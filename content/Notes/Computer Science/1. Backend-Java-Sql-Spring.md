---
title: Backend Engineering
draft: false
tags:
  - Java
  - SQL
  - Spring
---

# Contents
- [Contents](#contents)
- [Basics](#basics)
  - [OS Fundamentals](#os-fundamentals)
  - [Fullstack: The Big Picture](#fullstack-the-big-picture)
  - [Language Overview](#language-overview)
  - [Compilation Process](#compilation-process)
  - [JVM, JRE, JDK](#jvm-jre-jdk)
  - [Hello World](#hello-world)
  - [Entities of Java](#entities-of-java)
  - [Testing Through the Main Method](#testing-through-the-main-method)
  - [Methods](#methods)
  - [Primitive Types](#primitive-types)
  - [Working with Basic Operators](#working-with-basic-operators)
  - [String Basics](#string-basics)
  - [Flow Control Statements](#flow-control-statements)
  - [Debugging](#debugging)
  - [Troubleshooting a Technical Problem](#troubleshooting-a-technical-problem)
  - [Reading the Stack Trace](#reading-the-stack-trace)
  - [Packages and Imports](#packages-and-imports)
  - [Static Members](#static-members)
  - [Variable Scopes](#variable-scopes)
  - [Arrays](#arrays)
- [Java Basics/OOP](#java-basicsoop)
  - [Stack and Heap Memory](#stack-and-heap-memory)
  - [Casting](#casting)
  - [Classes and Objects](#classes-and-objects)
  - [Constructors](#constructors)
  - [Garbage Collection](#garbage-collection)
  - [Wrapper Classes](#wrapper-classes)
  - [Exceptions vs Errors and Hierarchy](#exceptions-vs-errors-and-hierarchy)
  - [Handling Exceptions](#handling-exceptions)
  - [Checked vs Unchecked Exceptions](#checked-vs-unchecked-exceptions)
  - [Creating Custom Exceptions](#creating-custom-exceptions)
  - [OOP Inheritance](#oop-inheritance)
  - [OOP Polymorphism](#oop-polymorphism)
  - [Object Class](#object-class)
  - [OOP Encapsulation](#oop-encapsulation)
  - [Access Modifiers](#access-modifiers)
  - [OOP Abstraction](#oop-abstraction)
  - [Abstract Classes and Methods](#abstract-classes-and-methods)
  - [Interfaces](#interfaces)
  - [List Interface](#list-interface)
- [OOP, Maven, and Dev Practices](#oop-maven-and-dev-practices)
  - [SOLID Design Principles](#solid-design-principles)
  - [Intro to Maven pom and xml Files](#intro-to-maven-pom-and-xml-files)
  - [Maven Build Lifecycle](#maven-build-lifecycle)
  - [Introduction To Sdlc](#introduction-to-sdlc)
  - [Agile](#agile)
  - [Scrum Ceremonies](#scrum-ceremonies)
  - [Story Pointing And Burndown Charts](#story-pointing-and-burndown-charts)
  - [Waterfall](#waterfall)
  - [Agile Vs Waterfall](#agile-vs-waterfall)
  - [Linux File Commands Using Gitbash](#linux-file-commands-using-gitbash)
  - [Moving and Deleting Files Using Gitbash](#moving-and-deleting-files-using-gitbash)
  - [Source Control Management(git,vcs,cvcs,dvcs)](#source-control-managementgitvcscvcsdvcs)
  - [Git Fundamentals](#git-fundamentals)
  - [Initializing A Repository](#initializing-a-repository)
  - [Pushing To A Remote Repository](#pushing-to-a-remote-repository)
  - [Git Commit Branch Merge Push Pull](#git-commit-branch-merge-push-pull)
  - [Gitignore](#gitignore)
- [is used to add comments to a .gitignore file.](#is-used-to-add-comments-to-a-gitignore-file)
  - [What Is A Database](#what-is-a-database)
  - [What Is SQL](#what-is-sql)
  - [Consistency](#consistency)
  - [Introduction To RDBMS](#introduction-to-rdbms)
  - [Schema](#schema)
  - [Table Structure](#table-structure)
  - [SQL Data Types](#sql-data-types)
- [SQL Basics and JDBC](#sql-basics-and-jdbc)
  - [Overview Of Sublanguages](#overview-of-sublanguages)
  - [DDL](#ddl)
  - [Defining Schema](#defining-schema)
  - [CREATE DROP TRUNCATE](#create-drop-truncate)
  - [DML](#dml)
  - [INSERT](#insert)
  - [UPDATE](#update)
  - [DELETE](#delete)
  - [DQL](#dql)
  - [Queries](#queries)
  - [Clauses](#clauses)
  - [CRUD Operations](#crud-operations)
  - [Constraints](#constraints)
  - [Auto Incrementing](#auto-incrementing)
  - [CHECK](#check)
  - [DEFAULT](#default)
  - [Primary Key](#primary-key)
  - [Referential Integrity](#referential-integrity)
  - [Foreign Key](#foreign-key)
  - [CASCADE](#cascade)
  - [Unique Key](#unique-key)
  - [Secondary Alternate Key](#secondary-alternate-key)
  - [What Is A Subquery](#what-is-a-subquery)
  - [What Is A Join](#what-is-a-join)
  - [Inner Join](#inner-join)
  - [Left And Right Joins](#left-and-right-joins)
  - [Outer Join](#outer-join)
  - [Equi And Theta Joins](#equi-and-theta-joins)
  - [Aliases](#aliases)
  - [Intro to JDBC](#intro-to-jdbc)
  - [JDBC Interfaces Classes](#jdbc-interfaces-classes)
  - [Result Set](#result-set)
  - [Data Access Object (dao)](#data-access-object-dao)
  - [Navigating Result Set Rows](#navigating-result-set-rows)
- [JDBC/SQL Intermediate](#jdbcsql-intermediate)
  - [Simple And Prepared Statements](#simple-and-prepared-statements)
  - [Reading from a Properties File](#reading-from-a-properties-file)
  - [Setting Up The Utility Class](#setting-up-the-utility-class)
  - [Preventing SQL Injection](#preventing-sql-injection)
  - [Sql Injection](#sql-injection)
  - [Composite Key](#composite-key)
  - [Normalization](#normalization)
  - [Multiplicity](#multiplicity)
  - [Data Modeling And ERD](#data-modeling-and-erd)
  - [Cross Join](#cross-join)
  - [Self-join](#self-join)
  - [Set-operators](#set-operators)
  - [Views](#views)
  - [Indexes](#indexes)
  - [Introduction To Http](#introduction-to-http)
  - [Http Verbs](#http-verbs)
  - [Http Status Codes](#http-status-codes)
- [APIs and Testing](#apis-and-testing)
  - [functional-interfaces](#functional-interfaces)
  - [lambdas](#lambdas)
  - [method-reference-syntax](#method-reference-syntax)
  - [Introduction To Rest](#introduction-to-rest)
  - [Introduction To Javalin](#introduction-to-javalin)
  - [Configuration](#configuration)
  - [Handlers](#handlers)
  - [Exception Handling](#exception-handling)
  - [JSON](#json)
  - [Exposing And Consuming Restful Api Endpoints](#exposing-and-consuming-restful-api-endpoints)
  - [Rest Resources And Url Construction](#rest-resources-and-url-construction)
  - [Authorization Vs Authentication](#authorization-vs-authentication)
  - [Java Introduction to Logback](#java-introduction-to-logback)
  - [Java Logback Logging Levels](#java-logback-logging-levels)
  - [Intro to TDD](#intro-to-tdd)
  - [Intro to Junit](#intro-to-junit)
  - [Junit Annotations and Assertions](#junit-annotations-and-assertions)
  - [Intro to Mockito](#intro-to-mockito)
  - [Mocking the DAO](#mocking-the-dao)
- [Java Collections and Algorithms](#java-collections-and-algorithms)
  - [Overview Of Collections Hierarchy](#overview-of-collections-hierarchy)
    - [Set Interface](#set-interface)
    - [SortedSet Interface](#sortedset-interface)
    - [Queue Interface](#queue-interface)
    - [Deque Interface](#deque-interface)
    - [Map Interface](#map-interface)
    - [SortedMap Interface](#sortedmap-interface)
    - [Methods of the Collection Interface:](#methods-of-the-collection-interface)
    - [Collections Class in Java](#collections-class-in-java)
  - [Generics](#generics)
  - [Set Interface](#set-interface-1)
  - [HashSet and TreeSet](#hashset-and-treeset)
  - [Queue Interface](#queue-interface-1)
  - [Map Interface](#map-interface-1)
  - [Map Interface](#map-interface-2)
  - [HashMap and HashTable](#hashmap-and-hashtable)
  - [Iterators](#iterators)
  - [ArrayList and LinkedList](#arraylist-and-linkedlist)
  - [Priority Queue](#priority-queue)
  - [Stacks and Vector](#stacks-and-vector)
  - [Comparable Interface](#comparable-interface)
  - [Comparator Interface](#comparator-interface)
  - [Creational: Factory](#creational-factory)
  - [Creational: Singleton](#creational-singleton)
  - [What is an Algorithm](#what-is-an-algorithm)
  - [Recursive Algorithm](#recursive-algorithm)
  - [Greedy Algorithm](#greedy-algorithm)
  - [Divide and Conquer Algorithm](#divide-and-conquer-algorithm)
  - [Backtracking Algorithm](#backtracking-algorithm)
  - [Time Complexity](#time-complexity)
  - [Linear Search](#linear-search)
  - [Binary Search](#binary-search)
  - [Bubble Sort](#bubble-sort)
  - [Merge Sort](#merge-sort)
- [SQL/Java Advanced](#sqljava-advanced)
  - [DCL](#dcl)
  - [TCL](#tcl)
  - [What Is A Transaction](#what-is-a-transaction)
  - [ACID Properties](#acid-properties)
  - [Transaction Commit Rollback Isolation Levels](#transaction-commit-rollback-isolation-levels)
  - [Aggregate Functions](#aggregate-functions)
  - [Scalar Functions](#scalar-functions)
  - [Sequence](#sequence)
  - [Trigger](#trigger)
  - [What Is A User Defined Function](#what-is-a-user-defined-function)
  - [What Is A Stored Procedure](#what-is-a-stored-procedure)
  - [Optional-class](#optional-class)
  - [stream-api](#stream-api)
  - [reflection-api](#reflection-api)
  - [thread-class](#thread-class)
  - [runnable-interface](#runnable-interface)
  - [states-of-a-thread](#states-of-a-thread)
  - [multithreading](#multithreading)
  - [synchronization](#synchronization)
  - [deadlock](#deadlock)
  - [livelock](#livelock)
  - [Criminal: waiting for police to give ransom.](#criminal-waiting-for-police-to-give-ransom)
  - [State: TIMED\_WAITING](#state-timed_waiting)
  - [State: TIMED\_WAITING](#state-timed_waiting-1)
  - [Criminal: waiting for police to give ransom.](#criminal-waiting-for-police-to-give-ransom-1)
  - [State: TIMED\_WAITING](#state-timed_waiting-2)
  - [State: TIMED\_WAITING](#state-timed_waiting-3)
  - [Criminal: waiting for police to give ransom.](#criminal-waiting-for-police-to-give-ransom-2)
  - [State: TIMED\_WAITING](#state-timed_waiting-4)
  - [State: TIMED\_WAITING](#state-timed_waiting-5)
  - [producer-consumer-problem](#producer-consumer-problem)
- [Spring/Spring Boot Basics](#springspring-boot-basics)
  - [Intro To Spring](#intro-to-spring)
  - [Spring Ioc Container](#spring-ioc-container)
  - [Overview Of Inversion Of Control](#overview-of-inversion-of-control)
  - [Overview Of Dependency Injection](#overview-of-dependency-injection)
  - [Types Of Dependency Injection](#types-of-dependency-injection)
  - [Injection Using Xml Based Configuration](#injection-using-xml-based-configuration)
  - [Injection Using Java Based Configuration](#injection-using-java-based-configuration)
  - [Annotation Based Configuration](#annotation-based-configuration)
  - [Component Scanning](#component-scanning)
  - [Stereotype Annotations](#stereotype-annotations)
  - [Bean Definition And Instantiation](#bean-definition-and-instantiation)
  - [Bean Lifecycle](#bean-lifecycle)
  - [Scopes Of A Bean](#scopes-of-a-bean)
  - [Lombok](#lombok)
  - [Overview Of Spring Boot](#overview-of-spring-boot)
  - [Using spring initializr](#using-spring-initializr)
  - [Auto Configuration](#auto-configuration)
- [In your application.properties file](#in-your-applicationproperties-file)
  - [Common Spring Boot Starters](#common-spring-boot-starters)
  - [Overview Of Spring Mvc \& Architecture](#overview-of-spring-mvc--architecture)
  - [Dev Tools](#dev-tools)
  - [Spring Environments](#spring-environments)
  - [@Controller MVC Annotations](#controller-mvc-annotations)
  - [@requestmapping \& @responsebody](#requestmapping--responsebody)
  - [Http Method Annotations](#http-method-annotations)
  - [Request Parameters And Path Variables](#request-parameters-and-path-variables)
  - [Request Body And @requestbody Annotation](#request-body-and-requestbody-annotation)
  - [Responseentity Class](#responseentity-class)
  - [Http Status Code \& Exception Handling With @exception Handler](#http-status-code--exception-handling-with-exception-handler)
  - [Restful Api Development With @restcontroller Annotation](#restful-api-development-with-restcontroller-annotation)
- [Spring Projects](#spring-projects)
  - [Relationship Between Jpa Hibernate And Spring Data Jpa](#relationship-between-jpa-hibernate-and-spring-data-jpa)
  - [Jparepository Vs Crud Repository](#jparepository-vs-crud-repository)
  - [Property Expressions](#property-expressions)
  - [Annotations](#annotations)
  - [@transactional](#transactional)
  - [Acid Properties Of Transactions](#acid-properties-of-transactions)
  - [Transaction Propagation Strategies](#transaction-propagation-strategies)
  - [Spring Boot Actuator Overview](#spring-boot-actuator-overview)
  - [Built In Actuator Endpoints](#built-in-actuator-endpoints)
  - [Unit Testing Service Layer Methods With Junit And Mockito](#unit-testing-service-layer-methods-with-junit-and-mockito)
  - [Intro To Integration Testing](#intro-to-integration-testing)
  - [Testing Restful Apis With Mockmvc And Resttemplate](#testing-restful-apis-with-mockmvc-and-resttemplate)
  - [Testing Database Interactions](#testing-database-interactions)

# Basics
## OS Fundamentals

**Learning Objectives**
After completing this module, associates should be able to:
Identify important concepts that a developer should know about operating systems.

**Description**
What is an Operating System?
An operating system, or "OS," is software that communicates with the hardware and allows other programs to run. It is comprised of system software, or the fundamental files your computer needs to boot up and function. Every desktop computer, tablet, and smartphone includes an operating system that provides basic functionality for the device.

Common desktop operating systems include Windows, OS X, and Linux. While each OS is different, most provide a graphical user interface, or GUI, that includes a desktop and the ability to manage files and folders. They also allow you to install and run programs written for the operating system. Windows and Linux can be installed on standard PC hardware, while OS X is designed to run on Apple systems. Therefore, the hardware you choose affects what operating system(s) you can run.

**How do Operating Systems impact software development?**
Being a developer you might have focused your skills on problem-solving and data structures. However, consider a scenario where you’re executing code, but your program runs too slowly. You check your code, and you find that there is nothing wrong with your code. What could be the reason behind this?

Well, one of the reasons could be your operating system. If you need to debug your program then how would you do that if you don’t know how your operating system works? Some possibilities are that you’re accessing too many files, you’re running out of memory or that swap is in high usage.

**Other considerations include:**
- Is it a local program or is the code running over the Internet?
- Why do some programmers prefer one OS over another?

As a developer, you should understand the importance of the operating systems. Today we are going to discuss some important concepts about operating systems that will help in your professional development.

Process and Process Management
The process is basically defined as a program in execution. The process should be executed sequentially. When you write a computer program in a text file and when you execute this program it becomes a process in your system. This process performs all the tasks mentioned in the program. A process generally (though not always) passes through six different states: Start, Ready, Running, Waiting, Terminated, or Exit. These states may have different names depending on the operating system.

Threads
You can define a thread as a flow of execution through the process code. The thread keeps track of all the instructions that need to be executed next in the program counter. Also, the thread contains system registers that hold the current working variables. Also, the thread's stack contains the execution history.

Scheduling
In scheduling, the process manager takes the responsibility to remove the running process from the CPU and chooses another process based on a specific strategy. For multiprocessing operating systems scheduling is the essential part. More than one process can be loaded into the executable memory at a time. The processes share the CPU using time multiplexing once they are loaded.

Memory Management
Memory management refers to the functionality of an operating system that handles and manages the primary memory. Processes move back and forth between the main memory and the disk during the execution.

Real World Application

Today we will compare the advantages and disadvantages of major operating systems with respect to software development

GNU/Linux
Linux comes with a large selection of distributions (or "distros"). Each one has the Linux basic system at its core, with other components built on top. Many Linux users will tend to switch between these distros until they find the perfect 'recipe' for their needs and tastes.

What are some of the pros of using Linux for software development?

One of the main benefits of Linux, not to mention the Linux ecosystem, according to software engineers, is the amount of choice and flexibility it provides. This really does make it the jewel in the crown of operating systems.
Linux is free and open-sourced. This means you don't have to fork out tons of cash on licenses for the OS and other apps used on it.
It is easy to install directly on your computer, or you can boot Linux from an external drive like a USB flash drive or CD. You can also install it with or inside Windows if you need both.
Linux is famous for its stability and security. While it can become infected with viruses, the chances are considerably lower than, say, Windows or macOS.
It consumes a very limited amount of your computer's resources while operating. It is quite possible to run it using only 500 MB of drive space and 300 MB of ram.
Linux has many alternatives to nearly all the programs on the market like Photoshop, MS Word, etc. These also tend to be entirely free.
Linux has an amazing support community for troubleshooting. This is worth more than its weight in gold (if you could ever measure such a thing).
What are some of the cons of using Linux for software development?

Learning Curve: Most users are already familiar with Windows/macOS, so switching to Linux requires learning new commands, system structures, and package management. This can be off-putting initially, but many resources are available to ease the transition.
Hardware Compatibility: While Linux supports a wide range of hardware, some peripherals (especially older ones) may require additional configuration or drivers to work properly.
Software Availability: Some proprietary software commonly used in development (e.g., Adobe products, MS Office) is not natively available on Linux, requiring alternatives or workarounds like Wine or virtual machines.
macOS
macOS is another very popular operating system for software engineers. Most programmers and software engineers will be divided on which is better - macOS or Linux, but it is clear that macOS is one of the best options.

It comes with a variety of built-in, or easily and freely available, UNIX-type development tools that also have excellent support, in most cases. The main bone of contention in any choice between macOS and Linux is the conflict between MacOS, which limits the user's operating system access, and Linux which allows the user to completely modify the operating system.

What are some of the pros of using macOS for software development?

One pro to using macOS is its intuitive, simple, and clean user interface. This is especially true when comparing it to something like Windows. If you are developing apps for iOS systems, the similarity with macOS' UI is a great advantage.
Multitasking is at the heart of macOS. The operating systems come with various native features that really make having various programs open at one time a breeze to navigate and manage. The ability to switch between them at the press of a button saves tons of time, along with your sanity, especially in the long run.
macOS is optimized for software and hardware compatibility. This means that it runs with zero to minimal conflicts throughout the lifetime of the system.
macOS is also famed for its reduced susceptibility to malware and other security issues. While, like Linux, it is not immune to security issues, the chances of getting infected with a nasty piece of software are reduced when compared to Windows.
When developing apps and other software for Apple products, macOS provides seamless compatibility. macOS and iOS have a very similar user interface and workflow logic making it the perfect choice for such software development. Files and data are readily synchronized between devices and can be shared through their dedicated Apple cloud servers.
What are some of the cons of using macOS for software development?

macOS can have limited, or restricted, availability for some applications. While other OS's, like Windows and Linux, have a literal plethora of applications, macOS is limited by comparison. There are fewer choices for software development applications on MacOS compared to Windows and Linux.
macOS can be inflexible when it comes to hardware upgrades or customization. This means that when you need more "bang for your buck" in the future, it can be more costly than for something like Windows. The deep integration of elements like CPU and RAM in a MacBook or iMac is a great boon initially, but they are not easily replaced in the future.
macOS devices tend to cost a pretty penny initially. When compared to Windows-based computers, for example, macOS systems are very much a high ticket item. This can be an issue if you are on a budget. If your company supplies the hardware, then this is not likely a problem.
Windows
The Windows OS does get a bad reputation a lot of the time. But for certain situations, Windows is actually an ideal OS for software engineers.

While macOS and Linux do have some serious advantages, Windows should not be completely ignored. Windows is also one of, if not the, most commonly used operating system for users around the world.

If you are targeting businesses, Windows' market dominance should always be catered to. It is also the OS of choice for many software engineering corporations.

What are some of the pros of using Windows for software development?

Windows offers free access to great development integrated development environment (IDE) through the Visual Studio Community.
Windows, especially Windows 10, supports a wide swathe of hardware almost unparalleled in other operating systems. Since it is an incredibly popular operating system, a large proportion of hardware manufacturers support Windows before any other systems, although not always.
Not to labor the point, but as Windows is practically everywhere around the world, it tends to support the newest hardware that is released. Owing to its market dominance, most manufacturers will have some form of support for Windows somewhere. For users, especially businesses, Windows' "Plug and Play" function has proved to be a winning formula and not something to ignore. For this reason, you can readily build a computer of your dreams from scratch.
One of the biggest advantages of Windows over macOS is that it is, relatively speaking, a lot cheaper with regards to the hardware you need. But while you could conceivably get the latest version of Windows to work on a sub-$200 machine (cost of Windows license excluded), it won't be the most responsive option.
Having such huge market dominance, Windows also has a lion's share of software and applications catering to it. This gives a Windows user a big choice of tools.
What are some of the cons of using Windows for software development?

As Windows is such a popular operating system, its success is also its weakness. Most malware, spyware, and ransomware viruses tend to target Windows operating systems. For this reason, Windows is one of the most vulnerable to such attacks.
As Windows allows for ease of customization, when it comes to hardware solutions, it can be problematic to get all the components to get along. Finding the right mix of drivers can cause some serious conflicts that could, conceivably, render your machine unworkable. But this can also prove to be a worthy challenge.
If you decide to buy an off the shelf computer with Windows pre-installed, the buyer (you) really should beware. Component quality, pre-ship testing, and long-term support can vary widely. Do your research first.
Forced updates. Windows tends to bloat very quickly and installs updates seemingly constantly. Not only that, but Windows has a history of apparently-botched update packages that can suddenly render some installed software and hardware unusable until a patch is created. This really is infuriating at times.
The most up-to-date Windows versions, like Windows 10, gather information related to contacts, location, calendar, and input (text and touch). The process for opting out of all data collection efforts is both time-consuming and requires a reasonable amount of technical know-how.
Summary

Here are some important concepts about operating systems that will help in your professional development.

Process and Process Management
Threads
Scheduling
Memory Management
Inter-Process Communication

## Fullstack: The Big Picture
Fullstack the Big Picture
Learning Objectives

After completing this module, associates should be able to:

Define the term "full stack developer"
Identify skill sets needed to become a full stack developer.
Description
What is a Full Stack Developer?
Full stack technology refers to the entire depth of a computer system application, and full stack developers straddle two separate development domains: the front end and the back end.
The front end includes everything that a client, or site viewer, can see and interact with.
By contrast, the back end refers to all the servers, databases, and other internal architecture that drives the application; usually, the end-user never interacts with this realm directly.
Front end developers work to optimize the visible parts of an application for web browsers and mobile devices.
Front end platforms are usually built with HTML, CSS, and JavaScript; however, they can also be made via pre-packaged code libraries or content management systems like WordPress.
Back end developers, in contrast, refine the software code that communicates with servers, databases, or other proprietary software that conveys information to front end interfaces.
Those knowledgeable in both front end and back end are called full stack developers, meaning they are well versed in both disciplines.
Today, modern businesses often rely on entire teams of developers to operate network equipment, work with virtual machines, and manage enormous databases.
It takes time to develop a comprehensive, nuts-and-bolts understanding of all these emerging technologies.
The developers who do so are, for that reason, versatile enough to shift fluidly between front and back end development and take on any task that their team might need them to tackle.
Real World Application

The easiest way to put the full stack into perspective is to imagine a restaurant:

Front End (Presentation Layer): This is like the well-decorated, comfortable seating area where visitors enjoy their food. It’s everything the user interacts with directly—what they see, click, and interact with on a website or application. This layer is focused on providing a great user experience, just as the dining area is designed to keep customers comfortable while they eat.
Back End (Business Logic and Data Layer): The kitchen and pantry make up the back end, typically hidden away from the customer’s view. The cook (or chefs) in the kitchen represent the operations and processes that take place on the server. They prepare the food (process requests) using ingredients (data) pulled from the pantry (database). The pantry (database) stores the ingredients (data) that are needed to create the meal, like storing user information or transaction records.
Developer (Manager / Full Stack Engineer): Developers are like the restaurant managers who oversee the entire operation. They organize the front end, the back end, and ensure the entire system works together seamlessly. They create the architecture of the restaurant (system architecture) and coordinate the flow of information between the kitchen (back end), pantry (database), and dining area (front end). They ensure the front end gets the right data from the back end and that everything runs efficiently.
In a full-stack system, the developer can manage both the front end (the user interface) and the back end (server-side logic, database), ensuring that all layers of the application work together smoothly:

The front end (presentation layer) is the user-facing part.
The back end (business logic layer) is where the servers process requests and handle the core logic.
The database layer stores the data used by the application, much like the pantry stores the ingredients.
Just as a restaurant manager makes sure the chefs (back-end) have the ingredients (data) and that the servers (front end) deliver the food to customers (users) effectively, developers ensure that the full-stack system operates efficiently.
The term “full stack developer” originated during the early days of the web, when websites were small and uncomplicated enough to allow a single person to tackle every aspect of site-building.

But in the decades since those initial days, the web has grown ever more complex.
The rise of machine learning, predictive computing, and responsive design has made it challenging — but not impossible! — for a single developer to handle every aspect of building and designing a site or application.
According to a 2020 Stack Overflow survey of 65,000 developers worldwide, roughly 54.9 percent identify as full stack.

Below is a diagram that illustrates full stack development:

Diagram of Full Stack Development

Summary

Full stack developers straddle two separate development domains: the front end and the back end.
The front end includes everything that a client, or site viewer, can see and interact with.
The back end refers to all the servers, databases, and other internal architecture that drives the application; usually, the end-user never interacts with this realm directly.
Front end platforms are usually built with HTML, CSS, and JavaScript; however, they can also be made via pre-packaged code libraries or content management systems like WordPress.
Back end developers, in contrast, refine the software code that communicates with servers, databases, or other proprietary software that conveys information to front end interfaces.

## Language Overview
Language Overview
Learning Objectives

After completing this module, associates should be able to:

Identify at least three reasons to select Java as a programming language.
Identify what makes a programming language object-oriented.
List at least five real-world uses of Java
Description
Who invented Java?
Java was invented at Sun Microsystems by a team led by James Gosling. It was first released in 1995.

Who currently maintains Java?
Java is currently owned and maintained by the Oracle Corporation, which acquired Java when it acquired Sun Microsystems in 2010.

Why Java?
Java is a high-level, compiled, strongly typed object-oriented programming (OOP) language. The advantages of Java are many: it is platform independent, has a C-language inspired syntax, provides automatic memory management, has an extensive built-in runtime library, is supported by the Oracle corporation, and has a rich open source community.

What is "object-oriented"?
When we say Java is object-oriented, we mean that it has the constructs of classes and objects built into the language. It also allows us to use various principles of object-oriented programming, which are covered in a separate module. An object in code can represent a real-world entity, or a conceptual entity.

Classes are the blueprints for how to create objects that contain a certain state - which is represented by fields (variables) - and behavior - which is defined via methods.

Objects are instances of class definitions. However, Java is not 100% object-oriented because it still has primitive values (or just: primitives), which are defined below:

Primitive type	Size	Description
boolean	not specified (JVM-dependent)	represents true and false values
byte	8-bit	numerical, integral value
short	16-bit	signed numerical, integral value
char	16-bit	unsigned numerical, Unicode character
int	32-bit	numerical, integral value
long	64-bit	numerical, integral value
float	32-bit	floating point value
double	64-bit	floating point value
What is the Java Language Specification?
It is the syntax and semantics of Java. Analogies to syntax and semantic errors in the English language follow:

"Bob are playing." is a syntax (Grammatical) error because the verb "is" should be used instead of "are."
"My freezer just rode a bicycle to Tampa." is a semantic (Meaning) error. Although the syntax is correct, the sentence does not make sense.
Real World Application

Java is a widely used programming language that has found applications in various domains. Here are some real-world examples of using Java:
Desktop GUI Applications

We use APIs like AWT, Swing, JavaFX to build these applications.
Examples of desktop GUI applications are Acrobat Reader, ThinkFree, Media Player, Antiviruses, etc.
Mobile Applications

A mobile application is an application created for mobile phones and tablets. In today’s era, the majority of phones and smart devices have Android OS and Android development is not possible without Java.
Examples of mobile applications are Photo and video gallery apps, Simple Calendar, Netflix, Tinder, QRReader, Google Earth, Uber, etc.
Enterprise Applications

An enterprise application is a large software system which operates in a corporate environment, to satisfy the needs of an organization, rather than of individual users.
Examples of enterprise applications are Business corporations, schools, banks, ERP (Enterprise Resource Planning) and CRM (Customer Resource Management) systems, clubs, charities, governments, interest-based user groups, etc.
Scientific Applications

A scientific application is an application that affects real-world activities using mathematics. Java supports the development of scientific applications, because of its powerful features.
Examples of scientific applications are applications related to research, science, medical science, space, aeronautics, etc.
Web-based Applications

A web application is a client-server program that is delivered on the Internet through a browser interface.
Examples of web-based applications are irctc.co.in, online forms, shopping carts, Gmail, Google Sheets, Google Slides and many more.
Embedded Systems

An embedded system, also known as an integrated system, is a combination of many small computing units that assemble together to perform dedicated functions for the larger systems.
Embedded systems are present everywhere. Don’t believe it? Most of us use them without knowing. For example, a motor system, entertainment and multimedia in a car, E-commerce, wireless communication, mobile computing and networking use an embedded system.
Embedded systems use Java for development. Originally, Java was designed for the purpose of developing embedded systems.
SIM (Subscriber Identity Module) cards in our phones have been running a variant of the JVM (Java Card) for nearly 20 years.
Big Data Technologies

The term big data is defined as “extremely large and complex datasets that may be analyzed to extract patterns, trends, and useful information. It is one of the most popular topics in the world of the latest technology.
Java is the perspective of big data. Today, many developers are switching their careers to Big Data Technology.
Hadoop and other big data technologies are also using Java in one way or the other. For example, Apache’s Java-based HBase and Accumulo (open source), and ElasticSearch as well.
Distributed Applications

A distributed application is an application or software that executes or runs on multiple computers within a network.
RMI (Remote Method Invocation) and CORBA (Common Object Request Broker Architecture) are the APIs to develop distributed applications.
Cloud-based Applications

Cloud computing means on-demand delivery of IT resources via the Internet, including storage, servers, databases, networking, and software with a pay-as-you-go pricing model.
It provides a solution for IT infrastructure at a low cost, as we can save files on remote databases and retrieve them on demand.
Java has long been the programming language that provides a structure for web applications, and now it has reached cloud applications, because of its distributed nature.
There are many Java cloud development tools. For example, Oracle Java cloud service provides a platform to develop and configure the Oracle servers
Web Servers and Application Servers

A web server is a computer program that uses HTTP (Hypertext Transfer Protocol) and other protocols, to store, process, and respond to client requests made over WWW (World Wide Web). A web server is a system that runs websites and delivers web pages to users.
An application server (or app server) is a software framework that stores the business logic for an application program and handles all operations between the client-end and the back-end of organizations.
Software Tools

A software tool is a set of computer programs that developers use to develop, analyze, maintain, debug, or support other applications and programs. Many developers use Java to write and develop useful software tools.
Examples of software tools are Eclipse, IntelliJ Idea, and NetBeans IDE.
Gaming Applications

Java proves to be one of the best platforms for developing 2-Dimensional games. Today almost every person has an Android phone that has Android games in it. Android games cannot be built without Java.
Android games use Java as a primary language because Java supports the Dalvik Virtual Machine (DVM) which is specially designed to run on the Android platform.
Summary

Java is a high-level, compiled, strongly typed object-oriented programming (OOP) language.

Java advantages include platform independence, C-language inspired syntax, automatic memory management, an extensive built-in runtime library, support from the Oracle corporation, and a rich open source community.

Classes are the blueprints for how to create objects. They are characterized by state and behavior.

Objects are instances of class definitions.

However, Java is not 100% object-oriented because it still has primitive values

The Java Language Specification involves

Syntax (Grammar)
Semantics (Meaning)
The Java Application Programming Interface (API) provides the programmer with a built-in library of functions that the programmer does not have to re-create.

## Compilation Process
Learning Objectives

After completing this module, associates should be able to:
Describe the compilation process for developing computer programs.
Description

Compilation means to transform a program written in a high-level programming language from source code into object code. Programmers write programs in a form called source code. Source code must go through several steps before it becomes an executable program. The first step is to pass the source code through a compiler, which translates the high-level language instructions into object code.

The final step in producing an executable program — after the compiler has produced object code — is to pass the object code through a linker. The linker combines modules and gives real values to all symbolic addresses, thereby producing machine code.

Java, being a platform-independent programming language, doesn’t just have a single compilation step. Instead, it involves a two-step execution, first through an OS-independent compiler; and second, in a virtual machine (JVM) which is custom-built for every operating system.

Real World Application

The two principal stages of compilation are explained below:

Stage 1: Compilation
First, the source .java file is passed through the compiler, which then encodes the source code into a machine-independent encoding, known as bytecode. The content of each class contained in the source file is stored in a separate .class file. While converting the source code into the bytecode, the compiler follows the following steps:

Parse: Reads a set of *.java source files and maps the resulting token sequence into AST (Abstract Syntax Tree)-Nodes.
Enter: Enters symbols for the definitions into the symbol table.
Process annotations: If requested, processes annotations found in the specified compilation units.
Attribute: Attributes the Syntax trees. This step includes name resolution, type checking and constant folding.
Flow: Performs dataflow analysis on the trees from the previous step. This includes checks for assignments and reachability.
Generate: Generates .class files.
Stage 2: Execution
The class files generated by the compiler are independent of the machine or the OS, which allows them to be run on any system. To run, the main class file (the class that contains the method main) is passed to the Java Virtual Machine and then goes through three main stages before the final machine code is executed. These stages are:

ClassLoader : The main class is loaded into the memory by passing its .class file to the JVM, through invoking the latter. All the other classes referenced in the program are loaded through the class loader.
Bytecode Verifier : After the bytecode of a class is loaded by the class loader, it has to be inspected by the bytecode verifier, whose job is to check that the instructions don’t perform harmful actions. The following are some of the checks carried out:
Variables are initialized before they are used.
Method calls match the types of object references.
Rules for accessing private data and methods are not violated.
Local variable accesses fall within the runtime stack.
The run-time stack does not overflow.
Just-In-Time Compiler
This is the final stage encountered by the Java program, and its job is to convert the loaded bytecode into machine code.
When using a JIT compiler, the hardware can execute the native code, as opposed to having the JVM interpret the same sequence of bytecode repeatedly.
This can lead to performance gains in the execution speed for instructions that are repeatedly executed.
Implementation

This demo below is to demonstrate how to compile and run a java program locally from the command line.

NOTE: This is NOT a required exercise. It's just beneficial to see how java compiles and runs programs

class Main {
    public static void main(String[] args)
    {
        System.out.println("Hello, World!");
    }
}
Output:

Hello, World!
Let us understand the compilation and execution process.

Enter the code above in a text file and save it with .java extension.
Open the terminal and go to the directory where you saved the program.
Try to compile the program with the below command
javac Main.java
Correct any syntax errors, then when the program successfully compiles, execute the following command:
java Main
Summary

Compilation means to transform a program written in a high-level programming language from source code into object code.
Source code must go through several steps before it becomes an executable program.
The first step is to pass the source code through a compiler, which translates the high-level language instructions into object code.
The final step in producing an executable program — after the compiler has produced object code — is to pass the object code through a linker. The linker combines modules and gives real values to all symbolic addresses, thereby producing machine code.
Java, being a platform-independent programming language, doesn’t just have a single compilation step. Instead, it involves a two-step execution
The first step is through an OS-independent compiler
The second, in a virtual machine (JVM) which is custom-built for every operating system.

## JVM, JRE, JDK
JVM, JRE, JDK
Learning Objectives

After completing this module, associates should be able to:
Define the terms JDK, JRE, and JVM
Description

Programs that are written in Java are executed utilizing the Java Virtual Machine (JVM). The JVM is a special program that knows how to execute the programs that you write in Java.

The Java Virtual Machine is able to run our code because it runs our compiled bytecode. This is unique as it does this in a virtual environment that is the same across every platform. This is known as Write Once, Run Anywhere (WORA). However, the JVM that you use is specific to your operating system.

In order to run our code it has something in it called the Just-In-Time Compiler (JIT). The JIT turns your bytecode into machine code, in most instances on a line-by-line basis.

Thus, programs in Java are technically compiled twice.

In order to run Java code, you also need a Java Runtime Environment (JRE), which contains all the runtime libraries that your code will be calling and using. The JRE contains the JVM within it, so if you want to run a Java program, all you need to install is the JRE.

But how do we actually compile the Java code that we write down to bytecode that the JVM will understand? For that, you need a JDK - Java Development Kit, which provides developer tools like a compiler, debugger, documentation tools, and other command-line utilities. The JDK also has a JRE inside of it, so if you install a JDK you can compile your Java code as well as execute it.

This diagram illustrates how these components work together:
Image of JDK

Step 1: The developer would write the source code that the JDK would compile into bytecode.

Step 2: The JVM processes the bytecode line by line using the JIT.

Step 3: Finally the JIT compiler turns the bytecode into machine code line by line.

Phases of the Process
Compile time: This is when source code is converted at one time to byte code.
Runtime: This is when Java uses the JIT. The bytecode that the developer wrote is then turned into instructions for the JVM to execute.
Recap
The JDK contains tools for Java development as well as JRE. The JRE contains the JVM which actually executes the Java bytecode and runs it on the specific operating system on which it is installed.

Real World Application

The following is a brief and non-exhaustive list of available Java Development Kits. This is to illustrate that Java programs can be developed using many different tools in many different operating environments.
JDK	Provider	Operating Systems
Oracle JDK	Oracle	Win, Mac, Linux
Adoptium	Eclipse	Win, Mac, Linux
Dragonwell	Alibaba	Linux only
Corretto	Amazon	Win, Mac, Linux
Zulu	Azul	Win, Mac, Linux
Implementation
Here we will download, install, and verify the Java Development Kit (JDK).
Here are instructions for downloading, installing, and verifying the Java Development Kit (JDK). Please note that installing Java locally is not required. This is an optional exercise.

Step 1: Download JDK

Go to the JDK download archive for Java 8 @ https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html
Under "Java SE Development Kit 8u331", select the appropriate download for your operating system.
Step 2: Install JDK

Run the downloaded installer. Accept the defaults and follow the screen instructions to complete the installation.
Step 3: Set the PATH environment variable

The PATH is the system variable that your operating system uses to locate needed executables from the command line or Terminal window. The PATH system variable can be set using System Utility in control panel on Windows, or in your shell's startup file on Linux and Solaris. Making changes to the system PATH variable is typically not necessary for computers running Windows or Mac OS X.

Instructions for Windows
In Search, search for “edit the system environment variables”
The System Properties window will show up. Click the Environment Variables... button.
In the Environment Variables window, In the section System Variables find the PATH environment variable and select it. Click Edit. If the PATH environment variable does not exist, click New.
In the Edit System Variable (or New System Variable) window, specify the value of the PATH environment variable. It should be the path to the JDK’s bin folder and the default location for the JDK is in the Java folder within the Program Files folder in your file system. Here’s an example of what the path could look like: C:\Program Files\Java\jdk-11\bin.
Click OK. Close all remaining windows by clicking OK.
Open the Command prompt window, and run the following command to verify the installation: java -version
Instructions for Mac OS X
To run a different version of Java, either specify the full path, or use the java_home tool:
% /usr/libexec/java_home -v 1.8.0_73 --exec javac -version
Instructions for Solaris and Linux
To find out if the path is properly set:
In a terminal window, enter:
% java -version
This will print the version of the java tool, if it can find it.
If the version is old or you get the error java: Command not found then the path is not properly set.
Determine which java executable is the first one found in your PATH
In a terminal window, enter:
% which java
  * Set the PATH permanently
To set the path permanently, set the path in your startup file.

Bash Shell

Edit the startup file (~/.bashrc)
Modify PATH variable
PATH=/usr/local/jdk1.8.0/bin:$PATH
export PATH
Save and close the file
Load the startup file
% . /.profile
Verify that the path is set by repeating the java command
% java -version
C Shell (csh)

Edit the startup file (~/.cshrc)
Set Path
set path=(/usr/local/jdk1.8.0/bin $path)
Save and close the file
Load the startup file
% source ~/.cshrc
Verify that the path is set by repeating the java command
% java -version
Summary

A Java Development Kit (JDK) is used to create Java code.
The Java code is then compiled into bytecode for a particular Java Runtime Environment (JRE).
The JRE runs the bytecode in a Java Virtual Machine (JVM) which is specific to the machine and operating system.
Since every machine can implement its own JREs and JVMs, Java source code does not have to be rewritten to be machine-specific. This is known as Write Once, Run Anywhere (WORA).

## Hello World
Learning Objectives

After completing this module, associates should be able to:
Analyze the java version of the "HelloWorld" program
Description
Introduction
Veteran software developers know the Hello World program as the first step in learning to code. The program, which outputs some variant of “Hello, World!” on a device’s display, can be created in most languages, making it some of the most basic syntax involved in the coding process.

Traditionally, Hello World programs are used to illustrate how the process of coding works, as well as to ensure that a language or system is operating correctly. They are usually the first programs that new coders learn, because even those with little or no experience can execute Hello World both easily and correctly.

Above all, Hello World is simple. That’s why it is so often used as a barometer of program success. If Hello World does not work effectively within the framework, then it is likely that other, more complex programs will also fail.

But this two-word phrase has big implications for the field of computer science. With Hello World as a foundation, novice programmers can easily understand computer science principles or elements. And professionals with years of coding experience can use it to learn how a given programming language works, especially in terms of structure and syntax. With applications at all skill levels and in almost every language, there is a long history behind such a short program.

Uses
The main use for Hello World programs was outlined above: It is a way for rookie coders to become acquainted with a new language. However, the applications of these programs go beyond an introduction to the coding world. Hello World can, for example, be used as a sanity test to make sure that the components of a language (its compiler, development and run-time environment) have been correctly installed. Because the process involved in configuring a complete programming toolchain is lengthy and complex, a simple program like Hello World is often used as a first-run test on a new toolchain.

It is also used as part of the debugging process, allowing programmers to check that they are editing the right aspect of a modifiable program at runtime and that it is being reloaded.

Hackers also use Hello World “as proof of concept that arbitrary code can be executed through an exploit where the system designers did not intend code to be executed,” according to programming consultants at Cunningham & Cunningham (C2). In fact, it’s the first step in using homemade content, or “home brew” on a device. When experienced coders are configuring a new environment or learning a previously unknown one, they verify that Hello World behaves correctly.

One more popular use for Hello World is as a basis for comparison. Coders can “compare the size of the executable that the language generates, and how much supporting infrastructure must exist behind the program for it to execute,” according to C2’s wiki.

Beginnings
Though the origins of Hello World remain somewhat unclear, its use as a test phrase is widely believed to have begun with Brian Kernigham’s 1972 book, A Tutorial Introduction to the Language B. In this text, the first known version of the program was used to illustrate external variables. Because the previous example in the tutorial printed “hi!” on the terminal, the more complex “hello, world!” required more character constants for expression and was the next step in the learning process.

From there, it was used in a Bell Laboratories memo in 1974, as well as The C Programming Language in 1978. This popular text is what made Hello World famous. The example from that book (the first, and most pure, example) printed “hello, world,” with no capital letters or exclamation point. At this time, Hello World was used almost solely to illustrate a few functions of a language— not to test whether the system was running.

Before Kernigham’s seminal texts on B and C, there was no standard first program. Even as late as 1972, it was not widely in use. The popular BASIC tutorial, “My Computer Likes Me, When I Speak in Basic,” starts with a simple program that writes a line of text. However, this message was “MY HUMAN UNDERSTANDS ME,” far from the two-word greeting programmers use today. But once Hello World was invented, it spread quickly, becoming well-known by the late 1970s. Its popularity continues to this day.

Hello World Today: A Standard Practice in Varied Forms
In modern coding languages, Hello World is deployed at different levels of sophistication. For example, the Go language introduced a multilingual Hello World program, and XL features a spinning, 3D version complete with graphics. Some languages, like Ruby and Python, need only a single statement to print “hello world,” but a low-level assembly language could require several commands to do so. Modern languages also introduce variations in punctuation and casing. These include the presence or absence of the comma and exclamation point, as well as the capitalization of both words.

With the increasing complexity of modern coding languages, Hello World is more important than ever. Both as a test and a teaching tool, it has become a standardized way of allowing programmers to configure their environment. No one can be sure why Hello World has stood the test of time in an industry known for rapid-fire innovation, but it is here to stay.

Source:
Adapted from "The History of Hello World." Retrieved from The Software Guild's web site at https://medium.com/the-software-guild-blog/the-history-of-hello-world-175440f77776

Real World Application
First Java Program - HelloWorld
Let's take a look at the java implementation of Hello World.

public class Main {
  public static void main(String[] args) {
    System.out.println("Hello, world!");
  }
}
There are some things to note about this program that will help us understand future programs:

Every piece of logic in Java is wrapped around a construct called a class (which will be explained more later)
The first line that is executed starts at the main method
The main method always has the following syntax
  public static void main(String[] args) {

  }
Note: if the main method is not written in the exact syntax above, the compiler will not be able to find it
The curly braces represent the opening and closing of the main method.
Any logic that is written inside of those curly braces is technically "inside" of the main method. Meaning when the program runs, those statements will execute.
Finally lets look at the one statement that our program is executing:
  System.out.println("Hello, world!");
This statement above is the syntax to print text out to the console (for this program, we decided to print "Hello, world!" to the console).
A console is a window that allows users to send commands to the operating system through text. Just about every operation you have done graphically on a computer has an equivalent command you can write in the console.
Throughout this course, we may use the terms "console", "terminal" or "command line" interchangeably.
We frequently use the print statement shown above to output information to the console so we can debug what our program is doing.
Summary

"Hello, World!" programs are used to illustrate how the process of coding works, especially for introductory programmers.
Sometimes more experienced programmers use "Hello, World!" programs as a "sanity test" to make sure the components of a language have been correctly installed.
"Hello, World!" can also be used to compare the executable file sizes of different programs.
The "Hello, World!" program has been implemented in nearly every programming language.

## Entities of Java
Learning Objectives

After completing this module, associates should be able to:

Define what a class is
Define what a method is
Define what a variable is
Description
Entities of Java
To really get a head start in understanding Java, we need to be able to identify the 3 core entities that you will see in any Java file.

Classes: The blueprint for creating objects
Variables: Entities that allow us to store data
Methods: Blocks of reusable code that can be invoked again and again
Classes
With a few exceptions, any line of code that you write in Java has to be within a class since it is an object-oriented programming language and classes are blueprints for creating objects. Objects in Java are constructs within a program that represent real world objects. Because real-world objects can have state and behavior, we can define state and behavior for our Java objects as well.

Note that when naming a class, we follow the PascalCase naming convention. This is where every word in the name of the class has a capital first letter.

Variables
State is a property or characteristic of an object. For example, if we look at a mug, the mug has properties such as a color, shape, or whether or not it is empty. If we want to define state for our Java objects, we can use a construct called a variable. A variable is a way to store data within code. In Java, we declare, or create, a variable using the following syntax: datatype variableName; Java is a statically-typed language which means that all variables in Java must define what type of data we can store into that variable. This statement creates a place in memory for Java to store information of that specific datatype. We can refer to this named place in memory using the variable name. If we want to store a value in the variable, we can assign it a value using the following syntax: variableName = value;. We can reassign the variable, or assign it a different value, using this same syntax. Keep in mind that this will only work as long as the new variable is of the same datatype.

Note that when naming a variable, we follow the camelCase naming convention. This is where the first word in the variable name is all lowercase and every consecutive word has a capital first letter.

Methods
When we talk about objects having behavior, this refers to a construct in code called a method. A method is a block of code that we can invoke and reuse again and again. Methods in Java usually represent some sort of action.

We can identify what a method looks like by its syntax. Methods always have parenthesis after the method name and they also follow the camelCase naming convention. Below is the syntax for defining a method:

 returntype methodName(datatype parameter1, datatype parameter2, etc) {
   // statements / functionality of method go in here
 }
Methods have a return type to specify what type of value can be returned from the method. If no values should be returned, you can specify void as the return type.

The parenthesis allow you to specify any input you want the method to take in by creating parameters. A parameter is a variable local to a method that receives and stores a value when the method is called. You create a parameter like you would a variable by specifying the datatype the parameter's name. For example, let's say we have the following method:

int sum(int a, int b) {
    System.out.println(a + b);
}
We see two parameters, named a and b, that are of the int datatype. We can then use these parameters by referencing their name within the method body. In the above example, we use our parameters to add the values within them and then print them out to the console.

The method body contains the functionality of the method. It begins and ends with curly braces. If your method does return a value, the last statement of your method should be include a return keyword followed by the value to return. Below is an example of returning a value from a method:

int sum(int a, int b) {
    return a + b;
}
We can use the above method somewhere else in our program by calling it. Below is the syntax to call the method:

sum(1, 2);
Notice that we pass in as arguments the values we want to add together. The method returns the sum of the two numbers. It is important to note that although the method returns a value, we need to write code that uses that value, too. We can update the previous code so that we save the returned value into a variable for later use:

int result = sum(1, 2);
Real World Application

Being able to represent real-world objects in a program is a powerful tool and this is what makes Object Oriented Programming so popular. Consider a Banking Application. How can you represent a user's accounts? You can create classes that represent the different types of accounts a user can have, and these accounts can have characerstics, like a balance, or behavior, like the ability to be withdraw or deposit.

Throughout the program, you'll see how you can not only represent real world objects with the help of classes, variables, and methods, you'll also see how you can have objects interact in order to create complex and reusable programming components.

Implementation

Let's create a class that represents a Dog. To start off, we will create our class using a **class definition** which specifies to Java that we are trying to create a class of the type Dog.
class Dog {

}
In the above code, the open curly brace represents the beginning of class Dog and the closing curly brace represents the end of class Dog. Any code within those two curly braces is considered to be “within” class Dog.

Next, let’s give our Dog class a variable called age. We just added one line of code within our Dog class. Note that the variable is within the class's curly braces. The word int in Java is a datatype that represents whole numbers and the word age is the name or label we use to refer to the value we store within it.

class Dog {
  int age;
}
If we were to create a Dog object, it now is given an age where we can store the Dog's age.

Since a common behavior of a dog is to bark, lets write a method to in our Dog class that represents the action of barking.

class Dog {
  int age;

  String bark() {
    return "WOOF!";
  }
}
In the above example, the method name is bark(). The datatype String before the method name represents what datatype we are expecting to return from this method. The String datatype represents text. Within the method's code block, we are returning the text value "WOOF!".

Now that we finished creating our class, let's create a Dog object:

Dog max = new Dog();
The above creates a dog object and assigns it to the variable max. Now we can use the name max to refer to our dog!

Let's use the dog's behavior:

String dogBarking = max.bark(); 
System.out.println(dogBarking);
We called our bark() method and assigned the returned value to a variable named dogBarking. We then print the value to the console.

Alright, so we created our first class. What’s the point of creating this class? Again, the point of classes is to define how to create an object. It is the “blueprint”. Anytime we want to create a dog, we can use this blueprint!

Summary

Classes are blueprints for creating objects in Java.
Classes can have variables and methods within them to represent state and behavior.
After reading this lesson, you should be able to identify classes, variables and methods in code.

## Testing Through the Main Method
Learning Objectives

After completing this module, associates should be able to:

Discuss the main() method of a Java program
Successfully implement the important features of the main() method in a Java program
Description

The main method is the entry point of any Java program. It looks like the code below:

public static void main(String[] args) {

}
All methods in a class are defined by their access modifier, any non-access modifiers, return type, method name, and parameter list. Together, these form the method signature. The main method is a special method - when the code is executed, the JVM looks specifically for and invokes this unique method signature.

Syntax
The public keyword is an access modifier that ensures that the method is available throughout the project. The static keyword is a non-access modifier that ensures that the method is not associated with an object. These two keywords will be discussed in other lessons, but just know for now that they are required parts of defining the main method.

void is the "return type" of the method - in this case, the method does not return any data. Finally, the method takes an array of String objects as its only parameter. The part of the method definition you can change is the name of the String array argument. For example, you can change args to myStringArgs.The method definition is wrapped in curly braces to define the beginning and end of the method.

One more note on the main method - the array of Strings defined in the method parameters are passed from the command line when the java command is run. For example, the following command:

java HelloWorld string1 string2 string3
will pass an array of three Strings containing "string1", "string2", and "string3" to the HelloWorld program. The program can then use the Strings passed at runtime just like any other variable.

Real World Application

In Java, the main method plays a crucial role as it serves as the entry point for the execution of a Java program. It is a special method that the Java Virtual Machine (JVM) looks for and invokes when running a Java application.

Here are the key points about the importance of the main method in applications:

Program Execution: The main method is where the program execution begins. When you run a Java program, the JVM looks for the main method and starts executing the code from that point.

Required by the JVM: The JVM requires the presence of a main method to run a standalone Java application. If the main method is missing, the JVM will throw an error and won't be able to execute the program.

Entry Point for Standalone Applications: The main method is essential for creating standalone Java applications that can be executed directly from the command line or an integrated development environment (IDE).

Initialization and Startup Logic: The main method is typically used to initialize variables, create objects, and set up any necessary configurations or resources required by the application before the actual program logic is executed.

Command-Line Arguments: The String[] args parameter in the main method allows the program to accept command-line arguments when it is executed. These arguments can be used to customize the program's behavior or pass input data to the application.

It's important to note that although the main method is the entry point for a standalone Java application, it is not the only method that can be executed. Other methods can be called from within the main method, allowing the program to perform various tasks and functionalities.

Implementation

The following is an example of a simple program that uses the main method. Following along is optional.

We can create a java file named HelloUser.java and add in the following code:

public class HelloUser {

    public static void main(String[] args) {
        System.out.println("Hello, " + args[0] + "!");
    }

}
In the main method, we have a print statement that prints a String that contains the first argument passed to the application. Remember that args is an array of Strings. To access the first element in the args array, we use args[0]. Arrays will be more fully discussed in another lesson.

In the terminal, we can use the following commands to compile and then run the application:

javac HelloUser.java
java HelloUser arg1
The first statement compiles the program. The second statement runs the program and passes in arg1 as an argument to the program. The program then access that value and prints it out to the console. We'll see the following output:

Hello, arg1!
If we call the program again with the following command:

java HelloUser Bob
Then we should see the output:

Hello, Bob!
Summary

The Java main method is the entry point of any Java program.
The main method can also accept parameters that can be passed to the method during runtime.

## Methods
Cumulative for the testing through the main method
Learning Objectives
Learning Objectives for the Methods topic
Learning Objectives
After completing this module, associates should be able to:

Understand what a method is and when to use a method
Identify the different parts of a method
Description
Description for the Methods topic
What is a Method?
A method is a block of reusable code that can be invoked as many times as we want.

Parts of a Method
There are 3 minimum required parts we need to know to write a method:

Method name: we give our method a unique name so we can identify it from other methods
Method parameters: variables passed inside of the parenthesis of the method which we are able to utilize inside of our method. These values are given to us from the entity that invokes the method.
Return type: The datatype that we are going to return from the method
Let's identify the 3 parts above in the "addNumbers" written below:

int addNumbers(int num1, int num2){
  return num1 + num2;
}
The method name of this specific method is the text right before the parenthesis. So the method name of this specific method is "addNumbers".
The method parameters in the above method are num1 and num2. The entity that invoked this method gave us those to values so we can add them together inside of the method.
The return type of the above method is an "int". This means that you need to return a whole-number value before the method ends. If you don't need to return anything from a method, you can utilize the "void" keyword in the return type instead.
How do we invoke a method?
Let's say we wanted to call our newly created "addNumbers" method. We can do that as shown below:

public class Main {
  public static void main(String[] args){
    addNumbers(1,3);
  }
}
In the example above, we are invoking the "addNumbers" method from our main method. All we needed to do was type out the method name and provide the values required to the parameter list. In the above example we aren't storing the result of the method however we can do this by assigning the method to a variable.

NOTE: The variable datatype and the return type of the method must be the same if you are going to store the value into a variable

public class Main {
  public static void main(String[] args){
    int sum = addNumbers(1,3);
  }
}
In the above example, we are storing the result of the addNumbers method into the variable sum. In this case, since we passed the values 1 and 3 to the method, the value in the variable sum will be 4.

Real World Application
Real World Application of the Methods topic
Let's say we wanted to create an application that calculates the area of a triangle:

public class Main {

  public static void main(String[] args) {

    /*
    The formula for a triangle is (base * height) / 2
    So lets define 2 variables for the needed values and do the calculation below
    */
    double triangleBase = 3.5;
    double triangleHeight = 7.0;

    double area = triangleBase * triangleHeight;
    area = area / 2;
    
    System.out.println("The area of the triangle is " + area);
  }
}
We did it! We made a program that can calculate the area of a triangle. But there is a fundamental problem here. What if we needed to do the calculation for the area 3 times with different values? Let's implement that logic below.

public class Main {

  public static void main(String[] args) {

    //triangle 1
    double triangleBase = 3.5;
    double triangleHeight = 7.0;

    double area = triangleBase * triangleHeight;
    area = area / 2;


    //triangle 2
    double triangleBase2 = 4.0;
    double triangleHeight2 = 10.0;

    double area2 = triangleBase2 * triangleHeight2 ;
    area2 = area2 / 2;

    //triangle 3
    double triangleBase3 = 12.0;
    double triangleHeight3 = 3.0;

    double area3 = triangleBase3 * triangleHeight3;
    area3 = area3 / 2;
    
    System.out.println("Area of Triangle 1: " + area);
    System.out.println("Area of Triangle 2: " + area2);
    System.out.println("Area of Triangle 3: " + area3);
  }
}
As you see above, our program got drastically larger because we had to do the exact same operation 3 times (and this is a fairly simple algorithm we are implementing). What if we wanted to solve the area of 100 triangles? Are we going to write out the same algorithm 100 times? The way we wrote the logic above is not a practical way to write this program if we needed to calculate more than one triangle. This is where methods come into play.

A method is a block of reusable code that we can invoke as many times as we want to. Instead of writing the same algorithm over and over in the main method, lets write a new method that does the algorithm for us.

public class Main {

  public static void main(String[] args) {

    /*
    The formula for a triangle is (base * height) / 2
    So lets define 2 variables for the needed values and do the calculation below
    */

    double area = calculateArea(3.5, 7.0);
    System.out.println("The area of the triangle 1 is " + area);

    area = calculateArea(4.0, 10.0);
    System.out.println("The area of the triangle 2 is " + area);

    area = calculateArea(12.0, 3.0);
    System.out.println("The area of the triangle 3 is " + area);

  }

  public static double calculateArea(double triangleBase, double triangleHeight){
    double area = triangleBase * triangleHeight;
    area = area / 2;

    return area;
  }
}
Notice in the above code we now have 2 methods in our class Main:

  public static void main(String[] args){

  }
The main method is the only method we seen so far and generally it only gets called once by the jvm so this method is a special case.
  public static double calculateArea(double triangleBase, double triangleHeight){

  } 
This is the first practical method we have written.
Summary
Summary of the Methods topic
Methods are a blocks of reusable code that can be invoked as many times as we would like.
The core parts of a method are:
method name
method parameters
return type
Anytime you can identify repetitive code in your application, abstracting that logic into a method is generally the best solution to keep your program manageable.


## Primitive Types
Learning Objectives

After completing this module, associates should be able to:

Understand what a variable is in a programming language
List the primitive data types and their respective memory allocations
Description

A variable is a container for storing data. This is a key component of any programming language.

Syntax for Using Variables in Java
DataType variableName;
Let's break down the variable syntax defined above. When creating a variable, you assign it a datatype and a name. The variable name is the unique identifier used to reference that variable again. The term data type refers to the type of data that can be stored in a variable. Java is a statically typed language because when you declare a variable, you must specify the variable's type. Then the compiler ensures that you don't try to assign data of the wrong type to the variable. Variable assignment looks like the following code:

variableName = value;
Assigning a value to a variable is done by using the assignment operator (=). Another lesson will discuss operators in more detail. When you assign a variable an initial value, this is known as initialization.

When we assign variables values, we must make sure that the values match the datatype that the variable can store. The following example code would generate a compiler error:

int x;
x = 3.1415;
Because x is declared as a variable of type int (which holds whole number values), it is not legal to assign the floating point value 3.1415 to it.

Primitive Types
Java makes an important distinction between primitive types and reference types:

Primitive types are the data types defined by the language itself.
Reference types are types defined by classes in the Java application Programming Interface (API) or by classes you create rather than by the language itself.
A key difference between a primitive data type and a reference type is that the memory location associated with a primitive-type variable contains the actual value of the variable. As a result, primitive types are sometimes called value types. By contrast, the memory location associated with a reference-type variable contains an address (called a pointer) that indicates the memory location of the actual object.

Java defines a total of eight primitive types, listed in the table below. Of the eight primitive types, six are for numbers, one is for characters, and one is for true/false values. int is the default whole number datatype and double is the default decimal type.

To memorize the 8 primitive datatypes of java, we have developed a saying: Bitter black coffee sure is for long days

The first character of each word in this sentence is equivent to the first character of a primitive datatype. If you can memorize that sentence, not only can you identify all of the datatypes, you will also know the sizes of the datatypes from smallest to largest.

Saying    	Primitive type  	Size     	Datatype Usage    
bitter	boolean	1-bit	true and false values
black	byte	1 byte (8 bits)	numerical values
coffee	char	2 bytes (16 bits)	1 character
sure	short	2 bytes (16 bits)	numerical values
is	int	4 bytes (32 bits)	numerical values
for	float	4 bytes (32 bits)	floating point value
long	long	8 bytes (64 bits)	numerical values
days	double	8 bytes (64 bits)	floating point value
Real World Application

Primitive data types in Java exist for several reasons:

Efficiency: Primitive data types are more memory-efficient and have better performance compared to their object counterparts. They directly represent simple values and don't require additional memory overhead.

Simplicity: Primitive data types provide a simple way to represent basic data values such as integers, floating-point numbers, characters, and booleans. This simplicity makes them easy to understand and use.

Language design: Primitive data types are an integral part of Java's language design. They allow for basic operations and manipulations at a low level, which is essential for programming tasks ranging from simple calculations to complex algorithms.

Historical reasons: Java inherited primitive data types from languages like C and C++, which also have similar concepts. This familiarity makes it easier for developers coming from these languages to transition to Java.

Overall, primitive data types in Java serve to provide a foundational set of data representations that are efficient, simple, and well-suited for various programming tasks.

Implementation

Below we have an example on how to create a variable with each datatype.

//boolean datatype (true or false)
boolean bool = true;

//byte datatype (whole number between -128 to 127)
byte b = 127;

//char datatype (1 character)
char c1 = 'B'; 

//short datatype (whole number between -32,768 to 32,767)
short s = 32767; 

//int datatype (whole number between -2,147,483,648 to 2,147,483,647)
int i = 2147483647;

//float datatype (floating point numbers (less accurate))
float f = 1.234F;

//long datatype (whole number between -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807)
long l = 9223372036854775807L; 

//double datatype (floating point numbers (more accurate))
double d = 1.234;

Summary

Java is statically typed, meaning that when a variable is declared in Java the type must be specified.
there are 8 primitive data types
primitive types are defined by the language itself and are stored directly in variables

## Working with Basic Operators
Learning Objectives

After completing this module, associates should be able to:

Discuss the different categories of operators in Java.
Discuss the different operators in each category.
Successfully execute a Java program that demonstrates the usage of different Java operators.
Description
Operators
If we want to make programs that "do something", we're going to need to create instructions that manipulate values and return new ones. Programming operations can be thought of just like a math equation such as 4 * 5 = 20. When programming, however, we can almost never assume that a value is known. Instead, we'd have to represent that equation as: int result = a*b; where a and b represent two unknown numbers. The symbols that we use to perform operations are called operators, and they use one or more expressions to perform a calculation and return a result.

There are several different operators in Java. We have already seen the assignment operator (=) which assigns a reference variable to a primitive value or object. This article explains in detail many of the other operators, some of which we will discuss below.

Increment and decrement operators
In order to increment or decrement integral values, we can use the common postfix operators: x++ and x--, where x is a byte, short, int, or long. A similar operator is the prefix increment or decrement: ++x and --x. The difference is that the prefix operator will change the value before the rest of the expression is evaluated, while the postfix operator changes the value after the entire expression is evaluated.

int a = 5;
int b = a++; // assign b=5, then increment a to 6
int c = ++a; // increment a to 7, then assign c=7
System.out.println(a); // 7
System.out.println(b); // 5
System.out.println(c); // 7
Logical operators
Logical, or boolean, operators perform operations that return boolean results. There are a few logical operators you should be aware of: && is the logical AND operator - it compares two boolean values. If both are true, the expression becomes true. Otherwise, the expression becomes false.

The logical OR operator || compares two boolean values - if either of the values are true, the expression evaluates to true. Otherwise, the expression is false.

Finally, the logical NOT operator ! reverses the state of the boolean - so true becomes false and false becomes true.

We can combine logical operations like so:

boolean a = true;
boolean b = false;
if (!(a && b)) {
  System.out.println("a and b are NOT both true");
}
We use parenthesis to prioritize the expression a && b, which returns false. We use the ! operator to reverse the false value to true. So the if statement's condition is true and the print statement is executed.

Comparison Operators
Comparison operators in Java are used to compare two values and return a boolean result. They are commonly used in control flow statements like if, while, and for loops.

Here are the main comparison operators:

== (equal to): Checks if two values are equal.

Example: a == b (true if a is equal to b).
!= (not equal to): Checks if two values are not equal.

Example: a != b (true if a is not equal to b).
> (greater than): Checks if the left operand is greater than the right operand.

Example: a > b (true if a is greater than b).
< (less than): Checks if the left operand is less than the right operand.

Example: a < b (true if a is less than b).
>= (greater than or equal to): Checks if the left operand is greater than or equal to the right operand.

Example: a >= b (true if a is greater than or equal to b).
<= (less than or equal to): Checks if the left operand is less than or equal to the right operand.

Example: a <= b (true if a is less than or equal to b).
Ternary Operator
The ternary operator uses the following syntax: x = condition ? expr1 : expr2. If the condition is true, x is assigned the value of expr1; if the condition is false, expr2 is assigned.

boolean skyIsBlue = true;
boolean twoAndTwoIsFour = true;
boolean makesSense = (skyIsBlue && twoAndTwoIsFour) ? true : false ;
Operator precedence
The operators below are listed in order of precedence. The closer an operator is to the top the table, the higher its precedence. Operators with higher precedence will be evaluated before operators with lower precedence.

When operators of equal precedence appear in the same line of code:

All binary operators with the exception of assignment operators are evaluated from left to right.
Assignment operators are evaluated from right to left.
Operators	Precedence
postfix	expr++ expr--
unary	++expr --expr +expr -expr ~ !
multiplicative	* / %
additive	+ -
shift	<< >> >>>
relational	< > <= >= instanceof
equality	== !=
bitwise	AND &
bitwise exclusive OR	^
bitwise inclusive OR	|
logical AND	&&
logical OR	||
ternary	? :
assignment	= += -= *= /= %= &= ^=
Real World Application

Operators are fundamental tools in programming languages like Java, and they are immensely useful when making applications for several reasons:

Data Manipulation: Operators enable developers to manipulate data efficiently. Arithmetic operators (e.g., +, -, *, /) are used to perform mathematical calculations, logical operators (e.g., &&, ||, !) are used to make decisions based on conditions, and bitwise operators (e.g., &, |, ^) are used to manipulate individual bits within data.

Expressing Logic: Operators allow developers to express complex logic concisely and clearly. For example, comparison operators (e.g., ==, !=, <, >) are used to compare values and make decisions based on the comparison result. This helps in writing code that is easier to understand and maintain.

Control Flow: Operators play a crucial role in controlling the flow of execution within an application. Conditional operators, such as the ternary operator, enable developers to create conditional expressions, which determine which code blocks are executed based on certain conditions. This facilitates the implementation of branching logic in applications.

Overall, operators are indispensable tools in application development, enabling developers to manipulate data, express logic, control program flow, manipulate strings, improve efficiency, adhere to standards, and abstract underlying computational concepts effectively.

Implementation
Java Unary Operators
The Java unary operators require only one operand. Unary operators are used to perform various operations such as:

incrementing/decrementing a value by one
negating an expression
inverting the value of a boolean
Java Unary Operator Example: ++ and --

public class OperatorExample {  
    public static void main(String args[]){  
        int x=10;  
        System.out.println(x++);//10 (11)  
        System.out.println(++x);//12  
        System.out.println(x--);//12 (11)  
        System.out.println(--x);//10  
    }
}  
Java Unary Operator Example: !

public class OperatorExample{  
    public static void main(String args[]){
        boolean c=true;  
        boolean d=false;  
        System.out.println(!c);//false (opposite of boolean value)  
        System.out.println(!d);//true  
    }
}  
Java Arithmetic Operators
Java arithmetic operators are used to perform addition, subtraction, multiplication, and division. They act as basic mathematical operations.

Java Arithmetic Operator Example

public class OperatorExample{  
    public static void main(String args[]){  
        int a=10;  
        int b=5;  
        System.out.println(a+b);//15  
        System.out.println(a-b);//5  
        System.out.println(a*b);//50  
        System.out.println(a/b);//2  
        System.out.println(a%b);//0  
    }
} 
Java AND Operator Example: Logical && and Bitwise &
The logical && operator doesn't check the second condition if the first condition is false. It checks the second condition only if the first one is true.

The bitwise & operator always checks both conditions whether first condition is true or false.

public class OperatorExample{  
    public static void main(String args[]){  
        int a=10;  
        int b=5;  
        int c=20;  
        System.out.println(a<b&&a<c);//false && true = false  
        System.out.println(a<b&a<c);//false & true = false  
    }
}  
Java OR Operator Example: Logical || and Bitwise |
The logical || operator doesn't check the second condition if the first condition is true. It checks the second condition only if the first one is false.

The bitwise | operator always checks both conditions whether first condition is true or false.

public class OperatorExample{  
    public static void main(String args[]){  
        int a=10;  
        int b=5;  
        int c=20;  
        System.out.println(a>b||a<c);//true || true = true  
        System.out.println(a>b|a<c);//true | true = true  
        //|| vs |  
        System.out.println(a>b||a++<c);//true || true = true  
        System.out.println(a);//10 because second condition is not checked  
        System.out.println(a>b|a++<c);//true | true = true  
        System.out.println(a);//11 because second condition is checked  
    }
} 
Java Ternary Operator
Java Ternary operator is used as one line replacement for if-then-else statement and used a lot in Java programming. It is the only conditional operator which takes three operands.

Java Ternary Operator Example

public class OperatorExample{  
    public static void main(String args[]){  
        int a=2;  
        int b=5;  
        int min=(a<b)?a:b;  
        System.out.println(min);  
    }
}  
Summary

There are different types of operators in Java
Arithmetic operators (+, -, *, /, %)
Logical operators (&&, ||, !)
Comparison operators (>, <, ==, >=, <=, !=)
Assignment operators (=, +=, -=, *=, /=, %=)
Increment and decrement operators (++, --)
Ternary operators
Just like in arithmetic, there is an operator precedence that should be noted so that operations are conducted in the order desired.


## String Basics
Learning Objectives

After completing this module, associates should be able to:

Discuss the String class
Successfully execute a Java program that demonstrates different operations on Strings using the String API.
Description
Strings
Strings are commonly used objects in Java that can hold multiple characters. In Java, Strings are not primitives - they are immutable objects derived from the String class. To be immutable means that the state or value of the object cannot be altered once created.

Because Strings are immutable, all of the methods in the String class return a new String - the original is not modified. For example:

String str1 = "my string";
str1.concat(" is the best!");
System.out.println(str1);
will print out my string. Why? Because the .concat() method returns a completely different string which we are not assigning to any variable, and the original object is not changed. Thus, str1 still refers to the String "my string". In order to make the code print "my string is the best!", we would need to change line 2: str1 = str1.concat(" is the best!"); which re-assigns the reference variable str1 to the new String returned from the method. Keep in mind that the original String hasn't changed.

Unique Properties of Strings in Java
Strings are very crucial in Java and also are very frequently used by Java programmers. It is important that the Java developers have a good knowledge about String class, in order to use them effectively. In this article, we will be focusing on some of the important topics about Java String.

Strings do not use any null character for termination.
The String does not use any null character for termination. On the contrary, strings and objects are backed by character array. Programmers can use the character array to represent a String in Java programming language. They can do this by using toCharArray() method of java.lang.String class of JDK.

Strings are not modifiable
One thing Java programmers must be aware of is that String is immutable, which means that once the programmer creates a string it cannot be modified. If the programmer tries to modify the content of the string, then it will create a new String. In other words, Strings does not allow inclusions of new content into the existing String and they also cannot override the functionality of a String.

Strings are placed in the String Pool
Strings are maintained in a separate String pool. String pool is a special memory location available inside Java memory. Whenever a programmer creates a String object using String literal, Java Virtual Machine first checks String pool and if JVM finds any similar content available in the String pool then it will return that string and does not create a new object. Java Virtual Machine does not check String pool if the programmer creates an object using the new operator.

Comparison of Strings is done using equals method
For comparing two Strings, the String class uses the equals() method instead of the equality operator. When comparing objects, the equality operator is used to check whether the values within the reference variables are the same.
Reference variables store memory addresses and act like pointers to the objects they reference. So even if two objects are equivalent, or have the same state, using the equality operator would return false when comparing them.

String word1 = new String("hello");
String word2 = new String("hello");
System.out.println(word1 == word2); // false
The String class uses the equals() method to check if the two objects are equivalent (have the same characters), rather than checking to see if they are the same object in memory.

String word1 = new String("hello");
String word2 = new String("hello");
System.out.println(word1.equals(word2)); // true
If you use literal notation, or double quotes, to create a String rather than the new operator, the String will be created within the String Pool. If you attempt to create another String literal that is equivalent to a String that is already in the pool, Java will use the one in the pool rather than creating a new String.

String word1 = "hello";
String word2 = "hello";
System.out.println(word1 == word2); // true
System.out.println(word1.equals(word2)); // true
In this example, because we used literal notation, Java first created a String that has the character sequence “hello” in the String Pool and assigned it to the reference variable word1. Then, Java reused the same String object by having the reference variable word2 point to it. In this case, using either equals() or the equality operator would return true, since the same object is being referenced.

It is best practice to use equals() over the equality operator when comparing objects, including Strings.

String Methods
To fully utilize the String class to its full potential, we need to understand the methods that come with a string object. Below are commonly used String methods:

toUpperCase() - Converts all the characters of a string to upper case.
toLowerCase()- Converts all the characters of a string to lower case
charAt(int index) - This returns the indexed character of a string, where the index of the initial character is 0
concat(String s) - This returns a new string consisting which has the old string + s
equals(String s) - Checks if two strings are equal
equalsIgnoreCase(String s) - This is like equals(), but it ignores the case(Ex: ‘Hello’ and ‘hello’ are equal)
indexOf(char c) - Returns the index of the first occurrence of a character within a String. Returns -1 if the character is not found.
length() - Returns the number of characters in the current string.
replace(char old, char new) - This returns a new string, generated by replacing every occurrence of old with new.
trim() - Returns the string that results from removing white space characters from the beginning and ending of the current string.
Real World Application

Strings are crucial in Java for several reasons:

Text Handling: Strings are used to represent text data in Java. They allow developers to manipulate and process textual information, such as user input, file contents, messages, and more.

Immutable: Strings in Java are immutable, meaning their values cannot be changed once they are created. This immutability ensures that string values remain constant throughout the program, which is important for maintaining data integrity and consistency.

Standard Library Support: Java provides a rich set of methods and functionalities for working with strings through the java.lang.String class. These methods allow developers to perform various string operations, such as substring extraction, searching, replacement, conversion, and more, without having to implement them from scratch.

Interoperability: Strings play a crucial role in interacting with external systems and APIs. They are used for communication between Java programs and databases, web services, network protocols, user interfaces, and other software components.

Overall, strings are indispensable in Java programming due to their versatility, immutability, widespread usage, standard library support, interoperability, support for internationalization and localization, and role as a key component in Java APIs and libraries.

Implementation
Demonstrating Various String Methods and Operations
toUpperCase() and toLowerCase()
We can utilize the toUpperCase()/toLowerCase() methods to return an uppercase/lowercase equivalent of an existing string.

String str = "Revature";

String uppercase = str.toUpperCase();  //value in the variable "uppercase" will be "REVATURE"

String lowercase = str.toLowerCase(); // value in the variable "lowercase" will be "revature"

charAt(int index)
We can utilize the charAt method to return an individual character at an index.

NOTE: The first character of a String starts at index 0

String str = "Revature";

char c1 = str.charAt(2);  // value in the variable character will be 'v'

char c2 = str.charAt(6); // value in the variable will be 'r'

concat(String s)
We can utilize the concat method to combine to two different Strings together

String str1 = "Rev";
String str2 = "ature";

String newStr = str1.concat(str2); // value in the variable "newStr" will be "Revature"

equals(String s)
This method is utilized to check if two strings are equal. This method will return a boolean

String str1 = "Apple";
String str2 = "apple";

boolean b = str1.equals(str2); // value in the variable "b" will be false because .equals is case-sensitive.

equalsIgnoreCase(String s)
This method is utilized to check if two strings are equal but is NOT case-sensitive. This method will return a boolean

String str1 = "Apple";
String str2 = "apple";

boolean b = str1.equalsIgnoreCase(str2); // value in the variable "b" will be true because .equalsIgnoreCase is NOT case-sensitive.

length()
This method is utilized to find the length of a string

String str = " Cheese ";

int len = str.length(); // value in the variable "len" will be 8 (characters dont have to be a letter. Spaces and other symbols count as a character as well).

replace(char old, char new)
this method is utilized to find a replace every instance of one character with another

String str = "Bitter*black*coffee*sure*is*for*long*days";

String newStr = str.replace('*', ' '); // value in the variable "newStr" will be "Bitter black coffee sure is for long days"

trim()
This method is utilized to remove white space characters from the beginning and end of a string

String str = "      Revature      ";

String newStr = str.trim(); // value in the variable "newStr" will be "Revature"

indexOf(char c)
This method is used to retrieve the index of the first occurrence of a given character within a String, or -1 if it is not found.

char character = 'a';
String s1 = "potatoes";
String s2 =  "vanilla";

s1.indexOf(character); // will return 3
s2.indexOf(character); // will return 1
String Concatenation
Concatenation is the process of combining two strings together using the + (plus) operator. You'll often see string concatenation when printing the output of a variable along with a sentence.

For example, say you have the following code snippet:

int x = 2;
String s = "The value of x is " + x;
The value of s after this snippet executes is, "The value of x is 2". What happens internally is the value of x is converted into a String and its value is combined with the value of the string "The value of x is ".

Notice we left a space after "is" so that the value of s would not be "The value of x is2".

int x = 2;
int y = 3;
System.out.println("The sum of x and y is " + x + y); // The sum of x and y is 23
When combining concatenation with addition operations, we need to ensure that the addition operation is evaluated before concatenation. The above examples shows that concatenation happened so that the numbers were just added to the end of the string.

int x = 2;
int y = 3;
System.out.println("The sum of x and y is " + (x + y)); // The sum of x and y is 5
In the above example, we see that by having the addition operation happen first, the sum is correctly calculated and then concatenated.

int x = 2;
int y = 3;
System.out.println(x + y + " is the sum of x and y"); // 5 is the sum of x and y
Because operations of the same precedence are read from left to right, if we put the addition before the String concatenation, we get the correct value.

Summary

Strings are among the most common types of Java objects.
Strings are immutable objects, meaning they cannot be altered.
When Strings are created they are placed in a special location within the heap called the String Pool.
The String API contains many useful methods for using String objects

## Flow Control Statements
Learning Objectives

After completing this module, associates should be able to:

Explain how if, else, and else if are used with Boolean expressions to provide alternate paths for Java program execution.
Explain how switch and case are used to provide an alternative to a chain of else if statements.
Debug, compile, and successfully execute a Java program that demonstrates both if and switch branching structures.
Justify the need for loops in programming.
Compare and contrast the different looping structures in Java.
Define break, continue, and labels.
Debug, compile, and successfully execute a Java program that demonstrates break, continue, and labels for flow control.
Create statements that utilize while loops
Be able utilize nested statements and loops that utilize conditional operators.
Description

Up to this point, the Java code we have written have run straight through, from beginning to end, without making any decisions. Here, we will discuss two Java statements that create variety in programs.

The if statement lets us execute a statement or a block of statements only if some conditional test turns out to be true.
The switch statement lets us execute one of several blocks of statements depending on the value of a variable of certain types. Not all data types can be used with a switch statement.
If statements
The if statement depends on the use of Boolean expressions.
A Boolean expression is an expression that returns a simple true or false result.
A statement or block of statements is executed should the result of the Boolean expression be true
else and else if statements provide alternate routes for program execution to follow should the result of the Boolean expression be false.
Boolean expressions can be complicated; however, frequently they involve the comparison of the value of one variable to another value, which could be another variable, a literal, or even an arithmetic expression. This comparison uses one of the relational operators listed below. All of these operators are binary operators which means they work on two operands, one to the left of the relational operator and one to the right of the relational operator.

Operator	Description
==	Returns true if the expression on the left evaluates to the same value as the one on the right
!=	Returns true if the expression on the left does not evaluate to the same value as the one on the right
<	Returns true if the expression on the left evaluates to a value that is less than the one on the right
<=	Returns true if the expression on the left evaluates to a value that is less than or equal to the one on the right
>	Returns true if the expression on the left evaluates to a value that is greater than the one on the right
>=	Returns true if the expression on the left evaluates to a value that is greater than or equal to than the one on the right
The basic syntax of if statements:

if (condition) { // this is the only block that is required - others are optional
  statement1;
} else if (condition2) {
  statement2;
} else {
  statement3;
}
Switch statements
Many applications call for a simple logical selection of things to be done depending on some value that controls everything.
Sometimes this is handled with large chains of else-if statements, which can get quickly out of hand.
The switch statement provides a more elegant solution.
switch statements attempt to match some variable with the value it contains.
This type of statement works with byte, short, char, and int primitives, along with enumerated types (enums) and - since Java 7 - Strings.
The basic syntax is:

switch(variable) {
  case 'A': System.out.println("Case A matches!"); break; 
  case 'B': System.out.println("Case B matches!"); break;
  case 'C': System.out.println("Case C matches!"); break;
  default: System.out.println("this will run if other cases don't match"); break;
}
Note the use of the break statement. It ensures that, if a given case executes, only its code will execute. You can choose the control flow to "fall through" to the next case.

Control Flow Statements
Now we look at Java statements that allow for the repeated execution of the same statement or block of statements. These are known as loops.

Loops are key for writing one of the most common types of programs- programs that get input from the user, process the output, then get more input the user and process that input, and so on.

Just like if statements, loops rely on conditional expressions to tell them when to stop looping. Otherwise, loops would continue without end.

The following are the Java keywords used to create a variety of loops, depending on the looping situation required:

for
while
do-while
For loops
For-loops are used for iteration, often in conjunction with data structures. They include 3 statements in parentheses - a declaration, condition, and statement (typically increment or decrement).

for (int i=0; i < myData.length; i++) {
  // typical for loop
  System.out.println(myData[i]);
}
Any object which implements the Iterable interface can be iterated over using an enhanced for-loop. The syntax is given in an example:

List<String> myList = getListOfStrings();
for (String myStr : myList) {
  System.out.println(myStr);
}
While statements
While statements test a condition - if the condition evaluates to true the block of code is run, otherwise the block is skipped. The block is looped through as long as the condition remains true.

while (true) {
  // infinite loop!
}
Do-while statements
An alternative to while loops is the do-while loop. This guarantees that the block will always run at least once:

do {
  // always runs at least once!
} while(condition); // condition evaluated after the block of code, and the do block will run subsequently until the condition evaluates to false
Overview of break and continue
A program generally is a linear flow of execution of consecutive statements. The programmer imposes certain controls to redirect the flow in an appropriate direction based upon some logic. Java’s break and continue statements are control statements that help in alternating the flow of the program. These two keywords can be used in association with a label or without a label.

The break keyword breaks out of the current control flow statement. We have seen it used with switch statements. The continue keyword is used to break out of the current iteration of a loop. This means that the remainder of the loop's code block will not run and the program's execution will start at the beginning of the next iteration of the loop.

Real World Application

Control flow statements are fundamental to programming and find numerous real-world applications across various domains. Here are some examples:

Decision Making in User Interfaces: Control flow statements, such as if-else and switch-case, are commonly used in user interfaces to make decisions based on user input or system state. For instance, in a web application, the UI might display different options or messages depending on the user's role or actions.

Business Logic in Applications: Control flow statements are used to implement business rules and logic in applications. For example, in an e-commerce platform, decision structures might be used to determine eligibility for discounts or promotions based on factors such as purchase amount, customer loyalty, or product category.

Data Processing and Filtering: Control flow statements are used to process and filter data based on specific criteria. For example, in a data analytics application, control flow structures can be used to filter out irrelevant data, perform calculations on relevant data, and generate insights or reports based on the processed data.

Overall, control flow statements are essential for controlling the execution flow of programs and implementing various logic and decision-making processes in real-world applications across diverse domains.

Implementation

Below are examples of using control flow statements.

Nested Conditionals
A nested statement is one that is defined inside another. Writing statements this way will create a limited context (or scope) for variables and provides a logical container for operations.

For example, look at the following:

if (condition1){
    if (condition2){
        System.out.println("This occurs inside an inner if-statement");
    }
}
In the above code, we first begin by defining an if-statement. Immediately, inside of the if-statement, we place another if-statement to create a nested statement. This is important because the only way our program will reach the nested statement, is if condition1 evaluates to true. If, it doesn't, then the nested statement will be completely skipped and never evaluated.

Writing Nested Conditional Statements
You may find yourself in a scenario in which you'll need to create a set of nested conditional statements in order to mimic a series of "if this happens, then do that" type of reasoning. For example, say we need to apply code for an automatic sprinkler system that will spray a specified amount of liters of water based on the chance of rain according to the following table:

Chance of Rain (%)	Water Output (L)
0 - 19	30
20 - 30	10
31-55	0
56 - 100	0
If we needed to design a program to take in this information and output the necessary water level then we could write something like the following:

public class WaterSystem {

    public static void main(String args[]){
        int precipitation = 0;
        int waterOutput = 0;

        //start an if-statement
        if (precipitation < 20){
            waterOutput = 30;
        }else if (precipitation < 31){
            waterOutput = 10; 
        }else if (precipitation < 56){
            waterOutput = 0;
        }else {
            waterOutput = 0;
        }

        System.out.println("The water output should be " + waterOutput);
    }
}
If we were to run the above code and change the precipitation value, then you'll get the expected waterOutput as defined by the above scenarios. We correctly modeled our system with a series of if-statements.

What if we needed to model a more complex situation?

Perhaps we need to also factor in current temperature into our design as follows:

Chance of Precipitation (%)	Temperature (F)	Water Output (L)
0 - 19	<= 75	30
> 75	45
20 - 30	 <= 75	10
 > 75	25
31 - 55	<= 75	0
> 75	15
56 - 100	<= 75	0
 > 75	0
For each chance of precipitation, there are 2 associated scenarios to determine the water output. One is when the current temperature is less than or equal to 75 degrees, and the other is when the temperature is greater than 75 degrees. 

What updates should you make to your code now in order to correctly model the above scenario?

Solution
The first thing we should do is define another variable to capture temperature. We'll define an int variable called temperature and provide an arbitrary value of 80.

public class WaterSystem {

    public static void main(String args[]){
        int precipitation = 0;
        int waterOutput = 0;
        int temperature = 80;
    ...
Next, we'll need to add an inner if-statement to each previous conditional statement in order to update our model to consider temperature. We'll start by adding an if-else-statement to the first conditional statement.

public class WaterSystem {

    public static void main(String args[]){
        int precipitation = 0;
        int waterOutput = 0;
        int temperature = 80;

        //start an if-statement
        if (precipitation < 20){
            if (temperature <= 75){
                waterOutput = 30;
            }
            else {
                waterOutput = 45;
            }
        }
        ...

        System.out.println("The water output should be " + waterOutput);
    }
}
If we continue to add similar if-else-statements to the other conditional statements to match our model. You should end up with the following:

public class WaterSystem {

    public static void main(String args[]){
        int precipitation = 0;
        int waterOutput = 0;
        int temperature = 80;

        //start an if-statement
        if (precipitation < 20){
        	if (temperature <= 75){
                waterOutput = 30;
            }
            else {
                waterOutput = 45;
            }
        }else if (precipitation < 31){
        	if (temperature <= 75){
                waterOutput = 10;
            }
            else {
                waterOutput = 25;
            } 
        }else if (precipitation < 56){
        	if (temperature <= 75){
                waterOutput = 0;
            }
            else {
                waterOutput = 15;
            }
        }else {
        	waterOutput = 0;
        }
        
        System.out.println("The water output should be " + waterOutput);
    }
}
Our console would display waterOutput value of 45.

Switches and Fall-through Logic
Background
In this exercise, you'll practice using a switch statement and understand what fall-through logic means.

Recall that a switch statement is a conditional block that will execute code if a value is equivalent to a specified case. If a case matches, then that block of code executes. It operates very much like an if-statement. 

Below is an example of a switch statement:

int x = 5;

switch (x) {
    case 1: ...
    case 2: ...
    default: ...
}
In the following steps, we'll walk you through creating a small project to define and operate a switch statement. 

public class SwitchStatement {

	public static void main(String[] args) {
		int x = 10;
	}
}
We will setup our class and we'll use our variable x to switch on.

Then we will add a switch statement and a case for the value 10:

public class SwitchStatement {

    public static void main(String[] args) {
        int x = 10;

        switch(x) {
        	case 10: System.out.println("case 10 ran");
        }
    }
}
Default Case
Now we'll add a default case to execute whenever the value is not 10. Change the initial value of the variable x to be 5 and add a default case to the switch statement. 

public class SwitchStatement {

	public static void main(String[] args) {
		int x = 5;

		switch(x) {
			case 10: System.out.println("case 10 ran");
			default: System.out.println("The default case ran.");
		}
	}
}
If we were to execute the program you would notice that once our program reaches the first case statement (case 10), it skips that case because x is not equal to 10. Next, our program reaches the default case and executes.

Think of the default case as a catch-all or "everything else" block that will run if no other specific cases are matched beforehand.

Fall-through
What do you think will happen if we change the order of the positions of the two cases (case 10 and default)?

public class SwitchStatement {

	public static void main(String[] args) {
		int x = 5;

		switch(x) {
			default: System.out.println("The default case ran.");
			case 10: System.out.println("case 10 ran");
		}
	}
}
The most curious thing occurs! You'll find that both cases executed their statements. You may be wondering why that is?

Well, switch statements support fall-through logic which means that whatever case is met first, all other cases below it will execute. In this scenario, case 10 was after the default case. The default case was the first to match the condition and thus both cases executed.

Let's see this again with more cases:

public class SwitchStatement {

    public static void main(String[] args) {
        int x = 5;

        switch(x) {
            case 6: System.out.println("case 6 ran");
            default: System.out.println("The default case ran.");
            case 7: System.out.println("case 7 ran");
            case 10: System.out.println("case 10 ran");
        }
    }
}
If we were to execute the program you'll again notice that all cases after the default one (which matched the value) executed.

Using break
In most scenarios, however, you'll use a break statement to exit a particular case to avoid fall-through logic.

Now we are going to place break statements after each case:

public class SwitchStatement {

    public static void main(String[] args) {
        int x = 5;

        switch(x) {
            case 6:
                System.out.println("case 6 ran");
                break;
            default:
                System.out.println("The default case ran.");
                break;
            case 7:
                System.out.println("case 7 ran");
                break;
            case 10:
                System.out.println("case 10 ran");
        }
    }
}
The last case doesn't need a break statement as there are no other cases below it. We also added line breaks (we skipped a line) to better organize our code to clearly visualize which lines of code belong to each case.

Now the console will display the output from only the single case that was matched and executed.

You can also use curly braces to group statements. Thus, the above code could be rewritten as follows:

public class SwitchStatement {

    public static void main(String[] args) {
        int x = 5;

        switch(x) {
            case 6: {
                System.out.println("case 6 ran");
                break;
            }
            default:{
                System.out.println("The default case ran.");
                break;
            }
            case 7:{
                System.out.println("case 7 ran");
                break;
            }
            case 10:{
                System.out.println("case 10 ran");
                break;
            }
        }
    }
}
Loops
For-Loop
  We'll create a loop that counts from 1 to 10, inclusively, and prints the number during each iteration.

public class Loops {

    public static void main(String[] args) {
    	//create a simple for-loop that prints the numbers 1 to 10     
    	for (int i = 1; i <= 10; i++){
    		System.out.println(i);
        }
    }
}
With our usage of a for-loop we printed the numbers 1 through 10 by using only 3 lines of code (ignoring our class setup).

One important note is that we used the variable i (which we defined in our declaration statement) inside of our loop. Because the increment statement increases the value by one each time, we are effectively counting from 1 to 10 and storing that value in the variable i during each iteration of the loop. So that's why we used it inside the loop and printed its value? 

This brings us to an interesting question. Can you use that same variable i outside of the for-loop?

The answer is no.

We cannot use our variable outside of the loop because it does not exist there. We defined the variable as part of the for-loop. It is said to have only that scope and therefore cannot be accessed outside of its scope or outside of the for-loop. 

If you're still trying to understand how our for-loop works, then you can review the following table which walks through the steps that our program takes when using a for-loop.

Steps of the for loop
Step	Statement	Description
1	for (int i = 1; i <= 10; i++ ){	This line of code defines the initial setup of our for-loop.

3 things are actually happening: 
1 The variable i is created and assigned a value of 1.
2. We then test if i is less than or equal to 10. It evaluates to true (1 is less than 10), so we'll begin executing the lines inside of the for-loop. 
3. We then create our increment statement i++. This line will execute after all other lines inside of our for-loop have executed once. 

The current value of i is 1. 
2	System.out.println(i);	This line will display the current value of i to the console.

The current value of i is 1; thus, 1 is printed to the console.
3	}	Our for-loop has now executed all enclosed statements. It will next run the increment statement (i++).

The current value of i is now 2. 
4		After one complete run or iteration of the for-loop we now check the condition (i <= 10).

The condition evaluates to true (since 2 is less than or equal to 10) . We will begin our execution of all enclosed statements. 
5	System.out.println(i);	This line will output to the console the current value of i.
The current value of i is 2; thus, 2 is printed to the console.
6		Our for-loop has executed all enclosed statements. It will now run the increment statement (i++) again.

The current value of i is now 3. 
...		Our program will continue looping through the code until i no longer satisfies the condition. This happens when i is equal to 11.
... 		At this point, when i is equal to 11, the condition statement fails. 11 is not less than or equal to 10. 
The loop will now exit and the Java program will execute any lines after the closing parentheses of the for-loop. As part of its cleanup, any variables declared inside of the for-loop or as part of the declaration statement are now erased.
While Loop
Background
A while loop is used to repeat a group of statements until a condition becomes false.

The following is the basic syntax for a while-loop:

while (condition){
    ...
}
The keyword while is a reserved word in Java; it indicates the start of a while loop. The condition refers to a boolean expression (a variable or statement that can be evaluated as a boolean; it will either be true or false).

Writing While Loops
We are going to create a boolean variable named, on, and set its initial value to true.

public class ExampleOne {

    public static void main(String args[]){
        boolean on = true;
    }
}
Now we will create a while loop, by first specifying the keyword, while. Afterwards type a set of parentheses and use the variable on inside it.

public class ExampleOne {

    public static void main(String args[]){
    	boolean on = true;

        while (on)
	}
}
The on variable is used to indicate whether or not to execute any statements associated with the while loop (which will be added in the next step). If we set the value of the variable to false, then those statements will be skipped. It is very similar to how if-statements use a condition to determine whether or not to skip a block of code. 

Next, we need a set of curly braces to indicate which statements belong to the while loop. Type out a set of curly braces.

public class ExampleOne {

    public static void main(String args[]){
    	boolean on = true;

    	while (on){
    		
    	}
	}
}
Inside of the while loop, let's use a simple print statement to print "Inside the while loop" to the console.

public class ExampleOne {

    public static void main(String args[]){
        boolean on = true;

        while (on){
            System.out.println("Inside the while loop");
        }
    }
}
There is one last thing we need to do. We need to tell our while loop when to stop. As of this moment, if you ran your code, then you would get a program that executes an infinite loop. This is a loop that runs forever. You would have to manually terminate/stop your program. I'll show you what this looks like in just a moment. For now, let's get this loop working the proper way.

After your print statement, we can type another line to assign false to the variable on.

public class ExampleOne {

    public static void main(String args[]){
    	boolean on = true;

    	while (on){
    		System.out.println("Inside the while loop");
    		on = false;
    	}
	}
}
NOTE: What is between the parentheses must evaluate to true or false. We just are using a boolean value but if we wanted all values less than 5 we could do something like follows:

public class ExampleOne {

    public static void main(String args[]){
        int x = 1;

        while (x <= 5){
            x++;
        }
    }
}
The while loop will execute as long as the condition x <= 5 is true. Inside the loop, x is incremented (x++), and the loop continues to execute until x becomes greater than 5. This ensures that the loop will run multiple times, incrementing x on each iteration, until the condition becomes false.

The cautionary note is added to emphasize the importance of ensuring that the loop condition eventually evaluates to false to avoid infinite loops. In this example, the condition becomes false when x exceeds 5, preventing the loop from running indefinitely. It serves as a reminder to be cautious when designing loops to prevent unintentional infinite loops.

Next we'll learn what it means to build and execute an infinite loop. 

Dealing with infinite loops
Now, we'll comment out code and show you what happens when your program executes an infinite loop. We are going to comment out the line where we assign false to the variable on.

The code would look like the following:

public class ExampleOne {

    public static void main(String args[]){
        boolean on = true;

        while (on){
            System.out.println("Inside the while loop");
            //on = false;
        }
    }
}
If we were to run our code. The text "Inside the while loop" continuously prints to the console.

To stop this, you must manually terminate your program.

When using loops, be sure that your code at some point changes the condition, so that your program won't stall. Otherwise, you'll have to terminate your program when it reaches a point of infinitely looping and identify what code block caused it.

Do-While Loop
Writing Do-While Loops
public class ExampleOne {

	public static void main(String args[]){
    
	}
}
Now, we will create a boolean variable with the name, on. Set its initial value to be false.

public class ExampleOne {

    public static void main(String args[]){
    	boolean on = false;
    }
}
Now we will create a do-while loop by first specifying the keyword, do. Afterwards type a set of curly braces and place a print statement that says "Inside the do-while loop".

public class ExampleOne {
	public static void main(String args[]){
		boolean on = false;
        
        do{
        	System.out.println("Inside the do-while loop");
        }
    }
}
Next, we need to specify the while condition. Type the keyword while followed by parentheses enclosing the variable on.

public class ExampleOne {

    public static void main(String args[]){
    	boolean on = false;
    	
    	do{
    		System.out.println("Inside the do-while loop");
    	} while (on);
	}
}
Now our code is complete.

Although we've specified our condition as false, the program will execute the statement associated with our do-while loop. It should be stated again, do-while loops always execute at least once.

Break statements
In Java, a break statement is mostly used:

To exit a loop.
As a form of goto.
Terminate a sequence in a switch statement.
Using break to exit a loop:
Using break, we can force immediate termination of a loop, bypassing the conditional expression and any remaining code in the body of the loop. When we use break inside the nested loops, it will only break out of the innermost loop.

// Java program to demonstrate using
// break to exit a loop
class Main {
	public static void main(String[] args)
	{
		// Initially loop is set to run from 0-9
		for (int i = 0; i < 10; i++) {
			// Terminate the loop when i is 5
			if (i == 5)
				break;
			System.out.println("i: " + i);
		}
		System.out.println("Out of Loop");
	}
}
Using break as a Form of Goto
Java does not have a goto statement because it provides a way to branch in an arbitrary and unstructured manner. Java uses a label. A Label is used to identify a block of code.

// Java program to demonstrates using break with goto
class Main {
	public static void main(String args[])
	{
	// First label
	first:
		for (int i = 0; i < 3; i++) {
		// Second label
		second:
			for (int j = 0; j < 3; j++) {
				if (i == 1 && j == 1) {

					// Using break statement with label
					break first;
				}
				System.out.println(i + " " + j);
			}
		}
	}
}

Using break to terminate a sequence in a switch statement
The switch statement is a multi-way branch statement. It provides an easy way to dispatch execution to different parts of code based on the value of the expression. The break statement is used inside the switch to terminate a statement sequence. The break statement is optional. If omitted, execution will continue on into the next case.

// Java program to demonstrate using break to terminate a
// sequence in a switch statement.
class Main {
	public static void main(String args[])
	{
		int i = 2;
		switch (i) {
		case 0:
			System.out.println("i is zero.");
			break;
		case 1:
			System.out.println("i is one.");
			break;
		case 2:
			System.out.println("i is two.");
			break;
		default:
			System.out.println("Invalid number");
		}
	}
}

Continue statements
The continue statement in Java is used to skip the current iteration of a loop. We can use a continue statement inside any types of loops such as for, while, and do-while loop. Basically continue statements are used in the situations when we want to continue the loop but do not want the remaining statement after the continue statement.

// Java program to demonstrates the continue
// statement to continue a loop
class Main {
	public static void main(String args[])
	{
		for (int i = 0; i < 10; i++) {
			// If the number is 2
			// skip and continue
			if (i == 2)
				continue;

			System.out.print(i + " ");
		}
	}
}

Exercises (Optional)
Exercise 1
Create an array of ten byte values. The values can be any arbitrary numbers that you choose. Now use a for loop to iterate over the elements in the array and print them to the console. 

Exercise 2
For this exercise, you need to write a while loop that prints the numbers 1 through 10, inclusively each on a separate line.

HINT: You can use a variable that you increase during each iteration (step) of the loop. You can also use an if-statement to determine when to exit the loop.

Exercise 3
For this exercise, you must write a while loop that prints all of the even numbers 100 to -100, inclusively, each on a separate line.

Exercise 4
For this exercise, you need to write a do-while loop that prints the odd numbers 1 through 49, inclusively, each on a separate line.

Exercise 5
For this exercise, you must write a do-while loop that prints the letters of the alphabet. You must use a single char variable that you manipulate to print to the console. (You shouldn't have 26 lines of code that each print a letter...)

Exercise 6:
Write a Java program that uses break, continue, and labels to alter the flow control. Use proper source code formatting.

Summary
The if statement lets us execute a statement or a block of statements only if some conditional test turns out to be true.
The if statement depends on the use of Boolean expressions.
A Boolean expression is an expression that returns a simple true or false result.
A statement or block of statements is executed should the result of the Boolean expression be true
else and else if statements provide alternate routes for program execution to follow should the result of the Boolean expression be false.
The switch statement provides an alternative to a long chain of else if statements.
switch statements attempt to match some variable with the value it contains.
This type of statement works with byte, short, char, and int primitives, along with enumerated types (enums) and - since Java 7 - Strings.
Loops are key for writing one of the most common types of programs- programs that get input from the user, process the output, then get more input from the user and process that input, and so on.
Just like if statements, loops rely on conditional expressions to tell them when to stop looping. Otherwise, loops would continue without end.
The following are the Java keywords used to create a variety of loops, depending on the looping situation required:
for : This type of loop is used to execute a block of code a specific amount of times.
while : This type of loop evaluates a Boolean expression before executing the block of statements. Therefore, it may never execute.
do-while : This type of loop executes the block of statements one time then evaluates the Boolean expression at the end. Therefore, this type of loop executes at least once.
The Java keyword break can be used to:
Exit a loop
Implement a "goto" flow control that exists in other programming languages
Terminate a switch statement sequence
The Java keyword continue is used to skip the current iteration of a loop.
Break can be used with labels to alter flow control.

## Debugging
Learning Objectives

After completing this module, associates should be able to:

Describe and understand the debugging process.
Understand and use several debugging techniques.
Description

Debugging in computer programming is a multi-step process that involves identifying a problem, isolating the source of the problem, and then either correcting the problem or determining a way to work around it. The final step of debugging is to test the correction or workaround and make sure it works.

In software development, the debugging process begins when a developer locates a code error in a computer program and is able to reproduce it. Debugging is part of the software testing process and is an integral part of the entire software development lifecycle.

Below are common steps in the debugging process:

Identify the Problem: This is the initial step where you recognize that something isn't working as expected. It involves:
Noticing unexpected behavior, error messages, or crashes in the software.
Gathering information about the issue, such as error logs, user reports, or screenshots.
Clearly defining what the expected behavior should be versus what's actually happening.
Reproduce the Issue: This step is crucial for understanding and eventually fixing the bug. It involves:
Creating a set of steps that consistently trigger the problem.
Identifying the specific conditions under which the issue occurs (e.g., certain input data, system configurations, or user actions).
Documenting the reproduction steps for future reference and for other team members.
Isolate the Source: Once you can reproduce the issue, you need to pinpoint its origin. This step includes:
Narrowing down the problem to a specific component, module, or section of code.
Using debugging tools, log analysis, or strategic print statements to trace the program's execution.
Examining relevant variables, data structures, and program flow at the point where the issue occurs.
Fix Implementation: After identifying the source of the problem, you implement a solution. This involves:
Modifying the code to address the root cause of the issue.
Ensuring that the fix is in line with the overall design and coding standards of the project.
Considering potential side effects or impacts on other parts of the system.
Test the Revised Implementation: The final step is to verify that the fix resolves the issue without introducing new problems. This includes:
Running the original reproduction steps to confirm the bug is fixed.
Conducting additional tests to ensure the fix hasn't caused regressions or new issues elsewhere in the system.
Performing broader testing, possibly including automated tests, to validate the overall stability of the system.
This process is often iterative. If the test in step 5 reveals that the issue persists or new problems have been introduced, you may need to cycle back to earlier steps, refining your understanding of the problem or adjusting your fix.

Here are some tips and suggestions for efficiently debugging code:

Compile/Run Your Code More Often:
This one is the most important advice, especially for the beginners, who write large quantities of code before compiling.
When you run your code frequently and test it, you get the feedback and you check that whether you are going in the right direction or not.
Use Print Statements Effectively:
One of the simplest and favorite tool for every programmer especially for beginners to debug the code.
Most of the debugging issues can be solved by inserting the print statements in your code.
Print out variables and check your console for correct values.
Also inspect values when possible
Google, Google, and Google:
The simplest thing you can do is to copy the error message and google it.
There is a good chance that you get your answer on StackOverflow (the largest community for developers) or on other forums or communities.
Try Alternate Solutions:
Try different solutions when you don’t understand the cause and don’t know how to fix the problem.
If still, it’s not working try another one.
Possibilities are also that you get the solution but you encounter a new error. Don’t panic in this case and accept that every developer has to go through this phase.
Use Comments Effectively:
In any language comments are not just to leave a note in the code or to explain the code.
You can also use them to debug by temporarily commenting out a piece of code that you don’t need to run at that time and isolate other parts of the code to execute.
Use Binary Search:
Finding a complex error in a buggy file is really difficult especially when it has thousands of lines of code. In those cases, you need to check more and more places and to avoid this case the best thing you can do is to apply binary search.
Divide the code into two parts. Comment out one part and run the other part. Whatever part is responsible for the error, repeat the division process with that part and keep repeating it until you find the line(s) that produce the error.
Interactive Debugging:
Many development environments come with debugging tools like Visual Studio Code and Eclipse.
As one example, these tools can pause execution and inspect data values line by line.
Automated Tests:
Automated tests and some other unit tests are performed to check if the actual output is matched with expected output or not.
This is done by writing test scripts where we execute the software with specific input.
Program Output Analysis:
Reviewing application logs to understand the sequence of events leading to an error.
Carefully reading and interpreting error messages and stack traces.
Rubber Duck Debugging:
Obtain a rubber duck (bathtub variety) by any means.
Place the rubber duck on desk and tell it you are just going to go over some code, if that’s okay with the duck.
Explain to the duck what your code is supposed to do, and go explain it line by line.
At some point you will tell the duck what you will do next. You may realize at this point that that is not what you are actually doing.
The duck will sit there serenely, happy in the knowledge that it has helped you on your way.

**Note:** If a duck is not readily available, a pet or a friend or even a colleague can be reasonable substitutes.
Ask for Help:
If you have tried everything to find the bug and to resolve it but nothing is working, you need to ask someone for help. Asking for help you often yields a solution you might not have considered before.
Real World Application

Scenario
A major bank has recently updated its mobile banking application. Shortly after the update, customer support starts receiving numerous complaints about failed transactions.

Problem
Users report that when they attempt to transfer money between accounts, the app shows a successful transaction message, but the money doesn't actually move.

Debugging Process
Identify the Problem:
Customer support logs the complaints and alerts the development team.
The team confirms the issue by attempting transactions in a test environment.
Reproduce the Issue:
Developers create a controlled test scenario to consistently reproduce the error.
They notice the issue occurs only for transfers above a certain amount.
Isolate the Source:
The team uses logging tools to examine the application's behavior during transactions.
They discover that the app is correctly sending transaction requests to the server, but there's an issue with how the server processes these requests.
Developers suspect that a recent change in the server-side code might be causing the problem.
They theorize that a new validation check for large transactions might be incorrectly implemented.
The team reviews recent changes to the server code.
They find a new function that was intended to add an extra security check for large transfers but has a logical error.
Fix Implementation:
Developers correct the logical error in the server-side validation function.
They also add more comprehensive error handling to provide clearer feedback if a transaction fails.
Test the Revised Implementation:
The team conducts extensive testing of various transaction scenarios.
They deploy the fix to a staging environment and perform user acceptance testing.
Debugging Techniques Used
Log Analysis: Examining server logs and application logs to trace the path of transactions.

Interactive Debugging: Using IDE tools to step through the server-side code and examine variable states during transaction processing.

Code Review: Carefully examining recent changes to identify the source of the problem.

Unit Testing: Writing new unit tests to cover the fixed functionality and prevent future regressions.

Integration Testing: Testing the entire transaction flow from the app through to the server and back.

Outcome
After implementing and verifying the fix, the bank rolls out an emergency update to the app and server. They also communicate with affected customers, explaining the issue and ensuring that all transactions are correctly processed. The debugging process not only solves the immediate problem but also leads to improvements in the app's error handling and transaction security checks.

This real-world example demonstrates how debugging is crucial in maintaining the reliability and trustworthiness of critical systems like banking applications. It showcases the systematic approach required to solve complex issues in production environments where errors can have significant financial and reputational impacts.

Implementation


In this example, we are completing an if-statement lab where one of our test cases fails. Our if statement should check if the value in the boolean variable bool is either true or false. If the value is true, we should return x, otherwise we should return y.

In the below GIF, you can see our solution code. We run our tests and notice the second test fails:



We start debugging by checkout the test’s output within the Test Results tab:



We could see what the test expected versus what it received. In this case, it expected a 1 but received a 0.

We look for more information by looking at the tests, themselves. When we hover over our test, we click on the fourth icon to go directly to the testing file:



We can see that, for our failing test case, bool has a value of false, which means that the value of y, which is 1, should have been returned, but it wasn’t.

If we aren’t very comfortable yet with looking at test code, we could instead use print statements within our solution:



When we run our failing test, we can switch to the Debug Console tab to see any print statement output. We can see the values of bool, x, and y printed to the console.

Alternatively, we can use a Debugging Tool that is commonly built-in to some IDE’s. We can use VSCode’s Debugger. We’ll set it up by putting breakpoints, or areas in our program that will pause so we can analyze our program in those moments. Once we’ve done that, we can hover over our failing test and click on the Debug icon:



After we get to the first breakpoint, we see everything is as expected. We step over to the next breakpoint, where we see our issue: we are unintentionally re-assigning bool so that it’s value changes to true, meaning the first branch of our if-statement will always run:



We have identified our problem, so we fix our if-statement’s condition and rerun our tests:



They pass!

Summary

Debugging is a multi-step process in software development that involves identifying, isolating, and fixing code errors.
The main steps of debugging include: identifying the problem, reproducing the issue, isolating the source, implementing a fix, and testing the revised implementation.
Various techniques are used in debugging, such as using print statements, interactive debugging tools, automated tests, and code reviews.
Effective debugging practices include compiling/running code frequently, using comments strategically, applying binary search for complex errors, and seeking help when needed.
The debugging process is often iterative and requires patience, systematic thinking, and sometimes creative problem-solving skills.


## Troubleshooting a Technical Problem
Learning Objectives

After completing this module, associates should be able to:

Discuss the difference between troubleshooting and debugging
List best practices when troubleshooting
Description
What is troubleshooting?
Troubleshooting is a process that helps people identify issues or problems occurring in a system. Troubleshooting tends to exist at a higher level than debugging and applies to many components of a system. It’s a process of parsing out the items that are causing problems. This process requires interviewing the end users of the system to find out the steps they took to cause the problems.

Troubleshooting can be applied to any system. For instance, doing your laundry can be considered a system that consists of a washer and dryer. If one of those components (i.e. the washer or dryer) fails, you need to troubleshoot where the failure is occurring. It may be beyond your expertise to fix it, but you start by trying to identify what may be causing the problem. It could be as simple as turning on the water or plugging the dryer into the wall.

Real World Application

Here are the most common IT problems and how to solve them.

Lack of Employee (Internal) Security Measures
Perhaps the most serious technology issue in business is employee security. In fact, 48% of all data breaches are because of negligent employees (CoxBlue). Employees carry sensitive data with them all the time, and can easily lose data to:

Phishing attacks
Weak passwords
Unauthorized access to information
To mitigate this risk, it’s important to create a map of who has access to sensitive information within the company’s computer network, and restrict access when necessary.

To tackle weak passwords and phishing attacks it’s important to educate your employees and implement policies such as BYOD (Bring Your Own Device Policy).

Outdated Equipment and Software
Using outdated equipment and software is another big IT problem in business. This is especially true for small businesses. A lot of business owners can suffer from the complexities of integrating new hardware or software into their existing network. Budgetary restraints are also a huge pain point that small businesses face.

It’s important to have regular maintenance done on your current devices and to use support services whenever necessary.

As a best practice, it’s always best to consult with IT experts. By getting outside help you can limit your IT spending and work within your budget. IT experts will also prevent any errors like crashing, freezing, and slow performance.

New Technology Integration
Before introducing new technology to a business, take the time to evaluate the current hardware to determine if it’s compatible and upgrade where necessary to avoid integration issues.

This can be accomplished easily by enlisting assistance from your current tech support or consulting with a managed service provider. It is even advisable to ask which new technology should be integrated before purchasing to make the transition easier and to ensure it is a wise investment.

Data Loss and Recovery
The risk of data loss is a fear that most businesses face regularly. It was reported that losing 100 files can cost a company between $18,000 – $35,730. Data can be lost from a number of events such as a:

Power outage
Cyber attack
Equipment malfunction
Human error
Having a disaster recovery plan in place that incorporates the use of a cloud-based data backup is essential to prevent data loss and losing lots of money,

Using a reputable cloud service with layers of security can allow a business to store sensitive information without the fear of losing that data. A disaster recovery plan should include:

Backup, disaster, and business continuity plan
IT support contacts
Backup servers
Cloud services
External storage
Implementation
Troubleshooting Best Practices
Bottom-up Approach
When an issue arises – either you have an alert or an end-user is experiencing difficulties – start with a bottom-up approach. Check to see if something is reported as an error, not ready, or crashing – this gives you the thread to pull on and move forward.

Drill down into the issue
Get more info on the specifications, the configurations that were set up, and the events that happened (in most cases it stops there).

­Example: Failed to pull image "localhost:53329/nginx:latest" – In this case somebody made a typo and included a docker image that is unreachable from the cloud. Describing the pod and carefully examining each line would reveal the root-cause, and make for a quick debugging session.

Start looking horizontally
It’s time to check your config maps, ingresses, secrets, volumes, nodes – or to drill down even more by reading your app logs. It could be a platform issue or an application issue to get down to the root cause.

Configurations or Secrets used in your application might not be aligned with what the app actually needs. It’s always a good idea to check if you’re using secrets from, and in, the proper environment.

Application logs are often the last piece of the puzzle when debugging (usually it will require a more intimate knowledge of the application, which the troubleshooter might not always have) but they are also the most efficient; if a pod is in a CrashLoopBackOff, the application logs will usually include a stacktrace of an exception that caused the container to crash.

Summary

Troubleshooting is a process that helps people identify issues or problems occurring in a system.
Troubleshooting tends to exist at a higher level than debugging and applies to many components of a system.
It’s a process of parsing out the items that are causing problems.

## Reading the Stack Trace
Learning Objectives

After completing this module, associates should be able to:

Describe how to read a stack trace
Description

Each JVM thread (a path of execution) is associated with a stack that's created when the thread is created. This data structure is divided into frames, which are data structures associated with method calls. For this reason, each thread's stack is often referred to as a method-call stack.

When an exception / error gets thrown, a stack trace is displayed to the console. New developers get concerned when they see a stack trace because they feel like they have broken their application (because you did!). However the purpose of a stack trace is to help find the problem in your application so you can resolve it. If you didn't have a stack trace when an exception in your program happens, it would be much harder to find out the fundamental issue with your logic. We will take a look at a simple example in the next section.

Below is a screenshot of a typical stack trace:

Stack Trace

Real World Application

Reading a stack trace is a crucial skill for developers as it provides valuable information about the runtime flow of a program and helps pinpoint the location and cause of errors or exceptions. Here are some real-world examples highlighting the importance of reading a stack trace:

Debugging Runtime Errors: Imagine you're working on a web application, and users start reporting a "NullPointerException" when trying to submit a form. By examining the stack trace, you can trace the sequence of method calls leading up to the error. This helps you identify which part of the code is causing the exception, such as a missing object reference or uninitialized variable, enabling you to fix the issue efficiently.

Understanding Code Flow in Frameworks: When working with frameworks or libraries, understanding the stack trace is essential for troubleshooting configuration or integration issues. By examining the stack trace, you can see the sequence of method invocations within the framework, helping you identify misconfigurations, incompatible versions, or incorrect usage of APIs.

Handling Exceptions in Production Environments: In production environments, where logging and debugging capabilities may be limited, stack traces become invaluable for diagnosing errors reported by users or monitoring systems. By analyzing stack traces from error logs or crash reports, you can quickly identify the root cause of issues, prioritize bug fixes, and deploy hotfixes or patches to address critical issues affecting users' experience.

Improving Code Quality: Regularly reviewing stack traces from test runs, code reviews, or post-mortem analyses of incidents helps identify recurring patterns of errors, performance bottlenecks, or architectural flaws in the codebase. This insight enables you to refactor code, enhance error handling, and implement best practices to improve code quality, maintainability, and reliability over time.

Overall, reading a stack trace is a fundamental skill for developers that empowers them to diagnose and resolve errors efficiently, optimize application performance, and ensure the reliability and stability of software systems in real-world scenarios.

Implementation

Let's consider the following program where we are attempting to add values into an array. An array allows us to store more than one value of a specific datatype into a variable. Arrays are explained in more detail in another written lesson. In our example, we will create an array that can hold three values at index 0, 1, and 2.

public class Main {
    public static void main(String[] args) {
        int arr[] = new int[3];
    }
}
Let's see what happens if we try to add a value at index 3 (which is out of range for an array of length 3).

public class Main {
    public static void main(String[] args) {
        int arr[] = new int[3];
        arr[3] = 10;
    }
}
We would get the following stack trace from the console

Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: Index 3 out of bounds for length 3
        at Main.main(Main.java:4)
Now since our program is so small, our stack trace is small, as well, because our execution stack isn't very big. But there are some things we can take away from the above stacktrace:

The first line that isn't indented will give us the type of exception that was thrown. For the above example, the exception that was thrown was "ArrayIndexOutOfBoundsException". The more you see different types of exceptions the easier you can understand what an exception is trying to tell you. For the "ArrayIndexOutOfBoundsException", it is pretty self describing. There is a value that we are trying to set in an array that is out of bounds for that array.
the indented portion of the stack trace goes through the programs stack to determine exactly what class, file and line number the exception was actually thrown at.
in the above example, The class "Main" threw an exception in the file "Main.java" at line number 4.
Since we now know what type of exception and where the exception was thrown in our program, we can take a look at our logic at that line of code to better help us resolve the issue.

Summary

Each JVM thread (a path of execution) is associated with a stack that's created when the thread is created.
A stack trace (also known as a stack backtrace) is a report of the active stack frames at a certain point in time during a thread's execution.


## Packages and Imports
Learning Objectives

After completing this module, associates should be able to:

Describe the concept of packages in Java.
Describe how packages are imported.
Describe how packages are created.
Description

What is a package? A package is a collection of classes, interfaces, and enums in a hierarchical manner.

Why packages?

Packages enable you to keep your classes separate from the classes in the Java API.
Packages allow you to reuse classes in other applications.
Packages allow you to distribute your classes to others.
Example:

package com.revature.mypackage;
This line declares the package in which the class will reside.
This must always be the first (non-commented) line in a .java file.
Packages follow a naming convention of lowercase characters separated by periods in the reverse way you would specify a web domain - thus, com.revature.mypackage instead of mypackage.revature.com.
Also, classes can be referenced anywhere in a program by their "fully qualified class name" - which is the package declaration followed by the class, in order to uniquely identify the class.
Typically we do not want to write out a verbose package and class name together.
Instead, we can use an import statement after our package declaration to pull in other classes.
We can then just use the class name without the package.
By default, everything in the java.lang package is imported.
Other packages and classes must be imported by the programmer explicitly.
Real World Application

Packages store and organize our Java class/interface files.
Usually we separate them into areas of shared functionality.
In our class we declare the package in which our class will reside.
Folders provide a good analogy for packages. Like folders, packages nest inside one another.
Typically, a class will be stored in packages within packages.
We indicate that one package is essentially a subfolder of another by separating the package names with dots in our package declaration.
Packages follow a naming convention of lowercase characters separated by periods in the reverse way you would specify a web domain.
We declare packages in our projects by using the keyword package.
Java packages must correspond to folders in your file system.
When working in most IDEs the folders will be automatically generated as soon as you create a new package.
In this example we have a HelloWorld project in an IDE workspace with all of the source code stored within the src folder. In this project there is a HelloWorld.java file that includes the following package declaration:

package com.revature.mypackage
If we were to navigate to that HelloWorld project outside of the IDE we would open the src folder, then the com folder, then the revature folder

IDE1IDE2IDE3IDE4

What happens in the file system if we add another package to the application?

If more than one package is in an application then the file structure would still correspond to the structure of the packages. If we choose to create a Java class called Driver, in the com.revature package, we would see the other package, mypackage, and the file in the revature folder.

IDE5IDE6

The package declaration must always be the first non-commented line in a .java file. If you create a package it will show up as the first line of code in all files under that package.

Java has many built-in packages that can be used that come along with the JDK. The most commonly used package is the java.lang package.

However, there are also other commonly used ones such as java.util, java.sql, java.security, java.net to name a few.

Implementation
Importing a Package
Some functionality of the Java API is not included by default and needs to be imported. An example is the Scanner class. It can be found in the util package, and because it is not in the lang package, it must be imported.

// Importing java utility package
import java.util.*;
 
class ImportingExample {
 
    public static void main(String[] args)
    {
 
        // Scanner to take input from the user object
        Scanner myObj = new Scanner(System.in);
        String userName;
 
        // Display message
        // Enter Your Name And Press Enter
        System.out.println("Enter You Name");
 
        // Reading the users input entered for the name using 
        // nextLine() method
        userName = myObj.nextLine();
 
        // Print and display
        System.out.println("Your Name IS : " + userName);
    }
}
Creating a Package
Packages are used to organize our code. If we have a file within a package, we need to add a package declaration to the first line of the file to specify where it resides in the project.

// Name of package to be created
package firstpackage;
 
// Class in which the above created package belongs to
class Welcome {

    public static void main(String[] args)
    {
        System.out.println("Package and class successfully created!");
    }

}
Exercise (Optional):
Note: These exercises are optional. Create a new Java Project in your favorite IDE and follow along with the examples.

Create a new package named secondpackage
Add a class named SecondClass in secondpackage with a public instance method that prints the following output: method in secondpackage successfully called!
In the Welcome class, import SecondClass by adding the following line of code: import secondpackage.SecondClass;
In Welcome's main method, create an object of SecondClass and use it to call the method you created.
Summary

A package is a collection of classes, interfaces, and enums in a hierarchial manner.
Packages enable you to keep your classes separate from the classes in the Java API.
Packages allow you to reuse classes in other applications.
Packages allow you to distribute your classes to others.
Packages follow a naming convention of lowercase characters separated by periods in the reverse way you would specify a web domain - thus, com.revature.mypackage instead of mypackage.revature.com.
Also, classes can be referenced anywhere in a program by their "fully qualified class name" - which is the package declaration followed by the class, in order to uniquely identify the class.
Typically we do not want to write out a verbose package and class name together.
Instead, we can use an import statement after our package declaration to pull in other classes.
We can then just use the class name without the package.
By default, everything in the java.lang package is imported.
Other packages and classes must be imported by the programmer explicitly.


## Static Members
Learning Objectives

After completing this module, associates should be able to:

Define and describe how static members work
Successfully implement static members in a Java program.
Description

In Java, a static member is a member of a class that isn’t associated with an instance of a class. Instead, the member belongs to the class itself. As a result, you can access the static member without first creating a class instance.

The static keyword in Java is mainly used for memory management. The static keyword in Java is used to share the same variable or method of a given class. The users can apply static keywords with variables, methods, blocks, and nested classes. The static keyword belongs to the class than an instance of the class. The static keyword belongs to the class itself rather than any particular instance. A static variable is shared among all instances of the class, and it can change unless also marked as final. The final is the modifier that makes a variable constant, meaning once it’s assigned, its value cannot be changed. To declare a true constant in Java, you use both static and final together: static final.

The static keyword is a non-access modifier in Java that is applicable for the following:

Blocks
Variables
Methods
Classes
When a member is declared static, it can be accessed before any objects of its class are created, and without reference to any object.

For example, let's say we have the following class:


public class MyStaticClass {
    
    public static int myStaticVariable = 10;

    public static void myStaticMethod() {
        System.out.println("myStaticMethod() from the MyStaticClass is called!");
    }
}

It has two class members, one is a static variable, the other is a static method.

In another file, we can access these members using the following syntax:


public class Main {

    public static void main(String[] args) {
        MyStaticClass.myStaticVariable = 5;

        MyStaticClass.myStaticMethod();
    }

}
Note that we do not have to create an object of the class in order to access myStaticVariable and myStaticMethod since static members are associated with the class itself, not objects of the class. Instead, we specify which class to find the variable or property in, then we use dot notation to access its static class members.

Real World Application

The static keyword in Java is used to define class-level variables and methods that are shared among all instances of the class. Here are some real-world examples of the importance of using the static keyword:

Constants: When defining constants that are shared across multiple instances of a class or are used globally within an application, the static keyword is valuable. For example, consider a MathUtils class that provides mathematical constants like pi (MathUtils.PI). By declaring PI as a static final variable, it ensures that all instances of the class share the same value and can access it without needing to create an instance of the class.

Utility Methods: Utility methods that perform common operations and are not tied to specific instances of a class can be declared as static. For instance, a StringUtils class might have a static method for trimming whitespace (StringUtils.trimWhitespace(String str)). By making this method static, it allows developers to call it directly without instantiating the StringUtils class, making the code cleaner and more readable.

Factory Methods: Factory methods are often declared as static when creating instances of a class. For example, consider a DatabaseConnectionFactory class that provides methods for creating database connections. By declaring a static factory method (DatabaseConnectionFactory.getConnection()), it allows users to obtain a connection without needing to instantiate the DatabaseConnectionFactory class, simplifying usage.

Counters and Trackers: static variables can be used to create counters, trackers, or shared state across multiple instances of a class. For example, in a multi-threaded application, you might use a static variable to track the number of active threads or monitor resource usage. This shared state can be accessed and updated by all instances of the class, ensuring consistency in data tracking.

Singleton Design Pattern: The static keyword is commonly used in implementing the Singleton design pattern, where only one instance of a class is allowed to exist. By declaring the instance variable and the method to retrieve the instance as static, it ensures that all access points to the singleton instance refer to the same object, preventing multiple instances from being created unintentionally.

In summary, the static keyword promotes code reusability, simplifies usage, and facilitates the implementation of design patterns and shared state management in real-world applications.

Implementation

In the Java program below, we are accessing static method m1() without creating any object of the Test class.

// Java program to demonstrate that a static member
// can be accessed before instantiating a class

class Test
{
  // static method
  static void m1()
  {
    System.out.println("from m1");
  }

  public static void main(String[] args)
  {
    // calling m1 without creating
    // any object of class Test
    m1();
  }
}

Note that we do not have to specify the class name on line 21 because we are working within the same class.

Static blocks
If you need to do the computation in order to initialize your static variables, you can declare a static block that gets executed exactly once, when the class is first loaded.

Consider the following java program demonstrating the use of static blocks.

// Java program to demonstrate use of static blocks
  
class Test
{
    // static variable
    static int a = 10;
    static int b;
      
    // static block
    static {
        System.out.println("Static block initialized.");
        b = a * 4;
    }
  
    public static void main(String[] args)
    {
       System.out.println("from main");
       System.out.println("Value of a : "+a);
       System.out.println("Value of b : "+b);
    }
}
Output

Static block initialized.
from main
Value of a : 10
Value of b : 40
Static variables
When a variable is declared as static, then a single copy of the variable is created and shared among all objects at the class level. Static variables are, essentially, global variables. All instances of the class share the same static variable.

Important points for static variables:

We can create static variables at the class level only.
Static block and static variables are executed in the order they are present in a program.
Below is the Java program to demonstrate that static block and static variables are executed in the order they are present in a program.

// Java program to demonstrate execution
// of static blocks and variables
  
class Test
{
    // static variable
    static int a = m1();
      
    // static block
    static {
        System.out.println("Inside static block");
    }
      
    // static method
    static int m1() {
        System.out.println("from m1");
        return 20;
    }
      
    // static method(main !!)
    public static void main(String[] args)
    {
       System.out.println("Value of a : "+a);
       System.out.println("from main");
    }
}
Output

from m1
Inside static block
Value of a : 20
from main
Static methods
When a method is declared with the static keyword, it is known as the static method. The most common example of a static method is the main( ) method. As discussed above, Any static member can be accessed before any objects of its class are created, and without reference to any object. Methods declared as static have several restrictions:

They can only directly call other static methods.
They can only directly access static data.
They cannot refer to the this or super keywords in any way.
Below is the java program to demonstrate restrictions on static methods.

// Java program to demonstrate restriction on static methods
  
class Test
{
    // static variable
    static int a = 10;
      
    // instance variable
    int b = 20;
      
    // static method
    static void m1()
    {
        a = 20;
        System.out.println("from m1");
          
         // Cannot make a static reference to the non-static field b
         b = 10; // compilation error
                  
         // Cannot make a static reference to the 
                 // non-static method m2() from the type Test
         m2();  // compilation error
           
         //  Cannot use super in a static context
         System.out.println(super.a); // compiler error 
    }
      
    // instance method
    void m2()
    {    
        System.out.println("from m2");
    }
      
      
      
    public static void main(String[] args)
    {
        // main method 
    }
}
Output:

prog.java:18: error: non-static variable b cannot be referenced from a static context
         b = 10; // compilation error
         ^
prog.java:22: error: non-static method m2() cannot be referenced from a static context
         m2();  // compilation error
         ^
prog.java:25: error: non-static variable super cannot be referenced from a static context
         System.out.println(super.a); // compiler error 
                            ^
prog.java:25: error: cannot find symbol
         System.out.println(super.a); // compiler error 
                                 ^
  symbol: variable a
4 errors
Regarding the error message non-static variable b cannot be referenced from a static context, this means that we cannot refer to an instance variable directly from a static method. We would need an object of the class.

Regarding the error message non-static method m2() cannot be referenced from a static context, this means that we cannot refer to an instance method directly from a static method. We would need an object of the class.

Regarding the last two error messages, super is a reference to the super class from the context of an object. Because we are working with a static method, and not using an object, we get a compilation issue.

Exercise (Optional)
Debug the above program to correct the erroneous statements.

Summary

A static member is a member of a class that isn’t associated with an instance of a class.
Instead, the member belongs to the class itself.
As a result, you can access the static member without first creating a class instance.


## Variable Scopes
Learning Objectives

After completing this module, associates should be able to:

Define the term variable scope.
List and discuss the different types of variable scope.
Successfully execute a Java program that demonstrates the different levels of variable scope.
Description
Variable scopes
When a variable is declared in a Java program, it is attached to a specific scope within the program, which determines where the variable resides. The different scopes of a variable in Java are:

Instance, or object, scope
Class, or static, scope
Method scope
Block scope
Instance scope means that the variable is attached to individual objects created from the class. When an instance-scoped variable is modified, it has no effect on other, distinct objects of the same class.

Class scoped variables reside on the class definition itself. This means that when objects update a class-scoped variable, the change is reflected across all instances of the class. Class scope is declared with the static keyword. Methods can be static, however, static methods cannot invoke instance methods or variables because there is not a specific object to reference. Static methods and variables should be referenced through the class directly, not through an object. For example: MyClass.myStaticMethod() or MyClass.myStaticVariable.

Method scope is the scope of a variable declared within a method block, whether static or instance. Method-scoped variables are only available within the method they are declared; they do not exist after the method finishes execution (the stack frame is popped from the stack and removed from memory after execution).

Block scoped variables only exist within the specific control flow block, of which there are several in Java: for, while, and do-while loops, if/else-if/else blocks, switch cases, or even just regular blocks of code declared via curly braces ({}). After the block ends, variables declared within it are no longer available.

Real World Application

Overview

In Java, as in any programming language, each variable has a scope. This is the segment of the program where a variable can be used and is valid.
Here we will introduce the available scopes in Java and discuss the differences between them.
Class Scope

Each variable declared inside of a class's brackets ( {} ) with the private access modifier but outside of any method, has class scope. As a result, these variables can be used everywhere in the class, but not outside of it:
public class ClassScopeExample {
    private Integer amount = 0;
    public void exampleMethod() {
        amount++;
    }
    public void anotherExampleMethod() {
        Integer anotherAmount = amount + 4;
    }
}
We can see that ClassScopeExample has a class variable amount that can be accessed within the class's methods.

If we don't use private, it will be accessible from the entire package.

Method Scope

When a variable is declared inside a method, it has method scope and it will only be valid inside the same method:
public class MethodScopeExample {
    public void methodA() {
        Integer area = 2;
    }
    public void methodB() {
        // compiler error, area cannot be resolved to a variable
        area = area + 2;
    }
}
In methodA, we created a method variable called area. For that reason, we can use area inside methodA, but we can't use it anywhere else.

Loop Scope

If we declare a variable inside a loop, it will have a loop scope and will only be available inside the loop:
public class LoopScopeExample {
    List<String> listOfNames = Arrays.asList("Joe", "Susan", "Pattrick");
    public void iterationOfNames() {
        String allNames = "";
        for (String name : listOfNames) {
            allNames = allNames + " " + name;
        }
        // compiler error, name cannot be resolved to a variable
        String lastNameUsed = name;
    }
}
We can see that method iterationOfNames has a method variable called name. This variable can be used only inside the loop and is not valid outside of it.

Bracket Scope

We can define additional scopes anywhere using brackets ({}):
public class BracketScopeExample {    
    public void mathOperationExample() {
        Integer sum = 0;
        {
            Integer number = 2;
            sum = sum + number;
        }
        // compiler error, number is out of scope at this line
        number++;
    }
}
The variable number is only valid within the brackets.

Instance scope

Instance Variables
Instance variables, also called instance fields, are data associated with a class object. They make up the state that each instance will possess. If we want each instance of a Car class object to have a color and speed associated with it, we can define instance variables color of type String and speed of type int within the class. These variables are available for assignment within the class constructor. Here we’ll define them as public.

public class Car {
    public String color;
    public int speed;

    public Car(String carColor, int carSpeed) {
        // Instantiate instance variables in constructor
        color = carColor;
        speed = carSpeed;
    }
    public static void main(String[]args) {
        Car carObject = new Car("red", 50);
    }
}
this is used within any class method or constructor to reference the current object. You can reference any instance variable of a class, from within the class, using this. It cannot be used in a static context. Using this easily clarifies which variables are being referenced. Typically, this is how a constructor is created:

public class Car {
    public String color;
    public int speed;

    public Car(String color, int speed) {
        // Use keyword this to instantiate variables
        this.color = color;
        this.speed = speed;
    }
}
We assign the constructor arguments color and speed to the class instance variables color and speed which are referenced using this.color and this.speed. It is important to distinguish between local variables and instance variables.

Implementation
Member Variables (Class Level Scope)
These are the variables that are declared inside the class but outside any function has class-level scope. We can access these variables anywhere inside the class. Note that the access specifier of a member variable does not affect the scope within the class. Java allows us to access member variables outside the class with the following rules:

Access Modifier	Package	Subclass	World
public	Yes	Yes	Yes
protected	Yes	Yes	No
private	No	No	No
default	Yes	No	No
Let's see an example.

public class ClassScopedVariable {

    // This is a class-level variable, also known as an instance variable
    // It is accessible throughout the entire class
    private int classVariable = 10;

    public void printClassVariable() {
        // Accessing the class variable within a method
        System.out.println("Class variable: " + classVariable);
    }

    public static void main(String[] args) {
        ClassScopedVariable example = new ClassScopedVariable();

        // Accessing the class variable from the main method
        example.printClassVariable();
    }
}
In this example, classVariable is a class-scoped variable (also known as an instance variable). It is declared within the class but outside of any specific method. The printClassVariable method demonstrates how to access and print the value of the class-scoped variable. The main method creates an instance of the class and calls the printClassVariable method to showcase the usage of the class-scoped variable.

Here is another example of an instance variable. These are declared inside a class but outside any method, constructor, or block. When an instance variable is declared using the keyword static is known as a static variable. Their scope is class level but visible to the method, constructor, or block that is defined inside the class.

Let's see an example.

public class Product {
//variable visible to any child class  
	public String pName;
//variable visible to product class only  
	private double pPrice;

//creating a constructor and parsed product name as a parameter  
	public Product(String pname) {
		pName = pname;
	}

//function sets the product price  
	public void setPrice(double pprice) {
		pPrice = pprice;
	}

//method prints all product info  
	public void getInfo() {
		System.out.println("Product Name: " + pName);
		System.out.println("Product Price: " + pPrice);
	}

	public static void main(String args[]) {
		Product pro = new Product("Mac Book");
		pro.setPrice(65000);
		pro.getInfo();
	}
}
Let's see another example.

StaticVariableScope.java

public class StaticVariableScope {
//declaring a private static variable  
	private static double pivalue;
//declaring a constant variable  
	public static final String piconstant = "PI";

	public static void main(String args[]) {
		pivalue = 3.14159265359;
		System.out.println("The value of " + piconstant + " is: " + pivalue);
	}
}
Local Variables (Method Level Scope)
These are the variables that are declared inside a method, constructor, or block have a method-level or block-level scope and cannot be accessed outside in which it is defined. Variables declared inside a pair of curly braces {} have block-level scope.

Declaring a Variable Inside a Method
public class DemoClass1 {
	void show() {
//variable declared inside a method has method level scope  
		int x = 10;
		System.out.println("The value of x is: " + x);
	}

	public static void main(String args[]) {
		DemoClass1 dc = new DemoClass1();
		dc.show();
	}
}
Let's see another example of method-level scope.

public class DemoClass2 {
	private int a;

	public void setNumber(int a) {
		this.a = a;
		System.out.println("The value of a is: " + a);
	}

	public static void main(String args[]) {
		DemoClass2 dc = new DemoClass2();
		dc.setNumber(3);
	}
} 
In the above example, we have passed a variable as a parameter. We have used this keyword that differentiates the class variable and local variable.

Declaring a Variable Inside a Constructor
public class VariableInsideConstructor {
//creating a no-args constructor    
	VariableInsideConstructor() {
		int age = 24;
		System.out.println("Age is: " + age);
	}

//main() method    
	public static void main(String args[]) {
//calling a no-args constructor     
		VariableInsideConstructor vc = new VariableInsideConstructor();
	}
}  
Declaring a Variable Inside a Block (Block Level Scope)
public class VariableInsideBlock {
	public static void main(String args[]) {
		int x = 4;
		{
//y has limited scope to this block only      
			int y = 100;
			System.out.println("Sum of x+y = " + (x + y));
			y = 10;
//gives error, already defined  
			int y = 200;
		}
//creates a new variable  
		int y;
	}
}
The line y = 10; is commented out because it will cause a compile-time error if the int y = 200; line is also present. If you uncomment y = 10; the code will compile and run without any issues. The line int y = 200; is commented out because it will cause a compile-time error due to the duplicate declaration of the variable y within the same block. The line int y; outside the block is commented out because it will cause a compile-time error due to the potential use of an uninitialized variable y. If you need to declare y outside the block, you must initialize it with a value.

Let's see another example.

public class BlockScopeExample1 {
	public static void main(String args[]) {
		for (int x = 0; x < 10; x++) {
			System.out.println(x);
		}
		System.out.println(x);
	}
}
When we run the above program, it shows an error at line 9, cannot find symbol because we have tried to print the variable x that is declared inside the loop. To resolve this error, we need to declare the variable x just before the for loop as shown below -

public class BlockScopeExample2 {
	public static void main(String args[]) {
		int x;
		for (x = 0; x < 10; x++) {
//prints 0 to 9      
			System.out.print(x + "\t");
		}
//prints 10      
		System.out.println(x);
	}
} 
Let us look at all the scopes together.

public class VariableScopeExample {

    // This is a class-level variable, also known as an instance variable
    // It is accessible throughout the entire class
    private int classVariable = 10;

    public void methodWithLocalVariable() {
        // This is a local variable, only accessible within this method
        int localVar = 5;

        // Accessing both local and class variables within this method
        System.out.println("Local variable: " + localVar);
        System.out.println("Class variable: " + classVariable);
    }

    public void anotherMethod() {
        // Attempting to access the local variable declared in another method will result in a compilation error
        // Uncommenting the next line will result in a compilation error
        // System.out.println("Trying to access local variable from another method: " + localVar);

        // Accessing the class variable is fine
        System.out.println("Class variable from another method: " + classVariable);
    }

    public static void main(String[] args) {
        VariableScopeExample example = new VariableScopeExample();

        // Accessing the class variable from the main method
        System.out.println("Class variable from main method: " + example.classVariable);

        // Invoking the method with local variable
        example.methodWithLocalVariable();

        // Invoking another method
        example.anotherMethod();
    }
}
Exercises (Optional)
Create a single Java program that shows the following levels of scope via output to the console:
Object
Class
Method
Block
Use proper source code commenting.
Summary

When a variable is declared in a Java program, it is attached to a specific scope within the program, which determines where the variable resides. The different scopes of a variable in Java are:

Instance, or object, scope - The variable is attached to individual objects created from the class.
Class, or static, scope- Resides on the class definition itself.
Method scope - Declared within a method block; only available within the method in which they are declared.
Block scope - Only exist within the specific control flow block (for, while, etc.)


## Arrays
Learning Objectives

After completing this module, associates should be able to:

Define an array
Discuss different features and advantages of using arrays.
Successfully execute a Java program that demonstrate basic operations of arrays.
Description

An array is a contiguous block of memory storing a group of sequentially stored elements of the same type. Arrays in Java are of a fixed size and cannot be resized after declaration. Arrays are declared with square brackets after the type of the array like so:

int[] myInts = new int[]{1, 2, 3, 4};
String languages[] = {"Java", "JavaScript", "SQL"};
Note that the square brackets can be placed before or after the name of the array.

Each item of an array is called an element.
All elements of an array must be of the same type.
Items in an array are referenced via their index in square bracket notation, which begins with 0 for the first element.
Below is an example of selecting an element:

String myElement = languages[0];
In the above code, we select the first element of the languages array and save the value into the variable myElement.

Arrays also have a length property specifying the length of the array. This is helpful when iterating over arrays with a for loop:
String[] myArr = {"first", "second", "third"};
for (int i = 0; i < myArr.length; i++) {
  System.out.println(myArr[i]);
}
Even though it has no corresponding class, an array is still an object.
Thus, you can refer to an entire array by using the array's name without brackets.
In the above example, myArr by itself refers to the whole array.
Real World Application
Application of Arrays
Arrays are the simplest data structures that store items of the same data type. A basic application of Arrays can be storing data in tabular format. For example, if we wish to store the contacts on our phone, then the software will simply place all our contacts in an array.

Some other applications of the arrays are:

Arrangement of the leader-board of a game can be done simply through arrays to store the score and arrange them in descending order to clearly make out the rank of each player in the game.
2D arrays, commonly known as, matrices, are used in image processing.
It is also used in speech processing, in which each speech signal is an array.
Your viewing screen is also a multidimensional array of pixels.
Book titles in a Library Management Systems.
Online ticket booking.
Contacts on a cell phone.
For CPU scheduling in computer.
To store the possible moves of chess on a chessboard.
To store images of a specific size.
Implementation
Background
In this exercise, we'll practice creating different arrays and using a few properties of arrays.

Arrays are an important construct in many modern languages because they serve as a backbone for higher level data structures. Arrays are also commonly used because they are fast and efficient at obtaining data because of the way the system stores them in memory.

Recall that an array can be thought of as a one-row grid. Each piece of data is referred to as an element and each element has a position referred to as an index. Array indexes start at 0 (zero).

To create an array, you'll specify the datatype followed by brackets ([]).

int[] intArray;
The above line will create an array of int datatype or an integer array.

To initialize an array, you'll need to know its length beforehand. For example, to create an array that has 5 elements you would use the following code:

int[] myArray = new int[5];
Notice that we've also used the new keyword. Anytime you create arrays or non-primitive datatypes, you'll use the new keyword.

public class DisplayingArrays {

  public static void main(String[] args) {
    int[] intArray = new int[3];

    System.out.println(intArray);
  }
}
If we were to run the program something peculiar happens:



The output looks like some mix of characters unrelated to your array. This is the default way that a program will print an array. You'll see this often with non-primitive datatypes. The output is the address of the memory location of the array. 

To print the contents of the array, we can refer to individual elements.

We can edit the class to assign values for two positions in the array and then to print the first value (which is index 0). 

public class DisplayingArrays {

  public static void main(String[] args) {
    int[] intArray = new int[3];
    intArray[0] = -5123;
    intArray[2] = 32;

    System.out.println(intArray[0]);
  }
}
We would see the following output:



Notice that the value of the first element is printed. Again, the first element has index 0.

We are going to edit the file again and print the second element (the second element will have index 1):

public class DisplayingArrays {

  public static void main(String[] args) {
    int[] intArray = new int[3];
    intArray[0] = -5123;
    intArray[2] = 32;

    System.out.println(intArray[1]);
  }
}
We would see the following output:



Notice that the program prints the default value of an integer, 0, since we did not specify a value.

We will edit the file again to try and print a value outside of the range specified for the array:

public class DisplayingArrays {

  public static void main(String[] args) {
    int[] intArray = new int[3];
    intArray[0] = -5123;
    intArray[2] = 32;

    System.out.println(intArray[3]);
  }
}
What do you think will happen?

We would see the following output:



Notice that we didn't get any compiler errors. This type of error is referred to as a Runtime exception (because it occurs during the execution or run of the program). Neat, huh?

Reading through the message, you'll see that it is an ArrayIndexOutOfBoundsException and the index that was attempted is 3. This error simply means that you're accessing a position not defined by the array. The index 3 is not part of our array (which has indexes 0, 1, and 2).

As long as you access indexes supported by the array, then you won't get this particular exception.

Array length property
What happens if you're writing a bit of code and you were passed or given an array? How would you know what indexes were available?

You can use the length property of the array to access its size. This property is automatically populated with the array's size on creation and exists whenever an array is created.

public class ArrayLength {

  public static void main(String[] args) {
    int[] arr = {1,2,3,4,5,6,7,8,9,10};
  }

}
Notice that we've used the shortcut way to create an array and initialize it with specific values. Instead of saying new int[10] we've used curly braces and separated each value with a comma.

This will create an array with 10 elements with the values 1 through 10.

Now print the length of the array to the console:

public class ArrayLength {

  public static void main(String[] args) {
    int[] arr = {1,2,3,4,5,6,7,8,9,10};
    System.out.println(arr.length);
  }

}
If we were to run the above code we would see the following output:



Exercises (Optional)
Write a Java program that demonstrates the following concepts with respect to Arrays (you may use the same array to demonstrate more than one concept, for example you may use one array to demonstrate both "array of objects" and "multidimensional arrays"):

Show two different ways to declare arrays
Show two different ways to initialize arrays
Iterate through an array using a standard for loop
Also iterate through an array using for-each
Create an array of objects
Create a multidimensional array
Summary

An array is a contiguous block of memory storing a group of sequentially stored elements of the same type. Arrays in Java are of a fixed size and cannot be resized after declaration.
Each item of an array is called an element.
All elements of an array must be of the same type.
Items in an array are referenced via their index in square bracket notation, which begins with 0 for the first element.
Arrays also have a length property specifying the length of the array.
Even though it has no corresponding class, an array is still an object.



# Java Basics/OOP
## Stack and Heap Memory
Learning Objectives

After completing this module, associates should be able to:

Identify and describe the terms stack and heap
Description

To run an application in an optimal way, JVM divides memory into stack and heap memory. Whenever we declare new variables and objects, call a new method, declare a String, or perform similar operations, JVM designates memory to these operations from either Stack Memory or Heap Space.
In this tutorial, we'll examine these memory models. First, we'll explore their key features. Then we'll learn how they are stored in RAM, and where to use them. Finally, we'll discuss the key differences between them.
Stack Memory in Java
Introduction

Stack Memory in Java is used for static memory allocation and the execution of a thread. It contains primitive values that are specific to a method and references to objects referred from the method that are in a heap.
Access to this memory is in Last-In-First-Out (LIFO) order. Whenever we call a new method, a new block is created on top of the stack which contains values specific to that method, like primitive variables and references to objects.
When the method finishes execution, its corresponding stack frame is flushed, the flow goes back to the calling method, and space becomes available for the next method.
Key Features of Stack Memory

It grows and shrinks as new methods are called and returned, respectively.
Variables inside the stack exist only as long as the method that created them is running.
It's automatically allocated and deallocated when the method finishes execution.
If this memory is full, Java throws java.lang.StackOverFlowError.
Access to this memory is fast when compared to heap memory.
This memory is threadsafe, as each thread operates in its own stack.
Heap Space in Java
Introduction
Heap space is used for the dynamic memory allocation of Java objects and JRE classes at runtime. New objects are always created in heap space, and the references to these objects are stored in stack memory.
These objects have global access and we can access them from anywhere in the application.
We can break this memory model down into smaller parts, called generations, which are:
Young Generation – this is where all new objects are allocated and aged. A minor Garbage collection occurs when this fills up.
Old or Tenured Generation – this is where long surviving objects are stored. When objects are stored in the Young Generation, a threshold for the object's age is set, and when that threshold is reached, the object is moved to the old generation.
Permanent Generation – this consists of JVM metadata for the runtime classes and application methods.
Key Features of Java Heap Memory
Some other features of heap space include:
It's accessed via complex memory management techniques that include the Young Generation, Old or Tenured Generation, and Permanent Generation.
If heap space is full, Java throws java.lang.OutOfMemoryError.
Access to this memory is comparatively slower than stack memory
This memory, in contrast to stack, isn't automatically deallocated. It needs Garbage Collector to free up unused objects so as to keep the efficiency of the memory usage.
Unlike stack, a heap isn't threadsafe and needs to be guarded by properly synchronizing the code.
Visualization
Below, you can see a visualization of Stack and Heap Memory in Java. Notice that there are three object references in Stack memory, but only two objects created in Heap memory. Keep in mind that reference variables will return the memory address of objects including Strings in the String pool.Stack vs Heap Visualization

Real World Application

Understanding the concepts of the stack and heap memory is essential for optimizing memory usage, managing resources efficiently, and avoiding memory-related issues in real-world software development. Here are some examples of real-world applications of knowing about the stack and heap:

Memory Management in Embedded Systems: In embedded systems programming, where resources are limited, knowledge of stack and heap memory usage is crucial for efficient memory management. Developers need to carefully allocate and deallocate memory to ensure that the system operates within its memory constraints without running out of memory or encountering memory leaks.

Performance Optimization in High-Performance Computing: In high-performance computing applications, such as scientific simulations, data processing, and numerical computations, optimizing memory usage can significantly improve performance. Understanding how data is stored in the stack and heap allows developers to design algorithms and data structures that minimize memory overhead and maximize cache efficiency, leading to faster execution times.

Multithreaded Programming: In multithreaded programming, each thread typically has its own stack but shares the heap with other threads. Knowledge of stack and heap memory usage is essential for managing thread-local data and synchronizing access to shared resources. Developers need to carefully design concurrent data structures and synchronization mechanisms to prevent race conditions, deadlocks, and memory corruption issues.

Resource Management in Mobile and Game Development: In mobile and game development, where memory resources are limited, efficient memory management is critical for ensuring smooth performance and responsiveness. Developers need to carefully manage memory allocation and deallocation to minimize memory usage, avoid garbage collection pauses, and optimize runtime performance on resource-constrained devices.

Overall, understanding the stack and heap memory is essential for optimizing memory usage, managing resources efficiently, and preventing memory-related issues in various real-world software development scenarios.

Implementation

Based on what we've learned so far, let's analyze a simple Java code to assess how to manage memory here:

class Person {
    int id;
    String name;

    public Person(int id, String name) {
        this.id = id;
        this.name = name;
    }
}

public class PersonBuilder {
    private static Person buildPerson(int id, String name) {
        return new Person(id, name);
    }

    public static void main(String[] args) {
        int id = 23;
        String name = "John";
        Person person = null;
        person = buildPerson(id, name);
    }
}
Let's analyze this step-by-step:

When we enter the main() method, a space in stack memory is created to store primitives and references of this method
Stack memory directly stores the primitive value of integer id.
The reference variable name will be created in stack memory and will point to a String object that resides in the heap (specifically the String Pool)
The reference variable person of type Person will also be created in stack memory.
The main method is further calling the buildPerson() static method, for which further allocation will take place in stack memory on top of the previous one.
The call to the parameterized constructor Person(int, String) will allocate further memory on top of the previous stack. This will store:
The this object reference of the calling object in stack memory
The primitive value id in the stack memory
The reference variable of String argument name, which will point to the actual string from string pool in heap memory
However, heap memory will store all instance variables for the newly created object person of type Person.
Summary

Parameter	Stack Memory	Heap Space
Application	Stack is used in parts, one at a time during execution of a thread	The entire application uses Heap space during runtime
Size	Stack has a size limit that depends on the OS and JVM settings, and it is usually smaller than the heap.	Heap size is configurable via JVM settings (-Xmx), and exceeding it can result in an OutOfMemoryError.
Storage	Stores only primitive variables and references to objects that are created in Heap Space	All the newly created objects are stored here
Order	It's accessed using Last-in First-out (LIFO) memory allocation system	This memory is accessed via complex memory management techniques that include Young Generation, Old or Tenured Generation, and Permanent Generation.
Life	Stack memory only exists as long as the current method is running	Heap space exists as long as the application runs
Efficiency	Much faster to allocate when compared to heap (due to LIFO and automatic deallocation)	Slower to allocate when compared to stack (requires garbage collection)
Allocation/Deallocation	This Memory is automatically allocated and deallocated when a method is called and returned, respectively	Heap space is allocated when new objects are created and deallocated by Garbage Collector when they're no longer referenced

## Casting
Learning Objectives

After completing this module, associates should be able to:

Discuss the differences between type conversion and type casting.
List the rules that Java follows for type conversion.
Describe how type casting works.
Compile, test, and successfully execute a Java program that demonstrates type conversion and type casting.
Description
Automatic conversions
Java can automatically convert some primitive types to others and do so whenever necessary. For example, an int can be converted to a float, but large int values won't be converted exactly because int values can have more digits than can be represented by the float type.

Whenever you perform a mathematical operation on two values that aren't of the same type, Java automatically converts one of them to the type of the other. Here are the rules Java follows when doing a type conversion:

If one of the values is a double, the other value is converted to a double
If neither is a double but one is a float, the other is converted to a float.
If neither is a double nor a float but one is a long, the other is converted to a long.
If all else fails, both values are converted to int.
Casting
Casting is the process of converting a data type to another data type. This is also called type casting. Casting is necessary in some situations when we receive a data type that needs to have some actions performed on it that its original form cannot handle.

When working with some primitive numeric types, Java will automatically do this conversion. This automatic conversion is supported when the types are compatible with range and precision. The original data type must have a smaller size in memory (and thus is capable of holding a smaller range in values) than the target type. The target type must also hold the same or greater precision past the decimal point, so that there is no data lost in the conversion.

Real World Application

In the example below, an int (size of 32 bits) is being cast to a double (size of 64 bits). Due to their comparable size in memory, Java is able to do this conversion automatically without the loss of any data. And because we're going from a data type that only supports integer values to a data type that holds decimals, we risk no loss of precision.

public class AutomaticCasting {
  public static void main(String[] args){
    int n = 10;
    double d = n; //This is now converted from an int to a double.
  }
}
However, cases where the original data type is larger than the target type, you will have to explicitly convert them using parentheses. There can be data loss using this method, as the target type is not always able to represent the original value.

public class ExplicitCasting {
  public static void main(String[] args){
    short s = 150;
    byte b = s; // this will not compile without an explicit cast 
    // and in this case, even with an explicit cast, there will be data loss
    // as bytes can only hold values up to 127
    // b would actually hold the value -106 here
  }
}
public class ExplicitCasting2 {
  public static void main(String[] args){
        float f = 10.23f;
        int i = f; // this wil also not compile without an explicit cast 
        // although floats and ints are both represented in memory with 32 bits, floats can hold decimal values while ints cannot
        // we can explicitly cast these values, but we lose .23
        // i holds the value 10
  }

}
In some cases you will have to use the data type's own methods to convert. Some of these methods are listed in the table below.

Original	Target	Method
String	int	Integer.parseInt(String);
int	String	String.valueOf(int);
You are also able to cast subclass objects to their superclasses which is called upcasting and cast superclass objects to their subclasses (This has some limitations and prerequisites).

Implementation
Background
Casting a data type to another allows you to convert one similar type to another. Really, this is only used for numbers as you can’t cast an int to a boolean.

To cast one type to another, you’ll specify the type that you want to cast to in parenthesis in front of the variable that you are converting.

For example, look at the following:

long a = 10393L;

int i = (int)a;
In the second statement, we’ve casted the variable a to an int type, and stored the value in a new variable labeled, i. To cast, you specify a datatype in parentheses in front of the variable that you're converting. 

Widening vs. Narrowing
There is an important distinction to be aware of when casting to a type that encompasses a larger range (widening) or to a type with a smaller range (narrowing).

When implementing a widening type conversion, you won’t have to worry about data loss, because you will have extra bits of information to store the number in. For example, when casting from a float to a double, you move from 32 bits to 64 bits.

When implementing a narrowing type conversion, however, you can potentially lose some data depending upon the range of the type that you’re casting to. For example, if you cast from an int to a byte, any number larger than 127 (8 bits, using two’s complement), will be truncated and you’ll lose some bits of information.

Casting an int to a short
public class Example {

  public static void main(String[] args) {

    //initial values
    int i = 200;

    //cast to a short
    short s = (short) i;

    System.out.println(s); //output of s will b 200

  }

}       

The output is the number 200.

Nothing special here. A short can represent 200 without any issues. If the value was below -32,768 or above 32,767 there would be potential data loss due to overflow or underflow. However, in this case, the value 200 falls within the valid range, and there is no data loss during the cast.



 

Casting an int to a double
Next, we’ll cast the same value to a double. Recall a double is a 64-bit floating point number, so it can easily represent the value 200.

public class Example {

  public static void main(String[] args) {

    //default values
    int i = 200;

    //cast to a short
    short s = (short) i;
    System.out.println(s);

    //cast to a double
    double d = (double) i;

    System.out.println(d); //output is 200.0

  }

}


Notice the decimal point? This confirms that we’ve successfully converted the integer value (200), to the floating point number (200.0) which is stored as a double.

Casting an int to a byte
So, let’s see what happens when we cast our value to a type that cannot hold all of the bits of information. A byte is an 8-bit number that follows two’s complement system. This means its range of values is from -128 to 127. 200 is outside of this range. What do you think will happen in our program? Will it compile?

public class Example {

  public static void main(String[] args) {

    //default values
    int i = 200;

    //cast to a short
    short s = (short)i;

    System.out.println(s);

    //cast to a double
    double d = (double)i;

    System.out.println(d);

    //cast to a byte
    byte b = (byte)i;

    System.out.println(b);

  }

}
You’ll notice that no compiler error occurs and no runtime errors occur. Something peculiar does happen though.



The value -56 would be printed to the console.

How did this happen?

In order to understand this, we need to think back to how Java interprets numbers using bits. An integer is a 32-bit binary number; meaning we have 32 bits each representing a power of 2.

So, the number 200 as a 32-bit binary number is represented as:

0000 0000 0000 0000     0000 0000 1100 1000.

When we convert this number to a byte (which is an 8-bit type), we must truncate the extra bits. This leaves us with:

1100 1000.

A byte follows two’s complement numbering system, thus the leading 1 indicates a negative number. To find out what number this is we must find the two’s complement by inverting the bits and adding 1 to the number.

Inverting the bits gives us:

0011 0111.

Adding one gives us:

0011 1000.

This value represents (32 + 16 + 8 or 56). Thus, our number 1100 1000 represents -56.

Thus, you can clearly understand why downcasting can cause issues in some calculations, because you may lose important data due to truncating bits.

Summary

Automatic conversions

Java can automatically convert some primitive types to others and do so whenever necessary.
Whenever you perform a mathematical operation on two values that aren't of the same type, Java automatically converts one of them to the type of the other.
Casting

Casting is the process of converting a data type to another data type.
This is also called type casting.
Casting is necessary in some situations when we receive a data type that needs to have some actions performed on it that its original form cannot handle.
When working with some primitive numeric types, Java will automatically do this conversion.

This automatic conversion is supported when the types are compatible with range and precision.
The original data type must have a smaller size in memory (and thus is capable of holding a smaller range in values) than the target type.
The target type must also hold the same or greater precision past the decimal point, so that there is no data lost in the conversion.


## Classes and Objects
Learning Objectives

After completing this module, associates should be able to:

Identify the terms class, method, object, and instance.
Implement the concepts above in a Java program.
Description
Classes vs Objects and Reference Variables
In Java, it's important to understand the difference between a class, an object, and a reference variable.

A class is a template used to instantiate objects. It's also called a type when used with a reference variable. A class that is used to instantiate an object determines what state and behavior an object will possess. A class used as the type for a reference variable determines what behaviors of an object can be invoked, and how any variables get initialized.

An object is an instance of a class in memory. In Java, you never interact with objects directly. Instead, you interact with them through their reference, which is the memory address used by the JVM to find a particular object.

A reference variable is a variable that stores the reference to an object in memory. Just like the type of a primitive variable determines the range of values that a primitive variable can store, the type of a reference variable determines what types of objects a reference variable can store a reference to.

When a class is used as the type of a reference variable, that reference can only be used to invoke behaviors of the object that are declared in the class/type.

Let's look at the following line of code that we've divided into three parts:

1	2	3
String	someVar	= new String("Hello World");
The class/type of the reference variable
The name of the reference variable
The instantiation of a new object using the new keyword to invoke the constructor
The someVar reference variable does not contain a String object, it contains a reference that points to it in memory
The String type means that someVar can only store a reference to an object that is an instance of the String class (directly or through inheritance)
The String type means that someVar can only be used to invoke methods or access public variables present in the String class (whether defined in String or inherited from a superclass)
The new String("Hello World") expression creates an object, it is not the object itself. You can never access the object directly.
Understanding these concepts and their implications is key to properly understanding the Java language.

Real World Application

A real-world use case for classes and objects can be found in a banking system application:

Class Definition: In the banking system, you would define a BankAccount class to represent individual customer accounts. This class would encapsulate data and functionality related to bank accounts, such as account number, account holder name, balance, and methods for depositing, withdrawing, and transferring funds.

Object Instantiation: Each time a new customer opens an account, you would create a new instance of the BankAccount class to represent their account. For example, when a customer fills out an account application form, the banking system creates a new BankAccount object with the provided account details.

Object Interactions: Once a BankAccount object is created, it can interact with other objects and the system. For example, a customer can deposit funds into their account by calling the deposit() method on their BankAccount object. Similarly, they can withdraw funds or transfer money to another account by invoking appropriate methods.

Data Encapsulation and Abstraction: The BankAccount class encapsulates account data (e.g., account number, balance) and functionality (e.g., deposit, withdraw) within a single entity. This encapsulation hides the internal implementation details of the account and exposes only relevant methods for interacting with it, providing a level of abstraction that simplifies usage and maintenance.

Object Persistence: In a real-world banking system, you would likely store account data persistently in a database. Each BankAccount object represents a record in the database, and methods for reading from and writing to the database are encapsulated within the BankAccount class. This allows the banking system to store and retrieve account data efficiently while maintaining data integrity and security.

Overall, classes and objects provide a powerful mechanism for modeling real-world entities, encapsulating data and functionality, promoting code reuse, and building modular, maintainable software systems like a banking application.

Implementation
An Object-Oriented implementation of "Hello, World!"
To give an introductory look at what object-oriented programming really looks like, the listings below show another version of the "Hello, World!" program using two classes, one of which is actually made into an object when the program is running. The first class, named HelloApp is shown in the first example. It uses an object instantiated from the second class, named Greeter, to actually display the "Hello, World!" message on the console. The Greeter class defines a method named sayHello that displays the message.

Both the HelloApp and the Greeter classes are public classes. Java requires that each public class be stored in a separate file with the same name as the class; the filename ends with the extension .java. As a result, the HelloApp class should be stored in a file named "HelloApp.java" and the Greeter class should be stored in a file named "Greeter.java".

The helloApp class is shown in the listing below:

// This application displays a hello message on
// the console by creating an instance of the
// Greeter class and then calling the Greeter
// object's sayHello method.
public class HelloApp
{
    public static void main(String[] args)
        {
            Greeter myGreeterObject = new Greeter();
            myGreeterObject.sayHello();
        }
}
Key points of the HelloApp example:
The HelloApp class begins with the public class declaration. Because the public keyword is used, a file named "HelloApp.java" must contain this class.
The first line in the body of the main method creates a variable named myGreeterObject that can hold objects created from the Greeter class. Then it creates a new object using the Greeter class and assigns this object to the myGreeterObject variable.
The second line in the body of the main method calls the myGreeterObject object's sayHello method. This method simply displays the message, "Hello, World!" on the console.
The Greeter class is shown in the listing below:

// This class represents a Greeter object that displays
// a hello message on the console
public class Greeter
{
    public void sayHello()
    {
        System.out.println("Hello, World!");
    }
}
Key points of the Greeter example:
The sayHello method is declared using the public keyword so that the method is available to other classes that use the Greeter class. The void keyword indicates that this method doesn't provide any data back to the statement that calls it, and sayHello simply provides the name of the method.
The body of this method consists of just one line of code that displays the "Hello, World!" message on the console.
Object-oriented implementation
This version of the "Hello, World!" program defines a class that knows how to say hello to the world, then creates an object from that class, and asks that object to say hello.

The application itself doesn't know (or care) exactly how the Greeter object says hello. It doesn't know exactly what the greeting will be, what language the greeting will be in, or even how the greeting will be displayed.

Thus, the Greeter class can be changed without having to change the HelloApp class. If, for example we wanted to change the Greeter class to display, "Hola, mundo!" we would make that change in the Greeter class and recompile it. The HelloApp class would not know the difference. This is one of the main benefits of object-oriented programming.

Summary

A class is a template used to instantiate objects.
A method is an executable element inside of a class.
An object is an instance of a class in memory.
A reference variable is a variable that stores the reference to an object in memory.
One advantage of using classes is that they can be developed independently. For example, if Class A calls for an instance of Class B, Class B can be modified without any changes in Class A.


## Constructors
Learning Objectives

After completing this module, associates should be able to:

Define and describe how a constructor works.
Discuss the default constructor.
Successfully implement a constructor and a no-parameter constructor in a Java program.
Description
Constructors
When we use the new keyword in order to create an object, the JVM is invoking a special class member called a constructor. A constructor declares how an object is to be instantiated and initialized from the class "blueprint". A constructor is declared like a method, except its method signature does not contain a return type, and a constructor always has the same name as the class. The new object created by the constructor is always of the class in which the constructor is declared. A simple example is shown below:

public class ConstructorExample {

  int myNumber;

  public static void main(String[] args) {
    ConstructorExample ce = new ConstructorExample(3); // a
	  System.out.println(ce.myNumber); // b
  }
  
  public ConstructorExample(int myNumber) { // c
    this.myNumber = myNumber; // d
  }
}
this keyword
When this program runs, it will print 3. How does this happen? The constructor is defined on line "c" with one input parameter. Note that the constructor does not have a return type. On line "d", the parameter myNumber is assigned to the instance variable myNumber via the this keyword. this refers to the object which is being instantiated - it is used to initialize instance variables, or - to call other constructors (this is called constructor chaining).

When the program above runs, the main method is executed. On line "a", a new ConstructorExample object is created and assigned to the variable ce. The constructor is invoked with the new keyword and the int 3 is passed as the argument. this assigns the value 3 to the instance variable myNumber on the object returned, as explained above. Finally, the instance variable is printed out and the program finishes execution.

super keyword
There is another keyword important for constructors - the super keyword, which references the "super", or parent, class. When invoked as a method (super()), the parent class constructor will be called. A super() call (or a this() call) must be the first line of any constructor. If not explicitly provided, the compiler will inject super() it on the first line implicitly.

Default constructor
In MySimpleClass below, we can see a constructor with no arguments used with the new keyword. This is valid code, but we didn't define a constructor, so how is this possible? It turns out the compiler will inject a "default" constructor for us if we do not define one ourselves. The "default" constructor takes no arguments and simply calls super() (see above) - sometimes it is referred to as the "default, no-args" constructor. However, be warned that if we define our own constructor(s) in the class, we will not receive a default constructor from the compiler.

Thus, the simple class:

public class MySimpleClass {}
has a default no-args constructor that can be called:

MySimpleClass someVariable = new MySimpleClass();
Real World Application

Constructors are essential in object-oriented programming for several reasons:

Initialization: Constructors are used to initialize objects by setting their initial state or assigning values to their member variables. This ensures that objects are in a valid and consistent state when they are created, ready to be used by the program.

Encapsulation: Constructors encapsulate the initialization logic within the class, hiding the details of object creation from the outside world. This promotes data encapsulation and prevents direct manipulation of object state by external code, improving code maintainability and reducing the risk of errors.

Default Initialization: Constructors provide a way to specify default values for object properties, ensuring that objects are properly initialized even if specific values are not provided explicitly. This helps prevent uninitialized variables and ensures predictable behavior of objects in the absence of explicit initialization.

Overloading: Constructors can be overloaded, allowing multiple constructors to be defined with different parameter lists. This enables objects to be created in different ways or with different initializations, providing flexibility and supporting various use cases without the need for separate factory methods.

Dependency Injection: Constructors can be used for dependency injection, where dependencies required by an object are passed to it through constructor parameters. This promotes loose coupling between classes and facilitates unit testing and dependency inversion, making code more modular, reusable, and testable.

Overall, constructors are fundamental in object-oriented programming for promoting encapsulation, enabling dependency injection, and ensuring that objects are properly initialized and ready for use in real-world applications.

Implementation
Background
A constructor is a block of code that is executed when a class is instantiated. This block of code is executed once per object that is created.

A constructor always have the same name as the class and can accept any number of parameters.

By default, if you create a class and don't specify a constructor the compiler will generate one for you. The one generated is called the default no-arg constructor.

For example, let's consider the following class

public class DefaultClass{

    public static void main(String[] args) {
        //create a new class instance
        DefaultClass dc = new DefaultClass();
    }
}
We have specified a constructor, so once compiled, the file will more resemble this structure:

public class DefaultClass{

    public DefaultClass(){ }

    public static void main(String[] args) {
        //create a new class instance
        DefaultClass dc = new DefaultClass();
    }
}
Instructions
Now that we have some background on constructors let's see another example of a class with a constructor that takes an argument of type int.

public class Constructors {

    public Constructors(int value){
        System.out.println(value);
    }

    public static void main(String[] args) {
        //New instance created
        Constructors c = new Constructors(5839);
    }
}
The variable c is assigned the value 5839 by being passed into the newly created Constructors object.

We can see that the Constructors body has been executed.



Now let's see another example of a class with different constructors.

public class Constructors {

    public Constructors(int value){
        System.out.println(value);
    }

    public static void main(String[] args) {
        //New instance created
        Constructors c = new Constructors(5839);

        //use the no-arg constructor
        Constructors cNoArg = new Constructors();
    }
}
The line that attempts to create a new Constructors object using the no-arg constructor will be underlined in red because, The constructor Constructors() is undefined.



Typically the compiler creates a no-arg constructor, however, if you specify one (as we defined, one with a single argument), then the compiler will not add the default no-arg constructor.

Let's look at this example that has a no-arg constructor defined:

public class Constructors {

    public Constructors(){
        System.out.println("Default constructor ran.");
    }

    public Constructors(int value){
        System.out.println(value);
    }

    public static void main(String[] args) {
        //create instances here
        Constructors c = new Constructors(5839);

        //use the no-arg constructor
        Constructors cNoArg = new Constructors();
    }
}
We can observe the following output:



Exercises (Optional)
Create a class named A that contains two constructors,
One with no arguments that prints "No Arguments to A." to the console
One with one int argument that prints the argument received.
Create a class named B that extends class A above:
Also implement this class with two constructors as described above but prints "No Arguments to B" when the no-arguments constructor is called.
NOTE: 'Extending' a class is in reference to the object oriented design principle 'inheritance'. Review this information for more details.
Create a class named C that extends class B above, and has a main that instantiates objects of classes A, B, and C with one argument each.
NOTE: 'Extending' a class is in reference to the object oriented design principle 'inheritance'. Review this information for more details.
Summary

A constructor declares how an object is to be instantiated and initialized from the class "blueprint".
A constructor is declared like a method, except its method signature does not contain a return type, and a constructor always has the same name as the class.
The new object created by the constructor is always of the class in which the constructor is declared.
this refers to the object which is being instantiated - it is used to initialize instance variables or to call other constructors (this is called constructor chaining)
There is another keyword important for constructors - the super keyword, which references the "super", or parent, class.
When invoked as a method (super()), the parent class constructor will be called.
A super() call (or a this() call) must be the first line of any constructor.
If not explicitly provided, the compiler will inject super() it on the first line implicitly.
The "default" constructor takes no arguments and simply calls super()- sometimes it is referred to as the "default, no-args" constructor.
However, be warned that if we define our own constructor(s) in the class, we will not receive a default constructor from the compiler.


## Garbage Collection
Learning Objectives

After completing this module, associates should be able to:

Define and describe how garbage collection works.
Successfully implement garbage collection in a Java program.
Description
Garbage Collection
Consider the following Java code snippet:

Object o1 = new Object(); // 1
Object o2 = new Object(); // 2
Object o3 = o1;           // 3
o2 = o3;                  // 4
How many objects have we created on the heap? We determine this by counting the number of new keywords: in this case, 2. However, we have declared 3 reference variables that point to these objects in memory. The third line does not create a new object, it simply creates a new reference variable pointing to the object created on the first line. The last line reassigns the o2 variable to reference the object o3 is referencing, which is the first object created.

But what happened to the object created on the second line? After line 4, there are no reference variables pointing to it, thus it can never be used again in the program. When a condition like this occurs, the object becomes eligible for garbage collection which is the process of removing objects from the heap which have no references to them. In lower-level programming languages, memory is manipulated directly in the code, but Java abstracts these details away from the developer by allowing the JVM to handle memory management itself. We already know that the JVM creates objects in the heap when we invoke the new keyword. It also will clean up objects for us, freeing up memory for new objects to be created.

Garbage collection is run in the background by the JVM. There is no way we can explicitly force garbage collection to happen, but we can request garbage collection to be run through the use of one of the following:

System.gc()
Runtime.getRuntime().gc()
System.runFinalization()
Real World Application
Benefits of Java Garbage Collection
The biggest benefit of Java garbage collection is that it automatically handles the deletion of unused objects or objects that are out of reach to free up vital memory resources. Programmers working in languages without garbage collection (like C and C++) must implement manual memory management in their code.

Despite the extra work required, some programmers argue in favor of manual memory management over garbage collection, primarily for reasons of control and performance. While the debate over memory management approaches continues to rage on, garbage collection is now a standard component of many popular programming languages. For scenarios in which the garbage collector is negatively impacting performance, Java offers many options for tuning the garbage collector to improve its efficiency.

Implementation

Suppose you were told to write a program to count the number of employees working in a company(excluding interns). To make this program, you have to use the concept of a garbage collector.

This is the actual task you were given at the company:

Write a program to create a class called Employee having the following data members.

An ID for storing unique id allocated to every employee.
Name of employee.
age of an employee.
Also, provide the following methods:

A parameterized constructor to initialize name and age. The ID should be initialized in this constructor. A method show() to display ID, name, and age. A method showNextId() to display the ID of the next employee.

Without garbage collection, the program might be written like this:

// Java Program to count number
// of employees working
// in a company
 
class Employee {
   
    private int ID;
    private String name;
    private int age;
    private static int nextId = 1;
    // it is made static because it
    // is keep common among all and
    // shared by all objects
   
    public Employee(String name, int age)
    {
        this.name = name;
        this.age = age;
        this.ID = nextId++;
    }
    public void show()
    {
        System.out.println("Id=" + ID + "\nName=" + name
                           + "\nAge=" + age);
    }
    public void showNextId()
    {
        System.out.println("Next employee id will be="
                           + nextId);
    }
}
 
class UseEmployee {
    public static void main(String[] args)
    {
        Employee E = new Employee("GFG1", 56);
        Employee F = new Employee("GFG2", 45);
        Employee G = new Employee("GFG3", 25);
        E.show();
        F.show();
        G.show();
        E.showNextId();
        F.showNextId();
        G.showNextId();
 
        { // It is sub block to keep
            // all those interns.
            Employee X = new Employee("GFG4", 23);
            Employee Y = new Employee("GFG5", 21);
            X.show();
            Y.show();
            X.showNextId();
            Y.showNextId();
        }
        // After countering this brace, X and Y
        // will be removed.Therefore,
        // now it should show nextId as 4.
           
          // Output of this line
        E.showNextId();
        // should be 4 but it will give 6 as output.
    }
}
Output

Id=1
Name=GFG1
Age=56
Id=2
Name=GFG2
Age=45
Id=3
Name=GFG3
Age=25
Next employee id will be=4
Next employee id will be=4
Next employee id will be=4
Id=4
Name=GFG4
Age=23
Id=5
Name=GFG5
Age=21
Next employee id will be=6
Next employee id will be=6
Next employee id will be=6
Modification using garbage collection:
Now garbage collector(gc) will see 2 objects free. Now to decrement nextId,gc(garbage collector) will call method to finalize() only when we programmers have overridden it in our class. And as mentioned previously, we have to request gc(garbage collector), and for this, we have to write the following 3 steps before closing brace of sub-block.

Set references to null(i.e X = Y = null;)
Call, System.gc();
Call, System.runFinalization();
Now the correct code for counting the number of employees(excluding interns)

// Correct code to count number
// of employees excluding interns.
 
class Employee {
   
    private int ID;
    private String name;
    private int age;
    private static int nextId = 1;
   
    // it is made static because it
    // is keep common among all and
    // shared by all objects
    public Employee(String name, int age)
    {
        this.name = name;
        this.age = age;
        this.ID = nextId++;
    }
    public void show()
    {
        System.out.println("Id=" + ID + "\nName=" + name
                           + "\nAge=" + age);
    }
    public void showNextId()
    {
        System.out.println("Next employee id will be="
                           + nextId);
    }
    protected void finalize()
    {
        --nextId;
        // In this case,
        // gc will call finalize()
        // for 2 times for 2 objects.
    }
}
 
public class UseEmployee {
    public static void main(String[] args)
    {
        Employee E = new Employee("GFG1", 56);
        Employee F = new Employee("GFG2", 45);
        Employee G = new Employee("GFG3", 25);
        E.show();
        F.show();
        G.show();
        E.showNextId();
        F.showNextId();
        G.showNextId();
 
        {
            // It is sub block to keep
            // all those interns.
            Employee X = new Employee("GFG4", 23);
            Employee Y = new Employee("GFG5", 21);
            X.show();
            Y.show();
            X.showNextId();
            Y.showNextId();
            X = Y = null;
            System.gc();
            System.runFinalization();
        }
        E.showNextId();
    }
}
Output

Id=1
Name=GFG1
Age=56
Id=2
Name=GFG2
Age=45
Id=3
Name=GFG3
Age=25
Next employee id will be=4
Next employee id will be=4
Next employee id will be=4
Id=4
Name=GFG4
Age=23
Id=5
Name=GFG5
Age=21
Next employee id will be=6
Next employee id will be=6
Next employee id will be=4
Exercise (Optional)
Consider the code snippet below. Write a line of code that at the comment line "POINT A" will cause exactly one additional object to be eligible for the garbage collector.

public class GC {
 public static GC doStuff() {
 GC newGC = new GC();
 return newGC;
 }
 public static void main(String [] args) {
 GC gc1;
 GC gc2 = new GC();
 GC gc3 = new GC();
 GC gc4 = gc3;
 gc1 = doStuff();
 // POINT A
Summary

Garbage collection is the process of removing objects from the heap which have no references to them.
Java abstracts the details away from the developer by allowing the JVM to handle memory management itself.
Garbage collection is run in the background by the JVM. There is no way we can explicitly force garbage collection to happen, but we can request garbage collection to be run through the use of one of the following:
System.gc()
Runtime.getRuntime().gc()
System.runFinalize()


## Wrapper Classes
Learning Objectives

After completing this module, associates should be able to:

Define the terms "wrapper class", "boxing", "autoboxing" and "unboxing"
Successfully execute a Java program that demonstrates wrapper classes.
Description

Wrapper classes are classes that let you treat primitives as Objects. This is necessary - for example - for certain methods which only accept objects and not primitives. Boxing is the process of converting a primitive to its wrapper class. Java has a feature called autoboxing which will automatically convert primitives to wrapper classes implicitly. Unboxing is the reverse - converting a wrapper class to its primitive. Below the wrapper classes are listed:

Primitive	Wrapper Class
boolean	Boolean
byte	Byte
short	Short
char	Character
int	Integer
long	Long
float	Float
double	Double
Wrapper classes have static helper methods like .parseX() and .valueOf() for explicit primitive conversion.

public class AutoboxingExample {

  public static void main(String[] args) {
    int n = 5; // We start by declaring an int for a primitive type.
    someMethod(n); // autoboxing is done here to wrap the Integer class around int n.
	// 8
  }
  
  public static void someMethod(Integer i) {
    System.out.println(i + 3);
  }
}
Real World Application

Wrapper classes in Java are essential for several reasons:

Integration with Collections: Java collections (such as ArrayList, LinkedList, HashMap) can only store objects, not primitive types. Wrapper classes allow primitive values to be stored in collections by providing object representations of primitive types. For example, ArrayList can store integer values using the Integer wrapper class.

Nullability: Primitive types in Java cannot be assigned a null value, whereas objects can. Wrapper classes allow for nullability by providing a way to represent null values for primitive types. For example, if you need to represent an integer value that could be null, you can use the Integer wrapper class instead of int.

Additional Functionality: Wrapper classes offer additional functionality and utility methods that are not available for primitive types. For example, the Integer class provides methods for parsing strings into integers, converting integers to strings, performing arithmetic operations, and comparing values.

Compatibility with Generics: Generics in Java only work with objects, not primitive types. Wrapper classes enable the use of generics with primitive types by providing object representations. This allows for type-safe collections and algorithms that work with both primitive types and objects.

Overall, wrapper classes play a crucial role in Java programming by bridging the gap between primitive types and objects, enabling compatibility with collections, generics, APIs, and additional functionality that primitive types lack. They provide flexibility, nullability, and enhanced functionality, making them indispensable in many programming scenarios.

Implementation

The example program below illustrates the following concepts:

Autoboxing
Unboxing
Wrapper classes
Autoboxing
Autoboxing is the automatic conversion of primitive types to an object of their corresponding wrapper classes.

Example:

// Java program to demonstrate Autoboxing

import java.util.ArrayList;
class Autoboxing
{
	public static void main(String[] args)
	{
		char ch = 'a';

		// Autoboxing- primitive to Character object conversion
		Character a = ch;

		ArrayList<Integer> arrayList = new ArrayList<Integer>();

		// Here, autoboxing of the int 25 takes place because arrayList only stores Integer objects.
		arrayList.add(25);

		// printing the values from object
		System.out.println(arrayList.get(0));
	}
}

Unboxing
Unboxing can be considered the reverse of autoboxing. It converts an object of a wrapper class to its corresponding primitive data type.

Example:

// Java program to demonstrate Unboxing
import java.util.ArrayList;

class Unboxing
{
	public static void main(String[] args)
	{
		Character ch = 'a';

		// unboxing - Character object to primitive conversion
		char a = ch;

		ArrayList<Integer> arrayList = new ArrayList<Integer>();
		arrayList.add(24);

		// unboxing because get method returns an Integer object
		int num = arrayList.get(0);

		// printing the values from primitive data types
		System.out.println(num);
	}
}

Comprehensive example
Here is an example of autoboxing and unboxing in one Java program:

// Java program to demonstrate Wrapping and UnWrapping
// in Java Classes
import java.util.ArrayList;

class WrappingUnwrapping {
    public static void main(String args[]) {
        // byte data type
        byte a = 1;

        // Wrapping around Byte object using valueOf()
        Byte byteobj = Byte.valueOf(a);

        // int data type
        int b = 10;

        // Wrapping around Integer object using valueOf()
        Integer intobj = Integer.valueOf(b);

        // float data type
        float c = 18.6f;

        // Wrapping around Float object using valueOf()
        Float floatobj = Float.valueOf(c);

        // double data type
        double d = 250.5;

        // Wrapping around Double object using valueOf()
        Double doubleobj = Double.valueOf(d);

        // char data type
        char e = 'a';

        // Wrapping around Character object (autoboxing)
        Character charobj = e;

        // Printing the values from wrapper objects
        System.out.println("Values of Wrapper objects (printing as objects)");
        System.out.println("Byte object byteobj: " + byteobj);
        System.out.println("Integer object intobj: " + intobj);
        System.out.println("Float object floatobj: " + floatobj);
        System.out.println("Double object doubleobj: " + doubleobj);
        System.out.println("Character object charobj: " + charobj);

        // Unwrapping objects to primitive data types
        byte bv = byteobj;
        int iv = intobj;
        float fv = floatobj;
        double dv = doubleobj;
        char cv = charobj;

        // Printing the values from primitive data types
        System.out.println("Unwrapped values (printing as data types)");
        System.out.println("byte value, bv: " + bv);
        System.out.println("int value, iv: " + iv);
        System.out.println("float value, fv: " + fv);
        System.out.println("double value, dv: " + dv);
        System.out.println("char value, cv: " + cv);

        // More about using Wrapper classes
        Integer i3 = 5000; // Autoboxing
        Integer i4 = 5000; // Autoboxing

        // i3 and i4 are reference variables, pointing to different objects
        System.out.println("i3 == i4: " + (i3 == i4)); // False (different objects)
        
        // Comparing internal values using equals()
        System.out.println("i3.equals(i4): " + i3.equals(i4)); // True (same value)

        // Useful constants and methods in wrapper classes
        System.out.println("Integer.MIN_VALUE: " + Integer.MIN_VALUE);
        System.out.println("Integer.MAX_VALUE: " + Integer.MAX_VALUE);

        // Convert String to int using parseInt()
        int age = Integer.parseInt("75");
        System.out.println("Converted age: " + age);

        // Collections such as ArrayList can only store objects, not primitives
        ArrayList<Integer> myInts = new ArrayList<>();
        myInts.add(10);
        myInts.add(1000);
    }
}

Summary

Wrapper classes are classes that let you treat primitives as Objects.
Boxing is the process of converting a primitive to its wrapper class.
Java has a feature called autoboxing which will automatically convert primitives to wrapper classes implicitly.
Unboxing is the reverse - converting a wrapper class to its primitive.
Wrapper classes have static helper methods like .parseX() and .valueOf() for explicit primitive conversion.


## Exceptions vs Errors and Hierarchy
Learning Objectives

After completing this module, associates should be able to:

Describe the difference between exceptions and errors in Java.
Successfully execute a Java program that demonstrates the difference between an exception and an error.
Description
Errors, Exceptions, and Compilation Errors
In Java, both errors and exceptions are subclasses of the java.lang.Throwable class. An error in Java signifies a critical issue that typically arises from factors beyond the control of the application, leading to abnormal program behavior. These errors can manifest as compile-time issues, hindering successful compilation, or as run-time issues, impacting program execution. It is essential to address errors, encompassing both compile-time and run-time, before entering the compilation and execution phases. It is important to note you should never handle an error even though it is possible.

Alternatively, exceptions in Java denote unexpected or undesirable events that occur during program execution (run-time), causing a disruption in the regular flow of program instructions. Unlike errors, exceptions do not necessarily indicate critical issues but rather unexpected conditions that the program can handle. It is crucial to distinguish between errors, which are categorized as compile-time, run-time, or logical, and exceptions, which represent unforeseen events during runtime. Addressing both errors and exceptions appropriately contributes to the overall robustness and reliability of the Java application.

In the Java compilation process, the compiler checks that any checked exceptions that could be thrown are handled (via either try/catch block or the throws declaration on the method signature). If this is not the case, the compiler cannot compile the code. This is an example of a compilation error - not an exception. Exceptions are never thrown during the compilation process - they can only be thrown when the code is executing (running). Compilation errors generally occur due to improper syntax, like calling a method that doesn't exist, forgetting a semicolon on a line, using the wrong data type, using a reserved keyword incorrectly, and as we've shown, not handling checked exceptions properly.

The table below summarizes the difference between Errors, Exceptions, and Compilation Errors.

Name	Description	Occurs At	Can be caught?	Must be handled?	Example
Compilation Error	The compiler cannot compile the source code	Compile-time	N/A	N/A	SyntaxError
Error	Severe problem with the program	Runtime	Yes	No	OutOfMemoryError
Checked Exception	Any exception not derived from RuntimeException class	Compile-Time	Yes	Yes	IOException
Unchecked Exception	Any exception derived from RuntimeException class	Runtime	Yes	No	NullPointerException
Exception Class Hierarchy
Exceptions hierarchy

The exception class hierarchy starts with the Throwable class which inherits from Object. Any object which is a Throwable can be "thrown" in a program by the JVM or by the programmer using the throws keyword. The Exception and Error classes both extend Throwable. An Error represents something that went so horribly wrong with your application that you should not attempt to recover from. Some examples of errors are:

ExceptionInInitializerError
OutOfMemoryError
StackOverflowError
Exception is a general exception class which provides an abstraction for all exceptions. There are many subclasses of Exception, as shown above.

Real World Application

Exceptions provide a way to separate the normal flow of program execution from error-handling code, making the code more readable, maintainable, and robust.
Using exception handling allows developers to gracefully handle unexpected situations by providing meaningful error messages, and, in some cases, take corrective actions to ensure the continued operation of the software.
Example of compile-time error:

public class CompileErrorExample {
    public static void main(String[] args) {
        // Creating a final variable. 
        // final means the variable cannot be changed to another value later.
        final int x = 10;
        x = 20; // attempting to reassign the variable x which is final will cause a compile-time error
    }
}
The compiler will catch this before the code is ever executed and provide a error letting you know that this will not compile.

Example of run-time error:

// MainClass.java
import org.apache.commons.math3.util.ArithmeticUtils;

public class MainClass {
    public static void main(String[] args) {
        // Attempt to use Apache Commons Math to compute the factorial of 5
        long factorial = ArithmeticUtils.factorial(5);
        System.out.println("Factorial of 5: " + factorial);
    }
}
The compiler will not catch this until run time when the program is executing.

When we imported the class using import org.apache.commons.math3.util.ArithmeticUtils;, it informs the compiler where to find the class during the compilation process. However, during runtime, the availability of the class depends on the classpath.

The NoClassDefFoundError occurs when the Java Virtual Machine (JVM) attempts to load the class at runtime, but the class definition is not found. This could be due to a number of reasons.

Now let's explore Exceptions, which represent conditions that a well-designed application might want to handle.

Exceptions are runtime conditions that may occur and could potentially lead to the termination of the program. However, they are recoverable using the try, catch, and throw keywords.

Exceptions in Java are broadly categorized into two types:

Checked exceptions:

Checked exceptions, such as IOException, are known to the compiler at compile time. Developers are required to either handle these exceptions using a try-catch block or declare that their method throws these exceptions.
Unchecked exceptions:

Unchecked exceptions, such as ArrayIndexOutOfBoundsException, are known to the compiler at runtime. Unlike checked exceptions, the compiler does not enforce explicit handling of unchecked exceptions, providing more flexibility but requiring developers to be vigilant about potential issues.
Example of a common Exception:

public class Student {
    private String name;

    // Constructor
    public Student(String name) {
        this.name = name;
    }

    // Method to get the length of the student's name
    public int getNameLength() {
        return name.length(); // Potential NullPointerException if name is null
    }

    public static void main(String[] args) {
        // Creating a student with a null name
        Student student = new Student(null);

        try {
            // Simulating a situation where the student's name is null
            int nameLength = student.getNameLength(); // This line will throw NullPointerException
            System.out.println("Name length: " + nameLength); // This line won't be reached
        } catch (NullPointerException e) {
            // Handling the NullPointerException
            System.err.println("An error occurred: " + e.getMessage());
        }
    }
}
This example reflects a situation where a real-world class method could encounter a NullPointerException if not handled appropriately.

Implementation
Exception Example
import java.util.Scanner;  
public class ExceptionExample  
{  
    public static void main(String args[])   
    {  
        Scanner sc = new Scanner(System.in);  
        System.out.print("Enter a number: ");  
        int number = sc.nextInt();  
        System.out.println("You have entered: "+ number);  
    }  
}  
This program will yield two different result depending on the value supplied:

First, when we enter an integer such as 3. The program will run normally with an output of 3.
If we enter a decimal number (float or double) such as 3.14159, the program will throw an exception.
It is important to note, during the compilation process, the Java compiler verifies that any checked exceptions, which could be thrown, are appropriately handled. If a checked exception is not handled, the compiler cannot proceed with the compilation of code, resulting in a compilation error. Again, keep in mind that an Error is distinct from an Exception.

import java.io.IOException;

public class ThrowCheckedException {

    // A method that throws a checked exception
    public static void readFile(String file) throws IOException {
        // Code to read a file (simulated)
        // For the sake of the example, let's assume this method can throw IOException
        throw new IOException("File not found: " + file);
    }

    public static void main(String[] args) {
     // Specifying the file path to be read
        String filePath = "path_to_file";
        // Calling the method without proper handling or declaration
        readFile(filePath); // Compilation error here if not handled or declared

        // The previous code will not compile unless you handle the exception or declare it.
        // Comment out the readFile() and we will handle the exception below
        // Uncommenting the next line would handle the compilation error.
//         try {
//             readFile(filePath); //note we are referencing the method here in the try block as this is the method that will throw the exception.
//         } catch (IOException e) {
//             // Handle the exception
//             System.out.println("IOException: " + e.getMessage());
//         }
    }
}
Exceptions are not thrown during the compilation process; they can only occur when the code is executing (running). Compilation errors typically result from syntax errors, such as calling a nonexistent method, forgetting a semicolon, using an incorrect data type, or misusing a reserved keyword. Additionally, compilation errors may arise when checked exceptions are not handled properly, or from a myriad of other issues.

Error Example
In the example below, we create an infinite loop, which continues to add more and more data to the stack. Since the amount of memory available is not unlimited, we will eventually run out, leading to an OutOfMemoryError..

import java.util.ArrayList;
import java.util.List;

public class OutOfMemoryErrorExample {
    public static void main(String[] args) {
        List<int[]> memoryHog = new ArrayList<>();
        
        try {
            while (true) {
                // Allocating memory continuously without freeing it
                memoryHog.add(new int[1_000_000]); // 1 million integers per iteration
            }
        } catch (OutOfMemoryError e) {
            System.err.println("OutOfMemoryError occurred: " + e.getMessage());
        }
    }
}
When run, this program will result in a OutOfMemoryError, which is not an exception.

Summary

It is crucial not to confuse exceptions and errors in Java.
Compilation errors occur during the compilation process when the compiler encounters issues in the code that prevent it from generating bytecode. These errors are related to syntax issues, missing semicolons, undeclared variables, etc.
Exceptions are runtime issues that occur when the code is executed. Checked exceptions, however, are enforced by the compiler during the compilation process, ensuring that they are either handled or declared in the method signature.
There is almost never a reason to handle an Error using try-catch blocks, though it is possible since Errors are subclasses of Throwable.
Errors are serious or critical problems in our program.
Exceptions are expected to be caught and handled using a try-catch block.
Exceptions are less serious problems; they are ways the compiler describes not understanding what action is supposed to take place.
Exception Hierarchy
The exception class hierarchy starts with the Throwable class, which inherits from Object. Any object that is a Throwable can be "thrown" in a program either by the JVM or by the programmer using the throws keyword.
Both the Exception and Error classes extend Throwable. Examples of errors include ExceptionInInitializerError, OutOfMemoryError, and StackOverflowError.
Exception is a general exception class that provides an abstraction for all exceptions. It has numerous subclasses, including those mentioned above.

## Handling Exceptions
Learning Objectives

After completing this module, associates should be able to:

Describe handling exceptions
Successfully execute a Java program that demonstrates handling exceptions.
Description
Exceptions
When an exceptional condition occurs in the course of a Java program, a special class called an Exception can be thrown, which indicates that something went wrong during the execution of the program. If the exception is not handled anywhere in the program, it will propagate up through the call stack until it is handled by the JVM which then terminates the program.

Exceptions Handling / Declaring Exceptions
When risky code is written that has the possibility of throwing an exception, it can be dealt with in one of two ways:

Handling means that the risky code is placed inside a try/catch block
Declaring means that the type of exception to be thrown is listed in the method signature with the throws keyword. This is also called "ducking" the exception - you let the code which calls the method deal with it.
Real World Application

The following Java class accounts for the possibility of division by zero by throwing an exception if the user-inputted denominator is zero.

package com.revature.main;

import java.util.Scanner;

import com.revature.exception.DenominatorCannotBeZeroException;

public class Driver {

	private static Scanner sc = new Scanner(System.in);
	
	public static void main(String[] args) {
		
		// Idea: create an application that takes in
		// 1. a number for the numerator
		// 2. a number for the denominator
		
		System.out.println("Welcome to the calculator app for dividing numbers");
		
		System.out.println("Please enter a numerator: ");
		double numerator = Double.parseDouble(sc.nextLine());
		
		System.out.println("Please enter a denominator: ");
		double denominator = Double.parseDouble(sc.nextLine());
		
		try {
			System.out.println("The result is: " + divide(numerator, denominator));
		} catch (DenominatorCannotBeZeroException e) {
			System.out.println(e.getMessage());
		} finally { // finally block will always run whether an exception occurs or not
			System.out.println("This will always run regardless of what happens");
		}
		
		System.out.println("=== PROGRAM ENDED SUCCESSFULLY ===");
		
	}
	
	public static double divide(double x, double y) throws DenominatorCannotBeZeroException {
		
		if (y == 0) {
			throw new DenominatorCannotBeZeroException("You cannot have a denominator of 0"); // Throw the checked exception
			// DenominatorCannotBeZeroException
		}
		
		double result = x / y;
		
		return result;
	}

}
Implementation


Below are examples of handling exceptions using try, catch, and finally blocks.

package com.revature.exceptions.intro;

import java.io.FileNotFoundException;
import java.io.IOException;

public class BasicExceptionExample {

	public static void main(String[] args) {
		try { // try/catch blocks allow you to try out risky code and handle any exceptions that are thrown
			throwManyExceptions(3);
		} catch(FileNotFoundException e) {
			System.out.println("FileNotFoundException caught");
		} catch (IOException e) {
			System.out.println("IOException caught");
		} catch (Exception e) { // if you have multiple catch blocks, broader exception classes must come after more specific ones
			System.out.println("Other exception caught");
		} finally {
			System.out.println("The finally block will always run! (unless System.exit(0) is called or power is lost)");
		}
	}
	
	public static void throwManyExceptions(int i)
			throws Exception { // throws declaration in the method signature means you are "ducking" the exception
		switch(i) {
		case 1: throw new IOException();
		case 2: throw new ClassNotFoundException();
		case 3: throw new FileNotFoundException();
		default: throw new Exception();
		}
	}

}

Something about exceptions to also keep in mind is you can just have a try/finally without a catch. The example below will demonstrate an example of this. In this example, we are attempting to catch exceptions associated with file handling and throw those exceptions when appropriate.

public static void main(String[] args) {
    BufferedReader reader = null;
        try {
            reader = new BufferedReader(new FileReader("demo.txt"));
            String line = reader.readLine();
            System.out.println("Read from file: " + line);
        } finally {
            // Cleanup operations in finally block
            if (reader != null) {
                try {
                    reader.close();
                    System.out.println("Reader closed successfully.");
                } catch (IOException e) {
                    System.err.println("Error closing reader: " + e.getMessage());
                }
            }
        }
    }
}
This approach is suitable when you want to perform cleanup operations but don't need to handle the specific exception within the same block.

Exercises (Optional)
Write a Java program that has an example of handling an exception and an example of "ducking" an exception.

Summary

When an exceptional condition occurs in the course of a Java program, a special class called an Exception can be thrown, which indicates that something went wrong during the execution of the program.
If the exception is not handled anywhere in the program, it will propagate up through the call stack until it is handled by the JVM which then terminates the program.
When risky code is written that has the possibility of throwing an exception, it can be dealt with in one of two ways:
Handling means that the risky code is placed inside a try/catch block
Declaring means that the type of exception to be thrown is listed in the method signature with the throws keyword. This is also called "ducking" the exception - you let the code which calls the method deal with it.


## Checked vs Unchecked Exceptions
Learning Objectives

After completing this module, associates should be able to:

Describe the difference between checked and unchecked exceptions
Successfully execute a Java program that demonstrates checked and unchecked exceptions.
Description

Exceptions that require mandatory handling are called checked exceptions. The compiler will check that such exceptions are handled by the program.

Suppose that some statement in the body of a subroutine can generate a checked exception, one that requires mandatory handling. The statement could be a throw statement, which throws the exception directly, or it could be a call to a subroutine that can throw the exception. In either case, the exception must be handled.

This can be done in one of two ways:

The first way is to place the statement in a try statement that has a catch clause that handles the exception; in this case, the exception is handled within the subroutine, so that no caller of the subroutine can ever see the exception.
The second way is to declare that the subroutine can throw the exception.
This is done by adding a "throws" clause to the subroutine heading, which alerts any callers to the possibility that the exception might be generated when the subroutine is executed.
The caller will, in turn, be forced either to handle the exception in a try statement or to declare the exception in a throws clause in its own header.
Exception-handling is mandatory for any exception class that is not a subclass of either Error or RuntimeException. These checked exceptions generally represent conditions that are outside the control of the programmer. For example, they might represent bad input or an illegal action taken by the user. There is no way to avoid such errors, so a robust program has to be prepared to handle them.

Real World Application

Checked Exceptions are great, so long as you understand when they should be used.

Checked Exceptions should be used for predictable, but unpreventable errors that are reasonable to recover from.

Unchecked Exceptions should be used for everything else.

Predictable but unpreventable: The caller did everything within their power to validate the input parameters, but some condition outside their control has caused the operation to fail. For example, you try reading a file but someone deletes it between the time you check if it exists and the time the read operation begins. By declaring a checked exception, you are telling the caller to anticipate this failure.
Reasonable to recover from: There is no point telling callers to anticipate exceptions that they cannot recover from. If a user attempts to read from an non-existing file, the caller can prompt them for a new filename. On the other hand, if the method fails due to a programming bug (invalid method arguments or buggy method implementation) there is nothing the application can do to fix the problem in mid-execution. The best it can do is log the problem and wait for the developer to fix it at a later time.
Unless the exception you are throwing meets all of the above conditions it should use an Unchecked Exception.
Reevaluate at every level: Sometimes the method catching the checked exception isn't the right place to handle the error. In that case, consider what is reasonable for your own callers. If the exception is predictable, unpreventable and reasonable for them to recover from then you should throw a checked exception yourself. If not, you should wrap the exception in an unchecked exception. If you follow this rule you will find yourself converting checked exceptions to unchecked exceptions and vice versa depending on what layer you are in.
For both checked and unchecked exceptions, use the right abstraction level. For example, a code repository with two different implementations (database and filesystem) should avoid exposing implementation-specific details by throwing SQLException or IOException. Instead, it should wrap the exception in an abstraction that spans all implementations (e.g. RepositoryException).
Implementation
Programming with Exceptions
Exceptions can be used to help write robust programs. They provide an organized and structured approach to robustness. Without exceptions, a program can become cluttered with if statements that test for various possible error conditions. With exceptions, it becomes possible to write a clean implementation of an algorithm that will handle all the normal cases. The exceptional cases can be handled elsewhere, in a catch clause of a try statement.

When a program encounters an exceptional condition and has no way of handling it immediately, the program can throw an exception. In some cases, it makes sense to throw an exception belonging to one of Java's predefined classes, such as IllegalArgumentException or IOException. However, if there is no standard class that adequately represents the exceptional condition, the programmer can define a new exception class. The new class must extend the standard class Throwable or one of its subclasses. In general, if the programmer does not want to require mandatory exception handling, the new class will extend RuntimeException (or one of its subclasses). To create a new checked exception class, which does require mandatory handling, the programmer can extend one of the other subclasses of Exception or can extend Exception itself.

Here, for example, is a class that extends Exception, and therefore requires mandatory exception handling when it is used:

public class ParseError extends Exception {
   public ParseError(String message) {
         // Create a ParseError object containing
         // the given message as its error message.
      super(message);
   }
}
The class contains only a constructor that makes it possible to create a ParseError object containing a given error message. The statement "super(message)" calls a constructor in the superclass, Exception. The class inherits the getMessage() and printStackTrace() routines from its superclass. If e refers to an object of type ParseError, then the function call e.getMessage() will retrieve the error message that was specified in the constructor. But the main point of the ParseError class is simply to exist. When an object of type ParseError is thrown, it indicates that a certain type of error has occurred. (Parsing, by the way, refers to figuring out the syntax of a string. A ParseError would indicate, presumably, that some string that is being processed by the program does not have the expected form.)

A throw statement can be used in a program to throw an error of type ParseError. The constructor for the ParseError object must specify an error message. For example:

throw new ParseError("Encountered an illegal negative number.");
or

throw new ParseError("The word '" + word + "' is not a valid file name.");
Since ParseError is defined as a subclass of Exception, it is a checked exception. If the throw statement does not occur in a try statement that catches the error, then the subroutine that contains the throw must declare that it can throw a ParseError by adding the clause "throws ParseError" to the subroutine heading. For example,

void getUserData() throws ParseError {
   . . .
}
This would not be required if ParseError were defined as a subclass of RuntimeException instead of Exception, since in that case ParseErrors would not be checked exceptions.

A routine that wants to handle ParseErrors can use a try statement with a catch clause that catches ParseErrors. For example:

try {
   getUserData();
   processUserData();
}
catch (ParseError pe) {
   . . .  // Handle the error
}
Note that since ParseError is a subclass of Exception, a catch clause of the form "catch (Exception e)" would also catch ParseErrors, along with any other object of type Exception.

Sometimes, it's useful to store extra data in an exception object. For example

class ShipDestroyed extends RuntimeException {
   Ship ship;  // Which ship was destroyed.
   int where_x, where_y;  // Location where ship was destroyed.
   ShipDestroyed(String message, Ship s, int x, int y) {
         // Constructor creates a ShipDestroyed object
         // carrying an error message plus the information
         // that the ship s was destroyed at location (x,y)
         // on the screen. 
       super(message);
       ship = s;
       where_x = x;
       where_y = y;
   }
}
Here, a ShipDestroyed object contains an error message and some information about a ship that was destroyed. This could be used, for example, in a statement:

if ( userShip.isHit() )
   throw new ShipDestroyed("You've been hit!", userShip, xPos, yPos);
Note that the condition represented by a ShipDestroyed object might not even be considered an error. It could be just an expected interruption to the normal flow of a game. Exceptions can sometimes be used to handle such interruptions neatly.

General Purpose Exception Handling
The ability to throw exceptions is particularly useful in writing general-purpose methods and classes that are meant to be used in more than one program. In this case, the person writing the method or class often has no reasonable way of handling the error, since that person has no way of knowing exactly how the method or class will be used. In such circumstances, a novice programmer is often tempted to print an error message and forge ahead, but this is almost never satisfactory since it can lead to unpredictable results down the line. Printing an error message and terminating the program is almost as bad, since it gives the program no chance to handle the error.

The program that calls the method or uses the class needs to know that the error has occurred. In languages that do not support exceptions, the only alternative is to return some special value or to set the value of some global variable to indicate that an error has occurred. It is very easy to be lazy about checking for special return values every time a subroutine is called. Exceptions are a cleaner way for a subroutine to react when it encounters an error.

It is easy to modify a function to use exceptions instead of a special return value to signal an error. This modified subroutine throws a ParseError when the user's input is illegal, where ParseError is the subclass of Exception that was defined above. (Arguably, it might be reasonable to avoid defining a new class by using the standard exception class IllegalArgumentException instead.)

/**
 * Reads the user's input measurement from one line of input.
 * Precondition:   The input line is not empty.
 * Postcondition:  If the user's input is legal, the measurement
 *                 is converted to inches and returned.
 * @throws ParseError if the user's input is not legal.
 */
static double readMeasurement() throws ParseError {

   double inches;  // Total number of inches in user's measurement.
   
   double measurement;  // One measurement, 
                        //   such as the 12 in "12 miles."
   String units;        // The units specified for the measurement,
                        //   such as "miles."
   
   char ch;  // Used to peek at next character in the user's input.

   inches = 0;  // No inches have yet been read.

   skipBlanks();
   ch = TextIO.peek();
   
   /* As long as there is more input on the line, read a measurement and
      add the equivalent number of inches to the variable, inches.  If an
      error is detected during the loop, end the subroutine immediately
      by throwing a ParseError. */

   while (ch != '\n') {
   
       /* Get the next measurement and the units.  Before reading
          anything, make sure that a legal value is there to read. */
   
       if ( ! Character.isDigit(ch) ) {
           throw new ParseError("Expected to find a number, but found " + ch);
       }
       measurement = TextIO.getDouble();
       
       skipBlanks();
       if (TextIO.peek() == '\n') {
          throw new ParseError("Missing unit of measure at end of line.");
       }
       units = TextIO.getWord();
       units = units.toLowerCase();
       
       /* Convert the measurement to inches and add it to the total. */
       
       if (units.equals("inch") 
               || units.equals("inches") || units.equals("in")) {
           inches += measurement;
       }
       else if (units.equals("foot") 
                  || units.equals("feet") || units.equals("ft")) {
           inches += measurement * 12;
       }
       else if (units.equals("yard") 
                  || units.equals("yards") || units.equals("yd")) {
           inches += measurement * 36;
       }
       else if (units.equals("mile") 
                  || units.equals("miles") || units.equals("mi")) {
           inches += measurement * 12 * 5280;
       }
       else {
           throw new ParseError("\"" + units 
                                + "\" is not a legal unit of measure.");
       }
     
       /* Look ahead to see whether the next thing on the line is 
          the end-of-line. */
      
       skipBlanks();
       ch = TextIO.peek();
       
   }  // end while
   
   return inches;
   
} // end readMeasurement()
In the main program, this subroutine is called in a try statement of the form

try {
   inches = readMeasurement();
}
catch (ParseError e) {
   . . .  // Handle the error.
}
Summary

For those exception classes that require mandatory handling, the situation is different.
If a subroutine can throw such an exception, that fact must be announced in a 'throws' clause in the routine definition.
Failing to do so is a syntax error that will be reported by the compiler.
Exceptions that require mandatory handling are called checked exceptions. The compiler will check that such exceptions are handled by the program.
Exception handling can be done in one of two ways:
The first way is to place the statement in a try statement that has a catch clause that handles the exception.
The second way is to declare that the subroutine can throw the exception. This is done by adding a "throws" clause to the subroutine heading, which alerts any callers to the possibility that the exception might be generated when the subroutine is executed. The caller will, in turn, be forced either to handle the exception in a try statement or to declare the exception in a throws clause in its own header.
Exception-handling is mandatory for any exception class that is not a subclass of either Error or RuntimeException.
Among the checked exceptions are several that can occur when using Java's input/output routines. This means that you can't even use these routines unless you understand something about exception-handling.


## Creating Custom Exceptions
Learning Objectives

After completing this module, associates should be able to:

Describe how custom exceptions work
Successfully execute a Java program that demonstrates custom exceptions
Description
Custom Exceptions
A programmer can create custom exceptions in Java by extending any exception class. If you extend RuntimeException, however, you will be creating an unchecked exception. This is a good idea if you do not want other code to have to handle your exception being thrown. If you do always want to require your exception to be handled, then create a checked exception by extending any existing one, or the Exception class itself.

public class MyCheckedException extends Exception {}
public class MyUncheckedException extends RuntimeException {}

public class ExceptionThrower {

  public static void main(String[] args) {
    try {
	  throw new MyCheckedException("uh oh");
	} catch(MyCheckedException e) {} // we're just ignoring it here
	
    if ( 100 > 1) {
	  throw new MyUncheckedException("you're not required to handle me!");
	}
  }
  
  public static void declareChecked() throws MyCheckedException {
    throw new MyCheckedException("this one is declared!");
  }
}
Real World Application

Custom exceptions in Java are important for several reasons:

Semantic Clarity: Custom exceptions allow developers to create exception types that are semantically meaningful and descriptive of the specific error conditions encountered in their applications. This enhances code readability and maintainability by providing clear and precise error messages, helping other developers understand the cause of the exception and how to handle it appropriately.

Granular Error Handling: By defining custom exceptions for different error scenarios, developers can implement more granular error handling strategies tailored to specific types of errors. This enables finer-grained control over exception propagation, allowing different parts of the code to handle exceptions differently based on their type or severity.

Enhanced Debugging: Custom exceptions can include additional metadata or diagnostic information, such as stack traces, error codes, or contextual data, to aid in debugging and troubleshooting. This helps developers identify the root cause of exceptions more quickly and effectively, leading to faster resolution of issues and improved system reliability.

Application-Specific Error Handling: Custom exceptions enable developers to define application-specific error-handling logic tailored to the requirements and constraints of their applications.

Overall, custom exceptions empower developers to create more robust, maintainable, and user-friendly Java applications. They are an essential tool in the developer's toolkit for building reliable and resilient software systems.

Implementation

This example will build a custom exception, a class that throws the exception, and a driver that will demonstrate handling exceptions and using the class that throws them.

We have a custom package called "com.revature.bicycle" and created three files, one for each of the classes below. We can compile, debug, and execute the code.

package com.revature.exceptions.bicycle;

public class Bicycle {

	public int speed = 0;
	public int gear = 1;

	public static String bikeShop = "RevaBikes";
	public static final int MAX_SPEED = 25;
	public static final int MAX_GEAR = 3;

	public void speedUp(int increment) {
		if (this.speed + increment > MAX_SPEED) {
			speed = 25;
			gear = 3;
			System.out.println("Cannot exceed maximum speed of 25.");
			return;
		} else {
			this.speed += increment;
			gear = gear < MAX_GEAR ? gear + 1 : 3;
		}
	}

	public void slowDown(int decrement) throws NegativeSpeedException {
		if (this.speed - decrement < 0) {
			throw new NegativeSpeedException();
		} else {
			this.speed -= decrement;
			gear = gear > 1 ? gear - 1 : 1;
		}
	}

}

package com.revature.exceptions.bicycle;

import java.io.IOException;

public class BicycleDriver {
	
	public static void main(String[] args) {
		
		System.out.println("This is the start of our program...");
		System.out.println("Stufffff......");
		
		try {
			Thread.sleep(1000);
//			throw new InterruptedException();	// we can throw exceptions ourselves
//			throw new OutOfMemoryError();		// we can also throw errors (but why?)
//			System.exit(0);	 // this will halt the app, nothing past this line will be executed
		} catch (InterruptedException ie) {
			ie.printStackTrace();
		} finally {
			// will almost always be executed
			System.out.println("This is in the finally block!"); 
		}
		
		System.out.println("This is after the try/catch/finally blocks.");
		
		// Order of catch blocks matters! Catch exceptions starting with the most specific!
//		try {
//			throwSomething();
//		} catch (Exception e) {
//			e.printStackTrace();
//		} catch (IOException e) {
//			e.printStackTrace();
//		}
		
		Bicycle myBike = new Bicycle();
		System.out.println("gear: " + myBike.gear + ", speed: " + myBike.speed);
		
		myBike.speedUp(24);
		System.out.println("gear: " + myBike.gear + ", speed: " + myBike.speed);
		
		myBike.speedUp(2);
		System.out.println("gear: " + myBike.gear + ", speed: " + myBike.speed);
		
		try {
			myBike.slowDown(26);
		} catch (NegativeSpeedException e) {
			myBike.speed = 0;
			myBike.gear = 1;
			e.printStackTrace();
		}
		System.out.println("gear: " + myBike.gear + ", speed:" + myBike.speed);
		
	}
	
	// Include a throws clause on a method's signature to force any calling method to handle it.
	// Also known as throwing or propagating.
	public static void throwSomething() throws IOException {
		System.out.println("This method might throw an exception");
	}

}


package com.revature.exceptions.bicycle;

public class NegativeSpeedException extends Exception {

	private static final long serialVersionUID = 1L;
	
	public NegativeSpeedException() {
		super("Cannot go a negative speed!");
	}

}

Exercises (Optional)
Adding to the above example, we can uncomment the code in the BicycleDriver class to see what occurs during runtime. A snippet of the commented code is below, be sure to use the full code above including all three classes.

public class BicycleDriver {
	
	public static void main(String[] args) {
		
		System.out.println("This is the start of our program...");
		System.out.println("Stufffff......");
		
		try {
			Thread.sleep(1000);
//			throw new InterruptedException();	// we can throw exceptions ourselves
//			throw new OutOfMemoryError();		// we can also throw errors (but why?)
//			System.exit(0);	 // this will halt the app, nothing past this line will be executed
		} catch (InterruptedException ie) {
			ie.printStackTrace();
		} finally {
			// will almost always be executed
			System.out.println("This is in the finally block!"); 
		}
		
		System.out.println("This is after the try/catch/finally blocks.");
		
		// Order of catch blocks matters! Catch exceptions starting with the most specific!
//		try {
//			throwSomething();
//		} catch (Exception e) {
//			e.printStackTrace();
//		} catch (IOException e) {
//			e.printStackTrace();
//		}
.
.
.
.
.
Summary

A programmer can create custom exceptions in Java by extending any exception class.
If you extend RuntimeException, however, you will be creating an unchecked exception.
This is a good idea if you do not want other code to have to handle your exception being thrown.
If you do always want to require your exception to be handled, then create a checked exception by extending any existing one, or the Exception class itself.

## OOP Inheritance
Learning Objectives

After completing this module, associates should be able to:

Describe the concept of inheritance in OOP.
Description
What is inheritance?
Inheritance is all about inheriting the common state and behavior of a parent class (super class) by its derived class (sub class or child class). A sub class can inherit all non-private members from the super class, by default.

Inheritance can be one of four types – depending on the class' hierarchy. Let’s learn about all four types of inheritances.

Single inheritance: There is one parent class and one child class. One child class extends one parent class.
Multi-level inheritance: In multi-level inheritance, there will be inheritance between more than three classes in such a way that a child class will act as parent class for another child class.
Hierarchical inheritance: In hierarchical inheritance, there is one super class and more than one sub class extends the super class.
Multiple inheritance: In multiple inheritance, a class can inherit the behavior from more than one parent classes as well.
Note: Not all programming languages support Multiple Inheritance. For instance, Java does not support multiple inheritance. This means that any given class can only have one direct parent or super class.
Real World Application

Inheritance is a fundamental concept in object-oriented programming (OOP) with several important benefits:

Code Reusability: Inheritance enables code reuse by allowing a subclass (derived class) to inherit properties and behaviors from a superclass (base class). This means that common attributes and methods can be defined in a base class and reused across multiple subclasses, reducing duplication and promoting modular, maintainable code.

Polymorphism: Inheritance enables polymorphism, which is the ability of objects to take on different forms or behaviors depending on their context. Subclasses can override or extend methods inherited from their superclasses to provide specialized behavior, while still adhering to the same interface defined by the superclass. This allows for more flexible and extensible designs, where objects can be treated uniformly based on their common superclass type, regardless of their specific subclass implementation.

Extensibility: Inheritance allows classes to be extended or specialized to meet specific requirements or use cases. Subclasses can add new attributes, methods, or behaviors to the ones inherited from their superclasses, enhancing the functionality of the base class without modifying its implementation. This promotes modular design and facilitates incremental development, where new features can be added incrementally without disrupting existing code.

Abstraction and Modularity: Inheritance promotes abstraction by allowing developers to define a common interface or behavior in a superclass, while leaving the implementation details to the subclasses. This separation of concerns between interface and implementation facilitates modularity and encapsulation, making it easier to manage and maintain complex systems by isolating changes to specific parts of the codebase.

Code Organization and Readability: Inheritance improves code organization and readability by providing a natural way to represent relationships between classes and their common characteristics. By leveraging inheritance, developers can create intuitive class hierarchies that accurately model real-world entities and concepts, making the codebase easier to understand and navigate.

Overall, inheritance is a powerful mechanism in OOP that promotes code reuse, polymorphism, extensibility, abstraction, modularity, and code readability. By leveraging inheritance effectively, developers can build more flexible, maintainable, and scalable software systems that meet the evolving requirements of complex applications.

Implementation

The following examples showcase Inheritance in Java

Take this sample code for a possible implementation of a Bicycle class:

public class Bicycle {
        
    // the Bicycle class has three fields
    public int cadence;
    public int gear;
    public int speed;
        
    // the Bicycle class has one constructor
    public Bicycle(int startCadence, int startSpeed, int startGear) {
        gear = startGear;
        cadence = startCadence;
        speed = startSpeed;
    }
        
    // the Bicycle class has four methods
    public void setCadence(int newValue) {
        cadence = newValue;
    }
        
    public void setGear(int newValue) {
        gear = newValue;
    }
        
    public void applyBrake(int decrement) {
        speed -= decrement;
    }
        
    public void speedUp(int increment) {
        speed += increment;
    }
        
}
A class declaration for a MountainBike class that is a subclass of Bicycle might look like this:

public class MountainBike extends Bicycle {
        
    // the MountainBike subclass adds one field
    public int seatHeight;

    // the MountainBike subclass has one constructor
    public MountainBike(int startHeight,
                        int startCadence,
                        int startSpeed,
                        int startGear) {
        super(startCadence, startSpeed, startGear);
        seatHeight = startHeight;
    }   
        
    // the MountainBike subclass adds one method
    public void setHeight(int newValue) {
        seatHeight = newValue;
    }   
}
MountainBike inherits all the fields and methods of Bicycle and adds the field seatHeight and a method to set it. Except for the constructor, it is as if you had written a new MountainBike class entirely from scratch, with four fields and five methods. However, you didn't have to do all the work. This would be especially valuable if the methods in the Bicycle class were complex and had taken substantial time to debug.

Exercise (Optional)
Create a subclass of Bicycle called KidsBicycle that:

Adds a new int field called "numberOfTrainingWheels"
Adds two new methods:
"addTrainingWheels" sets the numberOfTrainingWheels to the number 2.
"removeTrainingWheels" sets the numberOfTrainingWheels to the number 0.
Summary

Inheritance is all about inheriting the common state and behavior of a parent class (super class) by its derived class (sub class or child class).
A sub class can inherit all non-private members from super class, by default.
Inheritance can be one of four types:.
Single inheritance: There is one Parent class and one Child class. One child class extends one parent class
Multi-level inheritance: In multilevel inheritance, there will be inheritance between more than three classes in such a way that a child class will act as parent class for another child class.
Hierarchical inheritance: In hierarchical inheritance, there is one super class and more than one sub classes extend the super class.
Multiple inheritance: In multiple inheritance, a class can inherit the behavior from more than one parent classes as well (not supported by Java).

## OOP Polymorphism
Learning Objectives

After completing this module, associates should be able to:

Describe the concept of polymorphism in OOP.
Successfully execute a Java program that demonstrates polymorphism.
Description
Polymorphism
By definition, polymorphism means "taking on many forms". In the realm of programming, it describes how objects can behave differently in different contexts. The most common examples of polymorphism are method overloading and overriding.

Method Overloading
Method overloading is when there exist two or more methods in a class with the same method name, but different method signatures by changing the parameter list.

We can change the number of parameters, the types of the parameters, or the order in which the parameters are defined. Which version of the method is executed is determined by the arguments passed when the method is invoked. Note that varying the return type of the method alone is not permitted in some OOP languages.

Because the argument list is known at compilation, the compiler knows which version of the method will be executed. Therefore, method overloading is a type of compile-time - or static - polymorphism.

Method Overriding
Method overriding is when a method in a child class has the same method signature as a method in the parent class, but with a different implementation. Thus, child classes can change the default behavior given by a parent's method. Overriding methods makes class hierarchies more flexible and dynamic.

If we have a child object that overrides a parent class method, the child classes implementation of a method will run. This is referred to as virtual method invocation and is key to method overriding. The parent class (if it is abstract) does not even need to define any implementation for an overridden method, since the method to be executed will be determined at runtime depending on the object referred to in memory. This is the reason why method overriding is classified as runtime - or dynamic - polymorphism.

One more item to note with method overriding is that static methods cannot be overridden. Instead, if a subclass implements the same static method as its parent, the method is hidden. Method hiding replaces the parent method in the calls defined in the child class.

Covariant return types
When overriding a method, we also have the option of changing the return type - provided that the overridden return type is a subtype of the original type. This is called covariant return types. We can also choose to change the access modifier of an overridden method - provided that the new modifier for the overriding method provides more, not less, access than the overridden method.

Real World Application

In object-oriented programming, Polymorphism provides the means to perform a single action in multiple different ways. Taking the real world example of animals, if we ask different animals to speak, they respond in their own way. Dog barks, duck quacks, cat says meow and so on. So the same action of speaking is performed in different ways by different animals.

As another example, consider your Mobile phone. You can save your Contacts in it. Now suppose you want to save 2 numbers for one person. You can do it by saving the second number under the same name.

Similarly, in an object-oriented language like Java, suppose you want to save two numbers for one person. You must have a function, which will take the two numbers and the person name as arguments to some function.

Now it’s not necessary that every person will have 2 numbers. Many other contacts might have only a single number. In such a situation, instead of creating another method with different name to save one number for a contact, what you can do is that you can have the same name of the method i.e. createContact() but instead of taking 2 numbers as parameters, you can take only 1 number as parameter in it. This is an example of polymorphism. There is only one method (CreateContact) but it has two definitions.

Implementation
Polymorphism
There are two types of polymorphism, Static and Dynamic.

Static Polymorphism:
Polymorphism that is resolved during compile time is known as static polymorphism.
Method overloading can be considered as static polymorphism example.
Method Overloading: This allows us to have more than one methods with same name in a class that differs in signature.
Example:
  class DisplayOverloading
  {
    public void disp(char c)
    {
      System.out.println(c);
    }
    public void disp(char c, int num)  
    {
      System.out.println(c + " "+num);
    }
  }
  public class ExampleOverloading
  {
    public static void main(String args[])
    {
      DisplayOverloading obj = new DisplayOverloading();
      obj.disp('a');
      obj.disp('a',10);
    }
  }
Dynamic Polymorphism
It is also known as Dynamic Method Dispatch.
Dynamic polymorphism is a process in which a call to an overridden method is resolved at runtime. It is otherwise known as runtime polymorphism
Example:
class Animal{
  public void animalSound(){
    System.out.println("Default Sound");
  }
}
public class Dog extends Animal{

  public void animalSound(){
      System.out.println("Woof");
  }

  public static void main(String args[]){
      Animal obj = new Dog();
      obj.animalSound();
  }
}
Exercise (Optional)
Overload the DisplayOverloading class's disp method so that it can also accept two ints (int1 and int2) and prints out (int1 " + " int2).

Summary

By definition, polymorphism means "taking on many forms". In the realm of programming, it describes how objects can behave differently in different contexts. The most common examples of polymorphism are method overloading and overriding.
Method overloading is when there exist two or more methods in a class with the same method name, but different method signatures by changing the parameter list.
Method overriding is when a method in a child class has the same method signature as a method in the parent class, but with a different implementation. Thus, child classes can change the default behavior given by a parent's method. Overriding methods makes class hierarchies more flexible and dynamic.


## Object Class
Learning Objectives

After completing this module, associates should be able to:

Define the Object class
Describe the methods of the Object class
Create a program that will override the equals method to determine equality among objects based on the data contained within.
Description
Object class
Object is a special class in Java which is the root class from which all other classes inherit, either directly or indirectly.

Any class that doesn't have an extends clause implicitly inherits Object. If a subclass has an extends clause that specifies a superclass other than Object, the class still inherits Object.

Consider this example:

public class Manager extends SalariedEmployee...
public class SalariedEmployee extends Employee...
public class Employee extends Person....
public class Person...
The Manager class inherits the Object class indirectly because it inherits SalariedEmployee, which inherits Employee, which inherits Person, which inherits Object.

Creating a class that doesn't inherit Object is not possible.

Also, note that primitives (such as int variables) are not objects; therefore they do not inherit the Object class.

Since all objects inherit from the Object class, they have at least the methods defined in the Object class:

Object clone() - Returns a copy of this object.
boolean equals(Object o) - Indicates whether this object is equal to the o object.
void finalize() - Called by the garbage collector when the object is destroyed.
Class<?> getClass() Returns a Class object that represents this object's runtime class
int hashCode() - Returns this object's hash code.
void notify() - Is used with threaded applications to wake up a thread that's waiting on this object.
void notifyAll() - Is used with threaded applications to wake up all threads that are waiting on this object.
String toString() - Returns a String representation of this object.
void wait() - Causes this object's thread to wait until another thread calls notify or notifyAll.
void wait(long timeout) - Is a variation of the basic wait method.
void wait(long timeout, int nanos) - Another variation of the wait method.
Real World Application
Object class methods
The toString() method is automatically called if you print an Object. Usually, this is overridden to provide human-readable output. Otherwise, you will print out fully.qualified.ClassName@memoryAddress

The equals(Object o) method compares two Objects. The == operator also compares objects, but only the memory address (i.e. will return true if and only if the variables refer to the exact same object in memory). By default, and unless you explicitly override it, the equals method simply calls the == operator.

The hashCode() method returns a hash code - a number that puts instances of a class into a finite number of categories. There are a few rules that the method follows:

You are expected to override hashCode() if you override equals()
The result of hashCode() should not change in a program
if .equals() returns true, the hash codes should be equal
if .equals() returns false, the hash codes do not have to be distinct. However, doing so will help the performance of hash tables.
Finally, the .finalize() method is called by the garbage collector when it determines there are no more references to the object. It can be overriden to perform cleanup activities before garbage collection, although it has been deprecated in newer versions of Java.

Implementation
Object class
Today we will introduce two methods that closely belong together: equals() and hashCode(). We'll focus on their relationship with each other, how to correctly override them, and why we should override both or neither.

equals()
The Object class defines both the equals() and hashCode() methods, which means that these two methods are implicitly defined in every Java class, including the ones we create:

class Money {
    int amount;
    String currencyCode;
}
Money income = new Money(55, "USD");
Money expenses = new Money(55, "USD");
boolean balanced = income.equals(expenses)
We would expect income.equals(expenses) to return true, but with the Money class in its current form, it won't.

The default implementation of equals() in the Object class says that equality is the same as object identity, and income and expenses are two distinct instances.

Overriding equals() Let's override the equals() method so that it doesn't consider only object identity, but also the value of the two relevant properties:
@Override
public boolean equals(Object o) {
    if (o == this)
        return true;
    if (!(o instanceof Money))
        return false;
    Money other = (Money)o;
    boolean currencyCodeEquals = (this.currencyCode == null && other.currencyCode == null)
      || (this.currencyCode != null && this.currencyCode.equals(other.currencyCode));
    return this.amount == other.amount && currencyCodeEquals;
}
equals() conditions Java defines the conditions that our implementation of the equals() method must fulfill. Most of the criteria are common sense. The equals() method must be:

reflexive: an object must equal itself
symmetric: x.equals(y) must return the same result as y.equals(x)
transitive: if x.equals(y) and y.equals(z), then also x.equals(z)
consistent: the value of equals() should change only if a property that is contained in equals() changes (no randomness allowed)
Violating equals() Symmetry With Inheritance

If the criteria for equals() is such common sense, then how can we violate it at all? Well, violations happen most often if we extend a class that has overridden equals(). Let's consider a Voucher class that extends our Money class:
class WrongVoucher extends Money {

  private String store;

  @Override
  public boolean equals(Object o) {
      if (o == this)
          return true;
      if (!(o instanceof WrongVoucher))
          return false;
      WrongVoucher other = (WrongVoucher)o;
      boolean currencyCodeEquals = (this.currencyCode == null && other.currencyCode == null)
        || (this.currencyCode != null && this.currencyCode.equals(other.currencyCode));
      boolean storeEquals = (this.store == null && other.store == null)
        || (this.store != null && this.store.equals(other.store));
      return this.amount == other.amount && currencyCodeEquals && storeEquals;
      }

  // other methods
  }
At first glance, the Voucher class and its override for equals() seem to be correct. And both equals() methods behave correctly as long as we compare Money to Money or Voucher to Voucher. But what happens, if we compare these two objects:
Money cash = new Money(42, "USD");
WrongVoucher voucher = new WrongVoucher(42, "USD", "Amazon");

voucher.equals(cash) => false // As expected.
cash.equals(voucher) => true // That's wrong.
This violates the symmetry criteria of the equals() contract.

Fixing equals() Symmetry With Composition

To avoid this pitfall, we should favor composition over inheritance.
Instead of subclassing Money, let's create a Voucher class with a Money property:

  class Voucher {

      private Money value;
      private String store;

      Voucher(int amount, String currencyCode, String store) {
          this.value = new Money(amount, currencyCode);
          this.store = store;
      }

      @Override
      public boolean equals(Object o) {
          if (o == this)
              return true;
          if (!(o instanceof Voucher))
              return false;
          Voucher other = (Voucher) o;
          boolean valueEquals = (this.value == null && other.value == null)
          || (this.value != null && this.value.equals(other.value));
          boolean storeEquals = (this.store == null && other.store == null)
        || (this.store != null && this.store.equals(other.store));
      return valueEquals && storeEquals;
      }

      // other methods
  }
Now equals will work symmetrically as required.

hashCode()
hashCode() returns an integer representing the current instance of the class. We should calculate this value consistent with the definition of equality for the class. Thus, if we override the equals() method, we also have to override hashCode().

hashCode() conditions

Java also defines a set of conditions for the hashCode() method. A thorough look at it shows how closely related hashCode() and equals() are.
All three criteria for hashCode() mention the equals() method in some way:
internal consistency: the value of hashCode() may only change if a property that is in equals() changes
equals consistency: objects that are equal to each other must return the same hashCode
collisions: unequal objects may have the same hashCode
Violating the Consistency of hashCode() and equals()

The second criteria of the hashCode conditions has an important consequence: If we override equals(), we must also override hashCode(). This is by far the most widespread violation regarding the equals() and hashCode() methods contracts.
Let's see such an example:

class Team {

    String city;
    String department;

    @Override
    public final boolean equals(Object o) {
      // implementation
    }
}

The Team class overrides only equals(), but it still implicitly uses the default implementation of hashCode() as defined in the Object class. And this returns a different hashCode() for every instance of the class. This violates the second rule.

Now, if we create two Team objects, both with city “New York” and department “marketing,” they will be equal, but they'll return different hashCodes.

HashMap Key With an Inconsistent hashCode() But why is the violation in our Team class a problem? Well, the trouble starts when some hash-based collections are involved. Let's try to use our Team class as a key of a HashMap:

Map<Team,String> leaders = new HashMap<>();
leaders.put(new Team("New York", "development"), "Anne");
leaders.put(new Team("Boston", "development"), "Brian");
leaders.put(new Team("Boston", "marketing"), "Charlie");

Team myTeam = new Team("New York", "development");
String myTeamLeader = leaders.get(myTeam);
We would expect myTeamLeader to return “Anne,” but with the current code, it doesn't.

If we want to use instances of the Team class as HashMap keys, we have to override the hashCode() method so that it adheres to the contract; equal objects return the same hashCode.

Let's see an example implementation:

@Override
public final int hashCode() {
    int result = 17;
    if (city != null) {
        result = 31 * result + city.hashCode();
    }
    if (department != null) {
        result = 31 * result + department.hashCode();
    }
    return result;
}
After this change, leaders.get(myTeam) returns “Anne” as expected.

When Do We Override equals() and hashCode()? Generally, we want to override either both of them or neither of them. We just saw in Section 3 the undesired consequences if we ignore this rule.

Domain-Driven Design can help us decide circumstances when we should leave them be. For entity classes, for objects having an intrinsic identity, the default implementation often makes sense.

However, for value objects, we usually prefer equality based on their properties. Thus, we want to override equals() and hashCode().

Summary

Object is a special class in Java which is the root class from which all other classes inherit, either directly or indirectly.
Any class that doesn't have an extends clause implicitly inherits Object. If a subclass has an extends clause that specifies a superclass other than Object, the class still inherits Object.
Creating a class that doesn't inherit Object is not possible.
Also, note that primitives (such as int variables) are not objects; therefore they do not inherit the Object class.
Since all objects inherit from the Object class, they have at least the methods defined in the Object class:
Object clone() - Returns a copy of this object.
boolean equals(Object o) - Indicates whether this object is equal to the o object.
void finalize() - Called by the garbage collector when the object is destroyed.
Class<?> getClass() Returns a Class object that represents this object's runtime class
int hashCode() - Returns this object's hash code.
void notify() - Is used with threaded applications to wake up a thread that's waiting on this object.
void notifyAll() - Is used with threaded applications to wake up all threads that are waiting on this object.
String toString() - Returns a String representation of this object.
void wait() - Causes this object's thread to wait until another thread calls notify or notifyAll.
void wait(long timeout) - Is a variation of the basic wait method.
void wait(long timeout, int nanos) - Another variation of the wait method.

## OOP Encapsulation
Learning Objectives

After completing this module, associates should be able to:

Describe the concept of encapsulation in OOP.
Successfully execute a Java program that demonstrates encapsulation.
Description
Encapsulation
Encapsulation is the OOP principle of containing related state and behavior together inside a class, and also hiding and preventing change to an object's data members. An object encapsulates or controls the access to its internal state. This prevents arbitrary external interference, which could bring the object into an invalid or inconsistent state.

When encapsulating your code, certain conventions should be followed:

All instance fields for a class should be private
Each field should have getters and setters as needed (typically these are public, but other access modifiers can be used as needed).
Getter and Setter Methods (a.k.a. 'accessor' and 'mutator' methods) should use the following name convention:
getVariableName
setVariableName
Real World Application

Encapsulation is a fundamental concept in object-oriented programming (OOP) with several important benefits:

Encapsulation promotes code maintainability: By encapsulating data and behavior within a class, encapsulation reduces the likelihood of unintended side effects when making changes to the implementation. Modifications to the internal implementation can be made without affecting other parts of the codebase, as long as the public interface remains unchanged. This simplifies code maintenance, debugging, and refactoring, leading to more robust and maintainable software systems.

Encapsulation improves code readability and understandability: By encapsulating related data and behavior within a class, encapsulation enhances code readability and understandability. Classes serve as self-contained units of functionality, making it easier for developers to comprehend the purpose and behavior of individual components. Encapsulation also promotes code documentation and self-documenting code practices, further improving code readability and maintainability.

Overall, encapsulation is essential in OOP for promoting maintainability and readability. By encapsulating data and behavior within classes and providing controlled access through a public interface, encapsulation enables the creation of robust, modular, and maintainable software systems.

Implementation
Encapsulation
Encapsulation simply means binding object state(fields) and behavior(methods) together. If you are creating class, you are doing encapsulation.
The following example showcases the application of Encapsulation using Java.
The class EncapsulationExample that is using the Object of class EmployeeCount will not able to get the NoOfEmployees directly.
It has to use the setter and getter methods of the same class to set and get the value.
Example:
class EmployeeCount
{
   private int numOfEmployees = 0;
   public void setNumOfEmployees (int count)
   {
       numOfEmployees = count;
   }
   public double getNumOfEmployees () 
   {
       return numOfEmployees;
   }
}
public class EncapsulationExample
{
  public static void main(String args[])
  {
      EmployeeCount obj = new EmployeeCount ();
      obj.setNumOfEmployees(5613);
      System.out.println("No Of Employees: "+(int)obj.getNumOfEmployees());

      // System.out.println(numOfEmployees); // compilation error - cannot access numOfEmployees
  }
}
Notice the last println statement, which tries to access the field numOfEmployees. If uncommented, this would result in a compilation error.

Exercise (Optional)
Add a public method called Payroll that:

Takes as input an int called AverageSalary
Returns an int value TotalPayroll calculated as numOfEmployees multiplied by AverageSalary.
Summary
Summary for the OOP: Encapsulation topic

Encapsulation is the OOP principle of containing related state and behavior together inside a class, and also hiding and preventing change to an object's data members.
Encapsulation is the mechanism that binds together code and the data it manipulates and keeps both safe from outside interference and misuse.


## Access Modifiers
Learning Objectives

After completing this module, associates should be able to:

Define the access modifiers in Java
Successfully execute a Java program that demonstrates access modifiers.
Description

Access modifiers are simply keywords in Java that provide accessibility of a class and its member. They set the access level to methods, variable, classes and constructors.

There are 4 types of access modifiers available in Java. In order of most accessible to least accessible they are:

public: The member with public modifiers can be accessed by any classes. The public methods, variables or class have the widest scope.
protected: The protected modifier is used within same package. It lies between public and default access modifier. It can be accessed outside the package but through inheritance only.
default: When we do not mention any access modifier, it is treated as default. It is accessible only within same package.
private: The private methods, variables and constructor are not accessible to any other class. It is the most restrictive access modifier. Classes cannot be private unless the class is a nested class.
Real World Application

A real-world example of using access modifiers can be found in a banking application. Consider a scenario where you have different types of bank accounts, such as savings accounts and checking accounts, each with specific rules and restrictions regarding access to account information and operations.

In this scenario, access modifiers can be used to control the visibility and accessibility of various class members (fields and methods) within the bank account classes. Here's how access modifiers can be applied in this context:

Private Access Modifier:
The balance field in the bank account class could be marked as private. This ensures that the balance of an account can only be accessed and modified internally within the class itself. Other classes or external code cannot directly access or modify the balance field, preventing unauthorized changes to account balances.

Protected Access Modifier:
The interestRate field in a savings account class could be marked as protected. This allows subclasses (e.g., different types of savings accounts) to access and modify the interest rate field, while restricting access to other classes outside of the inheritance hierarchy. Subclasses can override methods or define additional behavior based on the protected members inherited from the superclass.

Public Access Modifier:
Getter and setter methods for accessing and modifying account balances (getBalance() and setBalance()) could be marked as public. This allows external classes or modules, such as the banking system's user interface or transaction processing module, to retrieve and update account balances as needed. Public methods provide a well-defined interface for interacting with the account class, while hiding the internal implementation details. Similarly, methods for depositing (deposit()) and withdrawing (withdraw()) funds from an account could be marked as public. This allows clients of the account class to perform transactions securely and efficiently, while encapsulating the transaction logic within the account class itself.

By using access modifiers appropriately, the banking application ensures that sensitive data and critical operations are protected from unauthorized access or modification, while providing a well-defined interface for interacting with account objects. This promotes security, encapsulation, and maintainability in the application's design, aligning with best practices in object-oriented programming.

Implementation
private
The private access modifier is accessible only within the class.

In this example, we have created two classes A and Simple. The A class contains a private data member and a private method. We are accessing these private members from outside the class, so there is a compile-time error.

class A{  
private int data=40;  
private void msg(){System.out.println("Hello java");}  
}  
  
public class Simple{  
 public static void main(String args[]){  
   A obj=new A();  
   System.out.println(obj.data);//Compile Time Error  
   obj.msg();//Compile Time Error  
   }  
}  
Role of Private Constructor
If you make any class constructor private, you cannot create the instance of that class from outside the class. For example:

class A{  
private A(){}//private constructor  
void msg(){System.out.println("Hello java");}  
}  
public class Simple{  
 public static void main(String args[]){  
   A obj=new A();//Compile Time Error  
 }  
}  
default
If you don't use any modifier, it is treated as having default access. Default classes or class members are accessible only within package. They cannot be accessed from outside the package. It provides more accessibility than private. But, it is more restrictive than protected, and public.

In this example, we have created two packages pack and mypack. We are accessing the A class from outside its package, since the A class is not public, so it cannot be accessed from outside the package.

//save by A.java  
package pack;  
class A{  
  void msg(){System.out.println("Hello");}  
}  
//save by B.java  
package mypack;  
import pack.*;  
class B{  
  public static void main(String args[]){  
   A obj = new A();//Compile Time Error  
   obj.msg();//Compile Time Error  
  }  
}  
In the above example, the scope of class A and its method msg() is default so it cannot be accessed from outside the package.

protected
The protected access modifier is accessible within package and outside the package but through inheritance only.

The protected access modifier can be applied on the data member, method and constructor. It can't be applied on the class.

It provides more accessibility than the default modifer.

In this example, we have created the two packages pack and mypack. The A class of pack package is public, so can be accessed from outside the package. But msg method of this package is declared as protected, so it can be accessed from outside the class only through inheritance.

//save by A.java  
package pack;  
public class A{  
protected void msg(){System.out.println("Hello");}  
}  
//save by B.java  
package mypack;  
import pack.*;  
  
class B extends A{  
  public static void main(String args[]){  
   B obj = new B();  
   obj.msg();  
  }  
}  
public
The public access modifier is accessible everywhere. It has the widest scope among all other modifiers.

Example of public access modifier

//save by A.java  
  
package pack;  
public class A{  
public void msg(){System.out.println("Hello");}  
}  
//save by B.java  
  
package mypack;  
import pack.*;  
  
class B{  
  public static void main(String args[]){  
   A obj = new A();  
   obj.msg();  
  }  
}  
Summary

There are 4 types of access modifiers available in Java.

public: The member with public modifiers can be accessed by any classes. The public methods, variables or class have the widest scope.
default: When we do not mention any access modifier, it is treated as default. It is accessible only within same package.
protected: The protected modifier is used within same package. It lies between public and default access modifier. It can be accessed outside the package but through inheritance only.
private: The private methods, variables and constructor are not accessible to any other class. It is the most restrictive access modifier. A class except a nested class cannot be private.


## OOP Abstraction
Learning Objectives

After completing this module, associates should be able to:

Describe the concept of abstraction in OOP.
Successfully execute a Java program that demonstrates abstraction.
Description
Abstraction
In object-oriented programming (OOP), abstraction is a key principle that allows developers to model real-world entities as classes and objects, focusing on the essential properties and behaviors while hiding the implementation details that are not relevant to the abstraction's purpose. Abstraction in OOP involves defining a clear and well-defined interface for interacting with objects.

Think of a car - you do not need to know how the car works, just how to use the interfaces you are given, like the accelerator, brake, and steering wheel. Thus, a car "abstracts" away the internal details of the engine, motor, driveshaft, and other parts.

Another example is, if an Animal class were part of a library for creating animals in Java, the user of the library wouldn't need to know exactly how each animal speaks, because the speak method is defined on the Animal class. We can also use the generic Animal type for reference variables without worrying about which specific animal the object is.

Let's assume that the Dog and Cat class extend the Animal class. When creating an instance of a Dog, we will declare the corresponding reference variable to be of type Animal, not Dog. The advantage of writing code this way is that later if we decide we instead need to create a Cat here, we can easily change the constructor being invoked. If we code with the abstract superclass in mind, which is Animal, then we know that we can have the animal use the .speak() method regardless of whether the object is a Dog or Cat because that behavior is guaranteed to exist for all animals.

Real World Application

Abstraction
Abstraction means hiding lower-level details and exposing only the essential and relevant details to the users.
Example 1: Let's consider a Car, which abstracts the internal details and exposes to the driver only those details that are relevant to the interaction of the driver with the Car. For example, does a drive need to know the fuel injection rate? No, all a driver needs to know is when they step on the accelerator, the car travels faster.
Example 2: Consider an ATM (Automatic Teller Machine); We perform operations on the ATM like cash withdrawal, money transfer, retrieve mini-statement…etc. but we don't know the internal details about ATM. For instance, when an ATM user selects a withdrawal amount, the user does not need to know how the machinery dispenses the right amount of bills. All the user needs to know is when the user requests $40, the ATM will dispense two $20 bills in return.
Implementation
Abstraction
The following showcases the use of Abstraction in a Java application:

// Java program to illustrate the
// concept of Abstraction
abstract class Shape {
	String color;

	// these are abstract methods
	abstract double area();
	public abstract String toString();

	// abstract class can have the constructor
	public Shape(String color)
	{
		System.out.println("Shape constructor called");
		this.color = color;
	}

	// this is a concrete method
	public String getColor() { return color; }
}
Here, we created a Shape class which different types of shapes will be created. Notice that this class is abstract and declares some methods abstract as well.

When we inherit from the Shape class, we must implement the inherited abstract methods:

class Circle extends Shape {
	double radius;

	public Circle(String color, double radius)
	{

		// calling Shape constructor
		super(color);
		System.out.println("Circle constructor called");
		this.radius = radius;
	}

	@Override double area()
	{
		return Math.PI * Math.pow(radius, 2);
	}

	@Override public String toString()
	{
		return "Circle color is " + super.getColor()
			+ "and area is : " + area();
	}
}
class Rectangle extends Shape {

	double length;
	double width;

	public Rectangle(String color, double length,
					double width)
	{
		// calling Shape constructor
		super(color);
		System.out.println("Rectangle constructor called");
		this.length = length;
		this.width = width;
	}

	@Override double area() { return length * width; }

	@Override public String toString()
	{
		return "Rectangle color is " + super.getColor()
			+ "and area is : " + area();
	}
}
Now we can test our code:

public class Test {
	public static void main(String[] args)
	{
		Shape s1 = new Circle("Red", 2.2);
		Shape s2 = new Rectangle("Yellow", 2, 4);

		System.out.println(s1.toString());
		System.out.println(s2.toString());
	}
}
Exercise (Optional)
Extend the Shape class with a Triangle class:

The constructor should accept color, Base, and Height as input
Override area() with the calculation 0.5 * Base * Height
Override toString with an output showing the Triangle color and area.
Summary

Abstraction is a key principle that allows developers to model real-world entities as classes and objects, focusing on the essential properties and behaviors while hiding the implementation details that are not relevant to the abstraction's purpose.
Abstraction in OOP involves defining a clear and well-defined interface for interacting with objects.
Through abstraction, we hide underlying complexity through a simplified interface.


## Abstract Classes and Methods
Learning Objectives

After completing this module, associates should be able to:

Define abstract classes.
Create a program that will define an abstract class, then implements the abstract class to create other classes.
Description
Abstract Classes
An abstract class is a class that is declared abstract — it may or may not include abstract methods. Abstract classes cannot be instantiated, but they can be subclassed.
Abstract classes have constructors but still cannot be instantiated. Even if you can't create an object of an abstract class, it still may have some state that it passes down to subclasses that need to be initialized. When a subclass constructor calls super(), it calls the abstract class's constructor and the state can be initialized through the abstract class constructor.
An abstract class can have 0 or more abstract methods, but if a class has at least one abstract method then the whole class has to be abstract.
An abstract class can have implemented methods as well.
Use the extends keyword to extend an abstract class.
public abstract class GraphicObject {
   // declare fields
   // declare nonabstract methods
   abstract void draw();
}

class Circle extends GraphicObject {
    void draw() {
        ...
    }
    void resize() {
        ...
    }
}

class Rectangle extends GraphicObject {
    void draw() {
        ...
    }
    void resize() {
        ...
    }
}
Real World Application

A real-world use case for abstract classes in Java can be found in the development of a game engine or simulation framework.

Consider a scenario where you are developing a game engine that supports different types of game objects, such as characters, enemies, and obstacles, each with common characteristics and behaviors shared among them. Abstract classes can be used to define a common interface and partial implementation for these game objects, while allowing specific objects to extend the abstract class and provide their own implementations for certain methods.

Abstract GameObject Class: You can define an abstract GameObject class that serves as the base class for all game objects. This abstract class can define common properties and behaviors shared by all game objects, such as position, velocity, collision detection, and rendering.

Concrete GameObject Classes: Concrete subclasses, such as Character, Enemy, and Obstacle, can extend the GameObject class and provide their own implementations for specific methods, such as updating the object's state and rendering it in the game world.

Usage in Game Engine: In the game engine, developers can use the abstract GameObject class to interact with game objects in a unified manner, without needing to know the specific subclass implementations. This promotes code reuse, modularity, and flexibility in designing and implementing game logic, rendering, and collision detection. java

In this real-world use case, abstract classes provide a way to define a common interface and partial implementation for related classes, enabling code reuse, polymorphism, and modularity in the design and implementation of a game engine or simulation framework in Java.

Implementation
Example of Using an Abstract Class
The keyword abstract can be applied to a class or a method.

When applied to a method, it specifies to the compiler that the marked method should not have any implementation. The line should end in a semi-colon as opposed to an open curly brace.

public abstract void methodName(...);
When you declare a method abstract, then its containing class must be declared abstract as well. You may, however, declare a class abstract, but not have any abstract methods.

If a class is declared abstract, then it cannot be instantiated. 

Defining abstract methods
As previously stated, abstract methods do not require a method body, i.e. statements enclosed in a set of curly braces after the parameter list. Notice the two abstract methods getName and setName in the following example:

package com.example;
public class Person{

    protected String name;

    public abstract String getName();

    public abstract void setName(String name);
}
However, there is an error in this code, hovering over the methods we see the following:



The error messages all pertain to the fact that we didn't declare Person as abstract. Recall that if a class has at least one abstract method, then the class must be declared abstract.

Defining an abstract class
We can update the class to declare it as abstract, which will resolve this error:

package com.example; 

public abstract class Person{

    protected String name;

    public abstract String getName();

    public abstract void setName(String name);
}
Now, in another package, com.example.test, we'll create another class, TestPerson. In this class, we create a main method and try to instantiate a Person object:

package com.example.test;

import com.example.Person;

public class TestPerson {

    public static void main(String[] args) {
        Person adam = new Person();
    }
}
However, notice the error:



You cannot instantiate an abstract class.

So why use an abstract class?

The major reason is to provide a template for other classes to start from. There is a term called Inheritance which Java supports and this corresponds with allowing classes to inherit or copy properties and behavior from others. In this case, you can define an abstract class as the one to copy from and force developers to write their own version of the class. 

Extending an abstract class
First, we create another class in the com.example package called Developer. The Developer class will use the extends keyword in inherit from Person. Like so:

package com.example;

public class Developer extends Person {

}
This creates an inheritance relationship between Developer and Person. Developer IS-A Person.

Consequently, the methods and variables declared in Person are passed down, or rather implicitly copied into, Developer.

But what about our abstract methods?

Even the abstract methods are passed down, which is why you should see a compiler error on the Developer class.

image of error in IDE-"the type Developer must implement..."

Recall that unless a class is abstract, it cannot contain abstract methods. This includes classes within an inheritance hierarchy.

To solve the error we must override the abstract methods in order to implement them.

This will give concrete instructions for the methods that, as of yet, do not have a method body:

package com.example;

public class Developer extends Person {

    public String getName() {
        return this.name;
    }

    public void setName(String name) {
        this.name = name; 
    }
}
We implemented our abstract methods by defining methods of the same names, same parameters and the same return types as the inherited methods.

Also notice that we can refer to name without re-defining it because Developer inherits the name property from Person.

Wonderful! Let's test our code.

We return to TestPerson and edit it to resemble the following:

package com.example.test;

import com.example.Developer;
import com.example.Person;

public class TestPerson {
	public static void main(String[] args) {
	    Person adam = new Developer();
	    
	    adam.setName("Adam");
	    System.out.println(adam.getName());
	    
	 }
}
Notice how we edited the instantiation of a Person object to instead call the Developer class constructor. This is how we can indirectly instantiate our abstract classes. This is also an example of Polymorphism, another Object Oriented Programming principle-though this is not the focus of this particular example. Additionally, notice that we have included another import statement for the Developer class. Finally, when we run our code we see that the methods operate as we defined them to in our Developer class.

Final result of TestPerson"

Exercises: (Optional)
Successfully create a class called Cat that extends the Animal class below.
public abstract class Animal {
	
	public int numberOfLives = 1;
	
	public Animal() {
		System.out.println("Animal constructor called!");
	}
	
	/*
	 * Abstract methods are methods that have no implementation (body). They only consist of a method 
	 * signature - also known as a method stub. 
	 * 
	 * Any concrete subclass that extends this class will be required to implement all of its abstract 
	 * methods.
	 */
	public abstract void makeSound();
	
	/*
	 * As was mentioned, abstract classes can have concrete methods (methods with implementation). However,
	 * subclasses can still override these concrete methods with their own implementation if desired.
	 */
	public void exist() {
		System.out.println("The animal is existing...");
	}

	@Override
	public int hashCode() {
		final int prime = 31;
		int result = 1;
		result = prime * result + numberOfLives;
		return result;
	}

	@Override
	public boolean equals(Object obj) {
		if (this == obj)
			return true;
		if (obj == null)
			return false;
		if (getClass() != obj.getClass())
			return false;
		Animal other = (Animal) obj;
		if (numberOfLives != other.numberOfLives)
			return false;
		return true;
	}

	@Override
	public String toString() {
		return "Animal [numberOfLives=" + numberOfLives + "]";
	}

Summary

An abstract class is a class that is declared abstract —it may or may not include abstract methods. Abstract classes cannot be instantiated, but they can be subclassed.
An abstract class can have 0 or more abstract methods, but if a class has at least one abstract method then the whole class has to be abstract.
An abstract class can have implemented methods as well.
Use the extends keyword to extend an abstract class.


## Interfaces
Learning Objectives

After completing this module, associates should be able to:

Define the term interface
Successfully execute a Java program that demonstrates interfaces.
Description
Interfaces
An interface is similar to an abstract class, but there's a key difference: A class can only inherit one other class, but a class can implement as many interfaces as it needs.

A class implements an interface using the implements keyword in the class definition and by providing implementations for any abstract methods defined by the interface.

Interfaces have these advantages over inheritance:

Implementation details do not need to be provided in the interface.
A class can only extend one other class, but it can implement as many interfaces as needed.
An interface acts as a contract for behaviors that a class can implement.

public interface InterfaceA {
 public void methodName(); //You don't implement the method!
}
Interfaces have implicit modifiers on methods and variables.

Methods are 'public' and 'abstract'
Variables are 'public', 'static', and 'final' To inherit interfaces, a class must implement them and they are REQUIRED to implement all methods, unless the class is abstract.
Real World Application

A real-world use case for interfaces in Java can be found in the development of a payment processing system that integrates with multiple payment gateways, such as credit card processors, digital wallets, and bank transfers.

Consider a scenario where you are designing the payment processing system to handle various payment methods, each with its own implementation details and APIs. Interfaces can be used to define a common contract for processing payments, allowing the application to accept payments using different payment gateways without tightly coupling the implementation to specific payment providers.

Here's how interfaces can be used in this scenario:

PaymentProcessor Interface: You can define a PaymentProcessor interface that serves as the common contract for processing payments via different payment gateways. This interface can declare methods for authorizing payments, capturing funds, and handling payment status.

Concrete PaymentProcessor Implementations: Concrete classes implementing the PaymentProcessor interface can provide specific implementations for processing payments via different payment gateways, such as credit card processors, digital wallets, and bank transfer APIs.

Usage in Payment Processing System: In the payment processing system, developers can use the PaymentProcessor interface to process payments via different payment gateways without needing to know the specific implementation details of each payment provider. This promotes flexibility, modularity, and extensibility in the application design.

In this real-world use case, interfaces provide a way to define a common contract for interacting with different payment gateways, enabling code reuse, polymorphism, and modularity in the design and implementation of the payment processing system in Java. Additionally, interfaces allow for easy integration of new payment gateways in the future without modifying existing code.

Implementation

An interface defines a new construct similar to a class. Likewise, they create a form of inheritance where an interface can be labeled as the parent class and an implementation can be a child class.

Interfaces, however, are limited in the variables and types of methods they can define, and as a result this form of inheritance relationship differs in important ways from classes.

By defining an interface you are creating a type of contract.

When a class implements an interface, it must adhere to the outline of the interface. In other words, it must provide an implementation for methods outlined in the interface.

An interface typically outlines some number of abstract methods as depicted below:

public interface Abstractor {
    String computeOutput(..);
}
Note that all interface methods are implicitly public and abstract.

If you want to implement this interface in a class, then you would have to provide an implementation for the inherited abstract method computeOutput().

public Class ComplexClass implements Abstractor{

    //instance variables
    ...

    //methods
    ...

    public String computeOutput(..){
        //provide an implementation that can use other
        //methods and performs any operations
        ...
    }
}
Below we will showcase the creation and implemention of an interface:

Mammal Class

package com.example.model;

public class Mammal{

}
Whale Class

package com.example.model;

public class Whale extends Mammal{

}
In the above, we defined an inheritance hierarchy between the class Mammal and Whale. They are empty classes that we'll later use to tie in the use of an interface.

Now let's say that we want to expand our program and we need a lot of pieces that will rely on the ability of an animal to be a swimmer. In order, to be considered a swimmer all an animal needs to do is to be able to swim.

This is the perfect use case for an interface.

Defining an Interface
We create a new file, but instead of creating a Class, we create an interface:

package com.example.model;

public interface Swimmer {

}
Notice that the only real difference between an interface and a class is the use of the interface keyword in place of the keyword class when defining the entity.

Recall that to be a Swimmer, the object must be able to swim, so we will outline that method.

We will not provide an implementation for the method, as interface methods are abstract. Later you will find a workaround for this by using default or static methods.

We will edit the code so that it looks like the following:

package com.example.model;

public interface Swimmer {
    void swim(); 
}
Let's say that we want the Whale class to be considered a Swimmer. In order to do so, we update the Whale class to implement this interface by using the implements keyword.

package com.example.model;

public class Whale extends Mammal implements Swimmer{

}
After making this change, our IDE underlines the Whale class in red, giving us the following compiler error:

image of error in Whale class

It says that our class needs to implement the swim() method.

When a non-abstract class inherits an abstract method, such as the swim() method from the Swimmer interface, the non-abstract class must provide an implementation.

We create the implementation as follows:

@Override
public void swim() {
    System.out.println("Swim swim swim");
}
Notice that we included the annotation @Override. This annotation is not required, but it provides useful functionality, and provides context to any other developers reading the code that the method in question was inherited and that the implementation differs from that of the parent class.

Great. Now we've implemented our interface. As a last task, we need to write a test class to check how this works.

To do so we create a new class TestSwimmer under a new package com.example.test:

package com.example.test;

import com.example.model.Whale;
import com.example.model.Swimmer;

public class TestSwimmer {

    public static void main(String[] args) {
        Swimmer beluga;
        beluga = new Whale(); 
        beluga.swim(); 
    }
}
We can assign an instance of the Whale class to our Swimmer reference variable due to the IS-A relationship between them. Remember Whale implements Swimmer.

Running the program we see the following output:

image of running the class

Everything works great! And "Swim swim swim" prints successfully to the console.

Default Methods
Now imagine that all Swimmer objects should also be able to dive.

Well then we would go back to our interface and outline the dive method.

We edit the Swimmer interface to resemble the following:

package com.example.model;

public interface Swimmer {
    void swim(); 
    void dive(); 
}
After saving this change the same error from before will appear on the implementing Whale class:

image of error in Whale class

Of course, now Whale is inheriting an abstract method once again.

To fix this we could simply edit Whale, but what if we had implemented Swimmer in a hundred different classes?

This is where default methods come into play. They allow us to provide an implementation for interface methods that should be used if the method is not overridden in the implementing class:

package com.example.model;

public interface Swimmer {
    void swim(); 
    default void dive(){
        System.out.println("Diving"); 
    }
}
When a method is default, a method body should be present.

After making this change the previous error disappears in the Whale class.

Editting the TestSwimmer class allows us to verify these changes work:

package com.example.test;

import com.example.model.Swimmer;
import com.example.model.Whale;

public class TestSwimmer {
	public static void main(String[] args) {
		Swimmer beluga;
        beluga = new Whale();
		beluga.swim();
		beluga.dive();
	}

}
Running this new change produces the following output:

image of running the lab

Excellent.

Defining Variables and Static Methods
We can also use interfaces to define public static variables and public static methods:

package com.example.model;

public interface Swimmer {
	int NUMBER_OF_FEET_IN_A_LEAGUE = 18_228;
	
	void swim();
	default void dive() {
		System.out.println("Diving");
	}
	
	static int convertFromFeetToLeagues(int feet) {
		return feet/NUMBER_OF_FEET_IN_A_LEAGUE; 
	}
}
Notice the naming convention used. When variables are both static and final they are considered to be "constants". In Java the naming convention for constants is to use UPPER_SNAKE_CASE.

Next we edit the TestSwimmer class to resemble:

package com.example.test;

import com.example.model.Swimmer;
import com.example.model.Whale;

public class TestSwimmer {
	public static void main(String[] args) {
		Swimmer beluga;
        beluga = new Whale();
		beluga.swim();
		beluga.dive();
		
		System.out.println("There are "+Swimmer.NUMBER_OF_FEET_IN_A_LEAGUE+" in a league.");
		
		int feetTraveled = 80_000;
		System.out.println("So if the whales went "+feetTraveled+" feet, they "
				+ "would have gone "+ Swimmer.convertFromFeetToLeagues(feetTraveled)+" leagues.");
	}

}
Notice that we access our static variable and method via Swimmer, the interface, and not through beluga, the reference variable.

When we run this we see the following:

image of running the lab

Exercise (Optional)
Write a class that implements the CharSequence interface found in the java.lang package. Your implementation should return the string backwards. Select one of the sentences from this book to use as the data. Write a small main method to test your class; make sure to call all four methods.

Summary

An interface is similar to an abstract class, but one of many differences is that a class can only inherit one other class, but a class can implement as many interfaces as it needs.
A class implements an interface using the implements keyword in the class definition and by providing implementations for any abstract methods defined by the interface.
Interfaces have these advantages over class:
Implementation details do not need to be provided in the interface.
A class can only extend one other class, but it can implement as many interfaces as needed.
Interfaces have implicit modifiers on methods and variables.
Methods are 'public' and 'abstract'
Variables are 'public', 'static', and 'final'


## List Interface
Learning Objectives

After completing this module, associates should be able to:

Describe the List interface and its hierarchy.
Successfully execute a Java program that demonstrates the List interface
Description

A List is a data structure where the elements within it are ordered. In Java, lists use indexes to represent positions of elements within them. The List Interface and its subclasses allow you to create implementations of lists.

The List interface is located within the java.util package and is part of the Collection API and inherits from the Collection and Iterable interfaces. This means a List should do the following:

Be able to be iterated over using an Iterator
Should be able to perform common collection operations, such as adding or removing elements, checking if an element is in the collection, or checking the size of the collection
Additionally, the List interface defines operations that are specific to lists such as accessing elements by index, searching using an index, and iterating backwards. List-specific operations are:

add(int index, E element) - adds an element at the given index
get(int index) - retrieves an element at a given index
indexOf(Object o) - retrieves the index of the given element
listIterator() / listIterator(int index) - returns an iterator at either the beginning of the list or at the specified index
remove(int index) - removes an element at the given index
set(int index, E element) - replaces the element at the given index with the given element
subList(int fromIndex, int toIndex) - returns a sub-list of the elements at the specific range
Resources

List Interface Java 8 Documentation: https://docs.oracle.com/javase/8/docs/api/java/util/List.html
Real World Application

Lists can be used whenever you need an ordered collection of elements. Here are some real-world examples of how lists are used in different applications:

To-Do Lists and Task Management Apps:
Many to-do list applications use lists to manage tasks. Each task is an element in the list, and users can add, remove, or update tasks easily.
Shopping Cart in E-commerce Websites:
When you add items to your shopping cart on an online store, those items are typically stored in a list. This allows for easy manipulation, such as adding or removing items from the cart.
Playlist in Music Apps:
Playlists in music applications are often implemented using lists. Each song is an element in the list, and users can modify the playlist by adding, removing, or rearranging songs.
Contact Lists in Phones:
Your phone's contact list is a classic example of a list data structure. Each contact entry contains information like name, phone number, and email, organized in a list for easy retrieval.
Social Media Feeds:
Social media feeds display content in a chronological order, making them a good example of a list. Each post or update is an element in the feed.
Menu Items in Software Applications:
Menus in software applications are often implemented using lists. Each menu item is an element, and users can navigate through and select different options.
These examples illustrate how lists are versatile and widely used in various applications to organize and manage data efficiently.

Implementation
Creating and Populating a List
List<String> list = new ArrayList<>();
list.add("Hello");
list.add("there");
list.add("how");
list.add("you?");
System.out.println(list); // [Hello, there, how, you?]
list.add(3, "are");
System.out.println(list); // [Hello, there, how, are, you?]
In the above code, we created a List reference variable. We can assign this variable any subclass object, like an ArrayList. We specify that the list stores String objects. Next, we use the add() method inherited from the Collection interface to add elements to the list. We then use the add() method provided by the List interface to directly assign an element to a specific index. We see the element "are" being assigned to index 3 and this causes the previous element at that index to shift downward towards the end of the list.

Accessing List Elements
System.out.println(list); // [Hello, there, how, are, you?]
System.out.println(list.get(2)); // "how"
System.out.println(list.indexOf("how")); // "2"

list.remove(1);
list.set(0, "Hi");
System.out.println(list); // [Hi, how, are, you?]

List<String> sublist = list.subList(1, 3); 
System.out.println(sublist); // [how, are]
In the above code we demonstrate the use of several methods that we can use to access and manipulate list elements.

The get() method retrieves an element at a specified index.
The indexOf() method retrieves an index given a specified element.
The remove() method can be given an index as an argument to remove an element at a given index.
The set() method allows us to replace an element at a given index.
The subList() method returns a sub-list of the elements at the specific range.
Iterating Over Lists
// using enhanced for loop
for (String element : list) {
  System.out.println(element);
}

// iterating forwards with ListIterator
//                      [ "hi", "how", "are", "you?" ]
//                         0      1      2       3
//                      ^
ListIterator<String> iterator1 = list.listIterator();
while (iterator1.hasNext()) {
  System.out.println(iterator1.next()); 
}

// iterating backwards with ListIterator
//                      [ "hi", "how", "are", "you?" ]
//                         0      1      2       3
//                                                 ^
 ListIterator<String> iterator2 = list.listIterator(list.size());
while (iterator2.hasPrevious()) {
  System.out.println(iterator2.previous()); 
}
In the above code we have three examples of iterating over our list.

Enhanced for loop: Behind the scenes, Java is actually using an iterator. The enhanced for loop is syntactic sugar, or an easier to read syntax, that is provided for our convenience.
The second example uses listIterator() to return an iterator that can traverse our list for us. We do not provide an argument so the cursor, or starting point, of the iterator is at the very beginning of the list, right before the first element. We use a loop to check if there is still an element to iterate over using hasNext(), and if there is, we access it using next() and print out the value.
The third example uses listIterator() to return an iterator that can traverse our list for us. However, this time we specify that the cursor, or starting point, of the iterator should be right after the last element. This allows us to traverse the list backwards. We use a loop to check if there is still an element to iterate over using hasPrevious(), and if there is, we access it using previous() and print out the value.
Exercise (Optional)
create your own to-do list program where the user can do the following:
create a task
delete a task
change the position of a task on the list
edit a task
Summary

A list is a data structure where the elements within it are ordered
The List interface defines operations that are specific to lists such as accessing elements by index, searching using an index, and iterating backwards




# OOP, Maven, and Dev Practices
## SOLID Design Principles
Learning Objectives

After completing this module, you should be able to:

Explain the five principles of SOLID
Description

The SOLID principles are best practices you should follow when you are working with any OOP language.

SOLID is an acronym to remember what the principles are:

Single Responsibility Principle (SRP)
Open/Closed Principle (OCP)
Liskov Substitution Principle (LSP)
Interface Segregation Principle (ISP)
Dependency Inversion Principle (DIP)
Single Responsibility Principle (SRP)
SRP states that a class should have only one reason to change. This means that each class should have only one responsibility or job to do. To apply SRP in Java, you should aim to create classes that have a clear and single responsibility. If a class has multiple responsibilities, you should consider splitting it into smaller, more focused classes.

Open / Closed Principle (OCP)
OCP states that software entities should be open for extension but closed for modification. This means that you should be able to add new functionality to a system without modifying its existing code. To apply OCP in Java, you should aim to design classes that are open for extension but closed for modification. This can be achieved by using interfaces, abstract classes, and polymorphism.

Liskov Substitution Principle (LSP)
LSP states that subtypes should be substitutable for their base types. This means that any instance of a base class should be able to be replaced by an instance of a subclass without affecting the correctness of the program. To apply LSP in Java, you should aim to create subclasses that can be used in place of their superclass. This means that the subclass should not change the behavior of the superclass.

Interface Segregation Principle (ISP)
ISP states that clients should not be forced to depend on interfaces they do not use. This means that interfaces should be small and focused, and clients should only depend on the interfaces they need. To apply ISP in Java, you should aim to create interfaces that are small and focused. This means that you should avoid creating large, general-purpose interfaces that are used by many clients. Instead, you should create multiple small interfaces that are each focused on a specific set of functionality. Clients can then depend on only the interfaces they need.

Dependency Inversion Principle (DIP)
DIP states that high-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions. This means that you should use interfaces or abstract classes to define the behavior of your classes, and then use dependency injection to provide the specific implementations. This allows you to easily swap out different implementations without affecting the rest of your code.

By following these principles, you can create Java code that is more modular, maintainable, and flexible.

Real World Application

In the following section, we will describe how each SOLID principle can be used in real world applications:

Single Responsibility Principle (SRP)
In a web application, a User class should handle user-specific operations such as authentication and authorization, while a separate EmailService class should handle sending emails. SRP ensures that each class should be responsible for specific functionality and this separation ensures that changes to the email functionality won't affect the user-related operations.

Open-Closed Principle (OCP)
Consider a payment system where new payment methods can be added. By leveraging OCP, rather than modifying the existing code, the system can define an interface called PaymentProcessor and have multiple classes implementing it, such as CreditCardProcessor and PayPalProcessor. This way, new payment processors can be added without modifying existing code.

Liskov Substitution Principle (LSP)
In a banking application with different account types like SavingsAccount and CheckingAccount inheriting from the base class Account, LSP ensures that code expecting an Account object can seamlessly work with any subclass object. Regardless of the specific account type, LSP guarantees that common functionalities like depositing and withdrawing funds, as well as any shared behaviors defined in the base class, remain consistent.

Interface Segregation Principle (ISP)
If we apply ISP to a messaging system, instead of having a single IMessage interface with multiple methods like SendMessage, ReceiveMessage, and DeleteMessage, separate interfaces like ISender and IReceiver can be created. Clients can then implement the interfaces that correspond to their specific needs.

Dependency Inversion Principle (DIP)
In an e-commerce application, instead of the OrderProcessor class depending directly on a specific payment gateway implementation, it can depend on an IPaymentGateway interface. Adherence to DIP would allow for easier switching between different payment gateway providers without impacting the core logic of the application.

Implementation
Single Responsibility Principle (SRP)
Let's say we have a User entity that we want to be able to authenticate. In the below example, we have persistence logic and authentication logic within the same class. This violates the SRP since the class has more than one responsibility.

    //INCORRECT
    class User {
        public boolean authenticateUser(){
            //authentication logic here
        }

        public void createUser(){
            //user persistence logic here
        }
    }
You should seperate these classes out to a UserAuthenticator.java and UserDAO.java (DAO stands for Data Access Object).

    //CORRECT
    class UserAuthenticator {

        public boolean authenticateUser(){
            //authentication logic here
        }

        //other methods that deal with user validation...
    }

    class UserDAO {

        public void createUser(){
            //user persistence logic here
        }

        //other methods that deal with user persistence...
    }
Open-Closed Principle (OCP)
In this example, we have a Vehicle class that has a speed instance variable and an accelerate() method. The logic below does NOT follow the open closed principle because you have to update the Vehicle class any time a new subclass is made.

//Incorrect
class Vehicle {

    int speed;

    public double accelerate(Vehicle v) {
        if (v instanceof Car) {
            return v.speed += 10;
        if (v instanceof Truck) {
            return v.speed += 5;
        }
    }
}
Instead of modifying an existing class to add new functionality, you could create a new class that implements the same interface and provides the new functionality.

//Correct
interface Vehicle {
    public double accelerate();
}

class Car implements Vehicle {

    int speed;

    @Override
    public double accelerate() {
        this.speed += 10;
        return this.speed;
    }
}

class Truck implements Vehicle {

    int speed;

    @Override
    public double accelerate() {
        this.speed += 5;
        return this.speed;
    }
}
Liskov Substitution Principle (LSP)
For this example, we have a Bird class and a Penguin subclass. The subclass changes the intent of the fly() method, which violates the Liskov Substitution Principle. We should ensure that the Penguin class does not change the behavior of the Bird class.

    //INCORRECT
    class Bird {
        public void fly() {

        }
    }

    class Penguin extends Bird {
        public void fly(){
            //implements walking logic since penguins can't fly
        }
    }
We can instead expand the inheritance hierarchy to have Flying and Flightless subtypes or using an interface that defines the fly() behavior.

    class Bird {
    }

    interface Flyable {
        public void fly();
    }

    class Penguin extends Bird {
        
    }

    class Albatross extends Bird implements Flyable {
        public void fly() {
            // implementation
        }
    }
Interface Segregation Principle (ISP)
In this example, we have a Vehicle interface that defines several behaviors. The Bike class can implement some of these behaviors, but not all of them. This violates the Interface Segregation Principle.

//INCORRECT
interface Vehicle {
    public void accelerate();
    public void break();
    public void openDoors();
}

class Bike implements Vehicle {
    public void accelerate(){
        //implementation...
    }

    public void break(){
        //implementation...
    }

    public void openDoors(){
        //Bikes dont have doors so this does not follow ISP and shouldnt be forced to implement a method it cannot use
    }
}
We can separate the behavior into separate interfaces or into a Vehicle class hierarchy instead.


class Vehicle {
    public void accelerate() {}
    public void break() {}
}

interface Enterable {
    public void openDoors();
}

class Bike extends Vehicle {
    public void accelerate(){
        // implementation...
    }

    public void break(){
        // implementation...
    }

}

class Truck extends Vehicle implements Enterable {
    public void accelerate(){
        //implementation...
    }

    public void break(){
        //implementation...
    }

    public void openDoors(){
        //implementation...
    }
}

Dependency Inversion Principle (DIP)
In this example, we have a NotificationService class that uses an EmailSender object to send a notification. However, this is tightly-coupled code that requires the NotificationService class to rely on a specific type of object.


class NotificationService {

    EmailSender emailSender = new EmailSender();

    sendNotification() {
        emailSender.send();
    }
}

class EmailSender {

    send() {
        // implementation
    }
}

We can instead have a general supertype, Sendable. This allows our NotificationService class the flexibility of swapping between different implementations of Sendable objects.


class NotificationService {

    Sendable sender = new EmailSender();

    sendNotification() {
        sender.send();
    }
}

interface Sendable {
    send();
}

class EmailSender implements Sendable {

    send() {
        // implementation
    }
}

class TextSender implements Sendable {

    send() {
        // implementation
    }
}

Summary

Single Responsibility Principle (SRP): A class should have only one purpose, focusing on a single responsibility or task.
Open-Closed Principle (OCP): Software entities should be open for extension but closed for modification, enabling flexibility and avoiding modification of existing code.
Liskov Substitution Principle (LSP): Objects of a superclass should be replaceable with objects of their subclasses without affecting the consistency of the program's behavior.
Interface Segregation Principle (ISP): Clients should not be forced to depend on interfaces they do not use, emphasizing specific interfaces tailored to clients' needs and reducing unnecessary dependencies.
Dependency Inversion Principle (DIP): High-level modules should not depend on low-level modules; both should depend on abstractions, promoting loose coupling and dependency inversion through abstractions.

## Intro to Maven pom and xml Files
Learning Objectives

After completing this module, associates should be able to:

Describe Maven and its implementation with project management.
Discuss the different fields in the pom.xml file.
Successfully execute a Java program that demonstrates Maven and pom.xml
Description
Introduction to Maven
Maven is a tool that can be used for building and managing any Java-based project.

Maven helps in the following ways:

Simplifies the build process
Adding jars and dependencies
Documenting project information with change logs and reports
Integration with source control systems (Git)
Features of Maven that we will go over:

Project Object Model (POM)
Maven lifecycles
Introduction to Maven
Maven is a dependency manager and build automation tool for Java programs. Maven project configuration and dependencies are handled via the Project Object Model, defined in the pom.xml file. This file contains essential project information such as dependencies and plugins required for building the project.

POM - Project Object Model
Maven identifies projects through project coordinates defined in the pom.xml file - these are:

<groupId> - for example: "com.revature"
<artifactId> - for example: "myproject"
<version> - for example: "0.0.1-SNAPSHOT"
Together, these uniquely identify a specific version of a program.

Some other important tags within the pom.xml file include:

<project> - this is the root tag of the file
<modelVersion> - defining which version of the page object model to be used
<name> - name of the project
<properties> - project-specific settings
<dependencies> - this is where you put your Java dependencies you want to use. Each one needs a <dependency>, which has:
<groupId>
<artifactId>
<version>
<plugins> - for 3rd party plugins that work with Maven
Here's an example:

<project>
  <modelVersion>4.0.0</modelVersion>
 
  <groupId>com.revature.app</groupId>
  <artifactId>my-app</artifactId>
  <version>1</version>
  
  <dependencies>
    <dependency>
      <groupId>org.apache.maven</groupId>
      <artifactId>maven-artifact</artifactId>
      <version>${mavenVersion}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.maven</groupId>
      <artifactId>maven-core</artifactId>
      <version>${mavenVersion}</version>
    </dependency>
  </dependencies>

</project>
Maven Build Lifecycle
When Maven builds your project, it goes through several steps called phases. The default maven build lifecycle goes through the following phases:

Validate => project is correct and all necessary information is available
Compile => compiles project source code
Test => tests all compiled code
Package => packages all compiled code to WAR/JAR file
Integration => performs all integration tests on WAR/JAR
Verify => runs any checks on the results of integration tests
Install => installs WAR/JAR to local repository
Deploy => copies final WAR/JAR to the remote repository
Each phase in turn is composed of plugin goals that are bound to zero or more build phases. A "goal" represents a specific task which contributes to the building or managing of the project.

For more information, see the Maven documentation.

Using the mvn command
To use the Maven CLI (command-line interface), first test that you have Maven installed:

mvn --version
Now, once you are in your project directory, you can run any phase in the default build lifecycle. Maven will look for the pom.xml file and use that to run the phase.

cd /path/to/myproject/
mvn package
To execute a specific Maven goal, use the plugin:goal syntax:

mvn dependency:copy-dependencies
Multiple phases or goals can be run sequentially. Again, see the Maven documentation for more information.

Code Coverage
One significant feature of Maven is its integration with various plugins that extend its functionality. These plugins enable tasks such as generating documentation, deploying artifacts to repositories, and performing code quality analysis.

Code coverage is a metric used to measure the percentage of code that is executed during automated testing. It helps assess the effectiveness of test suites by identifying areas of code that have not been tested. Code coverage tools analyze the relationship between the executed code and the source code to generate reports indicating the coverage percentage.

Maven integrates seamlessly with code coverage tools such as JaCoCo (Java Code Coverage) and Cobertura. These tools can be configured as Maven plugins to generate code coverage reports during the build process. By adding code coverage plugins to the Maven project configuration, developers can automatically generate detailed reports showing which parts of the codebase are covered by unit tests.

Real World Application
The Need for Maven
Maven is chiefly used for Java-based projects, helping to download dependencies, which refers to the libraries or JAR files. The tool helps get the right JAR files for each project as there may be different versions of separate packages.

After Maven, downloading dependencies doesn’t require visiting the official websites of different software. You can visit mvnrepository to find libraries in different languages. The tool also helps to create the right project structure in struts, servlets, etc., which is essential for execution.

Who is using Maven?
There are over 4,000 companies using Maven today. Maven is also used in industries other than computer science, like information technology, financial services, banking, hospital and care, and much more.

Some of the largest corporations that use Maven include:

Accenture
JPMorgan Chase & Co
Via Varejo
Red Hat
Mitratech Holdings, Inc.
KRG TECHNOLOGIES
Radio Canada
Implementation

Here we will describe the process for installing Maven and take a look at a sample pom.xml file. Note that following along is optional.
The Installation of Maven on Mac OS
Please use the following resource: https://www.digitalocean.com/community/tutorials/install-maven-mac-os

The Installation of Maven on Windows includes following steps:
Verify that your system has Java installed or not. if not then install Java.
Check if the Java Environmental variable is set or not. if not then set Java environment variable.
Unpack your Maven zip at any place in your system.
Add the bin directory of the created directory apache-maven-3.5.3(it depends upon your installation version) to the PATH environment variable and system variable.
open a terminal and run mvn -v command. If it prints the following lines of code (or similar) then installation completed.
Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-25T01:19:05+05:30)
Maven home: C:\apache-maven-3.5.3\bin\..
Java version: 1.8.0_151, vendor: Oracle Corporation
Java home: C:\Program Files\Java\jdk1.8.0_151\jre
Default locale: en_US, platform encoding: Cp1252
OS name: "windows 10", version: "10.0", arch: "amd64", family: "windows"
Maven pom.xml file
POM means Project Object Model is key to operate Maven. Maven reads pom.xml file to accomplish its configuration and operations. It is an XML file that contains information related to the project and configuration information such as dependencies, source directory, plugin, goals etc. used by Maven to build the project.

The sample of pom.xml

<project xmlns="http://maven.apache.org/POM/4.0.0" 
   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
http://maven.apache.org/xsd/maven-4.0.0.xsd">
          
         <modelVersion>4.0.0</modelVersion>
         <groupId> com.project.loggerapi </groupId>
         <artifactId>LoggerApi</artifactId>
         <version>0.0.1-SNAPSHOT</version>
           
       <!-- Add typical dependencies for a web application -->
       <dependencies>
               <dependency>
                       <groupId>org.apache.logging.log4j</groupId>
                       <artifactId>log4j-api</artifactId>
                       <version>2.11.0</version>
                 </dependency>
       </dependencies>
     
   </project>

Other Elements of Pom.xml file
dependencies- dependencies element is used to define a list of dependency of project.
dependency- dependency defines a dependency and used inside dependencies tag. Each dependency is described by its groupId, artifactId and version.
name- this element is used to give name to our maven project.
scope- this element is used to define scope for this maven project that can be compile, runtime, test, provided system etc.
packaging- packaging element is used to packaging our project to output types like JAR, WAR etc.
Code Coverage
Code coverage is a software metric used to measure how many lines of our code are executed during automated tests.

Here we are going to stroll through some practical aspects of using JaCoCo, a code coverage reports generator for Java projects.

Maven Configuration
In order to get up and running with JaCoCo, we need to declare this maven plugin in our pom.xml file:

<plugin>
    <groupId>org.jacoco</groupId>
    <artifactId>jacoco-maven-plugin</artifactId>
    <version>0.7.7.201606060606</version>
    <executions>
        <execution>
            <goals>
                <goal>prepare-agent</goal>
            </goals>
        </execution>
        <execution>
            <id>report</id>
            <phase>prepare-package</phase>
            <goals>
                <goal>report</goal>
            </goals>
        </execution>
    </executions>
</plugin>
Code Coverage Reports
Before we start looking at JaCoCo's code coverage capabilities, we need to have a code sample. Here's a simple Java function that checks whether a string reads the same backward and forward:

public boolean isPalindrome(String inputString) {
    if (inputString.length() <= 1) {
        return true;
    } else {
        char firstChar = inputString.charAt(0);
        char lastChar = inputString.charAt(inputString.length() - 1);
        String mid = inputString.substring(1, inputString.length() - 1);
        return (firstChar == lastChar) && isPalindrome(mid);
    }
}
Now all we need is a simple JUnit test:

@Test
public void whenEmptyString_thenAccept() {
    Palindrome palindromeTester = new Palindrome();
    assertTrue(palindromeTester.isPalindrome(""));
}
Running the test using JUnit will automatically set in motion the JaCoCo agent. It will create a coverage report in binary format in the target directory, target/jacoco.exec.

Obviously we can't interpret the output single-handedly, but other tools and plugins can, e.g. Sonar Qube.

The good news is that we can use the jacoco:report goal in order to generate readable code coverage reports in several formats, like HTML, CSV, and XML.

Report Analysis
JaCoCo reports help us visually analyze code coverage by using diamonds with colors for branches, and background colors for lines.

Red diamond means that no branches have been exercised during the test phase.
Yellow diamond shows that the code is partially covered – some branches have not been exercised.
Green diamond means that all branches have been exercised during the test.
The same color code applies to the background color, but for lines coverage.
JaCoCo mainly provides three important metrics:

Lines coverage reflects the amount of code that has been exercised based on the number of Java byte code instructions called by the tests.
Branches coverage shows the percent of exercised branches in the code, typically related to if/else and switch statements.
Cyclomatic complexity reflects the complexity of code by giving the number of paths needed to cover all the possible paths in a code through linear combination.
To take a trivial example, if there are no if or switch statements in the code, the cyclomatic complexity will be 1, as we only need one execution path to cover the entire code.

Generally, the cyclomatic complexity reflects the number of test cases we need to implement in order to cover the entire code.

Concept Breakdown
JaCoCo runs as a Java agent. It's responsible for instrumenting the byte code while running the tests. JaCoCo drills into each instruction, and shows which lines are exercised during each test.

To gather coverage data, JaCoCo uses ASM for code instrumentation on the fly, receiving events from the JVM Tool Interface in the process:

jacoco concept It's also possible to run the JaCoCo agent in server mode. In this case, we can run our tests with jacoco:dump as a goal in order to initiate a dump request.

Code Coverage Score
Now that we know a bit about how JaCoCo works, let's improve our code coverage score.

In order to achieve 100% code coverage, we need to introduce tests that cover the missing parts shown in the initial report:

@Test
public void whenPalindrom_thenAccept() {
    Palindrome palindromeTester = new Palindrome();
    assertTrue(palindromeTester.isPalindrome("noon"));
}
    
@Test
public void whenNearPalindrom_thanReject(){
    Palindrome palindromeTester = new Palindrome();
    assertFalse(palindromeTester.isPalindrome("neon"));
}
Now we have enough tests to cover our the entire code, but to make sure of that, let's run the Maven command mvn jacoco:report to publish the coverage report.

In a real world project, as developments go further, we need to keep track of the code coverage score.

JaCoCo offers a simple way of declaring minimum requirements that should be met, otherwise the build will fail.

We can do that by adding the following check goal in our pom.xml file:

<execution>
    <id>jacoco-check</id>
    <goals>
        <goal>check</goal>
    </goals>
    <configuration>
        <rules>
            <rule>
                <element>PACKAGE</element>
                <limits>
                    <limit>
                        <counter>LINE</counter>
                        <value>COVEREDRATIO</value>
                        <minimum>0.50</minimum>
                    </limit>
                </limits>
            </rule>
        </rules>
    </configuration>
</execution>
As we can see, we're limiting the minimum score for lines coverage to 50%.

The jacoco:check goal is bound to verify, so we can run the Maven command mvn clean verify to check whether the rules are respected or not. The logs will show something like:

[ERROR] Failed to execute goal org.jacoco:jacoco-maven-plugin:0.7.7.201606060606:check 
  (jacoco-check) on project mutation-testing: Coverage checks have not been met.
Today we learned how to make use of the JaCoCo maven plugin to generate code coverage reports for Java projects.

Keep in mind though, 100% code coverage doesn't necessarily reflect effective testing, as it only reflects the amount of code exercised during tests. In a previous article, we talked about mutation testing as a more sophisticated way to track the effectiveness of tests compared to ordinary code coverage.

Summary

Maven is a tool that can be used for building and managing any Java-based project.
Maven helps in the following ways:
Simplifies the build process
Adding jars and dependencies
Documenting project information with change logs and reports
Maven project configuration and dependencies are handled via the Project Object Model, defined in the pom.xml file. This file contains information about the project used to build the project, including project dependencies and plugins.
Some other important tags within the pom.xml file include:
<project> - this is the root tag of the file
<modelVersion> - defining which version of the page object model to be used
<name> - name of the project
<properties> - project-specific settings
<dependencies> - this is where you put your Java dependencies you want to use. Each one needs a <dependency>, which has:
<groupId>
<artifactId>
<version>
<plugins> - for 3rd party plugins that work with Maven


## Maven Build Lifecycle
Learning Objectives

After completing this module, associates should be able to:

Showcase the steps of the Maven Build Lifecycle
Description

The Maven Build Lifecycle is a well-defined sequence of phases that a Maven build goes through. Each phase represents a specific step in the build process, such as compiling the code, packaging it into a JAR file, and deploying it to a server. The phases are organized into a lifecycle, which is predefined and standardized by Maven.

The phases of the build lifecycle are:

validate- validate the project is correct and all necessary information is available
compile- compile the source code of the project
test- test the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed
package- take the compiled code and package it in its distributable format, such as a JAR.
verify - run any checks on results of integration tests to ensure quality criteria are met
install - install the package into the local repository, for use as a dependency in other projects locally
deploy - done in the build environment, copies the final package to the remote repository for sharing with other developers and projects.
After the validate phase and before the compile phase, the actual application code is written.

Real World Application

Understanding the Maven build lifecycle is crucial for effectively managing and controlling the build process of a project. Here are some key reasons why understanding the Maven build lifecycle is important:

Consistency and Standardization: Maven follows a predefined build lifecycle with well-defined phases such as compile, test, package, and install. Understanding this lifecycle ensures consistency across projects and standardizes the build process within an organization. Developers can easily navigate through the build phases and know what tasks are executed at each stage.

Customization and Extensibility: Maven allows customization and extension of the build process through plugins and configuration. By understanding the build lifecycle, developers can customize the build to meet project-specific requirements. They can define custom build phases, bind plugins to specific lifecycle phases, and execute additional tasks as needed, providing flexibility and extensibility.

Troubleshooting and Debugging: Understanding the Maven build lifecycle is essential for troubleshooting build failures and debugging issues. Developers can analyze the build logs, identify which phase or plugin encountered an error, and take appropriate corrective actions. Familiarity with the build lifecycle helps developers diagnose problems more efficiently and resolve them effectively.

Overall, understanding the Maven build lifecycle empowers developers to effectively manage the build process, leading to more efficient development workflows and higher-quality software deliverables.

Implementation

If you want to invoke any of the phases of the maven build lifecycle, all you need to do is type mvn phase_name within a Command Line Interface (CLI) which is open to the directory which contains your maven project.

For example, if you want to package your project into a jar executable, you type mvn package. This will generate a jar file in a folder called target.

NOTE: This will run every phase prior to package. So the flow would look like validate executed -> compile executed -> test executed -> package executed. If any of these phases failed, then the original command mvn package will also fail.

Plugins
Plugins assist with configuring the build lifecycle of your project. maven-assembly-plugin in particular will be required in training after PEP.

Maven Assembly Plugin
If your project has external dependencies and you need to build your maven project into a jar executable, mvn package won't work as intended. Running this command will generate a jar. However when the jar is ran, the output will state No Main Manifest Attribute. To fix this we are going to need to configure our pom.xml file to add the plugin maven-assembly-plugin.

    <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <archive>
            <manifest>
                <!-- Change the line below to reference the location of the class with main method -->
              <mainClass>MainClass</mainClass>
            </manifest>
          </archive>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id> 
            <phase>package</phase> 
            <goals>
              <goal>single</goal>
            </goals>
        </execution>
      </executions>
    </plugin>
Running the command mvn package should produce a PROJECT-jar-with-dependencies.jar executable in the target folder that integrates all of your dependencies when run.

Note: If the above command does not work, you can add the assembly:single goal to it. For example, you could run: mvn package assembly:single.
Summary

There are seven (7) phases of the Maven build lifecycle. They are:

validate: validate the project is correct and all necessary information is available
compile: compile the source code of the project
test: test the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed
package: take the compiled code and package it in its distributable format, such as a JAR.
verify - run any checks on results of integration tests to ensure quality criteria are met
install - install the package into the local repository, for use as a dependency in other projects locally
deploy - done in the build environment, copies the final package to the remote repository for sharing with other developers and projects.


## Introduction To Sdlc
Learning Objectives

After completing this module, associates should be able to:

Define the acronym "SDLC"
Elaborate on the stages of the SDLC
Give examples of how SDLC is implemented.
Description

The Software Development Life Cycle (SDLC) is the application of standard business practices to building software applications. It’s typically divided into six to eight steps: Planning, Requirements, Design, Build, Document, Test, Deploy, Maintain. Some project managers will combine, split, or omit steps, depending on the project’s scope. These are the core components recommended for all software development projects.

SDLC is a way to measure and improve the development process. It allows a fine-grain analysis of each step of the process. This, in turn, helps companies maximize efficiency at each stage. As computing power increases, it places a higher demand on software and developers. Companies must reduce costs, deliver software faster, and meet or exceed their customers’ needs. SDLC helps achieve these goals by identifying inefficiencies and higher costs and fixing them to run smoothly.

Real World Application

Organizations can use the SDLC process to provide structure when designing and building of applications.

This life cycle provides an effective plan for various activities in software development such as designing, building (developing), and maintaining software applications. It also provides a methodology for enhancing the quality of software applications in an organization. Organizations can choose an effective software development strategy from a variety of SDLC process models.

Various people in the organization can use the SDLC such as software engineers, developers, and cross-function teams. Developers and software engineers use it to create effective plans and designs. They also apply the various SDLC stages to develop innovative software products. Cross-function teams use the SDLC to collaborate across different software development stages.

Importance of SDLC
The software development life cycle adds value to software development in the following ways:

It provides an effective framework and method to develop software applications.
It helps in effectively planning before starting the actual development. SDLC allows developers to analyze the requirements.
It helps in reducing unnecessary costs during development. During the initial phases, developers can estimate the costs and predict costly mistakes.
It enables developers to design and build high-quality software products. This is because they follow a systematic process that allows them to test the software before it is rolled out.
It provides a basis when evaluating the effectiveness of the software. This further enhances the software product.
Implementation

The following are general stages of the Software Development Life Cycle. Depending on the scope of the project, some stages may be omitted, and other stages may be enhanced.

Planning
In the Planning phase, project leaders evaluate the terms of the project. This includes calculating labor and material costs, creating a timetable with target goals, and creating the project’s teams and leadership structure.

Planning can also include feedback from stakeholders. Stakeholders are anyone who stands to benefit from the application. Try to get feedback from potential customers, developers, subject matter experts, and sales reps.

Planning should clearly define the scope and purpose of the application. It plots the course and provisions the team to effectively create the software. It also sets boundaries to help keep the project from expanding or shifting from its original purpose.

Define Requirements
Defining requirements is considered part of planning to determine what the application is supposed to do and its requirements. For example, a social media application would require the ability to connect with a friend. An inventory program might require a search feature.

Requirements also include defining the resources needed to build the project. For example, a team might develop software to control a custom manufacturing machine. The machine is a requirement in the process.

Design and Prototyping
The Design phase models the way a software application will work. Some aspects of the design include:

Architecture – Specifies programming language, industry practices, overall design, and use of any templates or boilerplate
User Interface – Defines the ways customers interact with the software, and how the software responds to input
Platforms – Defines the platforms on which the software will run, such as Apple, Android, Windows version, Linux, or even gaming consoles
Programming – Not just the programming language, but including methods of solving problems and performing tasks in the application
Communications – Defines the methods that the application can communicate with other assets, such as a central server or other instances of the application
Security – Defines the measures taken to secure the application, and may include SSL traffic encryption, password protection, and secure storage of user credentials
Prototyping can be a part of the Design phase. A prototype is like one of the early versions of software in the Iterative software development model. It demonstrates a basic idea of how the application looks and works. This “hands-on” design can be shown to stakeholders, and feedback can be used to improve the application. It’s less expensive to change the Prototype phase than to rewrite code to make a change in the Development phase.

Software Development
This is the actual writing of the program. A small project might be written by a single developer, while a large project might be broken up and worked on by several teams. Use an Access Control or Source Code Management application in this phase. These systems help developers track changes to the code. They also help ensure compatibility between different team projects and to make sure target goals are being met.

The coding process includes many other tasks. Many developers need to brush up on skills or work as a team. Finding and fixing errors and glitches is critical. Tasks often hold up the development process, such as waiting for test results or compiling code so an application can run. SDLC can anticipate these delays so that developers can be tasked with other duties.

Software developers appreciate instructions and explanations. Documentation can be a formal process, including writing a user guide for the application. It can also be informal, like comments in the source code that explain why a developer used a certain procedure. Even companies that strive to create software that’s easy and intuitive benefit from the documentation.

Documentation can be a quick guided tour of the application’s basic features that display on the first launch. It can be video tutorials for complex tasks. Written documentation like user guides, troubleshooting guides, and FAQ’s help users solve problems or technical questions.

Testing
It’s critical to test an application before making it available to users. Much of the testing can be automated, like security testing. Other testing can only be done in a specific environment – consider creating a simulated production environment for complex deployments. Testing should ensure that each function works correctly. Different parts of the application should also be tested to work seamlessly together—performance test, to reduce any hangs or lags in processing. The testing phase helps reduce the number of bugs and glitches that users encounter. This leads to a higher user satisfaction and a better usage rate.

Deployment
In the deployment phase, the application is made available to users. Many companies prefer to automate the deployment phase. This can be as simple as displaying a payment portal and download link on the company website. It could also be downloading an application on a smartphone.

Deployment can also be complex. Upgrading a company-wide database to a newly-developed application is one example. Because there are several other systems used by the database, integrating the upgrade can take more time and effort.

Operations and Maintenance
At this point, the development cycle is almost finished. The application is done and being used in the field. The Operation and Maintenance phase is still important, though. In this phase, users discover bugs that weren’t found during testing. These errors need to be resolved, which can spawn new development cycles.

In addition to bug fixes, models like Iterative development plan additional features in future releases. For each new release, a new Development Cycle can be launched.

Summary

The Software Development Life Cycle (SDLC) is the application of standard business practices to building software applications.
It’s typically divided into six to eight steps: Planning, Requirements, Design, Build, Document, Test, Deploy, Maintain.
Some project managers will combine, split, or omit steps, depending on the project’s scope.


## Agile
Learning Objectives

After completing this module, associates should be able to:

Define the Agile development cycle.
Elaborate on the steps in the Agile cycle.
Explain how the Agile cycle is used in developing software.
Description
Agile
The Agile model was designed by developers to put customer needs first. This method focuses strongly on user experience and input. Plus, it makes the software highly responsive to customer feedback. Agile seeks to release software cycles quickly, to respond to a changing market. This requires a strong team with excellent communication. However, if not implemented carefully, it can also lead to a project going off-track by relying too heavily on customer feedback.

Advantages
It provides a responsive approach to the development of software since it involves user and customer input.
Agile is intended for everyone to focus on one task at a time.
When compared to the waterfall model, the Agile model has a reduced scope which results in better time allocation and estimation.
Disadvantages
The final product depends on the customer. If the customer is not clear on what is needed, the development team may move in the wrong direction.
If not implemented properly, it could lead to inadequate documentation which hinders technology transfer to new members.
Actors
There are three main roles or "actors":

Product Owner
Scrum Master
Development Team
Here's a brief overview of each:

Product Owner:
Responsible for maximizing the value of the product
Manages the product backlog
Defines and prioritizes user stories
Acts as the voice of the customer
Scrum Master:
Facilitates the Scrum process
Removes impediments for the team
Ensures Scrum practices are followed
Coaches the team on Agile principles
Development Team:
Cross-functional group responsible for delivering the product
Self-organizing and collaborative
Typically includes designers, developers, testers, etc.
Works together to complete sprint goals
These roles work together to ensure efficient and effective product development using the Scrum framework.

Real World Application

Below is a short list of companies that implement the Agile development model:

Philips
Philips is one firm that has adopted Agile principles. After numerous changes to management structure, the firm introduced several Agile coaches that went to deploy Scrum principles such as Scrum boards and breaking down teams into smaller ones. As a result of changes like this, teams could react to situations quicker, bureaucracy was removed, and it was ultimately easier for these smaller teams to take responsibility for their respective products.

VistaPrint
VistaPrint is the go-to marketing company for small businesses. The company performed some analysis of their existing waterfall methodology and found that teams were taking more than 60 days to move from the “ideation phase” to product delivery. The 60-day cycle only amounted to 40 hours of actual work.

The company looked further into why this was the case. They discovered that some of their processes were resulting in unusually large project lead times.

VistaPrint decided to switch to Agile practices with a focus on decreasing project lead time. VistaPrint was able to optimize their business processes and saw their Lead Time decrease from 40 days down to 15.

Implementation
What are the 4 core values of Agile?
The Agile Manifesto outlines 4 Core Values and 12 Guiding Principles which serve as a North Star for any team adopting an Agile methodology.

The 4 Core Values of Agile are:

Individuals and interactions over processes and tools
As sophisticated as technology gets, the human element will always serve as an important role in any kind of project management
Relying too heavily on processes and tools results in an inability to adapt to changing circumstances.
Working software over comprehensive documentation
As important as documentation is, working software is more. This value is all about giving the developers exactly what they need to get the job done, without overloading them.
Customer collaboration over contract negotiation
Your customers are one of your most powerful assets.
Whether internal or external customers, involving them throughout the process can help to ensure that the end product meets their needs more effectively.
Responding to change over following a plan
This value is one of the biggest departures from traditional project management.
Historically, change was seen as an expense, and one to be avoided.
Agile allows for continuous change throughout the life of any given project. Each sprint provides an opportunity for review and course correction.
Key components of Agile project management
User stories
Put simply, a user story is a high-level definition of a work request.
It contains just enough information so the team can produce a reasonable estimate of the effort required to accomplish the request.
This short, simple description is written from the user’s perspective and focuses on outlining what your client wants (their goals) and why.
The user story follows the general format "As a user I want to functionality so that motivation.
Example: As a manager, I want to be able to understand my team members' progress, so I can better report our success and failures.
Sprints
Sprints are a short iteration, usually between one to three weeks to complete, where teams work on tasks determined in the sprint planning meeting. As you move forward, the idea is to continuously repeat these sprints until your product is feature ready.
Once the sprint is over, you review the product see what is and isn’t working, make adjustments, and begin another sprint to improve the product or service.
Sprints are implemented in the Agile framework Scrum but not in some other frameworks like Kanban.
Stand-up meetings
Daily stand-up meetings (under 10 minutes), also known as “daily Scrum meetings,” are a great way to ensure everyone is on track and informed.
These daily interactions are known as “stand up” because the participants are required to stay standing, helping to keep the meetings short and to the point.
Agile board
An Agile board helps your team track the progress of your project.
This can be a whiteboard with sticky notes, a simple Kanban board, or a function within your project management software.
Backlog
As project requests are added through your intake system, they become outstanding stories in the backlog.
During Agile planning sessions, your team will estimate story points to each task.
During sprint planning, stories in the backlog are moved into the sprint to be completed during the iteration.
Managing your backlog is a vital role for project managers in an Agile environment.
What are steps in the Agile methodology?
The goal of Agile is to produce shorter development cycles and more frequent product releases than traditional waterfall project management. This shorter time frame enables project teams to react to changes in the client’s needs more effectively.

You can use a few different Agile frameworks—Scrum and Kanban are two of the most common. Each framework implements Agile in different ways. For example, Kanban teams use visuals to improve work-in-progress while Scrum teams reflect on wins-and-losses to continuously improve.

However, each Agile methodology tends to follow the same basic process, which includes:

Project planning
Like with any project, before beginning your team should understand the end goal, the value to the organization or client, and how it will be achieved.
You can develop a project scope here, but remember that the purpose of using Agile project management is to be able to address changes and additions to the project easily, so the project scope shouldn’t be seen as unchangeable.
Product roadmap creation
A roadmap is a breakdown of the features that will make up the final product. This is a crucial component of the planning stage of Agile, because your team will build these individual features during each sprint.
At this point, you will also develop a product backlog, which is a list of all the features and deliverables that will make up the final product. When you plan sprints later on, your team will pull tasks from this backlog.
Release planning
In traditional waterfall project management, there is one implementation date that comes after an entire project has been developed.
When using Agile, however, your project uses shorter development cycles (called sprints) with features released at the end of each cycle.
Before kicking off the project, you’ll make a high-level plan for feature releases and at the beginning of each sprint, you’ll revisit and reassess the release plan for that feature.
Some Agile frameworks, like Scrum, implement the following processes:

Sprint planning
Before each sprint begins, the stakeholders need to hold a sprint planning meeting to determine what will be accomplished by each person during that sprint, how it will be achieved, and assess the task load.
It’s important to share the load evenly among team members so they can accomplish their assigned tasks during the sprint.
You’ll also need to visually document your workflow for team transparency, shared understanding within the team, and identifying and removing bottlenecks.
Daily stand-ups
To help your team accomplish their tasks during each sprint and assess whether any changes need to be made, hold short daily stand-up meetings.
During these meetings, each team member will briefly talk about what they accomplished the day before and what they will be working on that day.
These daily meetings should be only 15 minutes long. They aren’t meant to be extended problem-solving sessions or a chance to talk about general news items. Some teams will even hold these meetings standing up to keep it brief.
Sprint review and retrospective
After the end of each sprint, your team will hold two meetings:
First, you will hold a sprint review with the project stakeholders to show them the finished product.
This is an important part of keeping open communication with stakeholders.
An in-person or video conference meeting allows both groups to build a relationship and discuss product issues that arise.
Second, you will have a sprint retrospective meeting with your stakeholders to discuss what went well during the sprint, what could have been better, whether the task load was too heavy or too light for each member, and what was accomplished during the sprint.
If your team is new to Agile project management, don’t skip this essential meeting.
It helps you gauge how much your team can tackle during each sprint and the most efficient sprint length for future projects.
Summary

The AGILE model was designed by developers to put customer needs first.
This method focuses strongly on user experience and input.
This solves much of the problems of older applications that were arcane and cumbersome to use.
Plus, it makes the software highly responsive to customer feedback.
Advantages
It provides a realistic approach to the development of software.
It requires minimum resources.
It provides greater flexibility to software developers.
It requires little planning.
Disadvantages
The final product depends on the customer. If the customer is not clear on what is needed, the development team may move in the wrong direction.
It does not generate adequate documentation which hinders technology transfer to new members.


## Scrum Ceremonies
Learning Objectives

After completing this module, associates should be able to:

List the four Scrum Ceremonies
Explain each ceremony..
Description

If you’re new to the Agile approach, or the Scrum methodology in particular, you might not know much about Scrum ceremonies. Here’s an overview of the four essential Scrum ceremonies and how they can help your product development team stay agile and efficient.

What are the four Scrum ceremonies?
Scrum ceremonies—also known as Scrum events, Scrum meetings, or Agile ceremonies—are a key component of the Scrum process.

The four Scrum ceremonies are:

Sprint planning
Daily stand-up
Sprint review
Sprint retrospective
Sprint planning occurs before the sprint, daily stand-up meetings take place during the sprint, and the review and retrospective come after the sprint has ended.

These four Scrum ceremonies form the backbone of the Scrum methodology. Proper planning, check-ins, and reflection empower teams to stay efficient, motivated, and mindful of their progress. Plus, reviewing each sprint’s challenges, accomplishments, and lessons helps make subsequent sprints more effective. Now, let’s take a closer look at each of the Scrum ceremonies.

Sprint planning
If your team starts sprinting without proper planning, everyone will end up running in different directions. The first Scrum ceremony—sprint planning—creates a road map for the upcoming product development sprint.

The sprint planning session doesn’t have to be time-consuming; an hour or two should be enough to get everyone on the same page for a one- or two-week sprint. The Scrum team, the product owner, and the Scrum Master should be present at the planning session.

During the sprint planning session, the team consults the backlog, which is a list of all desired features and bug fixes that the team can select from as they determine what to accomplish during the sprint.

During the sprint planning meeting, the Scrum team will estimate how many items from the backlog they can complete with their existing resources. The Scrum Master facilitates the meeting, the product owner clarifies the details and requirements of the backlog items, and the team members define the work and effort necessary to complete each backlog item chosen for the sprint. A centralized work management platform may be used to organize the full backlog as well as each individual sprint.

Daily stand-up
Stand-up meetings—also called the daily Scrum—ensure the sprint is proceeding effectively. Traditionally, Scrum Masters keep the daily Scrum to no longer than 15 minutes. These stand-up meetings are informal gatherings designed to help identify any roadblocks and allow team members to describe their current tasks, goals, and obstacles.

Ideally, the daily Scrum should take place at the beginning of the day with the Scrum Master, product owner, and complete Scrum team. While many teams choose to have their daily meetings in-person, remote get-togethers are also effective.

One way to prevent daily Scrums from exceeding 15 minutes is to ensure everyone is on the same page before the meetings begin. Many Scrum teams use a modern work management platform to help track each component’s progress. If the Scrum Master or product owner notices that someone is falling behind, they can ask about the issue even before the daily meeting.

The daily Scrum keeps each team member accountable. While leaders should never belittle or embarrass team members at standups, the requirement to report progress each day can motivate developers to stay efficient and productive.

Sprint review
After the team has completed the sprint, it’s time to meet with the stakeholders in a sprint review meeting (also called an “iteration review”). The Scrum team, Scrum Master, and product owner meet with other teams, managers, and executives to showcase what they accomplished during the sprint. Ideally, the sprint review allows each team member to participate. The review’s tone should be enthusiastic and positive—it’s an excellent opportunity to celebrate the team’s accomplishments.

Of course, Scrum teams should also solicit feedback from stakeholders. In many cases, teams may need to change or update the products they built during the sprint. Revision requests aren’t a bad thing; a continually iterative, evolving process is the essence of the Scrum philosophy.

The sprint review should last as long as necessary to fully demonstrate the team’s new technology and have a productive conversation with stakeholders. After the sprint review, Scrum teams move on to the sprint retrospective.

Sprint retrospective
The final Scrum ceremony is the sprint retrospective. During this last phase, the development team, the Scrum Master, and the product owner meet to discuss the sprint’s successes, challenges, and insights. The retrospective usually lasts around an hour.

Using feedback from stakeholders and the Scrum Master, the team should identify how it can improve its processes to have more effective sprints in the future. Agility and adaptability are core values of the Scrum process, so teams should strive to identify potential improvements without blame or judgment.

Real World Application
How to keep your Scrum ceremonies effective.
One way to ensure your Scrum sprints and ceremonies are as efficient as possible is to designate a Scrum Master. The Scrum Master is a team member responsible for ensuring the team stays organized and on task during its sprint. Scrum Masters coordinate daily Scrums and ensure everyone is on the same page. Some Scrum Masters rely on work management software to help track questions, accountability, and progress.

Despite being called a “master,” Scrum Masters don’t have direct authority over other team members. Instead, the Scrum Master is a servant-leader who works alongside product developers. You can think of a Scrum Master as more of a coach or a guide than a direct supervisor in a hierarchical structure.

Keep your team improving with Scrum ceremonies.
Scrum ceremonies are a vital part of the Scrum process. Don’t neglect them just because your team is short on time. According to the Scrum Alliance, 86% of respondents have initial planning sessions, 87% hold daily Scrum meetings, and 81% take the time for a Scrum retrospective. Scrum ceremonies empower teams to plan, maintain, and learn from sprints, ensuring a cycle of continuous improvement and steady productivity.

Implementation

Implementation for the Scrum Ceremonies topic

This video explains the four Scrum Ceremonies: Scrum Ceremonies

Summary

The four Scrum ceremonies are:
Sprint planning: Sprint planning creates a road map for the upcoming product development sprint.
Daily stand-up: Stand-up meetings ensure the sprint is proceeding effectively. Traditionally, Scrum Masters keep the daily Scrum to no longer than 15 minutes
Sprint review: After the team has completed the sprint, it’s time to meet with the stakeholders in a sprint review meeting
Sprint retrospective: During this last phase, the development team, the Scrum Master, and the product owner meet to discuss the sprint’s successes, challenges, and insights.
Sprint planning occurs before the sprint, daily stand-up meetings take place during the sprint, and the review and retrospective come after the sprint has ended.


## Story Pointing And Burndown Charts
Learning Objectives

After completing this module, associates should be able to:

Define Story Pointing
Explain how the Fibonacci Sequence is used to calculate story points.
Description
Agile Stories: A Refresher
Before we talk about story points in Agile, let’s remind ourselves what stories are in the Agile context.

A story is a simple, general explanation of a specific software feature. Stories are created from the customer or end-user’s perspective and show how the particular software feature will benefit the customer.

Here’s an example of a story: “I manage a small accounting team at work, and I need a way for us to easily share spreadsheets in real-time while being able to communicate with each other simultaneously.”

What are Story Points?
Story points (SP) are a type of comparative measurement unit. They are used by agile teams to provide estimates of the total amount of work required to completely implement items from the product backlog, or user stories. As a result, estimating story points actually entails estimating work and giving each backlog item a point value.

Put another way, it’s a numeric value that helps the development team understand the effort required to complete a story. The team assigns story points based on the work's complexity, amount, and uncertainty.

Story points in Agile are abstract measurements that developers use instead of hours. Points are relative values, so a story with a value of four is twice as hard as a story with a value of two. The actual numbers don’t matter — you could assign values between 1,000,000 and 5,000,000 if you want. Instead, you want to give the team an idea of the story’s relative difficulty. Story points tell you how much effort a given story will take to resolve.

Story Points vary depending on:
The team and individual member:
To the relative character of story points, there is an additional dimension: the effectiveness of the team responsible for estimations. One person differs from another based on a variety of criteria, including skill level, experience, and familiarity with specific duties. Accordingly, a backlog item may be worth five points for one side and only three for another. The same holds true for individual team members; junior and senior will likewise view the "size" of the effort required for a particular activity in various ways.

Another Value:
Because a story point may only exist in relation to another value, it is a relative unit. For example, if a task requires two narrative points to finish, it represents twice as much work as a task that just requires one story point. Comparably, a task estimated at one story point will require one-third the effort compared to a task assessed at three story points. To express the effort, you might utilize Fibonacci sequence values rather than linear 1 and 2. It is up to you and your group which option you choose. Rather than the exact numbers you allocate to Agile story points, what counts most is the ratio between the story points and the relative values.

What is a Burndown Chart?
An illustration of a team's progress through a customer's user stories in project management is provided by a burndown chart. Using an agile sprint or iteration, this tool records a feature's description from the viewpoint of the end user and compares the total effort to the amount of work.

In order to depict the past and future of the project, the amount of work still to be done is displayed on a vertical axis, and the amount of time since the project's start is displayed horizontally. As it is updated frequently for accuracy, the burndown chart is visible to all members of the agile project management team.

Types of Burndown Charts:
There are two burndown chart variants:

A Sprint Burndown: A sprint burndown shows the amount of work left in the iteration.
A Product Burndown: A product burndown shows the amount of work left on the project plan.
What is Fibonacci Agile Estimation?
Agile estimation refers to a way of quantifying the effort needed to complete a development task. Many agile teams use story points as the unit to score their tasks. The higher the number of points, the more effort the team believes the task will take.

The Fibonacci sequence is one popular scoring scale for estimating agile story points. In this sequence, each number is the sum of the previous two in the series. The Fibonacci sequence goes as follows: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89… and so on.

Fibonacci agile estimation refers to using this sequence as the scoring scale when estimating the effort of agile development tasks.

Why Use the Fibonacci Sequence for Agile Estimation?
Agile consultant Mike Cohn uses a helpful metaphor to explain why the Fibonacci sequence works well for estimating story points.

In his article on Fibonacci agile estimation, Cohn asks us to imagine holding a one-kilogram weight (2.2 pounds) in one hand and a two-kilogram weight (4.4 pounds) in the other. Without looking, could we determine which hand had a more substantial weight? Yes, easily. One is twice as heavy as the other.

But if those two weights were 20kg and 21kg, Cohn explains, we’d have more difficulty knowing which was heavier. In both scenarios, the difference in weight is one kilogram. But as we get into 20kg territory (45 pounds), the difference in the weights will need to be greater so that our brains can perceive it.

This is why Cohn recommends using the Fibonacci sequence for estimating agile story points. The numbers your team can choose from takes larger jumps as the sequence progresses, but they grow at a consistent rate — each number representing about a 60% jump. Cohn points out that even as the numbers get huge, our brains can still perceive the difference if the next number is 60% greater than the previous one.

How Does Fibonacci Agile Estimation Work in Practice?
Imagine your team wanted to estimate the effort needed to build a new widget in your app. Everyone agreed that this task would rate a high level of difficulty and take a long time to complete it.

But now imagine your team used a linear, even-number scoring scale for story point estimation: 2, 4, 6, 8, 10… up to 50. Even if everyone agreed this new widget would be on the high end of the point scale, could you all agree whether to assign it 42 points? How about 46? Or 48?

As the numbers get higher on this scoring scale, you will find it more difficult to determine the right number because there are too many options, and the numbers at the high end aren’t distinct enough from each other.

Moreover, remember that the goal with these story points is only to estimate the level of effort. There is no reason to try to zero in on the perfect story-point score. These numbers are just a guide to help your team gauge how much time a task will take and how many resources you will need to devote to it. This is why no viable agile estimation scale uses decimals.

If your team was using the Fibonacci sequence to estimate the effort to develop this new widget, you would have only a few numbers to choose from at the top end of the scale: 34, 55, or 89. (This is where your Fibonacci agile scale would stop.)

If you do the math, you’ll see Cohn is correct that each of these numbers jumps about 60% above the previous one in the sequence. And as you can see, it would be much easier to reach a consensus on whether your widget represented a 34-point task, or 55 points, or 89.

Real World Application
Why Should Teams Use Story Points in Agile?
Story points in Agile benefit development teams and product owners alike.

The effort is the result of several things coming together, and your team needs to take those into account when estimating story points. They consist of:

Amount of work: Since more work requires more effort, more work equates to more story points.
Work's complexity: An effort's level of difficulty can have a big impact.
Uncertainties and risks: Your team may need to make further efforts to handle potential risks, such as unclear requirements, legacy code lacking automated tests, and external stakeholders.
Stories points are also relative, so over time, repetition may have a role in determining how much weight you give certain actions. As a result, the "size" of the finished project will also depend on how experienced your team is with handling equivalent or related jobs.

The benefits to development teams can also vary:

Benefits for development teams:

The team gets a better grasp of what’s required of them, making it easier to develop a sound implementation strategy.

The team won’t over plan, so they have a better chance of finishing an increment.

The team knows how much to plan in a sprint, thereby letting them work at a sustainable pace.

The team can create a reasonable estimate without having to commit to a specific timeframe.

Benefits for product owners:

They can better assess an item’s return on investment (ROI) or the value for the money.

Owners can better understand the technical risk associated with their more oversized items.

They can better forecast the product’s longer-term delivery.

However, team members should be careful to avoid some common pitfalls:

Don’t give story points to small tasks that the team can easily estimate timewise.

Story point creation is a team effort, not a one-person show. Make sure the team stays engaged, and everyone contributes.

Don’t average story points.

Don’t let one variable influence the entire process — story points measure the whole picture (e.g., risk, complexity, uncertainty).

Who Uses a Burndown Chart?
Team managers use burndown charts as a way to see the overall progress of the project and the work remaining. Developers may also use burndown charts to measure progress or to show the team what’s left to do in an Agile sprint.

Managers tend to track high-level requirements, while developers tend to track specific tasks. This is because managers will need a higher-level summary but the developers will want the specific tickets or tasks that are associated with satisfying those requirements.

Components of a Burndown Chart:
Although the specifics can vary, it’s common to see the below sections of a burndown chart.

Axes:

The horizontal axis (X-axis): The X-axis typically tracks the time remaining until the project’s deadline.
The vertical axis (Y-axis): The Y-axis will track the amount of work remaining in the project. This will use different measurements depending on the project and what is being tracked. Some examples of measurement of progress are the number of Jira tickets there are left to complete, how many tasks need to be finished and how many requirements are left to satisfy.
Ideal Work Remaining Line:
The ideal work remaining line, as its name implies, shows the amount of work that a team has left to complete under ideal circumstances at a given point in the project or sprint. Project managers estimate this baseline and draw a straight line across the burndown chart using historical data. There should always be a negative slope on the optimal work remaining line.

Actual Work Remaining Line:
The real work remaining line shows the amount of work that a team has left to do at any given stage of the project. This is a realistic representation of the team's performance as opposed to an estimate, like the ideal work remaining line. As the team moves forward and finishes user stories, a boundary is established. Since teams work at varying speeds to finish projects, actual work remaining lines are typically not straight.

Implementation
Estimating with Story Points:
How are story points calculated?
Story points are essentially determined by comparing a project's features to those of a prior, comparable project. With this kind of approach, the team can comprehend the complexity of a given feature. They can also use it to assign a number that represents a specific feature effort.

What happens if this is the team's first time estimating effort?

A team must choose a baseline tale in such a situation. It doesn't have to be the tiniest one; just something that all the team members can relate to. After establishing the baseline tale, the group can start estimating story points by contrasting them with the baseline.

Let’s illustrate this approach using Circles as a simple example.

Story Point Illistration

Circle A is clearly the smallest one. B is about double the size of A. Whereas C is roughly four, maybe five times bigger than A, and two times bigger than B. Here, we do not have precise absolute values to assign to each circle. Please note that exact absolute values were not provided, but the sizes were determined relative to Circle A as the baseline.

The following video can provide you with additional insight into agile story points:

Agile Story Points

Story Points Sizing:
With an agile methodology, teams work with user stories rather than circles. Teams are able to assess the relative size of each user narrative and give them story points by comparing them with one another. Naturally, this approach works with requirements expressed in any other format as well as with user stories.

You may now be asking how the relativity ratio between individual user tales should be scaled. Naturally, there must be a greater disparity between them the larger the user story. If not, it would be challenging to distinguish between objects and estimate how much larger they actually are. Let's use circles once more to demonstrate this idea.

Story Point Illistration 2

A and B, as well as B and D, differ significantly from one another. However, if you compare D and E, the differences between them are so slight that it is impossible to determine how much they differ. This is the reason why several Agile relative estimating methods represent narrative point values using various scales, or sizes:

Fibonacci sequence: (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89,...), where each number equals the sum of the two numbers before it.

Benefits:
Non-Linear Scale: The gaps between consecutive numbers in the Fibonacci sequence increase as the numbers grow. This non-linear scale reflects the uncertainty and variability inherent in estimating the effort or complexity of tasks.
Reflects Relative Sizing: It emphasizes the relative sizing of tasks rather than an absolute scale, making it suitable for expressing the inherent uncertainty in estimating work.
Linear sequence in which the same amount is added or subtracted to go from one term to the next (e.g., 1, 2, 3, 4, 5; or 3, 6, 9, 12, 15, 18,...

Benefits:
Simplicity: Linear sequences provide a straightforward and simple scale. The effort or complexity increases uniformly with each increment.
Ease of Understanding: It can be easier for some teams, especially those new to agile practices, to work with a linear scale because of its simplicity.
Considerations for Choosing Between Fibonacci Sequence vs Linear Sequence:
Uncertainty and Non-Linearity:
Fibonacci: Recognizes that as tasks become larger, there's often an increasing level of uncertainty and non-linearity in effort.
Linear: Assumes a more straightforward and linear relationship between the size of a task and the effort required.
Ease of Use:
Fibonacci: Some teams find the Fibonacci sequence more intuitive for capturing the relative complexity of tasks, especially when the differences between task sizes are not consistent.
Linear: Offers simplicity and ease of use, particularly when team members prefer a straightforward progression.
Psychological Factors:
Fibonacci: Can sometimes encourage teams to think in terms of general categories rather than precise values, fostering a more flexible mindset.
Linear: May make it easier for teams to make precise estimates, but could potentially lead to overemphasis on exact values.
Ultimately, the choice between the Fibonacci sequence and a linear sequence depends on the team's preferences, the nature of their work, and their comfort with the chosen scale. Many teams find success with either approach, and some even use hybrid scales that combine elements of both sequences. The key is to establish a scale that works well for the team and facilitates effective estimation and planning.

How to Use a Burndown Chart in Agile & Scrum:
To plan and carry out projects, agile project management uses agile sprints. These are brief work periods known as sprints, during which a team works to achieve predetermined objectives that were first discussed and decided upon at the sprint planning meeting. Agile project managers find that burndown charts are quite useful since they help them stay on top of remaining work, assess performance against predetermined standards, and rapidly identify any delays. The following are instances of how an agile or scrum project can be managed with the aid of a burndown chart.

Create a work management baseline to compare planned vs. actual work
Complete a gap analysis based on discrepancies
Get information for future sprint planning meetings
Reallocate resources and manage tasks to complete sprints on time
How to Create a Burndown Chart?
Summing the Sprint Backlog estimations for each day of the Sprint will allow you to see how much work is left, as seen in this graph. The total quantity of work left in the Sprint Backlog is the amount of work that is still unfinished for each Sprint. Using the daily totals you keep track of, make a graph that illustrates the amount of work left over over time.

Burndown Chart Example:
Duration: 5 days
Sprint Backlog: 8 tasks
Velocity: 80 available hours
Step 1 – Create Estimate Effort

Suppose your ideal baseline for using the available hours over the sprint. So in the simplest for this is the available hours divided by number of days. In this example, 80 hours over 5 days equating to 16 hours a day. In order to create the project burn-down chart, the data needs to be captured as a daily running total starting with 80 hours than 64 hours left 1 (80 – 16) at end of day, 48 hours left at end of day 2, etc.

Burndown- Estimate effort:
Day	Effort Remaining
0	80
1	64
2	48
3	32
4	16
5	0
Step 2 – Track Daily Process

The daily progress is then captured in the table against each task. It is important to remember that the value captured for each day is the estimated effort to complete the task, not the actual effort.

Task	Hours	Day - 1	Day - 2	Day - 3	Day - 4	Day - 5	Total
Task - 1	10	3	2	0	1	4	10
Task - 2	10	3	2	0	1	4	10
Task - 3	10	3	2	0	1	4	10
Task - 4	10	3	2	0	1	4	10
Task - 5	10	3	2	0	1	4	10
Task - 6	10	3	2	0	1	4	10
Task - 7	10	3	2	0	1	4	10
Task - 8	10	3	2	0	1	4	10
Step 3 – Compute the Actual Effort

The total remaining effort needs to be captured at the end of each day. This is the total (sum) of all of the estimated time remaining at the end of each day.

Days	Actual Effort	Effort Remaining
0	80	80
1	56	64
2	40	48
3	40	32
4	32	16
5	0	0
Step 4 – Obtain the Final Dataset

When the data is available, the project burn-down chart can be created. This is relatively simple using the line chart option available within Excel.

Days	Actual Effort	Effort Remaining
0	80	80
1	56	64
2	40	48
3	40	32
4	32	16
5	0	0
Highlight the summary table that contains the daily total for baseline effort and estimated effort. You should also capture the heading of time period (Day 0, Day 1, etc).

Step 5 – Plot the Burndown using the Dataset

It is very simple to create a project burn-down chart as following, as long as you know what data you are tracking.

Burndown chart example:
Burndown Chart Example

Summary

Story points are units of measurement used to determine how much effort is required to complete a product backlog item or any other piece of work.

The team assigns story points based on the work's complexity, amount, and uncertainty.
Story Points are a numeric value that helps the development team understand how challenging the story is.

The Fibonacci sequence is one popular scoring scale for estimating agile story points.

In this sequence, each number is the sum of the previous two in the series.

The Fibonacci sequence goes as follows: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89… and so on.

Each of these numbers jumps about 60% above the previous one in the sequence after the first few terms which makes the sequence ideal for assigning story points since numbers on the high end are more distinct.


## Waterfall
Learning Objectives

After completing this module, associates should be able to:

Define the Waterfall development cycle.
Elaborate on the steps in the Waterfall cycle.
Explain how the Waterfall cycle is used in developing software.
Description
Waterfall
The Waterfall SDLC model is the classic method of development. As each phase completes, the project "spills" over into the next step like a waterfall. This is a tried-and-tested model, and it has worked in the past. The steps move in a linear fashion where each step needs to be fully completed before the next step takes place.

Advantages
It is a simple model that can be followed by developers in developing software.
It is easy to implement this model.
All the phases are clearly defined.
It works well in short projects.
Disadvantages
It is not ideal for complex software development projects.
It consumes a lot of time because each phase should be completed before moving to the next one.
Since each phase needs to be completed before the next one proceeds, the software product is not available for testing until later stages. This could lead to issues and increased technical debt.
Real World Application
Examples of the Waterfall Model
In the olden days, Waterfall model was used to develop enterprise applications like Customer Relationship Management (CRM) systems, Human Resource Management Systems (HRMS), Supply Chain Management Systems, Inventory Management Systems, Point of Sales (POS) systems for Retail chains etc.

With the evolution of technology, there were cases where large-scale enterprise systems, with the waterfall model as the default choice, were developed over a period of two to three years but became redundant by the time they were completed. Slowly, these enterprise systems switched over to more flexible and less expensive models, but the waterfall model continued to be preferred in systems where:

A human life is at stake and a system failure could result in fatalities
Money and time are secondary factors and what matters more is the safety and stability of a project
Military and aircraft programs where requirements are declared early on and remain constant
Projects with an extremely high degree of oversight and/or accountability such as those in the sectors of banking, healthcare and control systems for nuclear facilities
Implementation

The original Waterfall method's steps included Requirements Determination, Design, Implementation, Verification, and Maintenance.

Image of waterfall method

Other models change the Requirements phase into the Idea phase, or break the Requirements phase out into Planning and Analysis. Furthermore, some models further break the Design phase out into Logical and Physical Design sub-phases. However, the basic underlying principles remain the same.

Below, the major stages of the Waterfall method are elaborated:

The Waterfall method makes the assumption that all requirements can be gathered up front during the Requirements phase.
Communication with the user is front-loaded into this phase, as the Project Manager does his or her best to get a detailed understanding of the user's requirements. Once this stage is complete, the process runs "downhill" like a waterfall, where the water starts at the top and flows downward.
The Design phase is best described by breaking it up into Logical Design and Physical Design sub-phases.
During the Logical Design phase, the system's analysts makes use of the information collected in the Requirements phase to design the system independently of any hardware or software system.
Once the higher-level Logical Design is complete, the systems analyst then begins transforming it into a Physical Design dependent on the specifications of specific hardware and software technologies.
The Implementation phase is when all of the actual code is written. This phase belongs to the programmers in the Waterfall method, as they take the project requirements and specifications, and code the applications.
The Verification phase was originally called for to ensure that the project is meeting customer expectations.
Software testing takes place during the Verification phase.
The project is rolled out to the customer, and the Maintenance phase begins.
During the Maintenance phase, the customer is using the developed application.
As problems are found due to improper requirements determination or other mistakes in the design process, or due to changes in the users' requirements, changes are made to the system during this phase.
Summary

The Waterfall SDLC model is the classic method of development.
As each phase completes, the project spills over into the next step.
Advantages
It is a simple model that can be followed by developers in developing software.
It is easy to implement this model.
All the phases are clearly defined.
It works well in short projects.
Disadvantages
It is not ideal for complex software development projects.
It consumes a lot of time because each phase should be completed before moving to the next one.
It does not enhance customer integration during the development process.


## Agile Vs Waterfall
Learning Objectives

After completing this module, associates should be able to:

Compare and contrast the Agile methodology versus the Waterfall methodology.
Description

Agile and waterfall are two distinctive methodologies of processes to complete projects or work items. Agile is an iterative methodology that incorporates a cyclic and collaborative process. Waterfall is a sequential methodology that can also be collaborative, but tasks are generally handled in a more linear process.

Following the agile methodology, your project will move through a series of cycles throughout the lifetime of the project. The development phase, review, feedback, and then approval of the work item – either yes or no. If yes, implement and complete the task. If no, record and make any necessary changes, track and adjust the backlog or prioritization to reflect the newly acquired knowledge, and then move onto the next task or sprint.

Following the waterfall methodology is a simpler process of moving tasks through the phases of defining requirements, designing the implementation, implementing the work item, verification of implementation and quality assurance, and then maintenance of the feature in the end.

Selecting the right methodology for your projects will depend on preference and the nature of each project. Some projects require a more iterative process and others require a more sequential approach.

Real World Application
The advantages of waterfall
Requires less coordination due to clearly defined phases sequential processes
A clear project phase helps to clearly define dependencies of work.
The cost of the project can be estimated after the requirements are defined
Better focus on documentation of designs and requirements
The design phase is more methodical and structured before any software is written
The disadvantages of waterfall
Harder to break up and share work because of stricter phase sequences teams are more specialized
Risk of time waste due to delays and setbacks during phase transitions
Additional hiring requirements to fulfill specialized phase teams whereas agile encourages more cross-functional team composition.
Extra communication overhead during handoff between phase transitions
Product ownership and engagement may not be as strong when compared to agile since the focus is brought to the current phase.
The advantages of agile project management
Faster feedback cycles
Identifies problems early
Higher potential for customer satisfaction
Time to market is dramatically improved
Better visibility / accountability
Dedicated teams drive better productivity over time
Flexible prioritization focused on value delivery
The disadvantages of agile
Critical path and inter-project dependencies may not be clearly defined as in waterfall
There is an organizational learning curve cost
True agile execution with a continuous deployment pipeline has many technical dependencies and engineering costs to establish
Implementation

The following video compares and contrasts the differences between Agile and Waterfall: Agile vs. Waterfall

Summary

Agile and waterfall are two distinctive methodologies of processes to complete projects or work items.
Agile is an iterative methodology that incorporates a cyclic and collaborative process.
Waterfall is a sequential methodology that can also be collaborative, but tasks are generally handled in a more linear process.
Following the agile methodology, your project will move through a series of cycles throughout the lifetime of the project.
Following the waterfall methodology is a simpler process of moving tasks through phases, generally in a linear fashion.


## Linux File Commands Using Gitbash
``` TODO MARKDOWN FORMATTING FIX
Learning Objectives

After completing this module, associates should be able to:

List common file handling commands in UNIX/Linux.
Perform basic file manipulation functions using a command-line interface.
Description

Below are some useful UNIX/Linux commands for handling files using GitBash. Always remember that UNIX/Linux commands are case-sensitive.

ls
This command lists directory contents.
It lists files and directories.
Some versions may support color-coding. The names in blue represent the names of directories.
$ ls -l | more
This command helps paginate the output so you can view page by page. Otherwise the listing scrolls down rapidly. You can always use ctrl + c to return to the command line.
Note: more is not supported in GitBash for Windows.
$ ls –l 
This command shows more details of the contents in the directory. It lists the following:

Permissions associated with the owner of the file
The group associated with the file
The size of the file
The timestamp
The name of the file
cd

This changes the current directory. Note that it uses a forward slash.
$ cd /var/log
pwd
One way to identify the directory you are working in is the pwd command. It displays the current working directory path and is useful when directory changes are made frequently.
$ pwd
mkdir
The mkdir command makes a directory. The command is written as follows: mkdir [directory name]
$ mkdir myproject
cat
The cat command can be used to create, view, and concatenate files. The example below creates a new file called "newfilename".
$ cat > newfilename
When you type in the command below, the command prompt will disappear. This is because of the ">" (greater than symbol) which is known as "redirection." At this point the keyboard output is being re-directed into the file "newfilename". Type the text that you would like to place in the file. When you are done entering text, press the Control and D keys on your computer keyboard at the same time to save the file and return to the command prompt.

Another way to create a file using cat:
The cat example below will copy the file "sourcefilename" into the file "destinationfilename".
$ cat sourcefilename > destinationfilename
touch
The touch command creates an empty file for editing later. The command below creates an empty file called "filename". From there you can use a terminal-based editor like "vi" to edit the file.
$ touch filename
echo
The echo command prints the strings that are passed as arguments to the standard output, which can be redirected to a file. To create a new file run the echo command followed by the text you want to print and use the redirection operator > (as explained with cat above) to write the output to the file you want to create.
The command below will create the file "file1.txt" containing the text "Some line".
$ echo "Some line" > file1.txt 
grep
The grep command can be used to search files and directories (and subdirectories) for a string. The example below searches the file "filename" for the string "Aaron".
$ grep Aaron filename
diff
The diff command compares two files line by line to find differences. The output will be the lines that are different.
$ diff file1.txt file2.txt
Real World Application
Why use the command line?
GitBash, which we use for code repositories, uses UNIX/Linux based commands in a terminal (text-based) environment. Therefore, it is important to know why using the command line is crucial:

When Unix Was Developed, There Was No GUI
While Linux is not Unix, as it has no code from the system, its behavior is based on it, including its use of the command line. When Unix was developed at Bell Labs in the late '60s and early '70s, there was no such thing as a graphical user interface.
Command-line interfaces were natural for this type of terminal. The use of text terminals was also a major reason why Unix developers preferred short command names, as they were faster to type.
Programming Tools Use the Command Line
Programmers have been the staunchest advocates of Linux because it has so many tools for them to get their work done: interpreters, compilers, and debuggers. And all of these tools run on the command line.
While you can call all of these from a graphical IDE, it's just a front end to a command line somewhere.
The Command Line Is Fast
A lot of Linux users love to claim that the Linux command line is faster than using a GUI.
Command-line programs start faster than graphical ones because there's less overhead.
The Command Line Works Everywhere, Including on Servers
One big reason that the command line has survived on Linux systems is that it works just about everywhere.
If the X-windows system didn't like your graphics card, a problem that was also more common on early Linux systems, you'll find yourself dumped at the console.
This means you can fall back on the command line when you need to.
Command-Line Programs Can Be Scripted
One big advantage of command-line programs over graphical ones is that programmers can automate them.
For example, if you wanted to copy all your text files to a directory, you'd use this line:
cp *.txt /example
A script could be written if there were a need for repeated file copying.
Implementation
Online Linux Emulators
To get hands-on experience using Linux commands, we can use this online Linux emulator:

JSLinux

Specifically, you should attempt the following commands (with appropriate parameters) in any order:

ls
cd
pwd
mkdir
cat
grep
Note: The emulators do support file/directory creation and deletion, so feel free to create/delete files as appropriate!

Summary

Below are some useful UNIX/Linux commands for handling files using GitBash:

ls: Used to list files and directories
cd: Changes the current directory
grep: Used to search files and directories for a String
pwd: Prints the name of the directory you are currently in
mkdir: Creates a new directory
diff: Compares two lines to find differences
```

## Moving and Deleting Files Using Gitbash
``` TODO FIX MARKDOWN
Learning Objectives

After completing this module, associates should be able to:

List commands for moving and deleting files in UNIX/Linux.
Perform basic file moving and deleting functions using a command-line interface.
Description

Below are some useful UNIX/Linux commands for moving and deleting files using GitBash. Always remember that UNIX/Linux commands are case-sensitive.

mv
The mv command moves a file or renames it. Some examples; inline comments denoted with //

$ mv file1 directory1 // moves 'file1' to 'directory1'
$ mv file1 file2 file3 dir1 // moves 'file1', 'file2', and 'file3' to 'dir1'
$ mv file1 file2 // renames 'file1' with the new name 'file2'. This can also be used to rename directories.
$ mv -i file1 directory1 // just as above, this will move 'file1' into 'directory1' but the -i flag will prompt the operator should the command result in overwriting an existing file.
$ mv -n file1 directory1 // similar to the above, however the -n flag will not move 'file1' to 'directory1' if it causes an overwrite.
$ mv -u file1 directory1 // The -u flag will only move 'file1' to 'directory1' if the source file is newer than the destination file.
$ mv -b file1 directory1 // The -b flag will create a backup of any existing destination file overwritten by 'file1'.
cp
The cp command copies a file. Some examples; inline comments denoted with //

$ cp second.txt third.txt // copies 'second.txt' into a file 'third.txt'
$ cp -i second.txt third.txt // -i stands for Interactive copying. With this option system first warns the user before overwriting the destination file.
$ cp -b second.txt third.txt // -b will create a backup of the destination file in the same folder
$ cp -f second.txt third.txt // -f stands for force; if the system cannot open the destination file then the destination file is deleted first before the copying proceeds.
$ cp -r directory1 directory2 // -r is for Recursive copying which is used for copying the contents of 'directory1', including all subdirectories, into 'directory2'.
$ cp -p second.txt third.txt // -p stands for preserve. The command preserves some characteristics of 'second.txt' in the destination file 'third.txt' including times of last modification and access, ownership, and file permissions.
rm
The rm command deletes files/directories. Some examples; inline comments denoted with //

$ rm file1 // deletes 'file1' in the current working directory.
$ rm -i file1 // -i stands for interactive; this prompts the operator before deleting each file.
$ rm -f file1 // -f forces deletion if a file is write protected. It will not remove a write-protected directory.
$ rm -r directory1 // -r (recursive) will delete all files in 'directory1' and all subdirectories of 'directory1' and their contents.
Real World Application
Advantages of using the Command Line Interface (CLI)
When using a command-line interface, you can use detailed commands more efficiently and faster than you can with a graphical user interface. It demonstrates the advantages of using a command-line interface in this case, as it can handle extremely repetitive tasks across a wide range of systems.
With the assistance of a program like the computer cli or code, it is easier for the user to control everything. The user’s interface is slow when they navigate through different icons. This enables CLI to operate more quickly as commands are directly delivered to the computer. CLI is preferred by many professionals due to its speed and performance.
All options and operations are invoked in consistent form, while with GUIs similar operations often appear on different menus with different interfaces and different applications have different approaches.
All options and operations are documented (or should be), meaning that it is no more difficult to perform a rare operation than a common one.
CLIs double as scripting languages (see shell script) and can perform operations in a batch processing mode without user interaction. That means that once an operation is analyzed, it can be saved in a script and consistently performed without further effort. With GUIs, users must start over at the beginning every time, as GUI scripting is more limited (and often nonexistent). Simple commands do not even need a script, as the completed command can usually be assigned a name and executed simply by typing that name into the CLI.
Implementation
Online Linux Emulators
To get hands-on experience using Linux commands, we can use this online Linux emulator:

JSLinux

Specifically, you should attempt the following commands (with appropriate parameters) in any order:

cp
mv
rm
Note: The emulators do support file/directory creation and deletion, so feel free to create/delete files as appropriate!

Summary

Here some useful UNIX/Linux commands for moving and deleting files using GitBash. Always remember that UNIX/Linux commands are case-sensitive.

mv: The mv command moves a files or directories.
cp: The cp command is used to copy files or group of files or directories.
rm: The rm command is used to remove objects such as files, directories, symbolic links and so on from the file system.
```

## Source Control Management(git,vcs,cvcs,dvcs)
Learning Objectives

After completing this module, associates should be able to:

Explain the following terms:
VCS
CVCS
DVCS
Explain how Git implements Source Control Management (SCM)
Description

Version control, also known as Revision control a.k.a. Source Control Management (SCM), is a process to manage a collection of source code and changes that provide you with many capabilities, such as:

Maintain multiple versions of code
An ability to go back to any previous version.
Developers can work in parallel.
Audit traceability with clear picture on whom, which, when, where, and what are the changes.
Synchronize the code.
Copy/Merge/Undo the changes.
Find out the difference between versions.
Provides full backup without occupying much space.
Review the history of the change.
Capable for both small and large scale projects.
Ability to share and work on the code across the globe.
In simpler words, a Version Control System (VCS) allows you to manage and keep track of all your source code, along with the evidence that all your changes are being stored in a repository.

Importance of Version Control Systems
Let’s take an example of an organization with a project team of five developers.

Two of them are in one location and the other three are in a different location. They received a project and started developing the code, and then come across the following situations when they were working on the project:

Each developer is working on one module at a time whereas others were waiting for developers to complete their tasks so that they can start working.
Whenever they complete their work and decide to deploy the code, they were each taking a full backup of the project and one day they couldn’t do it because the disk was full.
One day one of them deleted one module which they couldn’t bring back and had to rework on the same module.
They were unable to find out who has done what changes and when the changes were made.
They couldn’t do any experiment for the new feature without interfering with other’s work.
They need to send the code via email as they were seated at different locations.
These situations outline the need for a Version Control System which can help avoid all the above situations as well as provide benefits to the development environment.

A version control system ensures that all the previous versions of all your code can be retrieved later on and all changes to the code can be traced over time.
It will help find what changes were made to which file, when, why and by whom.
It also explains what a file looked like on a specific date or at a specific release along with the ability to find the differences between any two versions of a file.
A version control system provides the ability to work in parallel.
Types of Version Control Systems
There are 2 types of Version Control Systems – the Centralized Version Control System (CVCS) and Distributed (DVCS).

Centralized Version Control
The concept of a centralized system is that it works on a Client-Server relationship. The repository is located in one place and provides access to many clients.

Distributed Version Control
On the contrary, in Distributed System, every user has a local copy of the repository in addition to the central repo on the server-side.

Real World Application
Benefits of Version Control Systems
A Version Control System works as a database of all your code and makes revisions instead of duplicating the files which helps you to save a lot of disk space.
It keeps all the history of all the files which gives you full traceability and audibility of what changes were made to which file, when, why, and by whom.
It provides an ability to revert back to the last revision or any previous stage as per requirement.
It prevents the risk of losing functioning code or breaking test scripts by overwriting files as you can always take out the last working code at any point in time.
It helps you to identify the difference in any set of files, compare the revisions and merge the changes as per requirements.
It provides you with the ability to have entirely independent code versions if you prefer to keep different development code. Once you are ready, you can merge the files to create a final working version.
It provides an ability to work as a distributed team with full collaboration across the globe and saves time and additional efforts for everyone. There is no need to wait for others to complete the work.
Summary

Version control, also known as Revision control a.k.a. Source Control Management (SCM), is a process to manage a collection of source code and changes that provide you with many capabilities.
A Version Control System (VCS) allows you to manage and keep track of all your source code, along with the evidence that all your changes are being stored in a repository.
There are 2 types of Version Control Systems – the Centralized Version Control System (CVCS) and Distributed (DVCS).
The concept of CVCS is that it works on a Client-Server relationship. The repository is located in one place and provides access to many clients.
On the contrary, in DVCS, every user has a local copy of the repository in addition to the central repo on the server-side.


## Git Fundamentals
``` TODO FIX MARKDOWN
Learning Objectives

After completing this module, associates should be able to:

Explain how Git is used for configuration management
Install Git
Description
What is version control?
Version control, also known as source control, is the practice of tracking and managing changes to software code. Version control systems (VCS) are software tools that help software teams manage changes to source code over time. As development environments have accelerated, version control systems help software teams work faster and smarter.

Version control software keeps track of every modification to the code in a special kind of database. If a mistake is made, developers can turn back the clock and compare earlier versions of the code to help fix the mistake while minimizing disruption to all team members.

One of the most popular VCS's is Git.

Nearly Every Operation Is Local
Because Git is a distributed version control system, most operations in Git need only local files and resources to operate — generally no information is needed from another computer on your network. Because you have the entire history of the project right there on your local disk, most operations seem almost instantaneous.

This means that there is very little you can’t do if you’re offline or off VPN. If you get on an airplane or a train and want to do a little work, you can commit happily (to your local copy, remember?) until you get to a network connection to upload. If you go home and can’t get your VPN client working properly, you can still work.

The Three States
Git has three main states that your files can reside in: modified, staged, and committed:

Modified means that you have changed the file but have not committed it to your database yet.
Staged means that you have marked a modified file in its current version to go into your next commit snapshot.
Committed means that the data is safely stored in your local database.
Main sections of a Git project
This leads us to the three main sections of a Git project: the working tree, the staging area, and the Git directory.

The working tree (or working directory, as in the diagram below) is a single checkout of one version of the project, downloaded to your local machine. The working tree is the set of all files and folders a developer can add, edit, rename and delete during application development. These files are pulled out of the compressed database in the Git directory and placed on disk for you to use or modify.
The staging area is a file, generally contained in your Git directory, that stores information about what will go into your next commit. Its technical name in Git parlance is the “index”, but the phrase “staging area” works just as well. (See diagram below)
The Git directory is where Git stores the metadata and object database for your project. This is the most important part of Git, and it is what is copied when you clone a repository from another computer. (See diagram below)
Image of a Git project's main sections

Git Workflow
The basic Git workflow goes something like this:

You modify files in your working directory.
You selectively stage just those changes you want to be part of your next commit, which adds only those changes to the staging area. (See "git add" in the diagram below)
You commit the changes, which takes the files as they are in the staging area and stores that snapshot permanently to your Git directory. (See "git commit" and "git push" in the diagram below)
Image of Git workflow overview

If a particular version of a file is in the Git directory, it’s considered committed. If it has been modified and was added to the staging area, it is staged. And if it was changed since it was checked out but has not been staged, it is modified.

What is Git Bash?
Git Bash is an application for Microsoft Windows environments which provides an emulation layer for a Git command line experience. Bash is an acronym for Bourne Again Shell. A shell is a terminal application used to interface with an operating system through written commands. Bash is a popular default shell on Linux and macOS. Git Bash is a package that installs Bash, some common bash utilities, and Git on a Windows operating system.

Real World Application

The following are bookmarked links to a video on getting started with Git

What is git?

What is version control?

Terms to be learned in Git

Commonly used Git commands

How to sign up for GitHub

How to use Git on your local computer

How to install Git

How to install a code editor like "VS Code"

How to use VS Code with Git

Implementation
Steps for Installing Git
Before you start using Git, you have to make it available on your computer. Even if it’s already installed, it’s probably a good idea to update to the latest version. You can either install it as a package or via another installer, or download the source code and compile it yourself.

Installing on Linux
If you want to install the basic Git tools on Linux via a binary installer, you can generally do so through the package management tool that comes with your distribution. If you’re on Fedora (or any closely-related RPM-based distribution, such as RHEL or CentOS), you can use dnf:

$ sudo dnf install git-all
If you’re on a Debian-based distribution, such as Ubuntu, try apt:

$ sudo apt install git-all
For more options, there are instructions for installing on several different Unix distributions on the Git website, at https://git-scm.com/download/linux.

Installing on macOS
Installing on macOS There are several ways to install Git on a Mac. The easiest is probably to install the Xcode Command Line Tools. On Mavericks (10.9) or above you can do this simply by trying to run git from the Terminal the very first time.

$ git --version
If you don’t have it installed already, it will prompt you to install it.

If you want a more up to date version, you can also install it via a binary installer. A macOS Git installer is maintained and available for download at the Git website, at https://git-scm.com/download/mac.

Installing on Windows
There are also a few ways to install Git on Windows. The most official build is available for download on the Git website. Just go to https://git-scm.com/download/win and the download will start automatically. Note that this is a project called Git for Windows, which is separate from Git itself; for more information on it, go to https://gitforwindows.org.

Summary

Git Bash is an application for Microsoft Windows environments which provides an emulation layer for a Git command line experience.
Bash is an acronym for Bourne Again Shell.
A shell is a terminal application used to interface with an operating system through written commands.
Bash is a popular default shell on Linux and macOS.
Git Bash is a package that installs Bash, some common bash utilities, and Git on a Windows operating system\dots
```

## Initializing A Repository
Learning Objectives

After completing this module, associates should be able to:

Explain how to initialize a repository in Git.
Use Git to create a new repository and/or clone an existing repository.
Description
What is a Git Repository?
A Git repository tracks and saves the history of all changes made to the files in a Git project. It saves this data in a directory called .git, also known as the repository folder.

Git uses a version control system to track all changes made to the project and save them in the repository. Users can then delete or copy existing repositories or create new ones for ongoing projects.

Real World Application

Here are some reasons for using a Git repository:

Cloud repositories
It is generally more secure.
It is easier to work collaboratively. Any team member can download the latest version of the repository from any machine.
It is cheaper than a traditional server.
Distributed file system
Git is distributed, meaning that every local copy of the global repository is a fully working copy.
In case there is a problem with the server and the global repository is corrupted or lost, any local copy can recreate the full history.
In a centralized version control system, the global server contains all changes in the project and the local copies are just light versions of it. If the server goes down, you lose all the history.
Perfect to work with others
Git is designed for creating projects where many contributors develop software in parallel.
Good documentation
Git has been around for many years now and it's really easy to find good documentation.
Branches allows for simultaneous code versions
Branches are one of the best features of a version control software and are used to develop in parallel to the main repository.
A branch is a fork of the main code to develop a new feature.
Encourages code reviews
Code reviews are a good practice that every developer team should follow.
Git facilitates code reviews with an operation called pull request.
Simpler to roll back mistakes
Every commit is referenced with a hash that uniquely identifies it, see this example.
With git, we can revert to any past commit and fix a mistake.
It's the current de facto open-source umbrella
If you want to develop open-source code, the biggest repository is GitHub.
Here you can find the most popular repositories in Github, it includes bootstrap, react, d3, tensorflow, angular, etc.
Implementation
Steps for Initializing a Repository
How to Get a Git Repository
There are two ways of obtaining a Git repository:

Turning an existing directory into a Git repository (initializing).
Cloning a Git repository from an existing project.
Initialize a Repository
To initialize a Git repository in an existing directory, start by using the Git Bash terminal window to go to your project's directory:

cd [directory path]
Where [directory path]: The path to your project directory.

Use Git Bash to go to your project directory
Once you navigate to the project directory, initialize a Git repository by using:

git init
Initializing a Git repository with git init
Initializing a repository creates a subdirectory called .git that contains the files Git needs to start tracking the changes made to the project files. The repository only starts tracking project versions once you commit changes in Git the first time.

Clone a Repository
Use the git clone command to clone an existing repository and copy it to your system:

git clone [url] [directory]
Where:

[url]: The URL of the Git repository you want to clone.
[directory]: The name of the directory you want to clone the repository into. Note: specifying the directory is optional; if not specified it will clone to the current directory.

Summary

A Git repository tracks and saves the history of all changes made to the files in a Git project.
It saves this data in a directory called .git, also known as the repository folder.
Git uses a version control system to track all changes made to the project and save them in the repository. Users can then delete or copy existing repositories or create new ones for ongoing projects.


## Pushing To A Remote Repository
``` TODO FIX MARKDOWN
Learning Objectives

After completing this module, associates should be able to:

Explain how to push content to a repository in Git.
Use Git to push new content into an existing repository.
Description

The git push command is used to upload local repository content to a remote repository. Pushing is how you transfer commits from your local repository to a remote repo. The fetch and push commands are counterparts; fetch imports remote commits to the local repository while push exports local commits to the remote repository.

Git push usage
git push <remote> <branch>
Push the specified <branch> to <remote>, along with all of the necessary commits and internal objects.

git push <remote> --all
Push all of your local branches to the specified remote.

Git push is one component of many used in the overall Git "syncing" process. The syncing commands operate on remote branches which are configured using the git remote command. git push can be considered an 'upload' command whereas, git fetch and git pull can be thought of as 'download' commands. Once change sets have been moved via a download or upload a git merge may be performed at the destination to integrate the changes.

Real World Application

The following are bookmarks of a video for learning how to handle remote repositories in Git:

cloning through vsCode

git commit command

git add command

committing

git push command

SSH Keys

git push

Review workflow

Compare between github workflow and local git workflow

git branching

Undoing in git

Forking in git

Implementation

This is a practical example of git push and an optional exercise.

Start by creating a new repository on GitHub. If you’re not already logged into your GitHub account, the link will take you to the login page. Just sign in to continue.
Open your Git Bash
Create your local project in your desktop directed towards a current working directory
pwd stands for 'print working directory', which is used to print the current directory.
If necessary, move to the specific path in your local computer by cd 'path_name'.
Create or copy some files into this directory. You don't have to do this step in GitBash; you can use a file explorer GUI instead.
$ pwd
<prints out current working directory>
$ cd 'path name' // replace path name with the desired directory for working with Git
Initialize the git repository
Use git init to initialize the repository.
It is used to create a new empty repository or directory consisting of files' with the hidden directory.
'.git' is created at the top level of your project, which places all of the revision information in one place.
$ git init
Add the file to the new local repository
Use git add . in your bash to add all the files to the given folder.
Use git status in your bash to view all the files which are going to be staged to the first commit.
$ git add .
$ git status
Commit the files staged in your local repository by writing a commit message
You can create a commit message by git commit -m 'your message', which adds the change to the local repository.
git commit uses '-m' as a flag for a message to set the commits with the content where the full description is included. It is recommended that you write in double-quotes an imperative sentence in 50 characters that describes what changed and why. For example, "Corrected println error in helloworld.java"
git commit -m 'your message"
Push the code in your local repository to GitHub
git push is used for pushing local content to GitHub.
git push
Summary

The git push command is used to upload local repository content to a remote repository.
Pushing is how you transfer commits from your local repository to a remote repo.
It's the counterpart to git fetch, but whereas fetching imports commits to local branches, pushing exports commits to remote branches.
Pushing has the potential to overwrite changes, caution should be taken when pushing.
git push is one component of many used in the overall Git "syncing" process.
The syncing commands operate on remote branches which are configured using the git remote command.
git push can be considered and 'upload' command whereas, git fetch and git pull can be thought of as 'download' commands.
Once change sets have been moved via a download or upload a git merge may be performed at the destination to integrate the changes.
```

## Git Commit Branch Merge Push Pull
Learning Objectives

After completing this module, associates should be able to:

Perform the following in Git
Commit changes
Merge changes
Push to a Git repository
Pull from a Git repository
Description
Git Merge
Merging is Git's way of putting a forked history back together again. The git merge command lets you take the independent lines of development created by git branch and integrate them into a single branch.

Note that all of the commands presented below merge into the current branch. The current branch will be updated to reflect the merge, but the target branch will be completely unaffected. Again, this means that git merge is often used in conjunction with git checkout for selecting the current branch and git branch -d for deleting the obsolete target branch.

How it works
Git merge will combine multiple sequences of commits into one unified history. In the most frequent use cases, git merge is used to combine two branches. The following examples in this document will focus on this branch merging pattern. In these scenarios, git merge takes two commit pointers, usually the branch tips, and will find a common base commit between them. Once Git finds a common base commit it will create a new "merge commit" that combines the changes of each queued merge commit sequence.

New merge commit node
Merge commits are unique against other commits in the fact that they have two parent commits. When creating a merge commit Git will attempt to auto magically merge the separate histories for you. If Git encounters a piece of data that is changed in both histories it will be unable to automatically combine them. This scenario is a version control conflict and Git will need user intervention to continue.

Preparing to merge
Before performing a merge there are a couple of preparation steps to take to ensure the merge goes smoothly.

Confirm the receiving branch
Execute git status to ensure that you are pointing to the correct merge-receiving branch.
If needed, execute git checkout to switch to the receiving branch. In our case we will execute git checkout main.
Fetch latest remote commits
Make sure the receiving branch and the merging branch are up-to-date with the latest remote changes.
Execute git fetch to pull the latest remote commits.
Once the fetch is completed ensure the main branch has the latest updates by executing git pull.
Merging
Once the previously discussed "preparing to merge" steps have been taken a merge can be initiated by executing git merge

Real World Application
Merge conflicts
It’s always nice when merges happened seamlessly, but on occasion, you could run into merge conflicts.

Merge conflicts happen when both branches you’re trying to merge change some part of the same file. Sometimes, Git is able to figure things out here.

However, Git often isn’t able to decide which version it should use if multiple people made changes to the same line in a file (or something similar), so it gives up right before creating the merge commit. That way, you can solve the problem.

Solving merge conflicts
First, you need to understand what caused the merge conflict. Did someone delete a file you made changes to? Or maybe you added a file with the same name as an existing file.

Regardless, Git will inform you that you have “unmerged paths,” meaning conflicts stopped your branches from merging.

Fortunately, Git uses visual markers (<<<<<<<, =======, and >>>>>>>) to help you find the problem area easily.

The equal signs separate the two branches being merged. The branch below the equal signs is the branch you’re merging, whereas the branch above the equals signs is the receiving branch.

Once you find these conflicting sections, you can edit them until they work well. You may have to collaborate with the contributors/team members involved to make sure everything looks right.

After fixing the sections, it’s time to finish the merge. Execute git add on the conflicting file(s) to let Git know you’ve fixed everything. Then, execute a git commit command to create the merge commit and finish up.

Implementation

This video goes through the steps involved in merging branches in Git: Merging branches in Git

Steps to Resolve a Merge Conflict on GitHub:
Sometimes when working on a project, we make multiple pushes to the remote repository without first pulling the most updated version of our application. When this happens, we are likley to encounter a merge conflict.

Don't worry, often times these conflicts can have easy resolutions when you know how to fix them. For this example, we will be using GitHub to resolve a merge conflict. In the second example we will use GitBash.

Step 1:
In your repository, click on the "Pull Requests" tab on the top of the page.

Pull Requests Picture

Step 2:
In the "Pull Requests" list, click the pull request with the merge conflict you'd like to fix.

Step 3:
Near the bottom of the pull request, click "Resolve conflicts".

Resolve conflict Picture

Step 4:
Choose which version of the code you'd like to keep or make necessary changes to the code as needed.

NOTE: Make sure that you delete the conflict markers <<<<<<<, =======, and >>>>>>>.

Also, if you have more than one merge conflict, you can feel free to do this step for all of them until the conflict is resolved.

Step 5:
After you've fixed all of your merge conflicts, you can click on the "Commit merge" button to merge the branch into the main branch.

Commit merge Picture

Steps to Resolve a Merge Conflict on GitBash:
Sometimes, the merge conflicts will be too complex to handle on GitHub, so you will need to do it through your local computer. Here is how to achieve that.

Step 1:
In GitBash, go to the local Git repository that has the merge conflict by using this command:

cd <Repository-Name>

Where you switch the <Repository-Name> with the name of the repository.

Step 2:
Get a list of the files affected by the merge conflict by using the git status command.

Step 3:
Open a text editor, such as VS Code, and go to the file with the merge conflict.

Step 4:
Just like in GitHub, the beginning of the conflict will be marked by the <<<<<<< MAIN, the ======= will show you where the main branch code ends and where the branch conflict starts, and the >>>>>>> will show you where the branch ends.

Figure out which part you'd like to keep or what further changes need to be made and make them.

REMEMBER: You need to delete the <<<<<<<, =======, and >>>>>>> from your code before you commit the changes.

Step 5:
Commit your changes as usual with the git add and git commit -m "Write your message here" commands.

Summary

Merging is Git's way of putting a forked history back together again.
The git merge command lets you take the independent lines of development created by git branch and integrate them into a single branch.
Git merge will combine multiple sequences of commits into one unified history.
In the most frequent use cases, git merge is used to combine two branches.
Git merge takes two commit pointers, usually the branch tips, and will find a common base commit between them.
Once Git finds a common base commit it will create a new "merge commit" that combines the changes of each queued merge commit sequence.


## Gitignore
Learning Objectives

After completing this module, associates should be able to:

Explain the .gitignore file and its purpose.
Description

The .gitignore file is a text file that tells Git which files or folders to ignore in a project. Files already tracked by Git are not affected. Each line in a gitignore file specifies a pattern. When deciding whether to ignore a path, Git normally checks gitignore patterns from multiple sources, with the following order of precedence, from highest to lowest.

Whatever files and folders from any project you want to ignore those files you will place in the .gitignore file by using the following patterns.

* is used as a wildcard match.
/ is used to ignore pathnames relative to the .gitignore file.
# is used to add comments to a .gitignore file.
** can be used to match any number of directories.
! to negate a file that would be ignored.
*.log
!example.log
In this example, example.log is not ignored, even though all other files ending with .log are ignored.

Real World Application

The .gitignore file is essential for managing a Git repository effectively. Here's why it's important:

Preventing Unnecessary Files from Being Tracked: The .gitignore file allows you to specify patterns for files or directories that Git should ignore. This prevents unimportant or generated files (e.g., build artifacts, log files, temporary files) from being accidentally tracked by Git. Ignoring such files helps keep the repository clean and focused on versioning the source code and essential project files.

Reducing Repository Size: By ignoring unnecessary files, you reduce the size of your Git repository. This is especially important when working with large binary files or autogenerated files that frequently change. Ignoring these files can significantly reduce the repository's size, making cloning, fetching, and pushing operations faster and more efficient.

Avoiding Accidental Commits of Sensitive Information: The .gitignore file helps prevent accidental commits of sensitive information such as passwords, API keys, and configuration files containing sensitive data. By explicitly ignoring files containing sensitive information, you reduce the risk of exposing confidential data in the version control system.

In summary, the .gitignore file is crucial for maintaining a clean, efficient, and secure Git repository. It helps prevent unnecessary files from being tracked, reduces repository size, and avoids accidental commits of sensitive information.

Implementation


In this section, you will learn about how the .gitignore file will work with examples step-by-step. This is an optional exercise.

First, go to your https://github.com/ and create a new repository GitIgnoreDemo. Copy the HTTPS URL to clone the project into the VS Code.
Note: First you must download a git and install. After that add the git extension in your VS Code as well. Otherwise, you can also download git bash.

Open your VS code go to open folder create a folder gitignoreexampledemo and select it.
Clone the project using the following git clone command and check git status in your VS code terminal.
git clone repository-HTTPS-URL
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo> git clone https://github.com/SrikanthMidathapalli/GitIgnoreDemo.git
Cloning into 'GitIgnoreDemo'...
remote: Enumerating objects: 3, done.
remote: Counting objects: 100% (3/3), done.
remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
Receiving objects: 100% (3/3), done.
The GitHub repository is successfully cloned into your local system. Now Check the status by using the git status command. You probably see the following error do not worry it is asking to initialize the git.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo> git status
fatal: not a git repository (or any of the parent directories): .git
Initialize the git using the git init command.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo> git init
Initialized empty Git repository in C:/Users/SrikanthMidathapalli/Desktop/gitignoreexampledemo/.git/
Now create a file HelloWorld.java and check the git status.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo> git status
On branch master

No commits yet

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        GitIgnoreDemo/

nothing added to commit but untracked files present (use "git add" to track)
Note: Here untracked files means that git does not know them.

Now you can see the untracked file which we newly created inside the GitIgnoreDemo/ folder now change the folder to GitIgnoreDemo then we can see the untracked file HelloWorld.java as shown below.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo> cd .\GitIgnoreDemo\
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is up to date with 'origin/main'.

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        HelloWorld.java

nothing added to commit but untracked files present (use "git add" to track)
We all know that from git basics any untracked files we want git to track we will add those files to track by using the following git command.
git add file-name // if we want to track one file
git add . // if we want all the files should be tracked
Here I am using the git add HelloWorld.java command to track my file and after that, I am seeing the status by using the git status command.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git add HelloWorld.java
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   HelloWorld.java
Now the file is ready to commit, but now let me add some content and check the status of the file.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   HelloWorld.java

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   HelloWorld.java
Here If you observe that the file is already ready to commit and the same file is not staged for commit it's because we already know that git works with data.
Now you can do again git add HelloWorld.java and check the status as shown below
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git add HelloWorld.java
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   HelloWorld.java
Here if you want to unstage the file you can follow the restore command as well
Now use the following git command to change the commit to save the files in your local system as shown below.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git commit -m "initial commit"
[main aa8ab33] initial commit
 1 file changed, 5 insertions(+)
 create mode 100644 HelloWorld.java
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
Now your local file is ready to push and pull them to the remote repository. As we know that we will use push or pull git commands to push the files from our local system to the remote repository.
Now let's consider a scenario where in your project you are having some log files which are not important to push to the repository. Therefore to achieve that you need to ignore those files that are saved inside the project folders. here you need a git that needs to ignore the files. To achieve this we use the .gitignore let's see in detail
Create a file which is a user.log inside the GitIgnoreDemo and create the file .gitignore as well and check git status you will see both files are untracked.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .gitignore
        user.log

nothing added to commit but untracked files present (use "git add" to track)
Now write the user.log inside the .gitignore file and check git status
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .gitignore

nothing added to commit but untracked files present (use "git add" to track)
Note: Here it says that you have committed some files that are ready to push into the remote repository.

Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)
you can see only one file which is .gitignore has to be tracked. Now you just add this file to tracked using the git add -A and git add . git commands as shown below and check the git status.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git add -A
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git add .  
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   .gitignore
Now the file is ready to commit just to do the same git commit -m "initial gitignore commit and check git status.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git commit -m "initial gitignore commit"
[main 16e5398] initial gitignore commit
 1 file changed, 1 insertion(+)
 create mode 100644 .gitignore
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
Now it's time to check if any modifications that you have performed on the user.log file git are tracking that file or not.
Open the user.log file and add some content and save the file and check the git status as shown below.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main  
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
None of the files are showing that files are modified its because you have added the user.log file in the .gitignore file so git is ignoring it to track that modified file.
This helps you when you are working on the project and you want that the files which you worked on are not to tracked by git we will use this file .gitignore. Similarly, create a folder users/ and add one more file inside the users/ folder like logginguser.log. that is users/logginguser.log. Perform the following commands as provided below.
Before the users/ folders are written in the .gitignore file it will show
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include what will be committed)
        users/

nothing added to commit but untracked files present (use "git add" to track)
After users/ folder written .gitignore file.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   .gitignore

no changes added to commit (use "git add" and/or "git commit -a")
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git add -A
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git add .
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git commit -m "initial gitignore commit"
[main ac1268b] initial gitignore commit
 1 file changed, 2 insertions(+), 1 deletion(-)
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status 
On branch main
Your branch is ahead of 'origin/main' by 3 commits.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
Now open the userlogging.log file and write some content and check for the git status you see that no modified message.
PS C:\Users\SrikanthMidathapalli\Desktop\gitignoreexampledemo\GitIgnoreDemo> git status
On branch main
Your branch is ahead of 'origin/main' by 3 commits.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
By this, you can say that whatever files you want that git to ignore to track those you can place inside the .gitignore file.
To demonstrate the ignore patterns create a file user1.log file inside the .gitignore file instead of adding individual files like user.log and user1.log you can use a wildcard like *.log inside the .gitignore here whatever the files are named .log those files will be ignored.
Summary

The .gitignore file is a text file that tells Git which files or folders to ignore in a project.
Using gitignore can help keep the repository clean and focused on versioning the source code and essential project files.
Using gitignore can help reduce the repository's size, making cloning, fetching, and pushing operations faster and more efficient.
The .gitignore file helps prevent accidental commits of sensitive information such as passwords, API keys, and configuration files containing sensitive data.


## What Is A Database
Learning Objectives

After completing this module, associates should be able to:

Identify and describe the use of data and databases
Description
What is data
Data is information with a purpose. In enterprise applications, data provides aggregated state information for the application. Businesses will use this data for various reasons including, marketing, usage statistics, error reporting, and more. Data is specifically designed to provide insights and persistence information for the applications which they support.

What is a database
A database is a system of software and capabilites that make validating, storing, searching, filtering, aggregating, grouping, and administering data possible. In enterprise applications databases fall into 2 main categories SQL and NoSQL.

SQL databases
SQL databases are a type of RDBMS which use the standard Structured Query Language to administer the data. Data in a SQL database are stored in objects called tables. Tables provide the relational information for the data stored in the database.

NoSQL databases
NoSQL (Not Only SQL) databases are not necessarily based on the relational model, unlike RDBMS. NoSQL databases typically use some other means or DSL (domain-specific language) for administering data and use different structures for storing data and relational information.

Real World Application

From the start of digital revolution, we have witnessed the utmost requirement of storing data effectively and efficiently. Data is the lifeblood of business solutions and having an accurate database management system is an important tool for handling such large volumes of data.

Each database management system offers particular solutions as per business requirements, and decision-makers are often left wondering which system would be appropriate for their use. We will cover various RDBMS vendors in another module to help understand these differences.

Implementation

The general syntax for creating a DATABASE in MySQL is:

CREATE DATABASE database_name;
Example:

CREATE DATABASE employee;
The general syntax to use a newly created schema:

USE database_name;
Example:

USE employee;
Summary

Data is information with a purpose. In enterprise applications, data provides aggregated state information for the application.
A database is a system of software and capabilites that make validating, storing, searching, filtering, aggregating, grouping, and administering data possible.
SQL databases are a type of RDBMS which use the standard Structured Query Language to administer the data.


## What Is SQL
Learning Objectives

After completing this module, associates should be able to:

Define SQL
Describe the purpose of using SQL
List the SQL Sublanguages
Description

Structured Query Language or SQL is the standard language for working with RDBM systems. SQL is used to administer and manipulate SQL servers. SQL is a scripting language that is interpreted by the database server. SQL is used to...

Define database structure
Manipulate stored data
Define data access permissions
Control concurrent data access
Query stored data
To accomodate the operations of the above categories, SQL is broken into 5 sublanguages.

Sublanguage	Description
DDL	Data Definition Language. Defines data structure
DML	Data Manipulation Language. Insert, Update, Delete record
DCL	Data Control Language. Grant or revoke access permissons to database object
TCL	Transaction Control Language. Defines concurrent operation boundaries
DQL	Data Query Language. Search, filter, group, aggregate stored data
Each sublanguage is responsible for a specific set of operations and have a specific set of commands associated with them. Breaking down each sublanguage is beyond the scope of this document, but they will be discussed in more depth in their own sections.

SQL itself is generally case-insensitive, but it's a good practice to follow a consistent naming convention (e.g., always using lowercase or uppercase) for tables, columns, and other identifiers to avoid potential issues related to case-sensitivity.

Real World Application

SQL is used to administer SQL-based RDBM systems. Below is a short list of some databases and their enterprise users.

Oracle
Wells Fargo
Verizon
Citi
ADP
FEMA
MySQL
Wordpress
NASA
Netflix
Youtube
Bank of America
PostgreSQL
Twitch
Apple
Spotify
Reddit
IMDb
Microsoft SQL server
Fisher Investments
Penske
Alarm.com
Citi
Humana
MariaDB
Moodle
Samsung
Nokia
Red Hat
Select Quote
Walgreens
Implementation

SQL is developed based on the ANSI SQL Standard. However, there are a lot of different vendor specific implementations available. Below is list of popular, but not exhaustive SQL implementations.

Oracle
MySQL
PostgreSQL
Microsoft SQL server
MariaDB
SQLite
Summary

Structured Query Language is the language used to administer SQL-based RDBM systems. SQL is based on ANSI standard ISO/IEC 9075:2016 (ANSI X3.135). There are many vendor specific implementations of the SQL standard, but the standard itself can be divided into 5 sublanguage categories: DDL, DML, DCL, TCL, DQL. Each sublanguage is responsible for a specific set of operations on the database.


## Consistency
Learning Objectives

After completing this module, associates should be able to:

Describe Database Consistency
Discuss the importance of database consistency
Provide examples of strong vs weak consistency
Description
Database Consistency
Database consistency is where all data points within the database system must align in order to be properly read and accepted. Information that does not meet the predefined values within the database will result in consistency errors with the dataset. Any transactions with the database involved in data written to the database must adhere to the database's established rules by developers. This is defined by specific constraints, triggers, variables, cascades, etc, that will all be described in the modules to follow.

While database consistency helps ensure the appropriate format for data written to the database, it does not account for what the data respresents. This means the information entered may match the appropriate format, but doesn't guarantee that the data corresponds with the actual information.

These rules applied to our data is what keeps databases working smoothly by keeping inconsistent data out, achieving higher accuracy, increasing database space, and improving data retrieval.

Database Inconsistency
Database consistency also applies to any changes of the data within the system. If one particular object in the database is updated, but also is present in another table in the database, it must be properly updated in all other tables it is present. If this fails, it results in database inconsistency.

Database inconsistency is when any portion of the information is updated in the table, but is not represented in all other tables utilizing that same information.

Real World Application
Database Consistency
Imagine working at the Pennsylvania DMV and you've been asked to work on the database for the new Driver's License. Due to growing population sizes, they've required a new driver's license number to help identify individuals. Your team has determined that every individuals driver's license number must include the following: 1 Alpha + 6 Numerics + 1 Alpha. All driver license numbers are now required to follow this rule, such that "P123456A" is a valid entry consistent with the format mentioned. Any entry that doesn't fit those stipulations, such as "P12345678", would result in error for inconsistent data as it's format is 1 Alpha + 8 Numerics.

Database Inconsistency
Keeping with the driver's license example, imagine a driver's home address changes. This update must be represented across all tables where that prior address existed. If any table contains the old address, but others contain the new address for the driver would be a prime example of database inconsistency.

Implementation

Database consistency implementation, which involves specific constraints, triggers, variables, cascades, etc., will be outlined throughout the remaining modules. These elements are established based on the rules set by you as a developer. Always keep in mind, "how is this affecting my data's consistency?" when applying any implementations in the future.

Summary

Database consistency is vital for accuracy, improved data retrieval and increasing the database space available.

Consistency using established rules by the developer allows for consistent data formats
Consistency does not account for what the data represents
Consistency ensures that any inconsistent data attempted to be written to the database results in an error
Database inconsistency is when tables that should have matching information is conflicting with one another.


## Introduction To RDBMS
Learning Objectives

After completing this module, associates should be able to:

Describe an RDBMS
Identifiy the benefits of using an RDBMS
Describe the basic components of an RDBMS
Description

Relational Database Management System or RDBMS is a set of software and capabilities that enable IT teams to create, update, administer and interact with a relational database. An RDBMS is a database management systems (DBMS) based on a relational model as defined by E.F. Codd in 1970.

The use of RDBMS is essential in large scale application development. The use is so widespread that it would be nearly impossible to find an enterprise application that doesn't utilize an RDBMS for data persistence. Below is a list of some reasons for using an RDBMS.

Structured data model
Large scale concurrent data access
Fault tolerance
Distributed data storage
Enforced data integrity
Support for multiple client types
Real World Application

Databases provide the backbone of data persistence in enterprise applications. Large business applications are designed around the idea of scalability which emphasizes stateless execution. This means that a single application in the system won't store its own data locally, but use some other type of data persistence. RDBMS is a solution for this problem.

Choosing persistence
System architects and project teams work to identify what type of persistence an application will need and incorporate it into the design of the system. RDBMS is just one solution, and there is a host of vendors to choose from. So how does one choose an RDBMS persistence provider. The following list is just a few features to consider.

Distribution type
Scalability
License types
Security features
Support
Distribution type
Installing the database is half the battle and of course there are a number of different ways to distribute the database software.

Distro Type	Description
Standalone	Individually downloaded and installed software built for specific OS architectures
Managed	Cloud-based installations like AWS RDS and GCP CloudSQL
Containers	Container images like Docker
Each distribution type has its pros and cons, which is out of the scope of this document, but each should be considered before choosing which distribution type works best for the solution.

Scalability
Much like the applications that use databases for persistence, the databases themselves should be scalable. The problem is, databases can't be stateless. The scalability of a database is centered around clustering. Clustering is a scheme where multiple nodes are orchestrated to behave as a single unit. In RDBMS clustering however, there is a slight difference. An RDBMS cluster is constructed with a single master and multiple other replicas. This means all reads and writes happen primarily on the master with data being propagated to the replicas over time.

Database cluster illustrations

Most vendors implement some kind of clustering capabilities into their databases, so the biggest decision on clustering is, how much work do you want to do. The distribution type will play a big part in this with Standalone distributions requiring the most effort, Managed distributions requiring the least effort, and Container distributions somewhere in between.

License type
Each vendor will have different license types. MySQL is free and so is PostgresSQL, but not all RDBMS systems are free. The project budget will dictate this decision. Feature sets are also often tied to license type so this will also help dictate this decision.

Security features
Security is paramount to data. One should consider security trade-offs when deciding to use any software tool. These features could also be tied to license type.

Support
A good support structure is essential for software especially for software not built by your team or company. It is good to have some way to get help when issues may arise. Support can come from different sources.

Documentation
Community forums
Email/Chat
Call center
Technician/Specialist
Many times the level of available support is based on license type.

Implementation

The implementation of an RDBMS is strictly dependent on the RDBMS vendor. Typically the implementation of the RDBMS is based on the SQL standard and there are many different RDBMS vendors. Below is a non-exhaustive list of popular RDBMS vendors.

Oracle
MySQL
PostgreSQL
SQL Server
MariaDB
SQLite
Since the implementation of the system is dependent on the RDBMS vendor, the architecture can vary drastically. Below is a simple breakdown of the MySQL architecture.

MySQL Architecture Simple breakdown

The architecture can be broken down into 6 parts.

Component	Purpose
Client	Provides users the ability to connect to database servers and execute commands.
Connection Handler	Receives incoming client connections and create new thread execution contexts for isolating client connections from each other.
Parser	Validate and convert SQL commands to system commands.
Query Cache	Store query results for later retrieval when the same commands are executed.
Optimizer	Apply available engine optimizations to execute queries in the most optimal way to reduce query execution time and ensure properly execution order.
Storage Engine	Stores data on hard drives for persistent access.
It is important to note that this diagram is not comprehensive and is not an accurate representation of all implementations of SQL based RDBMS, but does illustrate some important parts of the system. To understand the particular architecture of the DB vendor you are using, additional research will be needed.

Data structure
The discussion of vendor choice and vendor implementation aside an RDBMS is a DBMS based on relationships, all RDBMS implementations are based on some simple relation concepts.

An RDBMS manages data by storing them in tables. A single relational database can contain many tables along with many other types of objects. RDBMS serves as the basis for Structured Query Language (SQL) which is used to perform the various management operations on the system.

What is a table
One of the core components of an RDBMS is an object called a table. Data in the RDBMS is store in tables. A table consists of related data entries called rows or records and each row consist of numerous columns or fields. A column is defined by a number of constraints.

Let's look at an example table called 'Employees'.

+----+-----------+----------+------------+-----------+
| id | firstName | lastName | department | reportsTo |
+----+-----------+----------+------------+-----------+
|  1 | Jimmy     | Johnson  |          1 |      NULL |
|  2 | Aaron     | Anderson |          1 |         1 |
|  3 | Emily     | Emerson  |          1 |         1 |
+----+-----------+----------+------------+-----------+
What is a Row
A row is an individual entry that exists in a table. The terms row and record can be used interchangably. In the 'Employees' table there are 3 rows. Below is a single row.

+----+-----------+----------+------------+-----------+
|  1 | Jimmy     | Johnson  |          1 |      NULL |
+----+-----------+----------+------------+-----------+
What is a Column
A column is the smallest entity of a table. A single column makes up a single data point on a row. The 'Employees' table has 5 columns:

+----+-----------+----------+------------+-----------+
| id | firstName | lastName | department | reportsTo |
+----+-----------+----------+------------+-----------+
What is a constraint
A constraint is some restriction on the type or value that can be assigned to a column. In the 'Employee' table, each of the 5 columns has set of constraints on them, for instance the 'firstName' column has a constraint of not null which means this column must always have a value and can never be null.

Exercise (Optional)
Discuss: Why do you think the department and reportsTo columns have number values rather than some other meaningful values?

Discuss: What is the intended purpose of the id column?

Summary

An RDBMS is a data storage system based on a relational model where data that is related to a particular object is stored in tables with each entry being represented as a row and each data point is a column in the row that is validated by a set of constraints. The most common type of RDBMS is based on a standard called SQL, however there are many different implementations (vendors) of the standard each providing a unique set of benefits and features.


## Schema
Learning Objectives

After completing this module, associates should be able to:

Describe the purpose of schema in a database
Description

The term schema comes from Greek and is defined as 'form, figure'. In the world of databases, schema gives shape to the data stored in the database. In a DBMS, schema is the structure of a database which is described in a formal language which is supported by the DBMS. In layman's terms, schema refers to the organization of data as a blueprint for the construction of the database. In an RDBMS the schema of the particular data items are divided into tables.

Database schema is declared using a formal language, for RDBMS the language is SQL, the schema is structured using integrity constraints to ensure compatibility between the different parts of the schema. In enterprise application development, the definition of a schema is centered on modeling the data for a particular set of problems. The solutions will be implemented in software, but the data is stored in a database schema separate from the application of software logic.

Real World Application

The real world application of schema comes in two steps: modeling then implementation. Modeling can be done using tools to create an Entity Relation Diagram (ERD), then implementation uses SQL to create the table objects.

Let's look at a schema for an inventory management system.Example Schema

This schema defines a number of table objects and their relationships. There is a large amount of information here that defines the schema. This information is called the integrity constraints. Integrity constraints include:

Column names
Data types
Data constraints
Relationships
With the design done, let's view the script to create this schema.

-- MySQL Script generated by MySQL Workbench
-- Thu Jun 16 15:19:40 2022
-- Model: New Model    Version: 1.0
-- MySQL Workbench Forward Engineering

SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0;
SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0;
SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION';

-- -----------------------------------------------------
-- Schema mydb
-- -----------------------------------------------------
-- -----------------------------------------------------
-- Schema inventory
-- -----------------------------------------------------

-- -----------------------------------------------------
-- Schema inventory
-- -----------------------------------------------------
CREATE SCHEMA IF NOT EXISTS `inventory` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci ;
USE `inventory` ;

-- -----------------------------------------------------
-- Table `inventory`.`SALES_DEPARTMENTS`
-- -----------------------------------------------------
CREATE TABLE IF NOT EXISTS `inventory`.`SALES_DEPARTMENTS` (
  `id` INT NOT NULL AUTO_INCREMENT,
  `department_name` VARCHAR(20) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE INDEX `department_name_UNIQUE` (`department_name` ASC) VISIBLE)
ENGINE = InnoDB;


-- -----------------------------------------------------
-- Table `inventory`.`INVENTORY_ITEMS`
-- -----------------------------------------------------
CREATE TABLE IF NOT EXISTS `inventory`.`INVENTORY_ITEMS` (
  `id` INT NOT NULL AUTO_INCREMENT,
  `item_name` VARCHAR(45) NOT NULL,
  `short_name` VARCHAR(10) NOT NULL,
  `price` DECIMAL(10,2) UNSIGNED NULL DEFAULT 0.99,
  `qty` INT NULL DEFAULT 0,
  `dept_id` INT NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE INDEX `short_name_UNIQUE` (`short_name` ASC) VISIBLE,
  INDEX `fk_item_dept_idx` (`dept_id` ASC) VISIBLE,
  CONSTRAINT `fk_item_dept`
    FOREIGN KEY (`dept_id`)
    REFERENCES `inventory`.`SALES_DEPARTMENTS` (`id`)
    ON DELETE NO ACTION
    ON UPDATE NO ACTION)
ENGINE = InnoDB;


-- -----------------------------------------------------
-- Table `inventory`.`STORES`
-- -----------------------------------------------------
CREATE TABLE IF NOT EXISTS `inventory`.`STORES` (
  `id` INT NOT NULL AUTO_INCREMENT,
  `store_no` INT UNSIGNED NOT NULL,
  `address` VARCHAR(45) NOT NULL,
  `contact_no` CHAR(12) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE INDEX `address_UNIQUE` (`address` ASC) VISIBLE)
ENGINE = InnoDB;


-- -----------------------------------------------------
-- Table `inventory`.`CUSTOMERS`
-- -----------------------------------------------------
CREATE TABLE IF NOT EXISTS `inventory`.`CUSTOMERS` (
  `id` INT NOT NULL AUTO_INCREMENT,
  `name` VARCHAR(45) NOT NULL,
  `address` VARCHAR(45) NULL,
  PRIMARY KEY (`id`))
ENGINE = InnoDB;


-- -----------------------------------------------------
-- Table `inventory`.`PURCHASE_ORDERS`
-- -----------------------------------------------------
CREATE TABLE IF NOT EXISTS `inventory`.`PURCHASE_ORDERS` (
  `id` INT NOT NULL AUTO_INCREMENT,
  `open_date` TIMESTAMP NOT NULL,
  `close_date` TIMESTAMP NULL,
  `customer_id` INT NOT NULL,
  `store_id` INT NOT NULL,
  PRIMARY KEY (`id`),
  INDEX `fk_po_store_idx` (`store_id` ASC) VISIBLE,
  INDEX `fk_po_customer_idx` (`customer_id` ASC) VISIBLE,
  CONSTRAINT `fk_po_store`
    FOREIGN KEY (`store_id`)
    REFERENCES `inventory`.`STORES` (`id`)
    ON DELETE NO ACTION
    ON UPDATE NO ACTION,
  CONSTRAINT `fk_po_customer`
    FOREIGN KEY (`customer_id`)
    REFERENCES `inventory`.`CUSTOMERS` (`id`)
    ON DELETE NO ACTION
    ON UPDATE NO ACTION)
ENGINE = InnoDB;


-- -----------------------------------------------------
-- Table `inventory`.`PO_LINE_ITEMS`
-- -----------------------------------------------------
CREATE TABLE IF NOT EXISTS `inventory`.`PO_LINE_ITEMS` (
  `id` INT NOT NULL AUTO_INCREMENT,
  `po_id` INT NOT NULL,
  `item_id` INT NOT NULL,
  `price` DECIMAL(10,2) NULL DEFAULT 0.99,
  `qty` INT NULL DEFAULT 1,
  PRIMARY KEY (`id`),
  INDEX `fk_po_item_idx` (`po_id` ASC) VISIBLE,
  INDEX `fk_item_line_itm_idx` (`item_id` ASC) VISIBLE,
  CONSTRAINT `fk_po_line_itm`
    FOREIGN KEY (`po_id`)
    REFERENCES `inventory`.`PURCHASE_ORDERS` (`id`)
    ON DELETE NO ACTION
    ON UPDATE NO ACTION,
  CONSTRAINT `fk_item_line_itm`
    FOREIGN KEY (`item_id`)
    REFERENCES `inventory`.`INVENTORY_ITEMS` (`id`)
    ON DELETE NO ACTION
    ON UPDATE NO ACTION)
ENGINE = InnoDB;


SET SQL_MODE=@OLD_SQL_MODE;
SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS;
SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS;
There is a lot of SQL script here. This script was generated by MySQL Workbench Forward Engineering Tool. It inserts a lot of database specific code which can be omitted, but we can look at the create statements and observe the use of integrity constraints.

Implementation

The focus of this section is schema, however it is impossible to implement schema in an RDBMS without the use of SQL. The way to implement a schema is to first create it, this step is dependent on the database vendor. Generally the syntax is:

create schema <schema_name>;
After the schema has been created the data is divided in tables.

create table <table_name> (
    <col_name> datatype constraint1 constraint2 ...,
    ...
);
A full schema is created using multiple create statements that define each table and those table relationships.

Summary

A database schema defines the form of database and the data within it. An RDBMS schema can include objects like tables, triggers, functions, procedures, indexes, and views. In this section we focused on tables. In RDBMS table the schema defines the columns, their data types, and constraints.


## Table Structure
Learning Objectives

After completing this module, associates should be able to:

You will learn how to CREATE, DROP, TRUNCATE and ALTER a table in MySQL database.
Understand the difference between TABLE and DATABASE.
You will learn what a TUPLE is.
Description

In MySQL, a table stores and organizes data in columns and rows as defined during table creation.

The CREATE TABLE command is used to create a new table in a database.

The DROP TABLE command is used to completely remove the table from database.

The TRUNCATE TABLE command is used to remove all the data and not the table itself.

The ALTER TABLE command is used to alter the table.

The major differences between a database and a table are as follows:

Tables are a way to represent the division of information in a database while, database is a collection of tables and records.
Tables are used to group the records in relation with each other and create a dataset. This dataset will be used in the database. The data which are stored in the table in any form is a part of the database, but the vice-versa is not true.
TUPLE - A tuple refers to an ordered set of values. The number of values, or elements, in a tuple is fixed and is known as the arity of the tuple (arity - refers to number of columns in a table). Each value in the tuple is often called an attribute or a component.

In the context of databases and relational database management systems (RDBMS), a tuple is equivalent to a row in a table. Each tuple represents a single record, and the values in the tuple correspond to the attributes or columns of that record.

For example, consider a simple table representing employees:

Employee Table

EmployeeID	FirstName	LastName	Age	Salary
1	Susie	Que	30	50000
2	Billy	Bob	25	60000
3	Bacon	Hamburger	35	75000
In this table, each row is a tuple. For instance, the first tuple (or row) is (1, "Susie", "Que", 30, 50000), where each value corresponds to a specific attribute such as EmployeeID , FirstName, LastName, Age, and Salary.

Tuples are used in various areas of computer science, including databases, programming languages, and mathematical modeling, where the concept of an ordered set of values is fundamental.

Real World Application

A table is used in all forms of statistical report in today's world.
It's a sequence of labeled columns of data.
It can be used to both store and display data in a structured format.
Implementation

The general syntax for creating a table in MySQL is:

CREATE TABLE [IF NOT EXISTS] table_name(  
    column_definition1,  
    column_definition2,  
    ........,  
    table_constraints  
);
Example:

CREATE TABLE movies(m_name VARCHAR(45) NOT NULL,m_genre VARCHAR(20) NOT NULL,m_director VARCHAR(30) NOT NULL,m_year INT NOT NULL,PRIMARY KEY(m_name));
Add values to the newly created table.

INSERT INTO movies VALUE("Mr.Bean", "comedy", "Mr.ABC", 2019);
INSERT INTO movies VALUE("Fox", "horror", "Mr.XYZ", 2020);
DROP vs TRUNCATE
The general syntax for dropping a table in MySQL is:

DROP TABLE table_name;
We use DROP command to completely remove the table. Example:

DROP TABLE movies;
The general syntax for truncating a table in MySQL is:

TRUNCATE TABLE table_name;
The TRUNCATE command is used to delete the data inside a table, but not the table itself. Example:

TRUNCATE TABLE movies;
Modifying structure
To ADD a column:

ALTER TABLE table_name
ADD column_name datatype;
We use ALTER TABLE command to modify a table. Example:

ALTER TABLE movies
ADD ticket_price INT;
To DROP a column:

ALTER TABLE table_name
DROP COLUMN column_name;
Example:

ALTER TABLE movies
DROP COLUMN m_genre;
To MODIFY a column:

ALTER TABLE table_name
MODIFY COLUMN column_name datatype;
Example:

ALTER TABLE movies
MODIFY COLUMN ticket_price double;
Summary

After reading this module, you will be able to differentiate TABLE with DATABASE, and describe the following:

CREATE a table.
DROP a table.
TRUNCATE a table.
ALTER a table.


## SQL Data Types
Learning Objectives

After completing this module, associates should be able to:

Identify the categories of common SQL datatypes
List SQL data types
Description

In a database, each table is defined with a set of columns. Each column must have a data type which restricts the type of data that can be assigned to it. Each vendor can support any number of the data types. Below is a list of the most common types broken into categories.

Category	Sub 1	Sub 2	Sub 3
Character	Fixed-length	Variable-length	--
Numeric	Decimal	Integer	Floating point
Temporal	Date	Time	Timestamp
Character
Character types can represent character data as either fixed or variable-length character types like 'a' or 'cat' or a social media post.

Fixed-length character types
Fixed length character types store a fixed number of bytes in storage regardless of the string or number of characters being stored. The fix length types can be declared as char(n) or character(n) (depends on the database) and each character will take up 1 byte.

middle_initial character(1);
or

middle_initial character;
The above column middle_initial will be 1 byte in storage regardless if the column actually has a value or not. Remaining characters are padded with spaces.

pet_type char(10);
The column pet_type will always take up 10 bytes regardless if the column is empty or contains 'dog', 'feline', 'lizard', 'fish', or whatever. For instance if the value of pet_type is 'dog' the actual value stored in the database is, 3 bytes for 'd', 'o', 'g' + 7 bytes for the remaining spaces.

Variable-length character types
Variable length character types store a variable number of bytes in storage with a minimum of 2 bytes for storing the length of the string. Variable length types can be declared as character varying(n) or varchar(n)

first_name varchar(20);
Although the column first_name is declared with a 20 byte length, the actual length can range from 2-20 bytes. Remember the last 2 bytes are reserved for storing the length. If the column first_name contains the value 'John' the actual size in storage is 6 bytes. 4 bytes are used for the characters 'J', 'o', 'h', 'n' and 2 bytes are used for the length.

Numeric
Numeric types store type of numbers either whole or fractional. The minimum and maximum of the different numeric types will depend on the SQL implementation. The different type of numeric types are.

bit(n)
bit varying(n)
integer
smallint
bigint
real(s)
float(p,s)
double precision(p,s)
decimal(p,s)
Decimal
Decimal types are used to store exact fractional number values like money.

annual_income decimal(10,2);
The decimal data types decimal, float, double have a syntax that includes type(p,s). In the syntax 'p' is the precision that represents the number of significant digits, 's' is the scale that represents the number of digits after the decimal point. In the example of above annual_income will have a maximum of 10 digits with 8 before the decimal and 2 after it, with a max value of 99,999,999.99

Integer
Integer data types store signed and unsigned whole numbers.

age int;
Floating point
Floating point types store approximate fractional number values. The precision and scale of floating point numbers are variable length.

avg_temp float(10);
Temporal
Temporal types store data related to dates and times.

Date
The date type represents a date value with 3 parts year, month, and day. The range for date types is typically 0001-01-01 to 9999-12-31. Dates are also typically stored in the format YYYY-MM-DD.

dob date;
Time
The time type represents data related to time of day in hours, minutes, and seconds. The time type has the format HH:MM:SS.

received_at time;
Timestamp
The timestamp type represents data related to a date and time. Timestamps have the format YYYY-MM-DD HH:MM:SS. This format has a space between the date and time sections.

created_at timestamp;
On top of these very common data types, a database vendor can add any number of other data types to the implementation. Some vendors add spatial types, boolean types, json types, large object types, and more.

Real World Application

Understanding SQL data types is important for several reasons:

Data Integrity: Knowing the appropriate data types ensures data integrity by enforcing constraints and preventing invalid data from being stored in the database. For example, using the VARCHAR data type for storing strings ensures that the data length is within the specified limit, preventing data truncation.

Storage Efficiency: Choosing the correct data types helps optimize storage space in the database. Using data types with appropriate sizes reduces the amount of disk space required to store data. For example, using the INT data type for storing integer values requires less storage space compared to using VARCHAR for the same purpose.

Application Compatibility: Knowledge of SQL data types ensures compatibility between the database schema and application code. Application developers need to understand the data types used in the database to correctly map database columns to application variables and parameters.

In summary, knowing about SQL data types is essential for maintaining data integrity, optimizing storage and performance, and ensuring application compatibility.

Implementation

Here's an example SQL CREATE TABLE statement that demonstrates the use of various common data types:


CREATE TABLE Employee (
    EmployeeID INT,
    FirstName VARCHAR(50),
    LastName VARCHAR(50),
    Gender CHAR(1),
    DateOfBirth DATE,
    Salary DECIMAL(10, 2),
    IsManager BOOLEAN,
    DepartmentID INT,
    JoinDate TIMESTAMP
);

Let's explain the columns in this example:

EmployeeID: INT data type is used for storing integer values.
FirstName and LastName: VARCHAR data type is used for storing variable-length character strings.
Gender: CHAR data type is used for storing single-character values.
DateOfBirth: DATE data type is used for storing date values.
Salary: DECIMAL data type is used for storing fixed-point numbers with precision and scale.
IsManager: BOOLEAN data type is used for storing boolean values (true or false).
DepartmentID: INT data type is used for storing integer values.
JoinDate: TIMESTAMP data type is used for storing date and time values.
Summary

Data types enforce the expected types of values inserted into a table. SQL provides a standard list of data types which can be categorized into 3 main categories, each with its own subcategories.

Character types
Fixed-length
Variable-length
Numeric types
Decimal
Integer
Floating point
Temporal types
Date
Time
Timestamp



# SQL Basics and JDBC
## Overview Of Sublanguages
Learning Objectives

After completing this module, associates should be able to:

Define SQL Sublanguages
Describe the purpose of using SQL Sublanguages
Description

Structured Query Language or SQL is the standard language for working with RDBMS (Relational Database Management System). SQL is used to administer and manipulate SQL servers. SQL is a scripting language that is interpreted by the database server. SQL is used to:

Define database structure
Manipulate stored data
Define data access permissions
Control concurrent data access
Query stored data
To accommodate the operations of the above categories, SQL is broken into 5 sublanguages.

Sublanguage	Description
DDL	Data Definition Language. Defines the data structure
DML	Data Manipulation Language. Insert, Update, Delete record
DCL	Data Control Language. Grant or revoke access permissions to database object
TCL	Transaction Control Language. Defines boundaries for concurrent operations.
DQL	Data Query Language. Search, filter, group, aggregate stored data
Each sublanguage is responsible for a specific set of operations and have a specific set of commands associated with them. Further details on each sublanguage will be discussed in their respective sections.

Real World Application

Consider a scenario where we have to use all 5 SQL sublanguages commands, so we have to be familiar with all the commands in each of DDL, DML, DCL, and DQL and we should be able to use them in our project development. The commands are:

DDL

CREATE
ALTER
DROP
TRUNCATE
RENAME
DML

INSERT
UPDATE
DELETE
DQL

SELECT
DCL

GRANT
REVOKE
Some TCL commands include COMMIT, ROLLBACK, SAVEPOINT.

Implementation

The DDL sublanguage of SQL is utilized to create and manage the structure of a database. DDL consists of the commands CREATE, DROP, ALTER, TRUNCATE, and RENAME. Using DDL the overall structure is modeled by creating objects like tables where the specific columns, data types, constraints, and relationships are defined.

The DML sublanguage of SQL is utilized to create, update, and delete data in a database. DML consist of the INSERT, UPDATE, and DELETE commands. Using DML, the records in database are manipulated to reflect to overall state of the applications that utilize the database for persistence.

The SQL DQL sublanguage is the backbone for querying a database for data. The command set consists of the single SELECT command. However, the sublangage is built on a grammar structure that is used to:

Search data
Project record views
Filter records
Group values
Offset resultsets
The SELECT statement is the crux of the DQL sublanguage and is composed of clauses that determine how records are selected from the database.

Phrase	Clause 1	Clause 2
Search	... FROM table_ref	
Project	SELECT col_1 [, col_2] ...	
Filter	WHERE where_condition	
Group	Group By group_list	Having having_condition
Offset	Limit count	Offset count
The DCL sublanguage is used to GRANT or REVOKE access privileges to databases and database objects.

Summary

Structured Query Language is the language used to administer SQL-based RDBM systems. SQL is based on ANSI standard ISO/IEC 9075:2016 (ANSI X3.135). There are many vendor specific implementations of the SQL standard, but the standard itself can be divided into 5 sublanguage categories: DDL, DML, DCL, TCL, DQL. Each sublanguage is responsible for a specific set of operations on the database.


## DDL
Learning Objectives

After completing this module, associates should be able to:

Describe the DDL sublanguage
Identify the command set of DDL
Execute DDL statements on a RDBMS
Description

Data Definition Language is the SQL language subset used for defining data or altering structure in the database.

In order to utilize the functions of the DDL sublanguage, a database user must have the appropriate permissons on the server and on the particular parent object. For instance, to create a table on a database, the user must have the permission to create the table on the database object. That same user may not have any other permissions on the same parent object.

Commands
CREATE
DROP
ALTER
RENAME
TRUNCATE
COMMENT
Create
The CREATE command is used to create objects on the server. CREATE can be used to create:

Database
User
Table
Index
Trigger
Function
Stored Procedure
View
Drop
The DROP command is used to remove objects from the server. Any object created using the CREATE command can be dropped using the DROP command.

Alter
The ALTER command is used to change some characteristics of an object. The command will ultimately be used to add, drop, or modify some option on the object.

The ALTER command is commonly used to change table characteristics, like:

Add/Drop columns
Add/Drop constraints
Modify column data types
Modify column constraints
Rename
The RENAME command is used to rename objects.
NOTE: The availability and syntax of the RENAME command for database objects vary between database management systems (DBMS). Consult the specific DBMS documentation to ensure accurate command usage

Truncate
The TRUNCATE command is used to remove all data from a table along with all space allocated for the records. Unlike DROP truncate will preserve the structure of the table.

Comment
The COMMENT command is typically used to add comments or descriptions to database objects like tables, columns, or views. This information is not used by the database itself but can be helpful for documentation purposes or for providing additional information about the structure of the database. You could also write comments in SQL by using single line -- and multi-line /* */ comments.

Real World Application

The DDL sublanguage is used to define the structure of the database that will model the data which is used to persist application state. The main commands behind DDL are CREATE, DROP, ALTER, TRUNCATE, RENAME, COMMENT. The combination of these commands are used to maintain the structure of the database designed to support application functionality.

Database administrators use the DDL sublanguage to define complex tables and the relationships between them, the constraints on the data in the table, search indexs, large table partitions, and so on. Let's envision a complex system for user-identity management. Some of the concepts here go beyond the atomic concept of DDL and creating or maintaining structure. Focus on just the DDL specific tasks.

IAM databse ERD

The script required to create this ERD is a combination of the CREATE and ALTER commands.

CREATE DATABASE IF NOT EXISTS IAM;

USE IAM;

CREATE TABLE IF NOT EXISTS permission_categories (
	id BIGINT PRIMARY KEY,
    name VARCHAR(30) NOT NULL UNIQUE,
    description text(255)
);

CREATE TABLE IF NOT EXISTS permissions (
	id BIGINT PRIMARY KEY,
    categoryId BIGINT NOT NULL,
    name VARCHAR(30) NOT NULL UNIQUE,
    index(categoryId, name)
);

CREATE TABLE IF NOT EXISTS roles (
	id BIGINT PRIMARY KEY,
    name VARCHAR(30) NOT NULL UNIQUE
);

CREATE TABLE IF NOT EXISTS roles_permissions(
	roleId BIGINT,
    permissionId BIGINT,
    PRIMARY KEY(roleId, permissionId)
);

CREATE TABLE  IF NOT EXISTS login_activities_lu (
	id BIGINT PRIMARY KEY,
    type VARCHAR(10) NOT NULL UNIQUE
);

CREATE TABLE IF NOT EXISTS profiles (
	id BIGINT PRIMARY KEY,
    firstName VARCHAR(30) NOT NULL,
    lastName VARCHAR(40) NOT NULL,
    email VARCHAR(100) NOT NULL UNIQUE
);

CREATE TABLE IF NOT EXISTS users (
	id BIGINT PRIMARY KEY,
    username VARCHAR(20) NOT NULL UNIQUE,
    password VARCHAR(20) NOT NULL,
    profileId BIGINT NOT NULL,
    isActive BOOLEAN DEFAULT true,
    isLocked BOOLEAN DEFAULT false
);

CREATE TABLE IF NOT EXISTS login_activities (
	id BIGINT PRIMARY KEY,
    userId BIGINT NOT NULL,
    activityId BIGINT NOT NULL,
    activityDate DATETIME DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS users_roles (
	userId BIGINT NOT NULL,
    roleId BIGINT NOT NULL
);

ALTER TABLE permissions ADD CONSTRAINT fk_permissions_category_id FOREIGN KEY(categoryId) REFERENCES permission_categories(id);
ALTER TABLE roles_permissions ADD CONSTRAINT fk_permissions_role_id FOREIGN KEY(roleId) REFERENCES roles(id);
ALTER TABLE roles_permissions ADD CONSTRAINT fk_roles_permission_id FOREIGN KEY(permissionId) REFERENCES permissions(id);
ALTER TABLE users ADD CONSTRAINT fk_users_profile_id FOREIGN KEY(profileId) REFERENCES profiles(id);
ALTER TABLE login_activities ADD CONSTRAINT fk_login_user_id FOREIGN KEY(userId) REFERENCES users(id);
ALTER TABLE login_activities ADD CONSTRAINT fk_login_activity_id FOREIGN KEY(activityId) REFERENCES login_activities_lu(id);
ALTER TABLE users_roles ADD CONSTRAINT fk_ur_user_id FOREIGN KEY(userId) REFERENCES users(id);
ALTER TABLE users_roles ADD CONSTRAINT fk_ur_role_id FOREIGN KEY(roleId) REFERENCES roles(id);
Once the schema is created using DDL, the responsibility for maintaining the schema falls upon the database administrator to continuously manage the structure in response to changing requirements. The need to add new tables, remove tables, add constraints or columns could present themselves on a daily basis as teams coordinate their needs for persistent data structure.

Implementation
Create
Let's start out by creating a new database. The syntax for creating a database is,

CREATE {DATABASE | SCHEMA} [IF NOT EXISTS] db_name;
Let's create a database named my_database

CREATE DATABASE IF NOT EXISTS my_database;
The results:

Create database result

NOTE: it may be required to refresh your IDE view to see the results

The same CREATE command can be used to create a table in the newly created database, with slight change to the syntax.

CREATE [TEMPORARY] TABLE [IF NOT EXISTS] table_name (create_definition, ...) [table_options] [partition_options]
Focus on the create command and the (create_definition). The create_definition section of the statement can be summed up as the definition of the columns in the table. Generally in this format.

(
    col_name data_type constraints,
    ...
)
Let's now create a table call my_table

USE my_database;
CREATE TABLE my_table (
	id INT PRIMARY KEY,
    my VARCHAR(10) NOT NULL,
    my_other_value FLOAT DEFAULT 10.0
);
The results:

Create table results

NOTE: In MySQL it is necessary to select a database to which to apply statements. You should execute the USE db_name statement before the create statement

Now that the database has some structure, let's use the ALTER command to change the structure of the table. The syntax.

ALTER TABLE table_name [alter_option, [alter_option], ...] [partition_options]
NOTE: The ALTER command has a lot of different options depending on what and how you want to change the table.

Let's alter the table by adding a column using the syntax:

ALTER TABLE table_name ADD COLUMN col_name col_definition
ALTER TABLE my_table ADD COLUMN my_last_value boolean default true;
The results:

Alter table results

At this point the database has a working structure. Following is the syntax of the remaining commands which are trivial in comparison.

DROP [TABLE] table_name
Rename
RENAME TABLE old_name TO new_name [, old2_name TO new2_name] ...
Truncate
TRUNCATE [TABLE] table_name
Exercise (Optional)
Create a database named library
In the library database create a table name books
column: id INT PRIMARY KEY
column: title VARCHAR(100) NOT NULL
column: author VARCHAR(50) NOT NULL
Alter the books
Add column: publication_date not null timestamp
Add constraint: title unique
Summary

The DDL sublanguage of SQL is utilized to create and manage the structure of a database. DDL consist of the commands CREATE, DROP, ALTER, TRUNCATE, and RENAME. Using DDL the overall structure is modeled by creating objects like tables where the specific columns, data types, constraints, and relationships are defined.


## Defining Schema
Learning Objectives

After completing this module, associates should be able to:

Define what a Schema is
Explain advantages of the schema
Understand all components within a schema
Description

A database schema is the collection of database objects and elements that are logically connected through tables, stored procedures, views, triggers, functions and more. The schema defines how the table relationships should be built and organized within a particular database.

Note: Regarding MySQL, "schema" and "database" are synonymous. For other RDBMSs, multiple schemas are allowed in a database and each schema allows the ability to grant specific access and permissions to users.

Advantages
Allow several users per schema
Multiple Schemas
Move database objects between schema
Manage Database objects in a logical group
Real World Application

When a business is working with a relational database, it leverages schemas to manage all of its information, especially related to sales of product. The business has tables of relational data stored inside the schema for all of their products and related sales. Along with this they have stored procedures, views and functions associated with this sales information for repeated queries to generate for repeated analysis such as quarterly reports to deliver to the company, sales reports for specific items, profit margins, etc.

Implementation

When working with SQL you use the keyword CREATE to generate a schema, along with all the other components utilized by the schema.

Generate a new schema in SQL
CREATE SCHEMA employment;
GO
Generate a table inside a specific schema
CREATE TABLE employment.employees(
    employee_id INT PRIMARY  KEY IDENTITY,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    department VARCHAR(50)
);
Generate a view inside of a specific schema
CREATE VIEW employment.hr_employees AS
SELECT first_name, last_name
FROM employment.employees
WHERE department = 'HR';
Summary

A database schema is the collection of database objects and elements, such as tables, stored procedures, views, triggers, functions and more, that are logically connected to one another.

The schema defines how the table relationships should be built and organized within a particular database.
Multiple schemas are allowed in a database and each schema allows the ability to grant specific access and permissions to users.
We can move database objects between schema
Manage Database objects in a logical group


## CREATE DROP TRUNCATE
Learning Objectives

After completing this module, associates should be able to:

Describe and understand the difference between CREATE, DROP, and TRUNCATE
Description

CREATE

It is part of the DDL sublanguage
CREATE is a command that creates new database objects, like tables, in the database
Tables are used to store the data in the database with a unique name and schema
tables also require at least one column to be valid
Each column requires an associated data type
Certain RDBMS have transactional DDL language that allows you to rollback creates
PostgreSQL, SQL Server and SQLite among various others have transactional DDL
MySQL and Oracle Database do not support transactional DDL
Used to also define schemas, procedures, views, indexes and even databases
DROP

It is part of the DDL sublanguage
The DROP command removes the table from the database.
It removes all the indexes, privileges, rows and frees the memory space for other objects.
You can not drop the table referenced by foreign key constraint.
The objects related to the table like views, procedures needs to be explicitly dropped.
No DML triggers will be fired.
We can not roll back the drop table operation.
TRUNCATE

It is part of the DDL sublanguage
It is used to remove all the rows from the table.
It does not return number of rows truncated from the table.
It also deallocates the memory for that object and other object will use that deallocated space.
TRUNCATE operation can not roll back as it does not operate on each row.
We cannot use condition when it comes to using TRUNCATE.
Real World Application

FOR CREATE :

Consider a scenario where we want to create a fresh new table of course information, now when we create, we must specify all the information and appropriate data type for each column of information related to the course information. You can also use CREATE for schemas, procedures, views, indexes and even databases.

FOR TRUNCATE :

Consider a scenario where we want to delete or remove all records from table named student, now if we truncate, then the table structure remains and all the records will be removed.

FOR DROP :

Consider a scenario where we want to delete the whole table called Department, then we use DROP. Here, all the records and the table structure is completely removed.

Implementation

Syntax to DROP a table from database
DROP TABLE table_name;
Example:

DROP TABLE Movies;
Syntax to CREATE a table
CREATE TABLE table_name{
    table_id INT PRIMARY KEY IDENTITY(1,1),
    table_value VARCHAR (50) NOT NULL,
    table_value INT,
    foreign_key INT
    FOREIGN KEY (foreign_id) REFERENCES foreign_table_name (foreign_id)
}
Example:

CREATE TABLE courses (
    course_id INT PRIMARY KEY IDENTITY (1, 1),
    course_name VARCHAR (50) NOT NULL,
    max_capacity VARCHAR (50) NOT NULL,
    professor_id INT
    FOREIGN KEY (professor_id) REFERENCES professors (professor_id)
);
Syntax to TRUNCATE a table from database
TRUNCATE TABLE table_name;
Example:

TRUNCATE TABLE student;
Summary

DROP	CREATE	TRUNCATE
It removes a table form the database	It creates a new table with no information, along with views, procedures, schemas, indexes and databases	It is executed using table lock, where whole table is locked while removing the records
It maintains the log, so it is slower than TRUNCATE	It maintains the log	Least amount of logging needed so it is faster in performace
It cannot be rolled back	It cannot be rolled back	It cannot be rolled back


## DML
Learning Objectives

After completing this module, associates should be able to:

Describe the DML sublanguage
Identify the command set of DML
Execute DML statements on a RDBMS
Description

Data Modification Language is the SQL language subset used for modifying data in the database.

In order to utilize the functions of the DML sublanguage, a database user must have the appropriate permissons on the server and on the particular parent object.

Commands
INSERT
UPDATE
DELETE
The INSERT command is used to add records to a database table. The UPDATE command is used to modify existing data in a database table. The DELETE command is used to remove data from a database table.

Real World Application

DML is used to manipulate the data that is being stored in the database. Often DML is used in enterprise application solutions that utilize bridging libraries like JDBC in Java or EF in .NET, but of course it is possible to use DML directly in a SQL IDE or console. In the real world, DBA could be responsible for adding data to bootstrap the database into a working state for the applications that will rely on the data.

Let's bootstrap the permissions and roles tables in the following IAM schema:

CREATE DATABASE IF NOT EXISTS IAM;

USE IAM;

CREATE TABLE IF NOT EXISTS permission_categories (
	id bigint primary key,
    name varchar(30) not null unique,
    description text(255)
);

CREATE TABLE IF NOT EXISTS permissions (
	id bigint primary key,
    categoryId bigint not null,
    name varchar(30) not null unique,
    index(categoryId, name)
);

CREATE TABLE IF NOT EXISTS roles (
	id bigint primary key,
    name varchar(30) not null unique
);

CREATE TABLE IF NOT EXISTS roles_permissions(
	roleId bigint,
    permissionId bigint,
    primary key(roleId, permissionId)
);

CREATE TABLE  IF NOT EXISTS login_activities_lu (
	id bigint primary key,
    type varchar(10) not null unique
);

CREATE TABLE IF NOT EXISTS profiles (
	id bigint primary key,
    firstName varchar(30) not null,
    lastName varchar(40) not null,
    email varchar(100) not null unique
);

CREATE TABLE IF NOT EXISTS users (
	id bigint primary key,
    username varchar(20) not null unique,
    password varchar(20) not null,
    profileId bigint not null,
    isActive boolean default true,
    isLocked boolean default false
);

CREATE TABLE IF NOT EXISTS login_activities (
	id bigint primary key,
    userId bigint not null,
    activityId bigint not null,
    activityDate datetime default NOW()
);

CREATE TABLE IF NOT EXISTS users_roles (
	userId bigint not null,
    roleId bigint not null
);

ALTER TABLE permissions ADD CONSTRAINT fk_permissions_category_id FOREIGN KEY(categoryId) REFERENCES permission_categories(id);
ALTER TABLE roles_permissions ADD CONSTRAINT fk_permissions_role_id FOREIGN KEY(roleId) REFERENCES roles(id);
ALTER TABLE roles_permissions ADD CONSTRAINT fk_roles_permission_id FOREIGN KEY(permissionId) REFERENCES permissions(id);
ALTER TABLE users ADD CONSTRAINT fk_users_profile_id FOREIGN KEY(profileId) REFERENCES profiles(id);
ALTER TABLE login_activities ADD CONSTRAINT fk_login_user_id FOREIGN KEY(userId) REFERENCES users(id);
ALTER TABLE login_activities ADD CONSTRAINT fk_login_activity_id FOREIGN KEY(activityId) REFERENCES login_activities_lu(id);
ALTER TABLE users_roles ADD CONSTRAINT fk_ur_user_id FOREIGN KEY(userId) REFERENCES users(id);
ALTER TABLE users_roles ADD CONSTRAINT fk_ur_role_id FOREIGN KEY(roleId) REFERENCES roles(id);
We'll use DML's INSERT INTO commands to add information or data into our tables.

INSERT INTO roles (id, name) values (1, 'ADMIN'), (2, 'OWNER'), (3, 'EDITOR'), (4, 'VIEWER');
INSERT INTO permission_categories (id, name, description) VALUES (1, 'HR', 'Permissions assigned to members of the HR team'),
	(2, 'Accounting', 'Permissions assigned to members of the accounting team'), (3, 'Technology', 'Permissions assigned to members of the technology team');
INSERT INTO permissions (id, categoryId, name) VALUES (1, 1, 'TIMESHEETS'), (2, 2, 'PAYROLL'), (3, 3, 'EVALUATION');
The actual amount of bootstrapping that could be done is dependent on the problem at hand, and of course IAM is a very large problem. The above could demonstrates the use of the insert statements of DML in the schema. Pay attention to the order some of the statements. For instance the permissions table has a relationship with permission_catagories so in order to insert the categoryId into the permissions table we must first insert it into the permission_catagories table.

Should the need arise to modify a record in the database, like re-assign the 'PAYROLL' permission from 'Accounting' to 'HR'.

UPDATE permissions SET categoryId=1;
Or should the need arise to remove the 'VIEWER' role.

DELETE FROM roles WHERE name='VIEWER';
Implementation
Insert
The INSERT command is used to store new records in a database table.

INSERT INTO <table_name> (col1 [, col2, ...]) {VALUE | VALUES} (valuelist) [, (valuelist), ...]
The INSERT command to insert 1 or more records. Below is an example:

INSERT INTO my_table (id, my_value, my_other_value) VALUES (1, 'some data', 1.0);
The results of the insert statement are the number of rows affected by the statement. The results should be:Single insert result

Multiple records can be inserted using a single INSERT command.

INSERT INTO my_table (id, my_value, my_other_value) VALUES (2, 'misc data', 2.0), (3, 'even data', 100.13);
Multiple insert result

Update
The UPDATE command is used to modify whole records or parts of records in a database table.

UPDATE <table_name> SET assignment_list [WHERE where_list]
The UPDATE command can modify 0 or more records and the results of the command are the number of rows affected by the command.

UPDATE my_table SET my_value='new value';
Unfiltered update resultsAs you can see with the previous update statement all of the inserted rows were updated. This may or may not be your intended results.

To limit the number or rows that are updated, it is best practice to include the optional WHERE clause.

UPDATE my_table SET my_value='where val' WHERE id=1;
Filtered update resultsNow the results of the update statement are restricted to the WHERE clause. Since there is only a single record with the id of 1, only that single record is updated.

Delete
The DELETE command is used to remove data from a database table.

DELETE FROM <table_name> [WHERE where_condition]
Much like the UPDATE command the DELETE command has an optional WHERE clause. However, the DELETE command can be very destructive, it is best practice to always have a WHERE clause on the statement. If you exclude a WHERE clause on the DELETE command the database will remove all records from the table.

DELETE FROM my_table WHERE id=3;
The results of the DELETE command are the number of rows affected.Delete results

Exercise (Optional)
Using the library database and the books table from DDL implementation

Insert 3 books into the books table.
Insert a book titled, 'Learn SQL in a Snap', authored by 'Jim Dandy', published on 01-01-01
Modify the author of the first book inserted into the database
Delete the second and third books inserted into the database
Summary

The DML sublanguage of SQL is utilized to create, update, and delete data in a database. DML consist of the INSERT, UPDATE, and DELETE commands, Using DML the records in database are manipulated to reflect to overall state of the applications that utilize the database for persistence.


## INSERT
Learning Objectives

After completing this module, associates should be able to:

Define what the INSERT keyword accomplishes
Implement the INSERT keyword into SQL commands
Description

INSERT is a sql command to add a new record into the table within your database. INSERT statements only accepts the data that follows the rules defined through the creation of your table structure. The inserted data must satisfy all constraints and match the appropriate data types, otherwise you will result in an error. INSERT is always followed by the INTO keyword as well.

Real World Application

Every company requires the persistence of information, whenever new products are added to an e-commerce website the business will require the ability to add all of the details for the product into their database. They must maintain data consistency when applying their products to their database to capture the appropriate metrics for a successful business. These INSERT commands into their products table will follow the rules of the established table to prevent any data inconsistency.

Implementation

Basic INSERT command syntax is as follows:
INSERT INTO TABLE_NAME (column1, column2, columnN)
VALUES (value1, value2, valueN);
Example of INSERT command:
INSERT INTO products (product_id, p_name, quantity, p_description, category_id)
VALUES (42,'Supercomputer: Deep Thought', 1, 'Answers the ultimate question of Life, the Universe and Everything', 7)
Summary

INSERT command is used to add any record to a table as long as that information follows the constraints and data types applied to each of the columns.

Syntax:
INSERT INTO TABLE_NAME (column1, columnN)
VALUES (value1, valueN);


## UPDATE
Learning Objectives

After completing this module, associates should be able to:

Define UPDATE keyword
Leverage UPDATE syntax in database
Understand potential risks with UPDATE
Description

UPDATE statements allow us to modify existing records within a table. The UPDATE makes use of locks on each record while modifying the table and once the row is modified the lock is released, allowing for either single or multiple record changes with a single command.

NOTE: Be mindful and careful when you leverage UPDATE statements as they do not require a WHERE clause. Omission of the WHERE clause results in all records within the table being updated.
Real World Application

UPDATE commands are constantly being used to help manage employee information at both an individual record level and in mass. Such as, when HR needs to update the contact information for a employee that has changed addresses. Also, sometimes there is reorganization in departments so a team might be moved to a new department which also needs to be reflected by updating the employee's record.

Implementation

Basic Syntax Implementation for UPDATE
UPDATE TABLE_NAME
SET column1 = value1, column2 = value2, columnN = valueN
WHERE [condition];
Example of UPDATE for Single Record in employee with id = 12 Address
NOTE: Double check the ID whenever you use these commands to make sure you're updating the appropriate record
UPDATE employees
SET add_street = 'Philly Lane', add_number = '20211', add_zip = 19101
WHERE employee_id = 12;
Example of UPDATE for Multiple Records in Employee to transfer department_name from "QC" to "Tech"
UPDATE employees
SET department_name = 'Tech'
WHERE department_name = 'QC';
Summary

UPDATE allows us to change information for a single or multiple records inside the tables of our database. This is an extremely important tool that also requires close attention to detail, as the omission of the WHERE clause or inaccurate conditions specified could lead to drastic changes.


## DELETE
Learning Objectives

After completing this module, associates should be able to:

Define the DELETE command
Differentiate from TRUNCATE and DROP from DDL
Implementation for DELETE Commands
Understand the risks involved with DELETE
Description

DELETE is a keyword command in SQL used to remove a specific record from a table or relation. The DELETE command, generally, includes a WHERE clause to specify the records for deletion. However, much like the UPDATE command there is a risk that if the WHERE clause is not specified it will remove all records from the table. The DELETE command removes one record at a time and logs each deleted row, which is why it's not considered good practice to delete all records in a table using DELETE when you want to maintain the table structure. This is where TRUNCATE offers a better solution as it's both faster and removes the data by deallocating the data pages used to store the records. DELETE commands can also activate a trigger and work with indexed views.

Real World Application

Deleting records from a database is common when we need to remove unwanted data, such as looking at student records maintained at a college. When a student drops out from a course before the drop deadline, all grades and assessments from that course are deleted from the records as they no longer impact the student's GPA or show as a registered course. So they would include a few DELETE commands to both remove the student from the course, along with commands to remove any records or assessments associated with the course and that student.

Implementation

Basic Syntax for DELETE command
DELETE FROM TABLE_NAME
WHERE [condition];
Example to DELETE Student, student_id = 'student1@uni.edu' from an enrollment table for course_id = 101
DELETE FROM enrollment
WHERE student_id = 'student1@uni.edu' AND course_id = 101;
Example of DELETE all the above student's grades from a student_assessments table
DELETE FROM student_assessments
WHERE student_id = 'student1@uni.edu';
Summary

DELETE is a keyword command in SQL used to remove a specific record from a table or relation. The DELETE command, generally, includes a WHERE clause to specify the records for deletion.

However, much like the UPDATE command there is a risk that if the WHERE clause is not specified it will remove all records from the table.
The DELETE command removes one record at a time and logs each deleted row.
This why it's not considered good practice to delete all records in a table using DELETE when you want to maintain the table structure.
TRUNCATE offers a better solution for deleting all records from a table as it's both faster and removes the data by deallocating the data pages used to store the records.
DELETE commands can also activate a trigger and work with indexed views.


## DQL
Learning Objectives

After completing this module, associates should be able to:

Describe the DQL sublanguage
Identify the command set of DQL
Execute DQL statements on a RDBMS
Description

The Data Query Language is the SQL sublanguage used for querying data from the database. It is the major sublanguage used by applications to search, project, filter, join, aggregate and group data for displaying application state.

Commands
Unlike the other sublanguages, DQL is only associated with a single command

SELECT
The select command is used to build data queries.

Real World Application

The DQL sublanguage is the most widely used sublanguage in enterprise applications. Application developers use DQL select statements in combination with platform specific database libraries to query for data. Let's highlight a few enterprise application use cases of DQL.

E-Tail applications like Amazon, Walmart, and Best Buy may use DQL to

Query a database for the items in a user's shopping cart.
Query the database for the quantity of items left in inventory.
Query for a list of specials, sales, or targeted items based on user history.
Query a database for item information to craft UI elements.
Content driven applications like Youtube, Twitch, Twitter, and Facebook use DQL to

Query a database for user authentication information.
Query a database for content based on user feeds.
Query a database to highlight trending or viral content.
Query a database to deliver usage information for 3rd party applications.
Search engines like Google, Bing, and Yahoo use DQL to

Query for url data based on user queries.
Query for common questions to provide search suggestions.
Query for ads based on user queries.
Query for user search history.
The use cases go on and on. The main purpose of DQL is to provide access to the data stored in a database and structure queries in a way to deliver intelligent and useful information to apply to specific application problems.

Implementation

The DQL sublanguage has a simple one dimensional purpose: query data. However, this can be done in very complex ways depending on:

Desired projection
Relationships
Filtered records
Grouping
Aggregation details
Phrasing
A basic SQL query can be divided into clauses that describe different parts of the query.

SELECT <projection> FROM <table_name> <filter> <grouping> <ordering> <offset>
According to official documentation, this is how a query can be structured.

SELECT [ALL | DISTINCT]
    select_expr [, select_expr] ...
    [into_option]
    [FROM table_ref]
    [WHERE where_condition]
    [GROUP BY {col_name | expr | position}]
    [HAVING having_condition]
    [ORDER BY {col_name | expr | position}]
        [ASC | DESC]
    [LIMIT {[offset,] row_count | row_count OFFSET offset}];
That is a lot of information, but as an important note, most of those clauses are optional. Let's query the following simple table:

CREATE TABLE my_table (
	id INT PRIMARY KEY,
    my VARCHAR(10) NOT NULL,
    my_other_value FLOAT DEFAULT 10.0
);
After creating the table, we can run the following query:

SELECT * FROM my_table;
id	my_value	my_other_value
1	where val	1
2	new value	2
As you can see the SELECT statement can be very simple.

Let's go over some more advanced usages of a SELECT statement.

Projection
The projection clause of a SELECT statement is the set of columns(or aliases) that are returned from the statement. The projection is declared in the select_expr clause which comes after the SELECT command. In the previous example, the wildcard * projection was used. It is best practice to NOT use the wildcard projection, but to enumerate each column(or alias) you would like to use in the query. Let's update the previous SELECT statement.

SELECT my_value, my_other_value FROM my_table;
my_value	my_other_value
where val	1
new value	2
Observe that the new SELECT statement returns only the enumerated columns and excludes the id column.

Columns in the projection can also have aliases assigned to them. Let's use an alias for our columns in the projection.

SELECT my_value as value, my_other_value as other FROM my_table;
value	other
where val	1
new value	2
Observe that the column names in the projection have been replaced by their aliases. Aliases are often used for simple reusability in complex SELECT statements especially when related tables used in joins have columns with the same names.

Filtering
The filtering clause of a SELECT statement is a WHERE clause that defines how selected rows are filtered from the table. WHERE clause use logical operators to select records that meet specific conditions.

Where Logical operators
Operator	Meaning
AND	true if both boolean expressions evaluate to true
IN	true if the operand is included in a list of expressions
NOT	Reverses the value of any boolean expression
OR	true if either or both boolean expressions is true
LIKE	true if the operand matches a pattern
BETWEEN	true if the operand falls within a range
To demonstrate some the use of these filter operators, let's use the following code:

CREATE schema fruits_and_veggies;
USE fruits_and_veggies;
CREATE TABLE IF NOT EXISTS produce ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(20) NOT NULL UNIQUE, price DECIMAL(3,2), type VARCHAR(10) NOT NULL);
INSERT INTO produce (name, price, type) VALUES ('navel orange', 1.99, 'citrus'), 
('mandarin orange', 0.75, 'citrus'), 
('tangerine', 0.50, 'citrus'), 
('red delicious', 2.00, 'apple'),
 ('jona gold', 2.50, 'apple'), 
 ('granny smith', 1.00, 'apple'), 
 ('blueberry', 0.40, 'berry'), 
 ('raspberry', 0.35, 'berry'), 
 ('kiwi', 0.75, 'berry'), 
 ('watermelon', 3.99, 'melon'), 
 ('cantaloupe', 2.99, 'melon'), 
 ('honeydew', 2.00, 'melon'), 
 ('lettuce', 2.99, 'leafy'), 
 ('spinach', 1.99, 'leafy'),
  ('pumpkin', 4.99, 'marrow'), 
  ('cucumber', 0.99, 'marrow'), 
  ('potato', 0.45, 'root'), 
  ('yam', 0.25, 'root'),
   ('sweet potato', 0.50, 'root'), 
   ('onion', 0.33, 'allium'),
    ('garlic', 0.25, 'allium'), 
    ('shallot', 0.60, 'allium');
Before we start let's look at all of the records in the produce table.

SELECT id, name, price, type FROM produce;
id	name	price	type
1	navel orange	1.99	citrus
2	mandarin orange	0.75	citrus
3	tangerine	0.50	citrus
4	red delicious	2.00	apple
5	jona gold	2.50	apple
6	granny smith	1.00	apple
7	blueberry	0.40	berry
8	raspberry	0.35	berry
9	kiwi	0.75	berry
10	watermelon	3.99	melon
11	cantaloupe	2.99	melon
12	honeydew	2.00	melon
13	lettuce	2.99	leafy
14	spinach	1.99	leafy
15	pumpkin	4.99	marrow
16	cucumber	0.99	marrow
17	potato	0.45	root
18	yam	0.25	root
19	sweet potato	0.50	root
20	onion	0.33	allium
21	garlic	0.25	allium
22	shallot	0.60	allium
And
The AND operator compares 2 boolean expressions. Both sets of expressions must eval to true for the whole statement to be true.

Expression	Value
TRUE AND TRUE	TRUE
TRUE AND FALSE	FALSE
FALSE AND TRUE	FALSE
FALSE AND FALSE	FALSE
(TRUE AND TRUE) AND (TRUE AND TRUE)	TRUE
(TRUE AND TRUE) AND (FALSE AND FALSE)	FALSE
Select all records that have a type equal to 'apple' and price greater than 1.00;

SELECT name, price, type FROM produce WHERE type='apple' AND price>1.00;
name	price	type
red delicious	2.00	apple
jona gold	2.50	apple
In
The IN operator compares an operand to a list and evaluates to true if the operand is in the list.

Expression	Value
'cat' IN ('cat', 'dog', 'goat')	TRUE
1 IN (100, 25, 17, 10000)	FALSE
Select all produce that have the type 'apple', 'root', 'berry', or 'allium';

SELECT name, price, type FROM produce WHERE type IN ('apple', 'root', 'berry', 'allium');

name	price	type
red delicious	2.00	apple
jona gold	2.50	apple
granny smith	1.00	apple
blueberry	0.40	berry
raspberry	0.35	berry
kiwi	0.75	berry
potato	0.45	root
yam	0.25	root
sweet potato	0.50	root
onion	0.33	allium
garlic	0.25	allium
shallot	0.60	allium
Not
The NOT operator reverses the boolean value.

Expression	Value
NOT FALSE	TRUE
NOT TRUE	FALSE
Select all produce that are not an 'apple' and not an 'allium'

SELECT name, price, type FROM produce WHERE NOT (type='apple') AND NOT (type='allium');
name	price	type
navel orange	1.99	citrus
mandarin orange	0.75	citrus
tangerine	0.50	citrus
blueberry	0.40	berry
raspberry	0.35	berry
kiwi	0.75	berry
watermelon	3.99	melon
cantaloupe	2.99	melon
honeydew	2.00	melon
lettuce	2.99	leafy
spinach	1.99	leafy
pumpkin	4.99	marrow
cucumber	0.99	marrow
potato	0.45	root
yam	0.25	root
sweet potato	0.50	root
Or
The OR operator compares 2 boolean expressions. Either one or both of the expressions must evaluate to true for the whole statement to be true.

Expression	Value
TRUE OR TRUE	TRUE
TRUE OR FALSE	TRUE
FALSE OR TRUE	TRUE
FALSE OR FALSE	FALSE
Select all produce with a type of 'citrus' or 'berry'.

SELECT name, price, type FROM produce WHERE type='citrus' OR type='berry';

name	price	type
navel orange	1.99	citrus
mandarin orange	0.75	citrus
tangerine	0.50	citrus
blueberry	0.40	berry
raspberry	0.35	berry
kiwi	0.75	berry
Like
The LIKE operator is used to match patterns. LIKE can be used for fuzzy logic where a given value either fully or partially matches a pattern. Patterns in SQL can be 0 or more characters and include 0 or more valid wildcard characters.

Wildcard	Use
% (percent)	match any string of zero or more characters
_ (underscore)	match any single character
Select all produce with a type that starts with 'a'.

SELECT name, price, type FROM produce WHERE type LIKE 'a%';
name	price	type
red delicious	2.00	apple
jona gold	2.50	apple
granny smith	1.00	apple
onion	0.33	allium
garlic	0.25	allium
shallot	0.60	allium
Select all produce that have a type that is exactly 5 characters long.

SELECT name, price, type FROM produce WHERE type LIKE '_____';
name	price	type
red delicious	2.00	apple
jona gold	2.50	apple
granny smith	1.00	apple
blueberry	0.40	berry
raspberry	0.35	berry
kiwi	0.75	berry
watermelon	3.99	melon
cantaloupe	2.99	melon
honeydew	2.00	melon
lettuce	2.99	leafy
spinach	1.99	leafy
Between
The BETWEEN operator match expression that fall in a range.

Select all produce with a price between 0.50 and 1.00.

SELECT name, price, type FROM produce WHERE price BETWEEN 0.50 and 1.00;
name	price	type
mandarin orange	0.75	citrus
tangerine	0.50	citrus
granny smith	1.00	apple
kiwi	0.75	berry
cucumber	0.99	marrow
sweet potato	0.50	root
shallot	0.60	allium
Select all produce that have name between 'o' and 'y'.

SELECT name, price, type FROM produce WHERE name BETWEEN 'o' AND 'y';
name	price	type
onion	0.33	allium
potato	0.45	root
pumpkin	4.99	marrow
raspberry	0.35	berry
red delicious	2.00	apple
shallot	0.60	allium
spinach	1.99	leafy
sweet potato	0.50	root
tangerine	0.50	citrus
watermelon	3.99	melon
Grouping Pt 1
The group by clause of the SELECT statement groups rows that have the same values into summary rows. Group by is often used with aggregate functions like count, max, min. Note: Most databases require that the group by clause contain all projected columns. This may affect the results of the query

Select the types and avg price of each type of produce.

SELECT type, AVG(price) FROM produce GROUP BY type;
type	AVG(price)
citrus	1.080000
apple	1.833333
berry	0.500000
melon	2.993333
leafy	2.490000
marrow	2.990000
root	0.400000
allium	0.393333
Grouping Pt. 2
The HAVING clause is used to filter out groups that meet a condition.

Select the types of produce that have an avg cost higher than the average of all produce.

SELECT type, AVG(price) AS group_avg, (SELECT AVG(price) FROM produce) AS gross_avg FROM produce GROUP BY type HAVING AVG(price) > gross_avg;
type	group_avg	gross_avg
apple	1.833333	1.480000
melon	2.993333	1.480000
leafy	2.490000	1.480000
marrow	2.990000	1.480000
Ordering
The ORDER BY clause is used to sort the returned records by a specified column. The records can be ordered either ASC (ascending) or DESC (descending). Ascending order is default if not specified.

Select all produce ordered alphabetically ascending.

SELECT name, price, type FROM produce ORDER BY name ASC;

name	price	type
blueberry	0.40	berry
cantaloupe	2.99	melon
cucumber	0.99	marrow
garlic	0.25	allium
granny smith	1.00	apple
honeydew	2.00	melon
jona gold	2.50	apple
kiwi	0.75	berry
lettuce	2.99	leafy
mandarin orange	0.75	citrus
navel orange	1.99	citrus
onion	0.33	allium
potato	0.45	root
pumpkin	4.99	marrow
raspberry	0.35	berry
red delicious	2.00	apple
shallot	0.60	allium
spinach	1.99	leafy
sweet potato	0.50	root
tangerine	0.50	citrus
watermelon	3.99	melon
yam	0.25	root
Do the same, but in reverse.

SELECT name, price, type FROM produce ORDER BY name DESC;
name	price	type
yam	0.25	root
watermelon	3.99	melon
tangerine	0.50	citrus
sweet potato	0.50	root
spinach	1.99	leafy
shallot	0.60	allium
red delicious	2.00	apple
raspberry	0.35	berry
pumpkin	4.99	marrow
potato	0.45	root
onion	0.33	allium
navel orange	1.99	citrus
mandarin orange	0.75	citrus
lettuce	2.99	leafy
kiwi	0.75	berry
jona gold	2.50	apple
honeydew	2.00	melon
granny smith	1.00	apple
garlic	0.25	allium
cucumber	0.99	marrow
cantaloupe	2.99	melon
blueberry	0.40	berry
Offset Pt. 1
The LIMIT clause restricts number of records returned from the SELECT statement.

Select the first 5 produce records after ordering alphabetically.

SELECT name, price, type FROM produce ORDER BY name ASC LIMIT 5;
name	price	type
blueberry	0.40	berry
cantaloupe	2.99	melon
cucumber	0.99	marrow
garlic	0.25	allium
granny smith	1.00	apple
Offset Pt. 2
The OFFSET clause specifies from which record position to start counting from. This is often used in conjunction with the LIMIT clause. NOTE: Some SQL implementations use the SKIP keyword instead of OFFSET

Given the array [1,2,3,4,5,6,7,8,9,10], the following are facts in context of LIMIT and OFFSET

Clause	Fact
[1,2,3,4,5,6,7,8,9,10]
OFFSET 0	[1,2,3,4,5,6,7,8,9,10]
LIMIT 5 OFFSET 0	[1,2,3,4,5]
OFFSET 2	[3,4,5,6,7,8,9,10]
LIMIT 5 OFFSET 2	[3,4,5,6,7]
Select the second 5 produce records after ordering alphabetically.

SELECT name, price, type FROM produce ORDER BY name ASC LIMIT 5 OFFSET 5;
name	price	type
honeydew	2.00	melon
jona gold	2.50	apple
kiwi	0.75	berry
lettuce	2.99	leafy
mandarin orange	0.75	citrus
Summary
The SQL DQL sublanguage is the backbone for querying a database for data. The command set is simple consisting of the single SELECT command. However, The sublangage is built on a grammar structure that is used to...

Search data
Project record views
Filter records
Group values
Offset resultsets
The SELECT statement, at the core of the Data Query Language (DQL) sublanguage, consists of clauses that dictate the selection of records from the database.

Phrase	Clause 1	Clause 2
Search	... FROM table_ref	
Project	SELECT col_1 [, col_2] ...	
Filter	WHERE where_condition	
Group	Group By group_list	Having having_condition
Offset	Limit count	Offset count


## Queries
Learning Objectives

After completing this module, associates should be able to:

Define basic queries in MySQL
Description

Basic queries are important to work in many websites and applications, and are at the core of how data is stored and transferred across the internet. In RDBMS, a query is any command used to retrieve data from a table. Queries are almost always made using the SELECT statement.

In this module, we will discuss the basic syntax of SQL queries. We will also practice some important SQL queries using simple examples.

List of most widely used MySQL queries include:

CREATE database
USE database
CREATE table
INSERT values
UPDATE record
DELETE record
SELECT record
TRUNCATE table
DROP table
Real World Application

The ability to write SQL queries is crucial for various reasons:

Data Retrieval: SQL queries are used to retrieve data from databases. Being able to write SQL queries allows you to extract the information you need from large datasets efficiently and accurately.

Data Manipulation: SQL queries can also be used to manipulate data in databases. With SQL, you can insert, update, or delete records in tables, allowing you to modify the data according to your requirements.

Data Analysis: SQL queries enable data analysis by allowing you to perform calculations, aggregations, and filtering operations on datasets. You can use SQL to summarize data, calculate statistics, and generate reports to gain insights into the underlying data.

Database Administration: SQL queries are essential for database administration tasks such as creating and modifying database schema, managing users and permissions, and optimizing database performance. Database administrators rely on SQL to maintain and manage databases effectively.

In summary, the ability to write SQL queries is essential for developers working with relational databases and it empowers individuals to interact with databases effectively and leverage the full potential of their data assets.

Implementation

Syntax to create database:

CREATE DATABASE WORK;
Syntax to select/use database:

USE WORK;
Syntax to create TABLE:

CREATE TABLE EMP
(id INT, NAME VARCHAR(25), AGE INT, PRIMARY KEY(id));
Syntax to alter TABLE:

ALTER TABLE EMP
ADD SALARY DOUBLE;
Syntax to insert values:

INSERT INTO EMP VALUES(111, "Ryan", 24, 5000);
Syntax to update record:

UPDATE EMP SET AGE = 23 WHERE id = 111;
Syntax to delete record:

DELETE FROM EMP WHERE id = 111;
Syntax to select record:

SELECT * FROM EMP;
Syntax to truncate table:

TRUNCATE TABLE EMP;
Syntax to drop table:

DROP TABLE EMP;
Summary

In this module, we have implemented the following:

CREATE database
USE database
CREATE table
INSERT values
UPDATE record
DELETE record
SELECT record
TRUNCATE table
DROP table


## Clauses
Learning Objectives

After completing this module, associates should be able to:

Define DQL Clauses
Understand the purpose of WHERE , ORDERBY and GROUPBY
Description

WHERE clause is used to filter the data from the table based on user needs.
GROUP BY clause is used in MySQL queries to organize records which have same attribute values.
ORDER BY clause is used in MySQL queries to return the records in a sorted manner. By default, it returns in ascending order, which can also be specified as ASC. To return in descending order, We mention DESC after the expression along with ORDER BY.
The difference between ORDER BY and GROUP BY
ORDER BY	GROUP BY
It ensures presentation of columns	It ensures presentation of rows
It is always used after the GROUP BY clause in SELECT statement	It is always used before the ORDER BY clause in SELECT statement
It is not mandatory to use aggregate functions in the ORDER BY	It is mandatory to use aggregate functions in the GROUP BY
The output is sorted based on the column's attribute values	The grouping of records is done based on the similarity among the row's attribute values
Real World Application

Consider a scenario where we want to know the employee's salary in a particular department and organize results in descending order based on the department column. In the case, we would need both the group by and order by clause to get the desired result.

Implementation

The Syntax to use WHERE clause is:
SELECT column1, column2, ...
FROM table_name
WHERE condition;
Example:

SELECT title
FROM movies
WHERE ticket_price=200;
The Syntax to use ORDER BY clause is:
SELECT expressions    
FROM tables    
[WHERE conditions]    
ORDER BY expression [ ASC | DESC ];    
Example:

SELECT marks
FROM STUDENTS
WHERE marks>60
ORDER BY marks DESC;
The Syntax to use GROUP BY clause is:
SELECT column_name, function(column_name)  
FROM table_name   
WHERE condition   
GROUP BY column_name;  
Example 1:

SELECT Dept, Salary
FROM Employee
WHERE Salary > 15000
GROUP BY employee.salary, employee.dept
ORDER BY Dept DESC;
Example 2:

SELECT Dept, AVG(Salary) as AvgSalary
FROM Employee
WHERE Salary > 15000
GROUP BY employee.dept
ORDER BY Dept DESC;
Let's break down the differences:
Column Selection:

Example 1: Selects individual Dept and Salary values.
Example 2: Selects Dept and the average Salary for each department.
GROUP BY clause:

Example 1: Groups by both salary and department, which doesn't change the result set.
Example 2: Groups only by department, allowing for aggregation of salaries within each department.
Aggregation:

Example 1: No aggregation is performed.
Example 2: Uses AVG() to calculate the average salary for each department.
When to use each approach:
Example 1:

Use when you want to see individual salary records for each employee in departments where at least one employee earns more than $15,000.
It's essentially equivalent to removing the GROUP BY clause entirely, as it doesn't change the result set.
This query might be useful for getting a detailed view of high-earning employees across departments.
Example 2:

Use when you want to analyze salary data at the department level.
It provides a summary view, showing the average salary for each department (considering only salaries above $15,000).
This is useful for comparing compensation across departments or identifying departments with higher average salaries.
The key difference is the level of detail and the type of analysis you're performing:
The Example 1 query gives you a granular view of individual salaries.
The Example 2 query provides an aggregated view, summarizing salary data by department.
Summary

The WHERE clause is not only used in SELECT statements, it is also used in UPDATE, DELETE as well
The ORDER BY clause sorts the result and shows it in ascending or descending order
The GROUP BY clause is used to group data based on the exact value in a specific field


## CRUD Operations
Learning Objectives

After completing this module, associates should be able to:

Define all CRUD Operations
Understand which SQL commands are associated with each operation
Implement the use of each of these
Description

CRUD operations are the foundation of any computer programming language that is based on the four operations used to implement persistent storage for applications such as a relational database. These operations are not strictly for SQL, but can also be utilized by NoSQL as well. CRUD stands for the following:

Create: CREATE & INSERT commands from SQL allow us to facilitate this operation
Read: SELECT command from SQL allow us to facilitate this operation
Update: UPDATE command from SQL allow us to facilitate this operation
Delete: DELETE command from SQL allow us to facilitate this operation
Real World Application

You cannot interact with databases without using CRUD operations are they are essential for anything requiring persistent storage. Even Web Developers wouldn't be able to use REST, a superset of CRUD, to access HTTP requests and responses. CRUD is also crucial for the end-user to perform actions on the site they are visiting, creating bookmarks and interacting with anyone on the internet. CRUD Operation helps businesses facilitate control of security by satisfying various access requirements and simplify design to allow for more scalability and better performance.

Implementation

Basic Syntax for Create Operations you've seen previously
CREATE TABLE TABLE_NAME (
    col_1 INT PRIMARY KEY, 
    col_2 VARCHAR(20), 
    col_N INT
);

INSERT INTO TABLE_NAME (column1, column2, columnN)
VALUES (value1, value2, valueN);
Basic Syntax for Read Operations you've seen previously
SELECT * FROM TABLE_NAME;

SELECT col_1, col_N
FROM TABLE_NAME
WHERE id = 1;
Basic Syntax for Update Operations you've seen previously
UPDATE TABLE_NAME
SET column1 = value1, column2 = value2, columnN = valueN
WHERE [condition]
Basic Syntax for Delete Operations you've seen previously
DELETE FROM TABLE_NAME
WHERE [condition];
Summary

Understanding CRUD operations allow developers to build powerful web applications that can persist and retrieve information rapidly and easily from a database.

CRUD:

Create: CREATE & INSERT commands from SQL allow us to facilitate this operation
Read: SELECT command from SQL allow us to facilitate this operation
Update: UPDATE command from SQL allow us to facilitate this operation
Delete: DELETE command from SQL allow us to facilitate this operation


## Constraints
Learning Objectives

After completing this module, associates should be able to:

Define a constraint
Describe the different SQL constraints
Description

SQL constraints are used to help validate data beyond just a simple data type. Below is a set of commonly used SQL constraints.

Constraint	Use
Not Null	Ensures that a column's value is not null.
Unique	Ensures that a column's value is unique in the table.
Primary Key	Combines unique and not null. Uniquely identifies each row.
Foreign Key	Links to a row in another table. Prevents the destruction of those links.
Default	Specifies a value for a column, if one is not given.
Check	Ensures the value of a column satisfies a specific condition.
Each column of a table can use a combination of these constraints, but some are mutually exclusive in some vendors or context. For instance:

Although it is possible to define both NOT NULL and DEFAULT constraints for a column, they serve distinct purposes. The DEFAULT constraint provides a fallback value when no explicit value is provided, while NOT NULL ensures that NULL values are never inserted. Using both together ensures that the column always contains a valid value.
DEFAULT with a CHECK constraint might lead to conflicting requirements.
Since a FOREIGN KEY constraint establishes a relationship between two tables based on the values of a column in both tables, using a CHECK constraint to enforce similar conditions is redundant.
Lastly, a PRIMARY KEY constraint implies that a column (or a set of columns) is both UNIQUE and NOT NULL . Therefore, explicitly adding a UNIQUE constraint to the same column is redundant.
Constraints are used to define a database schema and are the backbone for defining integrity constraints of the schema. Integrity constraints are rules or conditions defined on a database schema to maintain the accuracy, consistency, and reliability of data within a database. Integrity constraints collectively help maintain the quality and reliability of data in a database, preventing errors, inconsistencies, and ensuring that data remains meaningful for the applications and users interacting with the database. They play a crucial role in upholding the integrity of the data model and supporting data accuracy and reliability.

Real World Application

Constraints are tied directly to the table and define the integrity of the data as part of the schema. Administrators use constraints to apply database level validation according to application and database requirements.

Table 1: Vendor

CREATE TABLE Vendor(
 vendorId  CHAR(2) NOT NULL,
 vendorName VARCHAR(25) UNIQUE NOT NULL,
 PRIMARY KEY (vendorId)
); 
This table demonstrated the NOT NULL, PRIMARY KEY, and UNIQUE constraints

Table 2: SalesTransaction

CREATE TABLE SalesTransaction(
 transId VARCHAR(8) PRIMARY KEY,
 customerId CHAR(7) NOT NULL REFERENCES Customer(customerId),
 storeId INT NOT NULL REFERENCES Store(StoreId),
-- Using DEFAULT to default to the current date cannot be null because a default value will be given
 transDate DATE DEFAULT now()
);
The above table demonstrated the use of PRIMARY KEY, NOT NULL, FORIEGN KEY, and DEFAULT constraints.

Implementation

Adding constraints to a column can be done when the table is being created or afterward if the requirements should change. Specific disscussion on the command associated with creating or altering tables is out of the scope of this document, but will discussed in the appropriate section.

-- adding constraints during creation
create table <table_name> (
    <col_name> datatype constraint1 constraint2,
    constraint <constraint_name> <constraint_type>(constraint_option, ...)
)
-- adding constraints after creation
alter table <table_name> add constraint <constraint_name> <constraint_type>(constraint_option, ...)
Exercises (Optional)
Discuss how to model a table for a rideshare application. The table will be called "scheduled_pickups". As the name suggests, the table should manage riders who have scheduled rides immediately and in the future.

Summary

Constraints help define the integrity constraints of the database schema. Common constraints include:

Primary key
Foreign Key
Not Null
Unique
Default
Check


## Auto Incrementing
Learning Objectives

After completing this module, associates should be able to:

To define and implement AUTO INCREMENT or SERIAL
Description

AUTO INCREMENT allows a unique number to be generated automatically when a new row is inserted into a table.
SERIAL is used to create an auto-incrementing integer column.
Most often, AUTO INCREMENT or SERIAL acts as a primary key field that we would like to be created automatically every time a new record is inserted.
Depending on the dialect of SQL you are using will determine if you use the AUTO INCREMENT keyword or SERIAL keyword.
The syntax for auto-incrementing columns varies among SQL dialects, with differences in implementation. While the concepts are the same, different database systems may use distinct keywords or mechanisms for achieving similar functionality. It is important to refer to the documentation of the specific database system you are working with to understand the correct syntax for auto-incrementing columns.
PostgreSQL - SERIAL implicitly creates a sequence to generate unique values.
MySQL - AUTO_INCREMENT automatically increments the value for each new row.
SQLite - AUTOINCREMENT ensures that each new row inserted into the table will automatically receive a unique and incrementing value.
Real World Application

Consider a table where we want to keep track of users. Users should not have the same identifier so as a new user is added to the database we would want to auto-increment their id.

Table: Users

CREATE TABLE users (
 user_id SERIAL PRIMARY KEY,
 first_name VARCHAR(30),
 last_name VARCHAR(30),
 address VARCHAR(30),
 city VARCHAR(30),
 state VARCHAR(2),
 zip VARCHAR(5),
 social_number VARCHAR(11),
 username VARCHAR(30),
 password TEXT,
 email VARCHAR(50)
);
Implementation

Syntax to create a table with employee_id as an attribute which will AUTO_INCREMENT on inserting new record.

CREATE TABLE table_name(
variable_name variable_datatype AUTO_INCREMENT
);
Example:

CREATE TABLE employees (
    employee_id INT AUTO_INCREMENT PRIMARY KEY,
    employee_name VARCHAR(50),
    department_id INT
);
This same table written using SERIAL would look as follows:

CREATE TABLE table_name (
    variable_name SERIAL PRIMARY KEY,
    -- Other columns...
);
Example:

CREATE TABLE employees (
    employee_id SERIAL PRIMARY KEY,
    employee_name VARCHAR(50),
    department_id INT
);
Notice the syntactical differences between the two table queries. This example is why it is important to always refer to documentation when working with different dialects of SQL.

You can modify auto incremented values, however, it is highly discouraged as the value is automatically managed by the system and should represent a unique identifier for each row.

Summary

In some cases, you may not have any unique identifying characteristics in data; therefore, it makes sense to create a Primary Key.
Explicitly initializing and modifying the AUTO-INCREMENT or SERIAL value is possible at any time though highly discouraged.
Record identifiers can be created that are unique to each record.
Automatic incrementing allows flexibility in handling gaps between rows.
Different database systems may use distinct keywords or mechanisms for achieving similar functionality. It is important to refer to the documentation.


## CHECK
Learning Objectives

After completing this module, associates should be able to:

To define and implement CHECK
Description

The CHECK constraint is used to limit the range value that can be placed in a column.
It determines whether the value associated with the column is valid or not with the given criteria.
It helps to check what type of values are to be stored in a table's column.
Real World Application

The CHECK constraint in SQL is used to enforce specific conditions on the data that is being inserted or updated in a table. The CHECK constraint ensures that values in a column meet certain criteria, and it helps maintain data integrity by preventing the insertion of incorrect or inappropriate data.

Data Validation: Developers use CHECK to validate the values that can be inserted into a column.
For example, you might want to ensure that a column only contains negative numbers or that a string column only accepts values with a certain length.

Date and Time Constraints: Developers often use CHECK to enforce date and time constraints. For instance, ensuring that a date falls within a certain range.

Enum-like Values: If a column is supposed to store values from a predefined set, you can use CHECK to restrict the allowed values.

An aquarium database may use the CHECK constraint to ensure that the tank size and temperature values for your fish tank are within acceptable ranges. A small example of this is below.

Table 1: FishTank

CREATE TABLE FishTank (
   FishID INT PRIMARY KEY,
   Species VARCHAR(50),
-- Ensure tank size is positive and not too large
   TankSize DECIMAL CHECK (TankSize > 0 AND TankSize <= 100), 
-- Ensure temperature is within a reasonable range
   Temperature DECIMAL CHECK (Temperature >= 0 AND Temperature <= 30)
);  
In this example, the TankSize CHECK constraint (TankSize > 0 AND TankSize <= 100) ensures that the value is not only within the specified range but is also greater than 0, effectively enforcing that TankSize must be a positive number.

Implementation

An example table for the syntax is as follows. Please note that certain column names and attributes in this example would need to be modified depending on what you are trying to accomplish in your table.

CREATE TABLE products (
    productID INT PRIMARY KEY,
    productPrice DECIMAL CHECK (productPrice > 0)
);
This particular example is checking that the NumericColumn contains values greater than 0.

Summary

CHECK constraint is declared during table creation.
It is used to protect the integrity of the table as similar to other constraint.
It helps us to get only those values that are valid for the condition and our requirements.


## DEFAULT
Learning Objectives

After completing this module, associates should be able to:

To define and implement DEFAULT
Description

The DEFAULT constraint is made use of to set a default value for a column.
The DEFAULT value will be added to all new records, if no other value is specified.
The DEFAULT values are typically literal constants, there's an important exception for timestamp and datetime columns. Many database systems allow the use of functions like now(), CURRENT_TIMESTAMP, or similar to set the default value to the current time for these specific column types. This is a commonly used feature and an exception to the general 'literal constant' rule.
Use ALTER TABLE table_name ALTER COLUMN col_name SET DEFAULT to create a DEFAULT constraint to an existing column of a table.
Use ALTER TABLE table_name ALTER COLUMN col_name DROP DEFAULT to delete a DEFAULT constraint from an existing column of a table.
Real World Application

DEFAULT keyword in SQL can include scenarios where you want to ensure that a certain column always has a value, even if it's not explicitly provided during data insertion. This can be useful for maintaining data consistency and handling cases where certain information may be missing but can be assumed or assigned a default value. For example, you might use DEFAULT for timestamp columns to automatically record when a record was created if no specific timestamp is provided.

Some examples of when you may want to use DEFAULT include:

Timestamps for Record Creation: prefill date with current date and time that the record was created.
Boolean Values: Is the user subscribed to receive notifications.
Numeric Defaults: prefill values with a 0 until otherwise modified. This could be useful in a database that stores user orders. All orders start at 0 until items are added to the cart and purchased.
Categorization or Status: All orders set to pending until being processed.
Default Text Values: Notes column in a database may be set to not have ‘no notes’ as a default value until modified.
These examples demonstrate how DEFAULT can be used to handle cases where certain values are expected to be common or assumed if not explicitly provided. It helps maintain data integrity and simplifies data insertion by providing reasonable default values.

Implementation

It's important to refer to the documentation of the specific database system you are using, as the syntax and supported data types for DEFAULT can vary between database management systems

CREATE TABLE UserOnline (
    ID INT PRIMARY KEY,
    UserName VARCHAR(25),
    Online BOOLEAN DEFAULT false
);
-- Inserting a user with online status not specified (defaults to false)
INSERT INTO UserOnline (ID, UserName) VALUES (1, 'LemonadeStandBoss');
-- Inserting a user with online status explicitly set to true
INSERT INTO UserOnline (ID, UserName, Online) VALUES (2, 'CodeLikeABoss', true);
In this example, the first user record inserted (“LemonadeStandBoss”) would have a value of ‘false’ for the column ‘Online’, since no value was provided. Note that some SQL dialects may use the keyword DEFAULT as well within the insert statement.

Summary

DEFAULT constraints set default values for columns.
Use DEFAULT default_value to set a default constraint to a column.
Use ALTER TABLE table_name ALTER COLUMN col_name SET DEFAULT to create a DEFAULT constraint to a an existing column of a table.
Use ALTER TABLE table_name ALTER COLUMN col_name DROP DEFAULT to delete a DEFAULT constraint from an existing column of a table.


## Primary Key
Learning Objectives

After completing this module, associates should be able to:

To define PRIMARY KEY
Description

The PRIMARY KEY constraint that is used to uniquely identify each record in a table.
Primary keys must contain values that are UNIQUE and NOT NULL.
A table can have only a single primary key.
Primary keys should not be sensitive information.
For example, although social security numbers are unique, assigning them as primary keys is a security risk.
Real World Application

Almost every table has a primary key. A few examples are:

the employee id in the employee details table is the primary key
the student id in the student table of a college database is the primary key
branch id of any company with more than one branch is the primary key
All these are used to uniquely identify the record.

Implementation

Below is an example of creating a table named employee with employee_id as primary key

 CREATE TABLE employee(
    employee_id INT PRIMARY KEY,
    first_name VARCHAR(20),
    last_name VARCHAR(20),
    emp_role VARCHAR(20)
    );
DESC employee;
The DESC or DESCRIBE commands describe the table.

result:

Field	Type	Null	Key	Default	Extra
employee_id	int	NO	PRI	NULL	
first_name	varchar(20)	YES		NULL	
last_name	varchar(20)	YES		NULL	
emp_role	varchar(20)	YES		NULL	
Summary

the primary key is used to uniquely identify a record in the table.
there can be only one primary key for one table
primary key values should be unique and not null


## Referential Integrity
Learning Objectives

After completing this module, associates should be able to:

To define Referential integrity
Description

Referential integrity is the relationship between tables. Each table in a database has a primary key and this primary key can appear in other tables because of its relationship to data within other tables. When a primary key from one table appears in another table, it is called a foreign key.
Referential integrity does not allow the addition of any record in a table that contains the foreign key unless the reference table contains a corresponding primary key.
Referential integrity does not allow deletion of a record in a table that contains the foreign key. To delete the record in the parent table, the corresponding record in the child table should be deleted first. To solve this issue ON DELETE CASCADE is used.
Other options are to set the foreign key to null or to its default value (only if the default value references an existing value in the primary-key table).
Real World Application

Consider any database, for example, an Office Database with Employee, Branch, and Client tables. Employee table will have a foreign key called branch_id which is the primary key for the Branch table and the Client table will have a foreign key called employee_id which is a Primary key for Employee table. Any changes made in the Branch table will affect the Employee table. If a branch is closed (deleted), all the employees in that branch are fired (deleted), and if an employee leaves the company(deleted), all the clients of the employee in the Client table are lost (deleted).

Implementation

step 1: Create a table named Student.

 CREATE TABLE Student(
    student_id INT PRIMARY KEY,
    first_name VARCHAR(20),
    last_name VARCHAR(20),
    Major VARCHAR(20)
    );

Step 2: create a table named InternationalStudent.

CREATE TABLE InternationalStudent(
    country_id INT PRIMARY KEY,
    name VARCHAR(20),
    student_id INT,
    FOREIGN KEY(student_id) REFERENCES Student(student_id)
    ON DELETE CASCADE
);
Step 3: Insert values into Student table and InternationalStudent table.

INSERT INTO Student VALUES(1,"Taylor", "Swift","English Literature");
INSERT INTO Student VALUES(2,"Stephen", "Hawking","Physics");
INSERT INTO InternationalStudent VALUES(1,"USA",1);
INSERT INTO InternationalStudent VALUES(2,"UK",2);
SELECT * FROM InternationalStudent;
Result:

country_id	name	student_id
1	USA	1
2	UK	2
Step 4: delete a record from Student table

DELETE FROM Student WHERE Major="English Literature";
Now the record in child table InternationalStudent with student_id as 1 is deleted.

SELECT * FROM InternationalStudent;
Result:

country_id	name	student_id
2	UK	2
Summary

Referential integrity is used to build and maintain logical relationships between tables to avoid logical corruption of data.

Referential integrity is made up of the combination of a primary key and a foreign key.

Normally the table with a Primary key is considered as a parent table and the table with a foreign key is considered a child table.

Updating and Deletion in primary key table and Insertion and Updating in the foreign key table could violate the referential integrity.

To avoid violating Referential Integrity, ON DELETE CASCADE, set the foreign key to NULL, or set foreign key to default are implemented.


## Foreign Key
Learning Objectives

After completing this module, associates should be able to:

To define and implement FOREIGN KEY
Description

A FOREIGN KEY is a field (or collection of fields) in one table, that refers to the PRIMARY KEY in another table.

Real World Application

Consider any database, for example, a company database, an employee table has details like id, name, branch id, address, manager id, etc. Consider another table called branch which an employee belongs to. branch id in the employee table is a foreign key referencing the primary key of the branch table.

Similarly, consider another table called client, which has details like client name, employee id, etc. the employee id in client table is a foreign key referencing the primary key of employee table.

Implementation

Step 1: Create a Branch and Employee table

CREATE TABLE Branch(
    id INT PRIMARY KEY,
    name VARCHAR(20)
     );

CREATE TABLE Employee(
    id INT PRIMARY KEY,
    name VARCHAR(20),
    address VARCHAR(50)
     );
Step 2: Add branch_id as foreign key for employee table

ALTER TABLE employee ADD branch_id INT;
ALTER TABLE employee ADD FOREIGN KEY (branch_id) REFERENCES Branch(id);
 DESC employee;
Result:

Field	Type	Null	Key	Default	Extra
employee_id	int	NO	PRI	NULL	
first_name	varchar(20)	YES		NULL	
last_name	varchar(20)	YES		NULL	
emp_role	varchar(20)	YES		NULL	
branch_id	int	YES	MUL	NULL	
The id column in the branch table is its PRIMARY KEY and it is used by the FOREIGN KEY of the employee table.

Summary

A Foreign key is used to connect two or more tables so that data from both can be put to use in parallel.

A foreign key is a field or collection of fields in a table that refers to the Primary key of the other table. It is responsible for managing the relationship between the tables.

The table which contains the foreign key is called the child table, and the table whose primary key is being referred by the foreign key is called the Parent Table.


## CASCADE
Learning Objectives

After completing this module, associates should be able to:

Define the CASCADE keyword
Implement examples of CASCADE
Differentiate between a CASCADE with DELETE or UPDATE
Description

In SQL, CASCADE is a keyword used to simultaneous delete or update data from both the child and parent tables and is used in conjunction while writing the query with ON DELETE or ON UPDATE. Applying CASCADE keyword to the command applies the changes in both parent and child tables accordingly to the execution of that query. CASCADE is appended to the reference command for the foreign key when creating a table.

ON DELETE
Appending ON DELETE CASCADE to the foreign keys within our child table allow us the opportunity to delete the parent record and subsequently delete all relational information in any referenced table through one command. Otherwise, due to referential integrity it would require multiple commands to delete all of the referenced records inside of the child tables first, before removing the parent record.

ON UPDATE
Appending ON UPDATE CASCADE to the foreign key within our child table allows us to update information in a singular command by updating the parent table record will apply to all subsequent child tables information. This also helps reduce the amount of commands and keeps our referential integrity intact.

Real World Application

Imagine students enrolled in a courses at a college, but some students drop out mid-way through the semester and the school is not required to keep the records of students who drop out. In this example, we would want to structure our tables such that the foreign keys have an ON DELETE CASCADE to maintain referential integrity when removing that students records from the gradebooks of all courses they attended.

Keeping with the student examples, imagine the students in the database need their primary key id reset to adjust for the students that dropped out and make room for the freshman. Through this we need to update all the current records for each of these individuals to match accordingly in both the parent and child tables. When the foreign key is applied with ON UPDATE CASCADE it will make sure that every record's primary key association in the child tables is updated accordingly.

Implementation

Sticking with the students in college theme let's look at students in a college course and use CASCADE to handle deletions and updates.

Define the table for students
CREATE TABLE students(
    student_id INT PRIMARY KEY,
    student_name VARCHAR(40),
    email VARCHAR(20) UNIQUE
);
Define the table for courses
CREATE TABLE courses(
    course_id INT PRIMARY KEY,
    course_name VARCHAR(20),
    course_length_weeks INT,
    credits INT
);
Finally, lets define a junction table to handle enrollment and uses CASCADE
for the purposes of this example we will assume that courses never get updated
CREATE TABLE enrollments(
    course_id INT,
    student_id INT,
    grade INT,
    completion_status boolean,
    PRIMARY KEY(course_id, student_id),
    FOREIGN KEY(course_id) REFERENCES courses(course_id) ON DELETE CASCADE
    FOREIGN KEY(student_id) REFERENCES students(student_id) ON DELETE CASCADE ON UPDATE CASCADE
);
The above command allows for the deletion of all referenced courses through a single command by removing the record from the courses table. Along with this, the command also accounts for any updates or deletion within the students table records.
Summary

In SQL, CASCADE is a keyword used to simultaneous delete or update data from both the child and parent tables and is used in conjunction while writing the query with ON DELETE or ON UPDATE.

ON DELETE CASCADE allows us with one command to delete records in a parent table and all subsequent records in the child tables.
ON UPDATE CASCADE allows us with one command to update records in a parent table and all subsequent records in the child tables.


## Unique Key
Learning Objectives

After completing this module, associates should be able to:

To define and implement UNIQUE KEY
Description

The UNIQUE constraint is used to ensure every value of a column is different. This means that each row must have a distinct value in the specified column.

The UNIQUE key interacts with NULL values somewhat, unintuitively, so let’s dive deeper into this.

A UNIQUE key allows for NULL column values for records.

There is often confusion around how many NULL values a UNIQUE constraint can have in a column. Each database may have slight differences in the amount of NULL values that are allowed with a UNIQUE constraint. Some database systems only allow a single NULL value; however, databases often allow multiple NULL values.

In other words, when a column is declared as UNIQUE and allows NULL, the UNIQUE constraint applies only to the non-null values. Rows with NULL values in that column are not considered duplicates in terms of the UNIQUE constraint. This is because NULL is actually the absence of a value. However, keep in mind that each non-null value must still be unique across all rows.

It's essential to consult the documentation of the specific database system you are using to understand its behavior regarding UNIQUE constraints and NULL values. Always consider the unique constraints enforced by your database to ensure the correct behavior based on your requirements.

Keep in mind that a column can have multiple constraints applied to it. The UNIQUE constraint ensures that all values in the column are distinct. However, in most databases, UNIQUE does not prevent multiple NULL values since NULL is treated as an unknown value. To ensure that no NULL values exist in a column, use the NOT NULL constraint in combination with UNIQUE.

Real World Application

Primary Keys: The primary key of a table is often enforced using a UNIQUE constraint. This ensures that each record in the table has a unique identifier, helping maintain data integrity and providing a reliable way to reference and link records from other tables.
Email Addresses and Usernames: In user management systems, it's common to enforce UNIQUE constraints on email addresses or usernames to ensure that each user has a distinct identifier. This helps prevent duplicate accounts and ensures a unique mapping between users and their login credentials.
Product Codes or SKUs: In inventory or e-commerce systems, product codes or Stock Keeping Units (SKUs) are often assigned as unique identifiers. Applying a UNIQUE constraint to these columns helps avoid confusion and ensures that each product is uniquely identified in the system.
Identification Numbers: Various entities, such as social security numbers, employee IDs, or customer IDs, are typically required to be unique. Enforcing a UNIQUE constraint on these columns helps prevent errors and ensures the uniqueness of each identifier.
Reference Codes in Relationships: Establishing relationships between tables, it's common to use UNIQUE constraints on columns that serve as foreign keys. This ensures that the relationship is well-defined and that each reference corresponds to a unique record in the referenced table.
Phone Numbers (if used as identifiers): In certain applications, phone numbers may serve as unique identifiers. Enforcing a UNIQUE constraint on phone number columns helps avoid confusion and ensures that each phone number corresponds to a single entity.
URLs or Website Addresses: In web-related applications, URLs or website addresses may be used as unique identifiers. Enforcing a UNIQUE constraint on these columns ensures that each web address is associated with a unique resource.
License Plate Numbers: In systems that track vehicles or manage parking, license plate numbers are often used as unique identifiers. Applying a UNIQUE constraint ensures that each vehicle is uniquely identified.
These are just a few examples and uses of why developers would use the UNIQUE constraint. Each of these cases, the UNIQUE constraint helps maintain data integrity and ensures that the data in the database accurately represents the real-world entities it is modeling. It prevents duplicate or conflicting information, which is crucial for reliable and effective database management.

Implementation

-- Create a table with a UNIQUE constraint allowing NULL values
CREATE TABLE students (
    studentId INT UNIQUE,
    firstName VARCHAR(255),
    lastName VARCHAR(255)
);

-- Insert rows with unique non-null values and multiple NULL values
-- This is valid, as NULL values are not considered duplicates for the UNIQUE constraint
INSERT INTO students (studentId, firstName, lastName) VALUES
    (1, 'Jane', 'Smith'),
    (2, 'Suzanne', 'Brown'),
    (NULL, 'Abdul', 'Singh'),
    (NULL, 'Soria', 'Waler');

In the example above, we can see the use of the UNIQUE constraint, as well as columns that lack this constraint. Notice that we have two students who do not have a student ID, represented by Null values. This may represent that their student account profiles are pending some information, so we have not generated a student ID for them, or perhaps these students moved and no longer have an active student Id, although we still keep records of them in our database.

The first and last name columns may or may not contain duplicate values so it is possible to have matching values for both first and last name (two students named “John Doe” for instance).

Summary

The UNIQUE constraint ensure that all values in a column are different and unique.
The UNIQUE as well as PRIMARY KEY constraints provide a guarantee for uniqueness for a column or set of columns.
A PRIMARY KEY constraint automatically has a UNIQUE constraint.
We can have many UNIQUE constraints per table, but only one PRIMARY KEY constraint per table.


## Secondary Alternate Key
Learning Objectives

After completing this module, associates should be able to:

To define and implement Secondary Key
Description

A candidate key is a column or a combination of columns that uniquely identifies each row in a table.
Any table can have only one primary key so all the candidate keys other than the primary key are secondary keys or alternate keys.
A secondary key and the foreign key are NOT related or the same.
Real World Application

Understanding secondary keys is important for several reasons:

Data Integrity: Secondary keys enforce data integrity by ensuring the uniqueness of values in specific columns. This helps prevent duplicate entries and maintains the consistency and accuracy of the database.

Indexing: Secondary keys are often used to create indexes, which improve the performance of data retrieval operations. By creating indexes on secondary keys, you can speed up queries that involve filtering, sorting, or joining data based on those columns.

Query Flexibility: Secondary keys provide flexibility in querying the database by allowing users to retrieve data based on different criteria. They enable users to perform complex searches and analysis, leading to better insights and decision-making.

In summary, knowing about secondary keys helps database administrators, developers, and users leverage the full potential of the database system and ensure its reliability and efficiency.

Implementation

The following table has a secondary key.

CREATE TABLE Product (
    ProductID INT PRIMARY KEY,
    Barcode VARCHAR(20) UNIQUE,
    Name VARCHAR(100),
    Description TEXT,
    Price DECIMAL(10, 2)
);

In this example:

ProductID is the primary key of the table.
Barcode is a candidate key since the column is marked as UNIQUE, ensuring each value in that column is unique.
Summary

all the candidate keys other than the primary key and foreign key are called secondary keys.

even if the secondary key is not a primary key, a secondary key or group of secondary keys is used to uniquely identify a record.


## What Is A Subquery
Learning Objectives

After completing this module, associates should be able to:

Describe the different types of subqueries
Write subqueries
Description

A SQL subquery is a query nested inside of a larger query. A subquery can occur in various subsections of a query:

SELECT clause (Inner Query)
WHERE clause (Nested Query)
FROM clause (Inline View)
Subqueries can be nested in a SELECT, INSERT, UPDATE, DELETE, or even inside of another subquery.

Logical operators can be used to compare the results of the subquery

Nested queries can return single or multiple rows

Inline views create temporary tables

Inner queries create temporary columns on the result set

The column created by the inner query has a value equal to the result of the query
The subquery can also be called inner query or inner select while the container query is called the outer query or outer select

The inner query executes before the outer query

Subqueries are convenient, but perform worse than joins.

Real World Application

Subqueries can be a convenient way of gathering information between related tables. However, in many cases the same data can be queried using a join. It is a better practice to prefer a join over a subquery since joins execute faster by replacing multiple queries with a single one.

It isn't often that you will find a subquery being used where a join could perform the same work, faster.

Implementation

For the following demonstrations, use the below table:

CREATE TABLE IF NOT EXISTS students(
    id INT PRIMARY KEY,
    name VARCHAR(40) NOT NULL UNIQUE
);

CREATE TABLE IF NOT EXISTS evals (
    id INT PRIMARY KEY,
    studentId INT NOT NULL,
    evalName VARCHAR(10) NOT NULL,
    mark INT DEFAULT 0,
    FOREIGN KEY(studentId) REFERENCES students(id),
    CONSTRAINT mark_check CHECK(mark >= 0), CHECK(mark <= 100)
);

INSERT INTO students (id, name) VALUES (1, 'Steve'), (2, 'Jane'), (3, 'Casey');
INSERT INTO evals (id, studentId, evalName, mark) VALUES (1, 1, 'quiz 1', 98),
 (2, 2, 'quiz 1', 80), 
 (3, 3, 'quiz 1', 95), 
 (4, 1, 'test 1', 72), 
 (5, 2, 'test 1', 100), 
 (6, 3, 'test 1', 68);
Nested Queries
A nested query is a subquery found in the WHERE clause of the outer query.

SELECT col1, ... FROM table_ref WHERE (subquery);
Using the students schema lets find all students that scored higher than Jane (2) on quiz 1. The problem is, we don't know what Jane scored on quiz 1. There is a simple solution for this first problem.

SELECT studentId, evalName, mark  FROM evals WHERE studentId = 2 AND evalName = 'quiz 1';
studentId	evalName	mark
2	quiz 1	80
Knowing this information we could write a simple query to obtain the other information.

SELECT studentId, evalName, mark FROM evals WHERE evalName = 'quiz 1' AND mark > 80;
studentId	evalName	mark
1	quiz 1	98
3	quiz 1	95
There is a problem with the second query, it isn't reusable. We can write the solution to use a subquery to make it more reusable where only the studentId and eval name would need to be known.

SELECT a.id, a.name, b.evalName, b.mark FROM students a, evals b WHERE a.id = b.studentId
    AND b.evalName = 'quiz 1'
    AND b.mark > (SELECT mark FROM evals WHERE evalName = 'quiz 1' AND studentId = 2);
id	name	evalName	mark
1	Steve	quiz 1	98
3	Casey	quiz 1	95
The subquery (SELECT mark FROM evals WHERE evalName = 'quiz 1' AND studentId = 2) is used to select a single record from the evals table. this value is then compared in the outer query which is used to filter the final results. It is important that the inner query only return a single record because the outer query uses the > operator.

Inline View
Inline views are subqueries that occur in the FROM clause of the outer query.

Let's write an inline view where any mark is greater than 90 on any eval.

SELECT a.name, b.evalName, b.mark FROM students a, (SELECT studentId, evalName, mark FROM evals WHERE mark > 90) b WHERE a.id = b.studentId;
name	evalName	mark
Steve	quiz 1	98
Casey	quiz 1	95
Jane	test 1	100
The subquery (SELECT studentId, evalName, mark FROM evals WHERE mark > 90) creates a temporary table, called an inline view. That inline view is used in the outer query to select records from.

Inner query
An Inner query is a subquery found in the SELECT clause of the outer query. This creates a temporary column on the result set where the value of that column is provided by the inner query.

Let's write an inner query to get the average score for each student on all exams.

SELECT a.id, a.name, (SELECT AVG(mark) FROM evals WHERE studentId = a.id GROUP BY studentId) avg FROM students a;
id	name	avg
3	Casey	81.5000
2	Jane	90.0000
1	Steve	85.0000
Each of these types of queries have their own advantages, but one of the most observable uses for subqueries is to gather data from related tables. As you can see in the above examples, when the evals table was queried in the subquery, the data was bound to the student name from the students table making the data more relevant and readable to the end user.

Summary

Subqueries are queries that are contained in another query. The container query is known as the outer query and the subquery is known as the inner query. There are 3 types of subqueries

Nest Query in the WHERE clause
Inline View in the FROM clause
Inner Query in the SELECT clause
Important Notes

Nested queries can return single or multiple rows
Nested query results can be compared using logical operators
Inline views create temporary tables
Inner queries create temporary columns on the result set
The column created by the inner query has a value equal to the result of the query
Subqueries are convenient, but perform worse than joins.


## What Is A Join
Learning Objectives

After completing this module, associates should be able to:

Define what a join is
Understand the importance of joins in SQL
Description

In relational databases, data stored in multiple tables is related to one another with a common key value. This relation is established by a foreign key inside of the child table linking to the parent. There is a constant need to extract the records from related tables to obtain the full picture of the information stored in our database. After applying some condition, this joined information is compiled together based on the type of join and using SQL's JOIN clause. This JOIN clause is a part of the DQL and used in queries to access multiple tables, based on the logical relationships of those two tables and join that information together in the output as a single table. There are several different types of SQL JOIN clauses that will be explained in other modules.

Real World Application

In businesses when they are looking to analyze metrics for quarterly business profits, they have queries that pull information from multiple tables across their entire database. This would include joining information from sales & product information to see how much profit is being obtained in sales versus the cost of maintaining all of those products. JOIN clauses are frequently used to handle such things as the above. Along with this you could also query and join information for customer orders that could track the category of products that are selling most. There is a nearly endless amount of uses for JOIN clauses in a business for analytics to help improve the business as a whole.

Implementation

Basic SQL JOIN Clause syntax, using INNER JOIN to show basic syntax.
SELECT 
TABLE_NAME1.column1, TABLE_NAME1.columnN, TABLE_NAME2.column1, TABLE_NAME2.columnN
FROM TABLE_NAME1
INNER JOIN TABLE_NAME2
ON TABLE_NAME1.matching_column = TABLE_NAME2.matching_column;
Summary

This JOIN clause is a part of the DQL and used in queries to access multiple tables, based on the logical relationships of those two tables and join that information together in the output as a single table.

There are several different types of SQL JOIN clauses.
This relation is established by a foreign key inside of the child table linking to the parent used at the 'matching_column' for the join.
There is a constant need to extract the records from related tables to obtain the full picture of the information stored in the business's database.
After applying some condition, this joined information is compiled together based on the type of join and using SQL's JOIN clause.


## Inner Join
Learning Objectives

After completing this module, associates should be able to:

Describe what is an INNER JOIN
Write commands that makes use of INNER JOIN
Description

An INNER JOIN selects all rows from both tables as long as there is a match between the columns. If there are data in the "Table1" table that do not have matches in "Table2", these orders will not be shown.

inner

Syntax:

SELECT column_name(s)
FROM table1
INNER JOIN table2
ON table1.column_name = table2.column_name;

Real World Application

INNER JOIN can be a faster way of gathering information between related tables when compared to subqueries.
INNER JOIN helps in selecting data that have matching values between two entities.
Implementation

Table: Users

userid	username	email
1	LolipopMagee	Sarah.Magee@example.com
2	ToBeOrNotToBe	harry.smith@example.com
3	JingleTrees	bob.johnson@example.com
Table: Account

accountid	userid	accountnumber	balance
101	1	A123456	1000.00
102	2	B789012	2500.50
103	3	C345678	500.25
In this example, we have two tables: Users and Account.
The Users table contains information about users, and the Account table contains information about their accounts.
The UserID in the Account table is a foreign key referencing the UserID in the Users table, establishing a relationship between the two tables.

Now, let's say you want to perform an INNER JOIN to get information about users and their corresponding accounts:


-- INNER JOIN example  

SELECT Users.UserID, Users.UserName, Users.Email, Account.AccountNumber, Account.Balance  
FROM Users  
INNER JOIN Account ON Users.UserID = Account.UserID;  

This query retrieves the UserID, UserName, Email, AccountNumber, and Balance for users who have accounts. The INNER JOIN is performed based on the matching UserID in both tables.

NOTE: If you are following along be sure to use the table name Users NOT User or you will get an error as User is a keyword in SQL.

OUTPUT:

userid	username	email	accountnumber	balance
1	LolipopMagee	Sarah.Magee@example.com	A123456	1000.00
2	ToBeOrNotToBe	harry.smith@example.com	B789012	2500.50
3	JingleTrees	bob.johnson@example.com	C345678	500.25
We could have achieved the same result without a Join by doing the following query:


SELECT Users.UserID, Users.UserName, Users.Email, Account.AccountNumber, Account.Balance
FROM Users, Account
WHERE Users.UserID = Account.UserID;

OUTPUT:

userid	username	email	accountnumber	balance
1	LolipopMagee	Sarah.Magee@example.com	A123456	1000.00
2	ToBeOrNotToBe	harry.smith@example.com	B789012	2500.50
3	JingleTrees	bob.johnson@example.com	C345678	500.25
This implicitly creates a join condition in the WHERE clause, and the result will include only the rows where the UserID matches between the User and Account tables. However, it's generally recommended to use explicit JOIN syntax for better readability and maintainability of your queries.

Summary

INNER JOIN restricts records retrieval from Table1 and Table2 to those that satisfy the join requirement.
INNER JOIN is the most widely used form of JOIN and is very common within the SQL community.


## Left And Right Joins
Learning Objectives

After completing this module, associates should be able to:

Describe the use of LEFT JOIN and RIGHT JOIN in a database
Differentiate when to use LEFT JOIN and RIGHT JOIN
Description

LEFT JOIN returns all records from the left table, and the records that match the condition from the right table.
left

Syntax:

SELECT column_name(s)
FROM table1
LEFT JOIN table2
ON table1.column_name = table2.column_name;

RIGHT JOIN returns all records from the right table, and the records that match the condition from the left table.
right

Syntax:

SELECT column_name(s)
FROM table1
RIGHT JOIN table2
ON table1.column_name = table2.column_name;

NOTE: LEFT and RIGHT tables do not actually exist in a database it simply means the table written first or second.

The table mentioned first in a LEFT JOIN (on the left side of the JOIN keyword) is considered the "left" table, and all its rows will be included in the result set, regardless of whether there are matching rows in the table mentioned second (on the right side of the JOIN keyword).
In a RIGHT JOIN, the table mentioned second is the "right" table, and all its rows will be included in the result set, regardless of whether there are matching rows in the table mentioned first.
Real World Application

Real world applications include:

To query the hierarchical data and compare records of official reports.
Retrieve comprehensive data, especially when dealing with tables that may not have matching records in both directions.
Implementation

Table: Customers

customerid	first_name	last_name	country
1	John	Carpenter	USA
2	Jane	Labumba	Canada
3	Bob	Johnson	UK
Table: Orders

orderid	customerid	orderdate	totalamount
101	1	2023-01-15	500.00
102	2	2023-02-10	750.25
103	1	2023-03-05	200.50
104	4	2023-04-20	300.75
In this example, we have two tables: Customers and Order. The Customers table contains information about customers, and the Order table contains information about their orders. Please note that in the Orders table, the CustomerID column is a foreign key referencing the CustomerID column in the Customers table. This establishes a relationship between the two tables.

You want to see a list of all customers and their orders, even if some customers haven't placed any orders.

SELECT Customers.CustomerID, Customers.First_Name, Customers.Last_Name, Orders.OrderID
FROM Customers
LEFT JOIN Orders ON Customers.CustomerID = Orders.CustomerID;
SQL query performs a LEFT JOIN between the Customers and Orders tables based on the common column CustomerID.

The result of this query will be a list of customers along with their associated orders (if any). If a customer has placed orders, the OrderID will be displayed. If a customer hasn't placed any orders, the corresponding columns from the Orders table will contain NULL values.

customerid	first_name	last_name	orderid
1	John	Carpenter	101
2	Jane	Labumba	102
1	John	Carpenter	103
3	Bob	Johnson	NULL
NOTE: John Carpenter had placed two orders so is listed twice.

In this example we want to see all the orders and their corresponding customer information.

SELECT Customers.CustomerID, Customers.First_Name, Customers.Last_Name, Customers.Country, Orders.OrderID, Orders.OrderDate, Orders.TotalAmount
FROM Customers
RIGHT JOIN Orders ON Customers.CustomerID = Orders.CustomerID;
Here, a RIGHT JOIN is used to retrieve all orders and their corresponding customer information. The result will include all orders, and if an order has a corresponding customer, the customer information will be included. If there's an order without a matching customer, the columns related to customers will contain NULL values.

customerid	first_name	last_name	country	orderid	orderdate	totalamount
1	John	Carpenter	USA	101	2023-01-15	500.00
2	Jane	Labumba	Canada	102	2023-02-10	750.25
1	John	Carpenter	USA	103	2023-03-05	200.50
NULL	NULL	NULL	NULL	104	2023-04-20	300.75
Summary

LEFT JOIN allows us to get all the entries from Table1 and those records from Table2 that meet the join criteria.
If there is no match in Table2 for a record in Table1, the result will still include the record from Table1, but with NULL values for columns from Table2.
RIGHT JOIN allows us to get all the entries from Table2 and those records from Table1 that meet the join criteria.
If there is no match in Table1 for a record in Table2, the result will still include the record from Table2, but with NULL values for columns from Table1.
Syntax LEFT JOIN:

SELECT column_name(s)
FROM table1
LEFT JOIN table2
ON table1.column_name = table2.column_name;

Syntax RIGHT JOIN:

SELECT column_name(s)
FROM table1
RIGHT JOIN table2
ON table1.column_name = table2.column_name;


## Outer Join
Learning Objectives

After completing this module, associates should be able to:

Describe what is an OUTER JOIN
Write commands that makes use of OUTER JOIN
Description

An OUTER JOIN, yields non-matching records as well as matching records from both tables.
If rows in a connected table don't match, the NULL values will be shown.
MySQL, does not support OUTER JOIN you must use a left LEFT JOIN or RIGHT JOIN to get the behaviors of a OUTER JOIN
Some other dialects of SQL you can use a FULL OUTER JOIN or FULL JOIN to combine two tables data together.
outer

Real World Application

Used to fetch data from different tables
Handling Incomplete Data
Analyzing Discrepancies between datasets
Combining Multiple Sources
Aggregating Data
Implementation

Table: PartyGuests

guestid	guestname
1	Alice
2	Bob
3	Charlie
4	David
Table: PartyAttendees

attendeeid	attendeename
1	Alice
3	Charlie
5	Eve
Let's consider a scenario where you have two tables: one containing a list of people invited to a party PartyGuests and another containing a list of people who actually attended the party PartyAttendees. We'll use a FULL OUTER JOIN to combine these tables and see the complete picture, including guests who didn't attend and attendees who were not on the original guest list.

SELECT PartyGuests.GuestID, PartyAttendees.AttendeeID, PartyGuests.GuestName,
    PartyAttendees.AttendeeName  
FROM PartyGuests  
FULL OUTER JOIN PartyAttendees ON PartyGuests.GuestID = PartyAttendees.AttendeeID;
This could also be written omitting OUTER and yield the same result.

SELECT PartyGuests.GuestID, PartyAttendees.AttendeeID, PartyGuests.GuestName,
    PartyAttendees.AttendeeName
FROM PartyGuests
FULL JOIN PartyAttendees ON PartyGuests.GuestID = PartyAttendees.AttendeeID;
OUTPUT:

guestid	attendeeid	guestname	attendeename
1	1	Alice	Alice
2	NULL	Bob	NULL
3	3	Charlie	Charlie
4	NULL	David	NULL
NULL	5	NULL	Eve
This example demonstrates how a FULL OUTER JOIN helps you see the complete picture of guests and attendees at a party, including those who were on the guest list but didn't attend and those who attended but were not on the guest list.

Summary

OUTER JOIN also known as FULL OUTER JOIN returns all records from both tables whether the other table matches or not.
OUTER JOIN is of two types: LEFT JOIN and RIGHT JOIN.
You can use a FULL OUTER JOIN or FULL JOIN to combine two tables data together.
Syntax:
SELECT column_name(s)
FROM table1
FULL OUTER JOIN table2
ON table1.column_name = table2.column_name;


## Equi And Theta Joins
Learning Objectives

After completing this module, associates should be able to:

Describe the use of EQUI JOIN and THETA JOIN in a database
EQUI JOIN vs THETA JOIN
Description

THETA JOIN allows us to merge two tables based on an arbitrary condition, and is represented by the symbol theta (Θ or θ).
In other words, a THETA JOIN is a join in which some arbitrary comparison relationship is used. For example (”>”, “<”, “>=”, etc...)
For a THETA JOIN we make associations on columns that have the same datatype, but may represent different information for each table. For instance, we may join tables based on how old someone was when they got their driver’s license by using comparisons which include a “birth_year” column and a “years_as_driver” column.
The general case of JOIN operation is called a THETA JOIN.
An EQUI-JOIN is a THETA-JOIN that uses the equality operator.
EQUI-JOIN is a common type of join, since it creates 1:1 relationships between matching data between tables.
EQUI JOIN performs a join against equality or matching column(s) values of the associated tables.
equi

Syntax for Equi-Join:
SELECT *
FROM Table1
JOIN Table2 ON Table1.Column = Table2.Column;

Syntax for Theta-Join:
SELECT *
FROM Table1, Table2
WHERE Table1.Column > Table2.Column;

Real World Application

Real world applications include:

EQUI-JOINS are suitable for scenarios where you need to match records based on equality of specific columns.
Matching Data for Analysis - you may have data spread across different tables, and you want to analyze or combine information based on common attributes.
THETA JOINS offer more flexibility by allowing a broader range of conditions, making them useful for scenarios involving complex criteria or non-equality conditions.
Pattern Matching - Finding all customers whose names contain a particular substring.
Date Range Matching - Finding all transactions that occurred within a certain time period.
Numeric Range Conditions - Finding customers with purchase amounts between certain thresholds.
Implementation

Table: Authors
CREATE TABLE Authors (
    authorid INT,
    authorf_name VARCHAR(50),
    authorl_name VARCHAR(50),
    birthyear INT
);
INSERT INTO Authors (authorid, authorf_name, authorl_name, birthyear)
VALUES
    (101, 'F. Scott', 'Fitzgerald', 1896),
    (102, 'Harper', 'Lee', 1926),
    (103, 'J.R.R.', 'Tolkien', 1892),
    (104, 'George', 'Orwell', 1903);
Table: Books
CREATE TABLE Books (
    bookid INT,
    title VARCHAR(100),
    genre VARCHAR(50),
    price DECIMAL(5, 2),
    authorid INT
);
INSERT INTO Books (bookid, title, genre, price, authorid)
VALUES
    (1, 'The Great Gatsby', 'Fiction', 14.50, 101),
    (2, 'To Kill a Mockingbird', 'Fiction', 12.75, 102),
    (3, 'The Hobbit', 'Fantasy', 19.50, 103),
    (4, '1984', 'Dystopian', 16.25, 104);
Equi-Join Example
Perform an Equi-Join to retrieve information about books and their corresponding author details
based on the equality of the AuthorID columns.
SELECT 
    Books.BookID, 
    Books.Title, 
    Books.Genre, 
    Books.Price, 
    Authors.AuthorF_Name, 
    Authors.AuthorL_Name, 
    Authors.BirthYear
FROM Books
JOIN Authors 
ON Books.AuthorID = Authors.AuthorID;
OUTPUT:

bookid	title	genre	price	authorf_name	authorl_name	birthyear
1	The Great Gatsby	Fiction	14.50	F. Scott	Fitzgerald	1896
2	To Kill a Mockingbird	Fiction	12.75	Harper	Lee	1926
3	The Hobbit	Fantasy	19.50	J.R.R.	Tolkien	1892
4	1984	Dystopian	16.25	George	Orwell	1903
True Theta Join Example
Use a Theta Join to retrieve book and author information based on a join condition
that involves a comparison operator other than equality.
SELECT 
    Books.BookID, 
    Books.Title, 
    Books.Genre, 
    Books.Price,
    Authors.AuthorF_Name,
    Authors.AuthorL_Name,
    Authors.BirthYear
FROM Books
JOIN Authors 
ON Books.Price > Authors.BirthYear;  
This query retrieves records where the price of a book is greater than the birth year of the author.
Another Theta Join Example with filtering

Retrieve books with price conditions as part of the join itself (beyond equality).
SELECT 
    Books.BookID, 
    Books.Title, 
    Books.Genre, 
    Books.Price,
    Authors.AuthorF_Name,
    Authors.AuthorL_Name,
    Authors.BirthYear
FROM Books
JOIN Authors 
ON Books.AuthorID = Authors.AuthorID
WHERE Books.Price > 15.00;  
OUTPUT:

bookid	title	genre	price	authorf_name	authorl_name	birthyear
3	The Hobbit	Fantasy	19.50	J.R.R.	Tolkien	1892
4	1984	Dystopian	16.25	George	Orwell	1903
Summary

EQUI JOIN performs a join against equality or matching column(s) values of the associated tables.
THETA JOIN allows us to merge two tables based on the condition represented by theta.
It works for all comparison operators. It is denoted by symbol theta.


## Aliases
Learning Objectives

After completing this module, associates should be able to:

Describe the use of alias in a database
Advantages of alias
Description

An alias is used to give a temporary name to a table or a column in a table for the intention to support a specific query.
Advantages of Aliases in MySQL
It provides a very useful feature that allows us to achieve complex tasks quickly.
It makes column or table name more readable.
It allows us to combine two or more columns
It makes the table more user-friendly.
Real World Application

Using aliases have a number of benefits for your applications, such as:

Resolving Naming Conflicts: When dealing with complex queries involving multiple tables with similar column names, aliases can help avoid naming conflicts.
Enhancing User Interface View: When developing applications with a database backend, developers can use aliases to provide more user-friendly and understandable names for the data presented to users.
Code Refactoring: Aliases make it easier to refactor (restructure existing computer code without changing its external behavior) code by providing a way to change the name of a table or column in one place without affecting the rest of the code.
Self-Joins or Complex Joins: Aliases can be used to distinguish between different instances of the same table in a SELF-JOIN or to simplify complex join conditions. You will see alias used often in all types of join statements.
Column Renaming: Aliases help in giving more meaningful names to columns, in turn making the output of a query easier to understand.
Table Renaming: When working with multiple tables, aliases can be used to create shorter and more readable aliases for table names.
These are just a few of many reasons that developers chose to use alias when writing queries.

Implementation

Table 1: Plants

plant_id	plant_name	species	price
1	Venus Flytrap	Dionaea muscipula	19.99
2	Pitcher Plant	Nepenthes	29.99
3	Sundew	Drosera	14.99
4	Cobra Plant	Darlingtonia californica	24.99
Table 2: Customers

customer_id	first_name	last_name	email	purchased_plant_id
1	John	Wheeler	john.wheeler@example.com	3
2	Jane	Smith	jane.smith@example.com	4
3	Sussie	Halloween	sussie.halloween@example.com	1
In this example we will have two tables Plants and Customers . The Plants table will contain information about the plants and the Customers table will contain information about the customer. The Customers table has a purchased_plant_id column, which is a foreign key referencing the plant_id in the Plants table.

In SQL, column aliases are used to provide a different name for a column in the query result. This is helpful for clarity and readability. In our case, we'll use a column alias to associate purchased plants with customers in the query result.

SELECT customer_id, first_name, last_name, email,
plant_name AS purchased_plant
FROM customers 
JOIN plants  ON customers.purchased_plant_id = plants.plant_id;
The keyword AS is used to create an alias named purchased_plant for the column plant_name. This way, when you read the query, it's clear that the plant_name column is being displayed as purchased_plant. This demonstrates alias by column.

Another method for creating an alias is on the table itself. The same query as above can be used to illustrate how this works.

SELECT cust.customer_id, cust.first_name, cust.last_name, cust.email,
plnt.plant_name AS purchased_plant
FROM customers cust
JOIN plants plnt  ON cust.purchased_plant_id = plnt.plant_id;
Notice how after customers there is the abbreviation cust. This is to tell the editor that we are giving an alias to the Customers table. The same can be said in the query about plants being abbreviated to plnt. Also, notice how the alias is used throughout the entire query. That is important as, if you are going to use a table alias, then it must be used throughout the entire query. This however, is not the only way to write this and get the same result. Some developers find it clearer to write the same query as follows:

SELECT cust.customer_id, cust.first_name, cust.last_name, cust.email,
plnt.plant_name AS purchased_plant
FROM customers AS cust
JOIN plants AS plnt  ON cust.purchased_plant_id = plnt.plant_id;
Something to keep in mind is that plant_name is a column in the table with the alias of purchased_plant while customers has an alias of cust which is referring to the Customers table and plants has an alias of plnt which is referring to the Plants table.

It is extremely common to see alias used in SQL queries as they help prevent ambiguity especially in larger databases where the same data may be in multiple tables. By providing aliases for tables and columns, developers make their queries more readable which helps avoid naming conflicts, and enhance overall code clarity.

Aliases are especially useful when dealing with complex joins or subqueries involving multiple tables, as they help distinguish between different instances of the same table and make the code more maintainable. In some cases aliases are required or the query will not work.

OUTPUT:

customer_id	first_name	last_name	email	purchased_plant
1	John	Wheeler	john.wheeler@example.com	Sundew
2	Jane	Smith	jane.smith@example.com	Cobra Plant
3	Sussie	Halloween	sussie.halloween@example.com	Venus Flytrap
Summary

Aliases are commonly used to simplify and improve the readability of SQL statements. They make it easier to write queries and enhance the overall clarity of the code.
In some scenarios, the use of aliases becomes necessary to construct specific queries. Aliases can be essential for addressing naming conflicts, especially in more complex queries involving multiple tables or calculations.
Aliases play a significant role in simplifying the writing of SQL statements by making them more readable, and contributing to easier maintenance of the code.


## Intro to JDBC
Learning Objectives

After completing this module, associates should be able to:

Describe JDBC and its uses
Successfully get started with using the JDBC
Description

JDBC stands for Java Database Connectivity. It is a relatively low-level API used to write Java code that interacts with relational databases via SQL.

Steps for Using the JDBC
In order to interact with a database, we need to do several things:

Obtain a JDBC driver
Open a connection using:
Database URL
Username
Password
Execute some SQL statement using either:
Statement
PreparedStatement
CallableStatement
Retrieve the results of executing the statement
Close the connection
Database JDBC Drivers
Because JDBC is a Java language API, it is database agnostic. It uses database drivers which implement the interfaces defined in the JDBC API for the given database. Many JDBC drivers are available through Maven's central repository and can be added as a dependency in the pom.xml file.

Creating a Connection
Now we can use the DriverManager class to get a Connection to the database, given that we have the JDBC URL, username, and password. Generally these parameters should be stored in an external configuration file that can be loaded dynamically and changed without affecting the application code.

try (Connection conn = DriverManager.getConnection(DB_URL,USERNAME,PASSWORD)) {
  // more code goes here
} catch (SQLException e) {}
Alternatively, the DataSource interface could be used to make connections and is covered extensively in this Oracle tutorial.

It's always a good idea to close your resources - here we've used the try-with-resources syntax to automatically close the Connection being created after the block ends.

Autocommit mode
By default, when a connection is created it is in auto-commit mode, so every SQL statement acts as a transaction and is committed immediately after execution. In order to manually group statements into a transaction, simply call:

Connection conn = DriverManager.getConnection(DB_URL,USERNAME,PASSWORD);
conn.setAutoCommit(false);
// execute some SQL statements...
conn.commit();
JDBC String
The database URL is an address pointing to the database to be used, also known as the JDBC String. The format of this URL varies between database vendors, as shown in the table below:

RDBMS	JDBC driver	URL format
MySQL	com.mysql.jdbc.Driver	jdbc:mysql://hostname/databaseName
Oracle	oracle.jdbc.driver.OracleDriver	jdbc:oracle:thin:@hostname:portNumber:databaseName
SQLServer	com.microsoft.sqlserver.jdbc.SQLServerDriver	jdbc:sqlserver://serverName:portNumber;property=value
PostgreSQL	org.postgresql.Driver	jdbc:postgresql://hostname:port/databaseName
Executing SQL
Once we have the Connection object, we can write our SQL and execute it:

Statement stmt = conn.createStatement();
String sql = "SELECT * FROM employees";
ResultSet rs = stmt.executeQuery(sql);
Alternatively, a PreparedStatement can be used. This interface gives us the flexibility of specifying parameters with the ? symbol. It also protects against SQL injection when user input is used by pre-compiling the SQL statement.

PreparedStatement ps = conn.prepareStatement();
String sql = "SELECT * FROM employees WHERE age > ? AND location = ?";
ps.setInt(1, 40);
ps.setString(2, "New York");
ResultSet rs = ps.executeQuery(sql);
The Statement and PreparedStatement also have additional methods for sending SQL, including:

.execute() - for any kind of SQL statement, returns a boolean
.executeUpdate() - for DML statements, returns an int which is the number of rows affected
Retrieving Results
Results from an SQL query are returned as a ResultSet, which can be iterated over to extract the data:

List<Employee> empList = new ArrayList<>();
while (rs.next()) {
  int id = rs.getInt("id");
  String name = rs.getString("first_name");
  empList.add(new Employee(id, name));
}
Further details about each of the steps for using the JDBC are given in other lessons.

Real World Application

(Source: https://www.oracle.com/java/technologies/industry-support.html)

The following companies have endorsed the JDBC TM database access API and have built or are building JDBC-based products:

Agave Software Design
Asgard Software
Atinav, Inc.
BEA
Borland
Caribou Lake Software
Daffodil Software
DataMirror
Dharma Systems Inc.
Empress Software Inc.
Esker Software
Fujitsu Siemens Computers
Gupta Technologies
Hewlett-Packard
Hit Software
Hummingbird
IBM's Database 2 (DB2)
IBM's Informix Software Inc.
IDS Software
i-net software
InterSystems Corporation
iWay Software
JNetDirect
Minisoft
mysql
NEON Systems
Novell
OpenLink Software
Oracle Corporation
Pervasive Software
POET Software
postgreSQL
Progress Software
Quest Software
RogueWave Software Inc.
SAS Institute Inc.
Siemens AG
Sun Software
Sunopsis S.A.
Sybase Inc.
Symantec
Thunderstone
Thought Inc.
Trifox, Inc.
Yard Software GmbH
Implementation

Java applications that need to communicate with the database have to be programmed using the JDBC API. A JDBC Driver that supports a data source, such as Oracle or SQL Server, has to be added in Java application for JDBC support. This JDBC driver intelligently communicates with the respective data source. Below is an example of a small application that uses the JDBC.

Creating a simple JDBC application
First, we'll add in a Database Driver. We can search Maven Central (https://central.sonatype.com/) for "mysql" to find drivers. We'll add the below dependency to our Maven project's POM.xml file:

<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>8.0.33</version>
</dependency>
Now we can communicate with a MySQL database. Let's create a class that connects to and uses a database. The database is named example and contains a table named example_table that has an id column of type INTEGER and a value column of type VARCHAR. Please note that this example database is local and not available online.

Below is the class that uses the database:

import java.sql.*;
  
public class JDBCDemo {
    
    public static void main(String args[]) throws SQLException {

      // database url + credentials
        String url = "jdbc:mysql://localhost:3306/example";
        String username = "root";
        String password = "pass";

        // Obtain a connection using credentials
        Connection con = DriverManager.getConnection(
                url, username, password);

        // create a Statement object
        Statement st = con.createStatement();

        // Execute the query using the Statement object
        String query = "INSERT INTO example_table VALUES (1, 'hello world')";
        int rowsUpdated = st.executeUpdate(query);
        
        System.out.println(rowsUpdated); // 1

        // Closing the connection
        con.close();
    }
} // class
The above example demonstrates the basic steps to access a database using JDBC.Note that you must import the java.sql package to provide basic SQL functionality and use the classes and interfaces of the package.

Summary

JDBC stands for Java Database Connectivity. It is a relatively low-level API used to write Java code that interacts with relational databases via SQL.
In order to interact with a database, we need to do several things:
Obtain a JDBC driver
Open a connection using:
Database URL
Username
Password
Execute some SQL statement using either:
Statement
PreparedStatement
CallableStatement
Retrieve the results of executing the statement
Close the connection


## JDBC Interfaces Classes
Learning Objectives

After completing this module, associates should be able to:

Describe JDBC classes and interfaces
Description

The JDBC classes and interfaces are located in the java.sql and javax.sql packages. There are several key classes and interfaces that are commonly encountered when writing JDBC code and in this topic we'll be going over each of them.

DriverManager class - Used to load JDBC drivers and for establishing connections to a database
has the getConnection(String url, String user, String password) method - Attempts to establish a connection to the given database URL
documentation: https://docs.oracle.com/javase/8/docs/api/java/sql/DriverManager.html
DataSource interface - for retrieving connections, an alternative to DriverManager
implementing classes are provided by the vendors, not by the JDBC
documentation: https://docs.oracle.com/javase/8/docs/api/javax/sql/DataSource.html?is-external=true
Connection interface - represents a physical connection with a database
can be used to create statements with the following methods:

createStatement(): creates a Statement object.
prepareStatement(String sql): creates a PreparedStatement object.
prepareCall(String sql): creates a CallableStatement object.
can be used to manage database transactions with the following methods:

setAutoCommit(boolean autoCommit): Sets this connection's auto-commit mode to the given state.
rollback(Savepoint savepoint): Undoes all changes made in the current transaction and releases any database locks currently held by this Connection object.
commit(): Makes all changes made since the previous commit/rollback permanent and releases any database locks currently held by this Connection object.
setSavepoint(String name): Creates a savepoint with the given name in the current transaction and returns the new Savepoint object that represents it.
needs to be closed after use:

has the close() method: Releases this Connection object's database and JDBC resources immediately instead of waiting for them to be automatically released.
has the isClosed() method: Retrieves whether this Connection object has been closed.
documentation: https://docs.oracle.com/javase/8/docs/api/java/sql/Connection.html

SQLException class - a general exception thrown when something goes wrong when accessing the database
Statement interface - used for executing static SQL statements
defines a constant, RETURN_GENERATED_KEYS to indicate that generated PKs should be made available for retrieval.
has the execute(String sql, int autoGeneratedKeys) method: Executes the given SQL statement, which may return multiple results. The second parameter is optional and can use RETURN_GENERATED_KEYS as a value.
has the executeQuery(String sql) method: Executes the given SQL statement, which returns a single ResultSet object.
has the executeUpdate(String sql) method: Executes the given SQL statement, which may be an INSERT, UPDATE, or DELETE statement or an SQL statement that returns nothing, such as an SQL DDL statement. If DML is executed, the number of rows affected will be returned.
documentation: https://docs.oracle.com/javase/8/docs/api/java/sql/Statement.html
PreparedStatement interface - represents pre-compiled SQL statements and extends the Statement interface.
documentation: https://docs.oracle.com/javase/8/docs/api/java/sql/PreparedStatement.html
CallableStatement interface - used to execute stored procedures and extends the PreparedStatement interface.
has methods that interact with stored procedures, such as getting and setting parameter values.
documentation: https://docs.oracle.com/javase/8/docs/api/java/sql/CallableStatement.html
ResultSet interface - represents data returned from the database
has the next() method: Moves the cursor forward one row from its current position.
includes getter methods for retrieving values from data the ResultSet object's rows
documentation: https://docs.oracle.com/javase/8/docs/api/java/sql/ResultSet.html
Real World Application

It's important to understand the JDBC (Java Database Connectivity) API's classes and interfaces for several reasons:

Database Operations: JDBC classes and interfaces enable developers to perform a wide range of database operations, including querying, inserting, updating, and deleting data. By understanding these classes and interfaces, developers can write Java code to interact with databases efficiently and effectively.

Transaction Management: JDBC supports transaction management, allowing developers to group multiple database operations into atomic units of work. Understanding JDBC's transaction-related classes and interfaces enables developers to implement transactional behavior in their applications, ensuring data integrity and consistency.

Error Handling: JDBC provides mechanisms for handling database errors and exceptions gracefully. By understanding JDBC's exception handling classes and interfaces, developers can write robust error-handling code to deal with database-related issues effectively, improving application reliability and stability.

Integration with Frameworks: Many Java frameworks and libraries rely on JDBC for database access. Understanding JDBC's classes and interfaces is essential for integrating Java applications with these frameworks and leveraging their database-related functionalities effectively.

Security Considerations: JDBC applications may be vulnerable to security threats such as SQL injection attacks. Understanding JDBC's classes and interfaces helps developers implement secure coding practices to mitigate these risks, such as using parameterized queries and input validation.

Overall, knowledge of JDBC's classes and interfaces is essential for Java developers who work with databases, as it forms the foundation for building robust, scalable, and secure database-driven applications.

Implementation

This example will demonstrate the use of the JDBC API and its classes and interfaces to successfully interface with a database.
Context
Let's say that we have a database with the following Notes table already persisted:

id	content	priority
1	take out trash	high
2	tues appt with dentist @10am	medium
3	remember to clean out the fridge	low
4	call Sammy	low
In our application, we have a Note Java class:

public class Note {

  // fields
  private long id;
  private String content;
  private String priority;

  // ... constructors, getters and setters omitted
  }
Processing SQL Statements with JDBC Classes and Interfaces
In general, to process any SQL statement with JDBC, you follow these steps:

Establishing a connection.
Create a statement.
Execute the query.
Process the ResultSet object.
Close the connection.
With our database and Note class set up, let's write a class that defines some database operations where we follow these steps:


import java.sql.*;

public class App {
    // database url + credentials
    String url = "jdbc:mysql://localhost:3306/example";
    String username = "root";
    String password = "pass";

    public Note addNote(String content, String priority) {

        // step 1 & 5: open connection to db and close when done
        try (Connection connection = DriverManager.getConnection(url, username, password)) {
            // step 2: create your statement
            PreparedStatement ps = connection.prepareStatement("INSERT INTO notes(content, priority) VALUES (?, ?)",
                    Statement.RETURN_GENERATED_KEYS);

            // assign any parameters their values
            ps.setString(1, content);
            ps.setString(2, priority);

            // step 3: execute statement
            ps.executeUpdate();

            ResultSet rs = ps.getGeneratedKeys();

            // step 4: process results:
            // while there is another record in the resultset to process...
            while (rs.next()) {
                // ... get the value of the first column in that resultset row...
                long resultId = rs.getLong(1);

                // ... and return a Note with the generated id in its state, as well as the
                // other values
                return new Note(resultId, content, priority);
            }
        } catch (SQLException e) {
            e.printStackTrace();
        }

        return null;

    }

    // read
    public Note getNoteById(long id) {

        // step 1 & 5: open connection to db and close when done
        try (Connection connection = DriverManager.getConnection(url, username, password)) {
            // step 2: create your statement
            PreparedStatement ps = connection.prepareStatement("SELECT * FROM notes WHERE id = ?");

            // assign any parameters their values
            ps.setLong(1, id);

            // step 3: execute statement
            ResultSet rs = ps.executeQuery();

            // step 4: process results:
            // while there is another record in the resultset to process...
            while (rs.next()) {
                // ... get the values from the respective columns ...
                long resultId = rs.getLong("id");
                String content = rs.getString("content");
                String priority = rs.getString("priority");

                // ... and return a Note with those values as its state
                return new Note(resultId, content, priority);
            }
        } catch (SQLException ex) {
            ex.printStackTrace();
        }

        // return null if unsuccessful
        return null;

    }
}


Establishing Connections
First, establish a connection with the data source you want to use. We used the statement Connection connection = DriverManager.getConnection(url, username, password) to establish a connection using the variables url, username, and password that we defined at the beginning of the class.

Creating Statements
A Statement is an interface that represents a SQL statement. You execute Statement objects, and they generate ResultSet objects, which is a table of data representing a database result set. You need a Connection object to create a Statement object.

Our example created PreparedStatements with the following code:

ResultSet rs = stmt.executeQuery(query);
and

PreparedStatement ps = connection.prepareStatement("SELECT * FROM notes WHERE id = ?");
Executing Queries
To execute a query, call an execute method from the Statement interface.

Our example executed statements with the following code:

ps.executeUpdate();
and

ResultSet rs = ps.executeQuery();
Processing ResultSet Objects
You access the data in a ResultSet object through a cursor. Note that this cursor is not a database cursor. This cursor is a pointer that points to one row of data in the ResultSet object. Initially, the cursor is positioned before the first row. You call various methods defined in the ResultSet object to move the cursor.

In our example, we used the following code to process a ResultSet:

// while there is another record in the resultset to process...
while (rs.next()) {
                // ... get the values from the respective columns ...
                long resultId = rs.getLong("id");
                String content = rs.getString("content");
                String priority = rs.getString("priority");

                // ... and return a Note with those values as its state
                return new Note(resultId, content, priority);
            }
Closing Connections
When you are finished using a Connection, Statement, or ResultSet object, call its close method to immediately release the resources it's using.

Alternatively, use a try-with-resources statement to automatically close Connection, Statement, and ResultSet objects, regardless of whether an SQLException has been thrown. The JDBC throws an SQLException when it encounters an error during an interaction with a data source. A try-with-resources statement consists of a try statement and one or more declared resources. Our example closes a connection using the following code:

 // step 1 & 5: open connection to db and close when done
        try (Connection connection = DriverManager.getConnection(url, username, password)) {
            // steps 2-4 ommitted...
        } catch (SQLException ex) {
            ex.printStackTrace();
        }
Summary

The JDBC classes and interfaces are located in the java.sql and javax.sql packages. There are several key classes and interfaces that are commonly encountered when writing JDBC code:

DriverManager class - to make a connection with a database driver
DataSource interface - for retrieving connections, an alternative to DriverManager
Connection interface - represents a physical connection with a database
SQLException class - a general exception thrown when something goes wrong when accessing the database
Statement interface - used for executing static SQL statements
PreparedStatement interface - represents pre-compiled SQL statements
CallableStatement interface - used to execute stored procedures
ResultSet interface - represents data returned from the database


## Result Set
Learning Objectives

After completing this module, you should be able to:

Describe the ResultSet Interface & its purpose

Generate a ResultSet from the execution of a Statement or Prepared Statement

Description

In the JDBC (Java Database Connectivity) Library, a ResultSet represents the result of executing a query against a database. It provides methods to navigate and access the data returned by the query. ResultSet objects are generated from Statement objects after execution of an associated SQL query. Simple Statements, PreparedStatement and CallableStatement can all produce ResultSets.

A ResultSet contains a cursor that can move forward and backward through the result set data, allowing access to individual rows and their column values. With a ResultSet, you can retrieve data from SELECT queries, view and manipulate the current row, and retrieve metadata about the result set, such as column names and types. ResultSet provides a powerful and flexible way to handle query results in Java applications.

When a ResultSet is first obtained from a query, the cursor is set to just before the first row of data. Calling the method next() is necessary initially to obtain the first row of data. The next() method returns a boolean value of true when there is data to return from the ResultSet and false when the end of the data has been reached. The default ResultSet type may only be traversed in one direction, meaning that once you have advanced the cursor using next(), you are unable to go back to a previous record.

Real World Application

ResultSet is commonly used in Java applications that interact with databases for a wide range of real-world scenarios, including:

Data Retrieval
ResultSet is used to fetch and process data retrieved from SELECT queries. It allows developers to extract the result set row by row and access individual column values.
Report Generation
When generating reports or exporting data from a database, ResultSet provides a convenient way to iterate over the query results and extract the necessary information for the report.
Data Analysis
ResultSet can be utilized for performing data analysis and statistical calculations on query results. It enables developers to aggregate, summarize, or perform calculations on the returned data.
Data Validation
ResultSet can be used to validate data against specific conditions or business rules. It allows developers to traverse the result set and apply custom validation logic.
Implementation

The following code snippet showcases a simple Employee class which is used when retrieving data from our database. This Employee model class uses similar structure and naming conventions to our employees table of our database.

// Class used to represent data from our Database
class Employee {
  private int empId;
  private String empName;
  private String empTitle;

  public Employee(int empId, empName, empTitle) {
    this.empId = empId;
    this.empName = empName;
    this.empTitle = empTitle;
  }
  // getters & setters below...
}
In this example, you can see that our Data Access Object (DAO) class, EmployeeDAO, creates a connection to the database, and executes queries. In each method, a ResultSet object is returned from the execution of the query on the database, and we in turn populate one or more Employee object(s), as appropriate.

The method next() is used to advance the ResultSet cursor and obtain data from the ResultSet. Since this method returns a boolean, it can be used with an if-statement to check if a unique record was found, or if can be used with a while loop to iterate through all records contained with the ResultSet if we expect multiple records.

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet
import java.sql.Statement;
import java.sql.SQLException;

import java.util.ArrayList;
import java.util.List;

class EmployeeDAO {
  // credentials for Database. Typically, these would be kept in a separate 
  // class, a config file or as environment variables
  String db_url = "replace_with_valid_database_url";
  String db_user = "replace_with_valid_database_username";
  String db_pass = "replace_with_valid_database_password";

  public Employee selectEmployeeByName(String name) {
    // We use prepared statement here to prevent SQL Injection
    PreparedStatement ps = null;

    // emp represents the data to return from this method
    Employee emp = null;

    // Our query to execute on the database
    String query = "SELECT * FROM employees WHERE empName=?";
  
    try {
      // Get a connection to the database, represented by a Connection object
      Connection conn = DriverManager.getConnection(db_url, db_user, db_pass);

      // the prepareStatement method on Connection objects returns a preparedStatement
      ps = conn.prepareStatement(query);

      // Set the placeholder (?) for our query
      ps.setString(1, name);

      // A ResultSet object with data from the query is returned from executeQuery
      ResultSet rs = ps.executeQuery();

      // Use the 'next()' method of ResultSet to check if there is data and
      // advance the cursor to that row, if available.
      if (rs.next()) {

        // Create an Employee populated with data from the ResultSet
        // we use the column names here, but we could also use the column index.
        emp = new Employee (
          rs.getInt("empId"),
          rs.getString("empName"),
          rs.getString("empTitle"),
        );
      }
    } catch (SQLException e) {
      e.printStackTrace();
    }

    // returns the data from the database. If no record existed, this will 
    // return null
    return emp;
 }

 public List<Employee> selectAllEmployees() {
    // Here we use a Simple Statement, since there are no parameters for the query
    Statement st = null;
    ResultSet rs = null;

    // This method returns a collection (List) of Employee Objects
    ArrayList<Employee> eList = new ArrayList<Employee> ();
    String query = "SELECT * FROM employees";
  
    try {
      // Get a connection to the database, represented by a Connection object
      Connection conn = DriverManager.getConnection(db_url, db_user, db_pass);
      st = conn.createStatement();

      // The executeQuery method for Statement takes a String argument for the query
      // this method returns a single ResultSet Object
      rs = st.executeQuery(query);
   
      // We use a while loop since we expect multiple records to be returned
      while (rs.next()) {
        eList.add(
          // Here we use each column's index, but we could use the column name
          // notice that the column indexing starts at 1 for the ResultSet data
          new Employee (
            rs.getInt(1),
            rs.getString(2),
            rs.getString(3),
          )
        );
      }
    } catch (SQLException e) {
      e.printStackTrace();
    }
    return eList;
 }

// Other code below...
}
Summary

ResultSet is a powerful class that is used to represent data retrieved from JDBC queries on a database.

ResultSet contains a cursor that allows for navigation and manipulation of data in the ResultSet.

ResultSet objects are created by executing a query using a Statement, PreparedStatement or CallableStatement object.


## Data Access Object (dao)
Learning Objectives

After completing this module, you should be able to:

Explain what a Data Access Object handles for us
Understand the role of abstraction in this design pattern.
Description

Data Access Objects (DAOs) are a design pattern commonly used in Java applications for managing the interaction between the application logic and the data persistence layer. DAOs provide an abstraction layer that separates the business logic from the underlying data access mechanisms, such as databases or APIs.

Allows data access mechanisms to change independently of the code that uses the information.
Can provide access to a particular resource without coupling to the business logic.
The interface allows for no direct relation to the data resource access mechanism.
This is a form of Abstraction from OOP to hide the implementation details of our DAO class.
Follows a clean and standardized interface performing CRUD Operations.
Real World Application

DAOs are widely used in various types of Java applications, including:

Enterprise Applications
DAOs are commonly used in large-scale enterprise applications where data access needs to be managed across multiple modules and components. They provide a centralized and consistent approach to accessing and manipulating data.
Web Applications
DAOs are essential for web applications that interact with databases. They help abstract the database operations, provide a layer of security, and ensure data integrity.
Implementation

There are two primary steps to applying the DAO design pattern to an application:

Create a DAO interface with the desired CRUD operation methods
Create subclasses to implement the interface
DAO Interface:
This defines the contract or API for accessing and manipulating the data. It specifies the methods for CRUD operations and any additional operations specific to the data entity.
The DAO design pattern creates a basic interface to allow for modularity, for instance, providing different implementations for based on SQL syntax.


public interface MemberDao{
    // Insert statement to add a new record
    public Member createMember(Member newMember);

    // Select query for all records
    public List<Member> findAllMembers();

    // Select query for a specific record
    public Member findMemberByEmail(String email);

    // Update statement for a specific record
    public boolean updateMember(Member updatedMember);

    // Delete statement for a specific record
    public boolean deleteMemberByEmail(String email);

}

DAO Implementation:
The DAO implementation class provides the actual implementation of the methods defined in the DAO interface. It interacts with the underlying data storage mechanism, such as a database, to perform the necessary operations.

public class MemberDaoImpl implements MemberDao {
    @Override
    public Member createMember(Member newMember) {

        // Here, we use a try-with-resources statement, which will close the connection automatically.
        // this is useful if we want to create new connections each time we query the database. Another strategy
        // would be to create a static connection that is not closed.
        try (Connection conn = ConnectionFactory.getConnectionFactory().getConnection()){
            // Here, we prepare an sql query to be executed on the database
            String sql = "INSERT INTO members (email, password, full_name, experience_months, registration_date) VALUES (?,?,?,?,?)"; 

            // the prepareStatement method will allow for setting the values in the placeholders (‘?’) in the query string
            PreparedStatement ps = conn.prepareStatement(sql);

            // Here, we set the values of the placeholders (’?’)
            // The first argument is the index of the placeholder, starting at 1, the second argument is the value
            // Although this example sets the values in order, 1 - 5, we can set them in any order.
            ps.setString(1, newMember.getEmail());
            ps.setString(2, newMember.getPassword());
            ps.setString(3, newMember.getFullName());
            ps.setInt(4, newMember.getExperienceMonths());
            ps.setDate(5, newMember.getRegistrationDate());

            // The executeUpdate method runs the query on the database and returns an int equal to the number of records affected
            int checkInsert = ps.executeUpdate(); 

            // Here, we check if the record was inserted or not. If the value is 0, the record was not inserted
            if(checkInsert == 0){
                // This ‘ResourcePersistanceException’ communicates to the rest of our code that the insertion was not successful.
                throw new ResourcePersistanceException("Member was not entered into the database.");
            }

            // If the insertion is successful, we will return a representation of the object inserted
            return newMember;

        } catch (SQLException e) {
            e.printStackTrace();
        }

        // return null if something went wrong otherwise.
        return null;
    }



    @Override
    public List<Member> findAllMembers() {
        try (Connection conn = ConnectionFactory.getConnectionFactory().getConnection()){
            // This method will return a collection of objects from the database
            List<Member> members = new LinkedList<>();

            // The query to execute on the database
            String sql = "SELECT * FROM members"; 

            // Here, we use a simple statement instead of a prepared statement. 
            // We have no fear of SQL injection in this case since the query does not take any input from the user.
            Statement s = conn.createStatement(); 

            // The executeQuery statement will return a resultset with data from the database.
            ResultSet rs = s.executeQuery(sql);

            // The next() method will return true while data still exists in the resultSet
            while(rs.next()){ 
                // for each record in the result set, we will create a Member object and add it to a list which we will return
                Member member = new Member();
                // We are using the setter methods on the ‘Member’ object to set values to instance fields.
                // using the ResultSet we can get the value of each column. Note that the column index can be used as well.
                member.setEmail(rs.getString("email"));
                member.setFullName(rs.getString("full_name"));
                member.setExperienceMonths(rs.getInt("experience_months"));
                member.setRegistrationDate(rs.getDate("registration_date"));
                member.setPassword(rs.getString("password"));

                // Add the we now have a completed member
                members.add(member);
            }

            // After populating the list, return the retrieved data
            return members;

        } catch (SQLException e) {
            e.printStackTrace();
        }

        // return null if no records are found, or if something went wrong.
        return null;
    }



    @Override
    public Member findMemberByEmail(String email) {
        try(Connection conn = ConnectionFactory.getConnectionFactory().getConnection()){
            String sql = "SELECT * FROM members WHERE email = ?";

            PreparedStatement ps = conn.prepareStatement(sql);

            ps.setString(1, email);

            ResultSet rs = ps.executeQuery();

            // if the next() method returns false when called no record was located, so we throw an exception
            if(!rs.next()){
                throw new InvalidUserInputException("Entered information is incorrect for login, please try again");
            }

            // Create object to return
            Member member = new Member();

            // Here, we use the column index instead of the column name. This indexing starts at 1 instead of 0
            member.setEmail(rs.getString(1)); // Column 1 is email
            member.setPassword(rs.getString(2)); // Column 2 password
            member.setFullName(rs.getString(3)); // Column 3 is full_name
            member.setExperienceMonths(rs.getInt(4)); // Column 4 is experience_months
            member.setRegistrationDate(rs.getDate(5)); // Column 5 is registration_date

            // return populated object from database
            return member;

        } catch (SQLException e){
            e.printStackTrace();
        }

        // return null if no record is found
        return null;
    }



    @Override

    public boolean updateMember(Member updatedMember) {
        try (Connection conn = ConnectionFactory.getConnectionFactory().getConnection()){
            String sql = "UPDATE members SET password = ? , full_name = ? , experience_months = ? , registration_date = ? where email = ?";
            PreparedStatement ps = conn.prepareStatement(sql);

            ps.setString(1, updatedMember.getPassword());
            ps.setString(2, updatedMember.getFullName());
            ps.setInt(3, updatedMember.getExperienceMonths());
            ps.setDate(4, updatedMember.getRegistrationDate());

            // Remember that we set the value of the placeholders for the PreparedStatement query.
            // Here, we are setting the email for the search condition ‘where email = ?’
            ps.setString(5, updatedMember.getEmail());

            int checkInsert = ps.executeUpdate(); 

            if(checkInsert == 0){
                throw new ResourcePersistanceException("Member was not entered into the database.");
            }

            // instead of returning a representation for the object, we can return true or false to represent if the update was successful or not
            return true;

        } catch (SQLException e) {
            e.printStackTrace();
            return false;
        }
    }



    @Override
    public boolean deleteMemberById(String email) {

        try (Connection conn = ConnectionFactory.getConnectionFactory().getConnection()){
            String sql = "DELETE FROM members WHERE email = ?"; 

            PreparedStatement ps = conn.prepareStatement(sql);

            ps.setString(1, email);

            int checkInsert = ps.executeUpdate(); 

            if(checkInsert == 0){
                throw new ResourcePersistanceException("Member was not entered into the database.");
            }

            // deletion was successful
            return true;

        } catch (SQLException e) {
            e.printStackTrace();
            // deletion was unsuccessful
            return false;
        }
    }
}

Summary

Data Access Objects (DAOs) provide a standardized and abstracted way of accessing and manipulating data in Java applications. By separating the data access logic from the business logic, DAOs promote modularity, maintainability, and flexibility. They are commonly used in enterprise applications, web applications, and mobile applications to interact with various data storage systems. Implementing DAOs involves creating a DAO interface, its implementation class, an entity class, and configuring the data source.


## Navigating Result Set Rows
Learning Objectives

After completing this module, you should be able to:

Navigate ResultSet
Understand the different methods available for us and how they work
Effectively retrieve and process the desired data from the query results.
Description

When working with JDBC (Java Database Connectivity) and retrieving data from a database using a ResultSet, it is essential to understand how to navigate through the rows of the result set. Navigating result set rows allows you to access and process individual records returned by a SQL query.

ResultSet provides methods to move the cursor within the result set, allowing traversal of rows in different directions. By understanding how to navigate result set rows, you can effectively retrieve and process the desired data from the query results.

Methods available on ResultSet to navigate
next()

Moves the cursor to the next row in the result set and returns true if a valid row exists. This method is typically used in a loop to iterate through all rows.
previous()

Moves the cursor to the previous row in the result set and returns true if a valid row exists.
first()

Moves the cursor to the first row in the result set and returns true if a valid row exists.
last()

Moves the cursor to the last row in the result set and returns true if a valid row exists.
absolute(int row)

Moves the cursor to the specified row number, where the first row is 1. Returns true if the specified row is valid.
relative(int rows)

Moves the cursor forward or backward by the specified number of rows. The argument can be positive or negative, indicating the direction and magnitude of movement.
isFirst()

Returns true if the cursor is currently positioned at the first row in the result set.
isLast()

Returns true if the cursor is currently positioned at the last row in the result set.
Real World Application

Navigating result set rows is a common requirement in various Java applications that interact with databases. Some real-world scenarios where navigating result set rows is applicable include:

Data Display
Displaying data in a user interface or console application where each row represents a record from the query results.
Data Processing
Performing calculations, transformations, or validations on individual rows of the result set.
Data Export
Exporting query results to different file formats or external systems, where the navigation is necessary to process each row and generate the desired output.
Implementation
Examples of the syntax with comments on how each navigation method works for ResultSet
// Assuming a valid Connection object and SQL query execution

try (Connection connection = ConnectionFactory.getConnectionFactory().getConnection()){

    // Create a Statement or PreparedStatement
    Statement statement = connection.createStatement();

    // Execute the SQL query
    ResultSet resultSet = statement.executeQuery("SELECT * FROM members");

    // Move the cursor to the next row
    while (resultSet.next()) {
        // Retrieve column values from the current row
        int id = resultSet.getInt("id");
        String name = resultSet.getString("name");
        int age = resultSet.getInt("age");

        // Process or display the retrieved data
        Member member = new Member();
        member.setId(id);
        member.setName(name);
        member.setAge(age);

        System.out.println(member);
        // Move to the next row and repeat the loop
    }

    // Move the cursor to the previous row
    resultSet.afterLast(); // Move to the end of the result set
    while (resultSet.previous()) {
        // Retrieve column values from the current row
        // Process or display the retrieved data

        // Move to the previous row and repeat the loop
    }

    // Move the cursor to the first row
    resultSet.first();
    if (!resultSet.isBeforeFirst()) {
        // Retrieve column values from the first row
        // Process or display the retrieved data
    }

    // Move the cursor to the last row
    resultSet.last();
    if (!resultSet.isAfterLast()) {
        // Retrieve column values from the last row
        // Process or display the retrieved data
    }

    // Move the cursor to a specific row
    resultSet.absolute(5); // Move to the 5th row
    if (resultSet.isBeforeFirst() || resultSet.isAfterLast()) {
        // The specified row is invalid
    } else {
        // Retrieve column values from the specified row
        // Process or display the retrieved data
    }


}
Summary

Navigating result set rows is an essential aspect of working with JDBC and retrieving data from a database. By using methods like next(), previous(), first(), last(), absolute(), and relative() provided by the ResultSet interface, developers can traverse through the result set and process each row individually. This capability is valuable in various real-world applications, including data display, processing, and export. Understanding and implementing result set row navigation enables efficient and effective handling of query results in Java applications.





# JDBC/SQL Intermediate
## Simple And Prepared Statements
Learning Objectives

After completing this module, associates should be able to:

Define what a Statement and PreparedStatement are from JDBC
Understand the implementation of each interface
Differentiate between the two statements
Description

In Java, Statement and PreparedStatement are two types of objects used for executing SQL queries and statements against a database. They are part of the JDBC (Java Database Connectivity) API, which provides a standard way to interact with databases using Java.

A Statement object allows you to execute SQL queries directly, while a PreparedStatement object provides a pre-compiled SQL statement that can be executed multiple times with different parameter values. Both Statement and PreparedStatement can be used for executing SQL queries and updates, but PreparedStatement offer several advantages in terms of performance and security.

Once we have the Connection object, we can write our SQL and execute it:

Statement stmt = conn.createStatement();
String sql = "SELECT * FROM employees";
ResultSet rs = stmt.executeQuery(sql);
Alternatively, a PreparedStatement can be used. This interface gives us the flexibility of specifying parameters with the ? symbol. It also protects against SQL injection when user input is used by pre-compiling the SQL statement.

String sql = "SELECT * FROM employees WHERE age > ? AND location = ?";
PreparedStatement ps = conn.prepareStatement(sql);
ps.setInt(1, 40);
ps.setString(2, "New York");
ResultSet rs = ps.executeQuery();
The Statement and PreparedStatement also have additional methods for sending SQL, including:

.execute() - for any kind of SQL statement. Returns a boolean representing a successful or unsuccessful execution of the query
.executeUpdate() - for DML statements. Returns an int which is the number of rows affected
.executeQuery() - for DQL statements. Returns a ResultSet with records from the database
In addition, the CallableStatement interface, created by calling the prepareCall() method on a connection object, is used for executing stored procedures. A CallableStatement can return one ResultSet object or multiple ResultSet objects.

Real World Application

Statement and PreparedStatement are widely used in Java applications for various database-related tasks, including:

Data Retrieval
To fetch data from a database by executing SELECT queries.
Data Modification
Perform updates, inserts, and deletes on the database using SQL statements.
Batch Processing
When multiple SQL statements need to be executed as a batch, PreparedStatement can provide better performance compared to executing individual Statement.
Parameterized Queries
PreparedStatement are commonly used when executing parameterized queries, where the query has placeholders that can be filled with specific values at runtime. This helps prevent SQL injection attacks and improves code readability.
Implementation

To create and execute a statement, we follow these steps:

Create a database connection using the DriverManager class of JDBC.
Create a String object to represent the desired query you want to execute.
For PreparedStatement: the query should include ‘?’ in the String as placeholders to represent where input values will later be set.
Create a Statement or PreparedStatement using the Connection object (call the createStatement() or prepareStatement() methods to create the Statement or PreparedStatement respectively.).
For PreparedStatement: Set values for the placeholders in the prepared statement query.
Execute the Statement or PreparedStatement by calling the execute(), executeUpdate() or executeQuery() methods.
As a pre-requisite, you should have a database established. For clarity, the following examples will assume an established database schema as follows:

CREATE SCHEMA JDBCDemo;
 
CREATE TABLE JDBCDemo.EMPLOYEE
(
  ID INT NOT NULL DEFAULT 0,
  FIRST_NAME VARCHAR(100) NOT NULL,
  LAST_NAME VARCHAR(100) NULL,
  STAT_CD TINYINT NOT NULL DEFAULT 0
);
Let’s write above steps in code. Note, although the instructions above include steps for using Statement or PreparedStatement, we will showcase using PreparedStatements, as they are typically advantageous to use as compared to simple Statement objects:

Step 1: Make JDBC database connection
Connection connection = DriverManager
  .getConnection("jdbc:mysql://localhost:3306/JDBCDemo", "root", "password");
Step 2: Write Your Query
Create a String Object to represent the query for the PreparedStatement

String sql = "INSERT INTO EMPLOYEE (ID,FIRST_NAME,LAST_NAME,STAT_CD) VALUES (?,?,?,?)";
Step 3: Create Your Statement
Create a PreparedStatement from the Connection

PreparedStatement pstmt = connection.prepareStatement(sql);
Set values for the PreparedStatement

pstmt.setInt(1, 87); // insert the value 87 into the first question mark
pstmt.setString(2, "Lokesh"); // insert the value Lokesh into the second question mark
pstmt.setString(3, "Gupta"); // insert the value Gupta into the third question mark
pstmt.setInt(4, 5); // insert the value 5 into the fourth question mark
For these setter methods, the first argument is the position of which placeholder to insert into. The second argument is the actual value to insert.

Step 4: Execute Your Statement
Execute the PreparedStatement

int affectedRows = pstmt.executeUpdate();
Let’s see the whole working example:

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
 
public class PreparedStatementDemo 
{
  public static void main(String[] args) 
  {
    Connection connection = null;
    PreparedStatement pstmt = null;
    String sql = "INSERT INTO EMPLOYEE (ID,FIRST_NAME,LAST_NAME,STAT_CD) VALUES (?,?,?,?)";
    try
    {
      connection = DriverManager.getConnection("jdbc:mysql://localhost:3306/JDBCDemo", "root", "password");
       
      pstmt = connection.prepareStatement(sql);
      pstmt.setInt(1, 87);
      pstmt.setString(2, "Lokesh");
      pstmt.setString(3, "Gupta");
      pstmt.setInt(4, 5);
      int affectedRows = pstmt.executeUpdate();
      System.out.println(affectedRows + " row(s) affected !!");
    } 
    catch (Exception e) {
      e.printStackTrace();
    }finally {
      try {
        pstmt.close();
        connection.close();
      } catch (Exception e) {
        e.printStackTrace();
      }
    }
  }
}
Output:

1 row(s) affected !!
Summary

The Statement interface is used for executing static SQL statements.
The PreparedStatement interface is used for executing pre-compiled SQL statements.
Once we have the Connection object, we can write our SQL and execute it.
Alternatively, a PreparedStatement can be used. This interface gives us the flexibility of specifying parameters with the ? symbol. It also protects against SQL injection when user input is used by pre-compiling the SQL statement.
The Statement and PreparedStatement also have additional methods for sending SQL, including:
.execute() - for any kind of SQL statement, returns a boolean
.executeUpdate() - for DML statements, returns an int which is the number of rows affected
.executeQuery() - for SELECT statements, returns a ResultSet containing the query results
The CallableStatement interface is used to execute stored procedures. You create an instance of a CallableStatement by calling the prepareCall() method on a connection object. A CallableStatement can return one ResultSet object or multiple ResultSet objects.


## Reading from a Properties File
Learning Objectives

After completing this module, associates should be able to:

Describe JDBC properties files.
Successfully modify an existing properties file (or create a new one) for the current environment.
Description
JDBC Properties Files
When making a JDBC connection to a database you may have noticed the code typically looks like this.

 DriverManager.getConnection(URL, CONNECTION_USERNAME, CONNECTION_PASSWORD);
But, where are we getting those values? Well up until now you have probably seen them as plain text Strings. As you can imagine this is not secure and requires you to change the code anytime the credentials are updated. To solve this problem developers began to collect all of this information into properties files.

We need to implement the following steps in order to use a properties file to store our credentials:

Create a properties file that contains key-value pairs, where the values are the credentials
In our data access class, we can extract the credentials from the properties file
To have our credentials be even more secure, we can use our system's environment variables to store the information and have our properties files refer to those variables. This will ensure the credentials are not in our project code at all.

We need to implement the following steps in order to use a properties file to refer to environment variables that will store our credentials:

Create environment variables that contain the information our application will need
Create a properties file that contains key-value pairs, where the values are the names of our environment variables
In our data access class, we can extract the credentials from our system using the properties file
Real World Application

Knowing how to use a properties file for storing database credentials is important for several reasons:

Security: Storing database credentials in a properties file helps enhance security by keeping sensitive information separate from the codebase. It prevents credentials from being hard-coded directly into the source code, reducing the risk of accidental exposure or unauthorized access to sensitive information.

Ease of Maintenance: Using a properties file allows for easy management and updating of database credentials without modifying the application code. Administrators can update the properties file with new credentials or connection details without requiring changes to the deployed application, simplifying maintenance tasks.

Flexibility: Storing database credentials in a properties file provides flexibility in deployment environments. It allows different configurations for development, testing, and production environments by simply updating the properties file for each environment, rather than modifying the application code.

Separation of Concerns: Separating database credentials into a properties file promotes the principle of separation of concerns. It separates configuration details (such as database credentials) from the application logic, making the codebase cleaner and easier to understand.

Overall, knowing how to use a properties file for storing database credentials is a best practice in application development and promotes flexibility, maintainability, and compliance with security standards and regulations.

Implementation

The following is an example of how we can use a properties file without using environment variables:

Step 1: Create your .properties file.
A properties file stores information as key value pairs, each on their own line and the file has a .properties extension. For instance, you might have the following content in a properties file named application.properties:

URL=jdbc:postgresql://localhost:5432/PubHub
CONNECTION_USERNAME=user
CONNECTION_PASSWORD=password
Step 2: Extract the Information
Then you can use the Properties class and FileInputStream class to use these properties in your class.

// create a stream from properties file so we can read from it
FileInputStream fileStream = new FileInputStream("pathtopropertiesfile"); 

// create a Properties object and get information from it
Properties properties = new Properties(); 
properties.load(fileStream);

// extract values from the keys into variables
URL = properties.getProperty("URL");	
CONNECTION_PASSWORD = properties.getProperty("CONNECTION_PASSWORD"); 
CONNECTION_USERNAME = properties.getProperty("CONNECTION_USERNAME"); 
Great! So this solves some of our problem, but we are still saving our information as plain text. Now it's just in a different kind of file.

Aside: If you are using this method to read in properties for a web application that will be deployed on a server, you may need to use a different approach. When reading in a file, if you use a relative path it will be relative to the working directory. This can be unpredictable when it comes to servers, so instead we can use this method in those instances.

Properties prop = new Properties();
try {
   InputStream dbProps = ConnectionUtil.class.getClassLoader().getResourceAsStream("database.properties");
   prop.load(dbProps);
} catch (Exception e) {
   LogUtil.logException(e, ConnectionUtil.class);
}

The following is an example of how we can use a properties file and environment variables:

Step 1: Create your environment variables
We can create environment variables and name them url, connectionUsername, and connectionPassword.

Windows instructions: https://docs.oracle.com/en/database/oracle/machine-learning/oml4r/1.5.1/oread/administrative-tasks-oracle-machine-learning-r.html#GUID-DD6F9982-60D5-48F6-8270-A27EC53807D0
Mac OS instructions: https://phoenixnap.com/kb/set-environment-variable-mac
Step 2: Refer to our environment variables in the properties file
We use our properties file to store the names of environment variables we want to use in our application. Let's store the names of the environment variables we created in the last step.

URL=url
CONNECTION_USERNAME=connectionUsername
CONNECTION_PASSWORD=connectionPassword
Step 3: Extract the Information
Then in our Java class we'll use our properties file and the System.getenv() method to extract the credentials from our system.


    // create a stream from properties file so we can read from it
    FileInputStream fileStream = new FileInputStream("pathtopropertiesfile");

    // create a Properties object and get information from it
    Properties properties = new Properties();
    properties.load(fileStream);
    
    // extract values from keys (the values are the names of our environment variables)
	String url = properties.getProperty("URL"); // the value is "url"
	String password = properties.getProperty("CONNECTION_PASSWORD"); // the value is "connectionPassword"
	String username = properties.getProperty("CONNECTION_USERNAME"); // the value is "connectionUsername"

    // now that we have our environment variable names, let's use those to get their associated values, the actual credentials!
	URL = System.getenv(url);
	CONNECTION_PASSWORD = System.getenv(password);
	CONNECTION_USERNAME = System.getenv(username);
And then we should be good to go! Notice that for our application's security, we don't have either the environment variables names or our credentials anywhere in our Java code and the properties file doesn't directly contain our credentials.

Summary

When making a JDBC connection to a database, up until now you have probably seen the connection implemented as plain text Strings. As you can imagine this is not secure and requires you to change the code anytime our credentials are updated.
To solve this problem developers began to collect all of this information into properties files. A properties file stores information as key value pairs, each on their own line and the file has a .properties extension.
To have our credentials be even more secure, we can use our system's environment variables to store the information and have our properties files refer to those variables. This will ensure the credentials are not in our project code at all.


## Setting Up The Utility Class
Learning Objectives

After completing this module, you should be able to:

Implement the Utility Class to provide our Connection for JDBC API interaction with our database
Learn the importance of a Utility Class to provide Connections to our applications
Description

Within our Java Application, we can leverage a utility class to handle the distribution of connectionFactory objects to other classes requiring a need to make connections and requests of our database.

This allows for an ease of use when establishing the connection to the database by providing sensitive authorization credentials within a db.properties file that can be accessed by this utility class.
This produces a single instance of the utility.
Real World Application

Having a connection utility class in a Java application offers several benefits:

Centralized Configuration: A connection utility class allows you to centralize database connection configuration, such as JDBC URL, username, and password. This makes it easier to manage and update database connection settings without modifying multiple parts of the application.

Abstraction of Connection Logic: The utility class abstracts away the details of creating and managing database connections, providing a simple and consistent interface for obtaining connections. This abstraction shields the rest of the application from the complexities of JDBC connection management.

Facilitation of Testing: Having a connection utility class makes it easier to write unit tests for database-related code. You can mock the connection utility class in unit tests to simulate database interactions, enabling isolated testing of other application components.

Security: A connection utility class can help enhance security by encapsulating sensitive information, such as database credentials, and restricting access to them. This reduces the risk of exposing sensitive information in the application codebase.

Overall, a connection utility class provides a convenient and reliable way to manage database connections in Java applications, promoting centralizing configuration, abstracting connection logic, and improving security.

Implementation

First we must establish a db.properties file inside of our src/main/resources directory with the following information. This is important to include within your .gitignore as we want to obfuscate this sensitive information.

url=jdbc:postgresql://localhost:5432/postgres
username=postgresql
password=password
Next, we will create our utility class ConnectionFactory in the util package. Pay close attention to the comments outlining what each block of code is handling for us inside of this class.

package com.revature.example.util;

import java.io.FileReader;
import java.io.IOException;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.SQLException;
import java.util.Properties;

public class ConnectionFactory {

    /**
     * There are two class variables included within our ConnectionFactory 
     * utility class. First, is the single connectionFactory object itself
     * This will be passed along to any layer that will make requests to 
     * the database. Second, is the props object of the Properties Class that
     * will allow us the ability to access our db.properties to obtain our 
     * sensitive information.
     */
    private static final ConnectionFactory connectionFactory = new  ConnectionFactory(); 
    private Properties props = new Properties();

    /**
     * We include a private constructor here to make sure that there are 
     * no other possibilities to create another instance of our 
     * ConnectionFactory object. Along with this, during the construction
     * of our connectionFactory object we make sure we can load in 
     * our db.properties file, handling any potential exception thrown.
     */
    private ConnectionFactory(){ 
        try {
            props.load(new FileReader("src/main/resources/db.properties"));
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Next, we must also include a static method to obtain the single
     * instance of our connectionFactory, to allow this to be accessible
     * within other classes. This way we can provide the connectionFactory
     * which can subsequently invoke the getConnection() method when we
     * need to make requests of our database.
     */
    public static ConnectionFactory getConnectionFactory() {
        return connectionFactory;
    }

    /**
     * This method provides the ability for classes to utilize the
     * getConnection() method from our Utility class and establish a
     * connection with our database that can be used to execute SQL 
     * statements through the Statement or PreparedStatement interfaces.
     * This will also check for any SQLException, incase the information
     * provided in the db.properties is incorrect.
     */
    public Connection getConnection(){
        try {
            return DriverManager.getConnection(
                props.getProperty("url"),    
                props.getProperty("username"), 
                props.getProperty("password")
            );
        } catch (SQLException e) {
            e.printStackTrace();
            return null;
        }
    }
}
Summary

The ConnectionFactory utility class handles the distribution of connectionFactory objects to other classes requiring a need to make connections and requests of our database.

This allows for an ease of use when establishing the connection to the database by providing sensitive authorization credentials within a db.properties file that can be accessed by this utility class.
This produces a single instance of the utility class.


## Preventing SQL Injection
Learning Objectives

After completing this module, associates should be able to:

Discuss how SQL injection occurs and the need to prevent it.
Successfully execute a Java program that prevents SQL injection.
Description

Today we'll explore common coding mistakes in Java that lead to a vulnerable application and how to avoid them using the APIs available in the JVM's standard runtime library.

How Do Applications Become Vulnerable to SQL Injection?
Injection attacks work because, for many applications, the only way to execute a given computation is to dynamically generate code that is in turn run by another system or component. If in the process of generating this code we use untrusted data without proper sanitization, we leave an open door for hackers to exploit.

This statement may sound a bit abstract, so let's take look at how this happens in practice with a textbook example:

public List<AccountDTO>
  unsafeFindAccountsByCustomerId(String customerId)
  throws SQLException {
    // UNSAFE !!! DON'T DO THIS !!!
    String sql = "select "
      + "customer_id,acc_number,branch_id,balance "
      + "from Accounts where customer_id = '"
      + customerId 
      + "'";
    Connection c = dataSource.getConnection();
    ResultSet rs = c.createStatement().executeQuery(sql);
    // ...
}
The problem with this code is obvious: we've put the customerId‘s value into the query with no validation at all. Nothing bad will happen if we're sure that this value will only come from trusted sources, but can we?

Let's imagine that this function is used in a REST API implementation for an account resource. Exploiting this code is trivial: all we have to do is to send a value that, when concatenated with the fixed part of the query, change its intended behavior:

curl -X GET \
  'http://localhost:8080/accounts?customerId=abc%27%20or%20%271%27=%271' \
Assuming the customerId parameter value goes unchecked until it reaches our function, here's what we'd receive:

abc' or '1' = '1
When we join this value with the fixed part, we get the final SQL statement that will be executed:

select customer_id, acc_number,branch_id, balance
  from Accounts where customerId = 'abc' or '1' = '1'
The expression 1 = 1 will always return true, and when true is returned in a WHERE clause, all records are returned.

A smart developer (aren't we all?) would now be thinking: “That's silly! I'd never use string concatenation to build a query like this”. Not so fast… This canonical example is silly indeed but there are situations where we might still need to do it:

Complex queries with dynamic search criteria: adding UNION clauses depending on user-supplied criteria
Dynamic grouping or ordering: REST APIs used as a backend to a GUI data table
Prevention Techniques
Now that we know what a SQL injection is, let's see how we can protect our code from this kind of attack. Here we're focusing on a couple of very effective techniques available in Java and other JVM languages, but similar concepts are available to other environments, such as PHP, .Net, Ruby and so forth.

Parameterized Queries
This technique consists of using prepared statements with the question mark placeholder (“?”) in our queries whenever we need to insert a user-supplied value. This is very effective and, unless there's a bug in the JDBC driver's implementation, immune to exploits.

Let's rewrite our example function to use this technique:

public List<AccountDTO> safeFindAccountsByCustomerId(String customerId)
  throws Exception {
    
    String sql = "select "
      + "customer_id, acc_number, branch_id, balance from Accounts"
      + "where customer_id = ?";
    
    Connection c = dataSource.getConnection();
    PreparedStatement p = c.prepareStatement(sql);
    p.setString(1, customerId);
    ResultSet rs = p.executeQuery(sql)); 
    // omitted - process rows and return an account list
}
Here we've used the prepareStatement() method available in the Connection instance to get a PreparedStatement. This interface extends the regular Statement interface with several methods that allow us to safely insert user-supplied values in a query before executing it.

[DEBUG][SQL] select
  account0_.id as id1_0_,
  account0_.acc_number as acc_numb2_0_,
  account0_.balance as balance3_0_,
  account0_.branch_id as branch_i4_0_,
  account0_.customer_id as customer5_0_ 
from accounts account0_ 
where account0_.customer_id=?
As expected, the ORM layer creates a prepared statement using a placeholder for the customerId parameter. This is the same we've done in the plain JDBC case – but with a few statements less, which is nice.

As a bonus, this approach usually results in a better performing query, since most databases can cache the query plan associated with a prepared statement.

Please note that this approach only works for placeholders used as values. For instance, we can't use placeholders to dynamically change the name of a table:

// This WILL NOT WORK !!!
PreparedStatement p = c.prepareStatement("select count(*) from ?");
p.setString(1, tableName);
The main reason behind this is the very nature of a prepared statement: database servers use them to cache the query plan required to pull the result set, which usually is the same for any possible value. This is not true for table names and other constructs available in the SQL language such as columns used in an order by clause.

User Data Sanitization
Data Sanitization is a technique of applying a filter to user supplied-data so it can be safely used by other parts of our application. A filter's implementation may vary a lot, but we can generally classify them in two types: whitelists and blacklists.

Blacklists, which consist of filters that try to identify an invalid pattern, are usually of little value in the context of SQL Injection prevention – but not for the detection! More on this later.

Whitelists, on the other hand, work particularly well when we can define exactly what is a valid input.

Let's enhance our safeFindAccountsByCustomerId method so now the caller can also specify the column used to sort the result set. Since we know the set of possible columns, we can implement a whitelist using a simple set and use it to sanitize the received parameter:

private static final Set<String> VALID_COLUMNS_FOR_ORDER_BY
  = Collections.unmodifiableSet(Stream
      .of("acc_number","branch_id","balance")
      .collect(Collectors.toCollection(HashSet::new)));

public List<AccountDTO> safeFindAccountsByCustomerId(
  String customerId,
  String orderBy) throws Exception { 
    String sql = "select "
      + "customer_id,acc_number,branch_id,balance from Accounts"
      + "where customer_id = ? ";
    if (VALID_COLUMNS_FOR_ORDER_BY.contains(orderBy)) {
        sql = sql + " order by " + orderBy;
    } else {
        throw new IllegalArgumentException("Nice try!");
    }
    Connection c = dataSource.getConnection();
    PreparedStatement p = c.prepareStatement(sql);
    p.setString(1,customerId);
    // ... result set processing omitted
}
Here, we're combining the prepared statement approach and a whitelist used to sanitize the orderBy argument. The final result is a safe string with the final SQL statement. In this simple example, we're using a static set, but we could also have used database metadata functions to create it.

Are We Safe Now?
Let's assume that we've used parameterized queries and/or whitelists everywhere. Can we now go to our manager and guarantee we're safe?

Well… not so fast, there are other aspects we must consider:

Stored Procedures: These are also prone to SQL Injection issues; whenever possible please apply sanitation even to values that will be sent to the database via prepared statements
Triggers: Same issue as with procedure calls, but even more insidious because sometimes we have no idea they're there.
Insecure Direct Object References: Even if our application is SQL-Injection free, there's still a risk that associated with this vulnerability category – the main point here is related to different ways an attacker can trick the application, so it returns records he or she was not supposed to have access to – there's a good cheat sheet on this topic available at OWASP's GitHub repository
In short, our best option here is caution. Many organizations nowadays use a “red team” exactly for this. Let them do their job, which is exactly to find any remaining vulnerabilities.

Damage Control Techniques
As a good security practice, we should always implement multiple defense layers – a concept known as defense in depth. The main idea is that even if we're unable to find all possible vulnerabilities in our code – a common scenario when dealing with legacy systems – we should at least try to limit the damage an attack would inflict.

Of course, this would be a topic for a whole article or even a book but let's name a few measures:

Apply the principle of least privilege: Restrict as much as possible the privileges of the account used to access the database
Use database-specific methods available in order to add an additional protection layer; for example, the H2 Database has a session-level option that disables all literal values on SQL Queries
Use short-lived credentials: Make the application rotate database credentials often
Log everything: If the application stores customer data, this is a must; there are many solutions available that integrate directly to the database or work as a proxy, so in case of an attack we can at least assess the damage
Use WAFs (Web Application Firewalls) or similar intrusion detection solutions: those are the typical blacklist examples – usually, they come with a sizeable database of known attack signatures and will trigger a programmable action upon detection. Some also include in-JVM agents that can detect intrusions by applying some instrumentation – the main advantage of this approach is that an eventual vulnerability becomes much easier to fix since we'll have a full stack trace available.
Real World Application

SQL Injection vulnerabilities can be found in various Java applications that use JDBC to interact with databases. Some real-world scenarios where SQL Injection attacks can occur include:

Web Applications

Web applications that accept user input for generating dynamic SQL queries, such as search forms, user authentication, or data filtering, are particularly susceptible to SQL Injection attacks.
E-commerce Systems

Online stores that handle customer data, process transactions, or manage inventory should carefully validate and sanitize user input to prevent SQL Injection attacks that could compromise sensitive information.
Content Management Systems

CMS platforms that allow user-generated content or provide administrative interfaces are potential targets for SQL Injection attacks. Proper input validation and parameterization are crucial to prevent unauthorized access or data manipulation.
Real-world SQL injection examples
In 2008, payment processor Heartland Payment Systems was hacked via SQL injection for over $130 million in losses. The attackers stole a whopping 130 million credit card numbers in one of the biggest data breaches of credit card data in history.
In 2014, a hacker gang collected over 1.2 billion unique IDS and password combinations from over 420,000 websites all across the internet. The Russian hacker group used SQL injections to command databases to reveal and dump their contents.
UK telecom giant TalkTalk came under fire in 2015 for weak web security that compromised hundreds of thousands of customers’ personal information. Even though SQL security risks were well-known at the time, the company was helpless against the attack.
For an SQL injection example that concerns the everyday gamer, Epic Games had their forums hacked in 2016, and 800,000 user accounts were leaked. SQL injections targeted the popular online message board software vBulletin, which has become infamous for its vulnerability to SQL exploits. In general, SQL injection attacks are spreading through the gaming industry like wildfire.
Breaches Enabled by SQL Injection
GhostShell attack—hackers from APT group Team GhostShell targeted 53 universities using SQL injection, stole and published 36,000 personal records belonging to students, faculty, and staff.
Turkish government—another APT group, RedHack collective, used SQL injection to breach the Turkish government website and erase debt to government agencies.
7-Eleven breach—a team of attackers used SQL injection to penetrate corporate systems at several companies, primarily the 7-Eleven retail chain, stealing 130 million credit card numbers.
HBGary breach—hackers related to the Anonymous activist group used SQL Injection to take down the IT security company’s website. The attack was a response to HBGary CEO publicizing that he had names of Anonymous organization members.
Notable SQL Injection Vulnerabilities
Tesla vulnerability—in 2014, security researchers publicized that they were able to breach the website of Tesla using SQL injection, gain administrative privileges and steal user data.
Cisco vulnerability—in 2018, a SQL injection vulnerability was found in Cisco Prime License Manager. The vulnerability allowed attackers to gain shell access to systems on which the license manager was deployed. Cisco has patched the vulnerability.
Fortnite vulnerability—Fortnite is an online game with over 350 million users. In 2019, a SQL injection vulnerability was discovered which could let attackers access user accounts. The vulnerability was patched.
Implementation
Using Prepared Statements
This lab covers the following topics:

Overview of Prepared Statements
Creating a PreparedStatement Object
Supplying Values for PreparedStatement Parameters
Overview of Prepared Statements
Sometimes it is more convenient to use a PreparedStatement object for sending SQL statements to the database. This special type of statement is derived from the more general class, Statement, that you already know.

If you want to execute a Statement object many times, it usually reduces execution time to use a PreparedStatement object instead.

The main feature of a PreparedStatement object is that, unlike a Statement object, it is given a SQL statement when it is created. The advantage to this is that in most cases, this SQL statement is sent to the DBMS right away, where it is compiled. As a result, the PreparedStatement object contains not just a SQL statement, but a SQL statement that has been precompiled. This means that when the PreparedStatement is executed, the DBMS can just run the PreparedStatement SQL statement without having to compile it first.

Although you can use PreparedStatement objects for SQL statements with no parameters, you probably use them most often for SQL statements that take parameters. The advantage of using SQL statements that take parameters is that you can use the same statement and supply it with different values each time you execute it. Examples of this are in the following sections.

However, the most important advantage of prepared statements is that they help prevent SQL injection attacks. SQL injection is a technique to maliciously exploit applications that use client-supplied data in SQL statements. Attackers trick the SQL engine into executing unintended commands by supplying specially crafted string input, thereby gaining unauthorized access to a database to view or manipulate restricted data. SQL injection techniques all exploit a single vulnerability in the application: Incorrectly validated or nonvalidated string literals are concatenated into a dynamically built SQL statement and interpreted as code by the SQL engine. Prepared statements always treat client-supplied data as content of a parameter and never as a part of an SQL statement. See the section SQL Injection in Database PL/SQL Language Reference, part of Oracle Database documentation, for more information.

The following method, CoffeesTable.updateCoffeeSales, stores the number of pounds of coffee sold in the current week in the SALES column for each type of coffee, and updates the total number of pounds of coffee sold in the TOTAL column for each type of coffee:

  public void updateCoffeeSales(HashMap<String, Integer> salesForWeek) throws SQLException {
    String updateString =
      "update COFFEES set SALES = ? where COF_NAME = ?";
    String updateStatement =
      "update COFFEES set TOTAL = TOTAL + ? where COF_NAME = ?";

    try (PreparedStatement updateSales = con.prepareStatement(updateString);
         PreparedStatement updateTotal = con.prepareStatement(updateStatement))
    
    {
      con.setAutoCommit(false);
      for (Map.Entry<String, Integer> e : salesForWeek.entrySet()) {
        updateSales.setInt(1, e.getValue().intValue());
        updateSales.setString(2, e.getKey());
        updateSales.executeUpdate();

        updateTotal.setInt(1, e.getValue().intValue());
        updateTotal.setString(2, e.getKey());
        updateTotal.executeUpdate();
        con.commit();
      }
    } catch (SQLException e) {
      JDBCTutorialUtilities.printSQLException(e);
      if (con != null) {
        try {
          System.err.print("Transaction is being rolled back");
          con.rollback();
        } catch (SQLException excep) {
          JDBCTutorialUtilities.printSQLException(excep);
        }
      }
    }
  }

Creating a PreparedStatement Object
The following creates a PreparedStatement object that takes two input parameters:

    String updateString =
      "update COFFEES " + "set SALES = ? where COF_NAME = ?";
	// ...
    PreparedStatement updateSales = con.prepareStatement(updateString);
Supplying Values for PreparedStatement Parameters
You must supply values in place of the question mark placeholders (if there are any) before you can execute a PreparedStatement object. Do this by calling one of the setter methods defined in the PreparedStatement class. The following statements supply the two question mark placeholders in the PreparedStatement named updateSales:

updateSales.setInt(1, e.getValue().intValue());
updateSales.setString(2, e.getKey());
The first argument for each of these setter methods specifies the question mark placeholder. In this example, setInt specifies the first placeholder and setString specifies the second placeholder.

After a parameter has been set with a value, it retains that value until it is reset to another value, or the method clearParameters is called. Using the PreparedStatement object updateSales, the following code fragment illustrates reusing a prepared statement after resetting the value of one of its parameters and leaving the other one the same:

// changes SALES column of French Roast### Prerequisite:

Prepare your system according to the Implementation found in the Intro to JDBC topic
//row to 100

updateSales.setInt(1, 100);
updateSales.setString(2, "French_Roast");
updateSales.executeUpdate();

// changes SALES column of Espresso row to 100
// (the first parameter stayed 100, and the second
// parameter was reset to "Espresso")

updateSales.setString(2, "Espresso");
updateSales.executeUpdate();
Using Loops to Set Values
You can often make coding easier by using a for loop or a while loop to set values for input parameters.

The CoffeesTable.updateCoffeeSales method uses a for-each loop to repeatedly set values in the PreparedStatement objects updateSales and updateTotal:

for (Map.Entry<String, Integer> e : salesForWeek.entrySet()) {
  updateSales.setInt(1, e.getValue().intValue());
  updateSales.setString(2, e.getKey());
  // ...
}
The method CoffeesTable.updateCoffeeSales takes one argument, HashMap. Each element in the HashMap argument contains the name of one type of coffee and the number of pounds of that type of coffee sold during the current week. The for-each loop iterates through each element of the HashMap argument and sets the appropriate question mark placeholders in updateSales and updateTotal.

Executing PreparedStatement Objects
As with Statement objects, to execute a PreparedStatement object, call an execute statement: executeQuery if the query returns only one ResultSet (such as a SELECT SQL statement), executeUpdate if the query does not return a ResultSet (such as an UPDATE SQL statement), or execute if the query might return more than one ResultSet object. Both PreparedStatement objects in CoffeesTable.updateCoffeeSales(HashMap<String, Integer>) contain UPDATE SQL statements, so both are executed by calling executeUpdate:

updateSales.setInt(1, e.getValue().intValue());
updateSales.setString(2, e.getKey());
updateSales.executeUpdate();

updateTotal.setInt(1, e.getValue().intValue());
updateTotal.setString(2, e.getKey());
updateTotal.executeUpdate();
con.commit();
No arguments are supplied to executeUpdate when they are used to execute updateSales and updateTotals; both PreparedStatement objects already contain the SQL statement to be executed.

Note: At the beginning of CoffeesTable.updateCoffeeSales, the auto-commit mode is set to false:

con.setAutoCommit(false);
Consequently, no SQL statements are committed until the method commit is called. For more information about the auto-commit mode, see Transactions.
Return Values for the executeUpdate Method Whereas executeQuery returns a ResultSet object containing the results of the query sent to the DBMS, the return value for executeUpdate is an int value that indicates how many rows of a table were updated. For instance, the following code shows the return value of executeUpdate being assigned to the variable n:

updateSales.setInt(1, 50);
updateSales.setString(2, "Espresso");
int n = updateSales.executeUpdate();
// n = 1 because one row had a change in it
The table COFFEES is updated; the value 50 replaces the value in the column SALES in the row for Espresso. That update affects one row in the table, so n is equal to 1.

When the method executeUpdate is used to execute a DDL (data definition language) statement, such as in creating a table, it returns the int value of 0. Consequently, in the following code fragment, which executes the DDL statement used to create the table COFFEES, n is assigned a value of 0:

// n = 0
int n = executeUpdate(createTableCoffees); 
Note that when the return value for executeUpdate is 0, it can mean one of two things:

The statement executed was an update statement that affected zero rows. The statement executed was a DDL statement.

Summary

SQL Injection attacks work because, for many applications, the only way to execute a given computation is to dynamically generate code that is in turn run by another system or component.
If in the process of generating this code we use untrusted data without proper sanitization, we leave an open door for hackers to exploit.
Some techniques for preventing SQL injection include:
Parameterized queries
User Data Sanitization
Other factors to consider:
Stored Procedures: These are also prone to SQL Injection issues; whenever possible please apply sanitation even to values that will be sent to the database via prepared statements
Triggers: Same issue as with procedure calls, but even more insidious because sometimes we have no idea they're there…
Insecure Direct Object References: Even if our application is SQL-Injection free, there's still a risk that associated with this vulnerability category – the main point here is related to different ways an attacker can trick the application, so it returns records he or she was not supposed to have access to – there's a good cheat sheet on this topic available at OWASP's GitHub repository
Some Damage Control Techniques
Apply the principle of least privilege: Restrict as much as possible the privileges of the account used to access the database
Use database-specific methods available in order to add an additional protection layer; for example, the H2 Database has a session-level option that disables all literal values on SQL Queries
Use short-lived credentials: Make the application rotate database credentials often
Log everything: If the application stores customer data, this is a must; there are many solutions available that integrate directly to the database or work as a proxy, so in case of an attack we can at least assess the damage
Use WAFs or similar intrusion detection solutions: those are the typical blacklist examples – usually, they come with a sizeable database of known attack signatures and will trigger a programmable action upon detection. Some also include in-JVM agents that can detect intrusions by applying some instrumentation – the main advantage of this approach is that an eventual vulnerability becomes much easier to fix since we'll have a full stack trace available.


## Sql Injection
Cumulative for the SQL Injection
Prerequisites and Learning Objectives
Prerequisites and Learning Objectives for SQL Injection
Prerequisites
Topic: Simple and PreparedStatement
Learning Objectives
Define SQL Injection
Understand the importance of preventing SQL Injection
What tools JDBC API offers us to prevent this
How to prevent any risk of SQL Injection when implementing DAO
Description
Description for SQL Injection
SQL Injection is a common security vulnerability that occurs when untrusted user input is included directly in SQL queries without proper sanitization or parameterization. When using JDBC (Java Database Connectivity) to interact with databases, it is crucial to be aware of and mitigate the risk of SQL Injection.

An SQL Injection attack allows malicious users to manipulate or execute unintended SQL queries, potentially leading to unauthorized access, data leakage, or data corruption. It is important to properly validate and sanitize user input before including it in SQL queries to prevent such attacks.

Real World Application
Real World Application for SQL Injection
SQL Injection vulnerabilities can be found in various Java applications that use JDBC to interact with databases. Some real-world scenarios where SQL Injection attacks can occur include:

Web Applications
Web applications that accept user input for generating dynamic SQL queries, such as search forms, user authentication, or data filtering, are particularly susceptible to SQL Injection attacks.
E-commerce Systems
Online stores that handle customer data, process transactions, or manage inventory should carefully validate and sanitize user input to prevent SQL Injection attacks that could compromise sensitive information.
Content Management Systems
CMS platforms that allow user-generated content or provide administrative interfaces are potential targets for SQL Injection attacks. Proper input validation and parameterization are crucial to prevent unauthorized access or data manipulation.
Mitigation of SQL vulnerabilities:
Parameterized Queries
Instead of concatenating user input directly into SQL queries, use parameterized queries with placeholders for dynamic values. This ensures that user input is treated as data rather than executable code.
Input Validation and Sanitization
Validate and sanitize user input before using it in SQL queries. Apply appropriate input validation techniques, such as whitelisting, to ensure that only expected values are accepted.
Stored Procedures
Consider using stored procedures or prepared statements to encapsulate SQL logic. These provide an extra layer of security by separating the code from user input.
Least Privilege Principle
Ensure that the database user account used by the application has the minimum required privileges. Restricting access rights helps limit the potential impact of a successful SQL Injection attack.
Regular Updates and Security Patches
Keep the JDBC driver and database software up to date to benefit from the latest security fixes and improvements.


## Composite Key
Learning Objectives

After completing this module, associates should be able to:

To define and implement Composite key
Description
Composite Key
Composite key is a combination of columns used to uniquely identify a table.
A composite key can also be a combination of candidate keys.
A group of all the foreign keys can also be used to uniquely identify a table, even in this scenario is also considered a composite key.
Real World Application

Consider a scenario where there is a student table, mentor table, and another table named mentor_mentee. A student can have multiple mentors, and a mentor can have multiple students. In this type of relationship, the mentor_mentee table stores pairings between a student and their mentor. Records in the student table can be uniquely identified by student id, records in mentor table can be identified by mentor id, but the table mentor_mentee needs both student id and mentor id to uniquely identify a record. Both student id and mentor id are grouped as a primary key.

Implementation

Below is an example of using a composite key.

Step 1: Create the tables called employee, client, and branch.
CREATE TABLE employee(
    employee_id INT PRIMARY KEY,
    first_name VARCHAR(20),
    last_name VARCHAR(20),
    manager_id INT,
    FOREIGN KEY(manager_id) REFERENCES employee(employee_id) ON DELETE SET NULL
    );
CREATE TABLE client(
    client_id INT PRIMARY KEY,
    client_name VARCHAR(20),
    branch_id INT,
    FOREIGN KEY(branch_id) REFERENCES branch(branch_id) ON DELETE SET NULL

);
CREATE TABLE branch (
  branch_id INT PRIMARY KEY,
  client_name VARCHAR(40),
  manager_id INT,
  FOREIGN KEY(manager_id) REFERENCES employee(employee_id) ON DELETE SET NULL
);
Step 2: Create a table works_with
CREATE TABLE works_with (
  employee_id INT,
  client_id INT,
  sales INT,
  PRIMARY KEY(employee_id, client_id),
  FOREIGN KEY(employee_id) REFERENCES Employee(employee_id) ON DELETE CASCADE,
  FOREIGN KEY(client_id) REFERENCES Client(client_id) ON DELETE CASCADE
);
In the above table works_with, employee_id and client_id are combined to create a composite key.

Summary

Composite key is a combination of columns used to uniquely identify a table.
A composite key can also be a combination of candidate keys.
A group of all the foreign keys can also be used to uniquely identify a table, even in this scenario is also considered a composite key.


## Normalization
Learning Objectives

After completing this module, associates should be able to:

To define Normalization
To define Normal forms 1NF, 2NF, 3NF and BCNF.
Description
Normalization
Normalization is the process of efficiently organizing data in a database. The two main objectives of normalization are to eliminate redundant data (to make sure that the same data is not stored twice) and to ensure data dependencies make sense (to store only relational data in tables). Both of these are important because they reduce the amount of space of a database and ensure that data is logically stored.

Normal Forms
First, second, and third normal forms are stepping stones to the Boyce-Codd normal form and, when appropriate, the higher normal forms.

First Normal Form
The first normal form (1NF) is conclusive of a relational database. If we are to consider a database relational, then all relations in the database are in 1NF.

A database is considered relational if all the fields in the tables are atomic, every column is a unique attribute, and a unique identifier or primary key is used. It can also be described as the elimination of recurring groups of relations. 1NF doesn't allow hierarchies of data values.

Second Normal Form
Second normal form (2NF) deals with the elimination of circular dependencies from a relation. We say a relation is in 2NF if it is in 1NF and if every non-key attribute is completely dependent only on the Primary Key.

A non-key attribute is any column that cannot be used to uniquely identify the table.

Third Normal Form
Third normal form (3NF) deals with the elimination of non-key attributes that do not describe the Primary Key. For a relation to be in 3NF, the relationship between any two non-key attributes, or groups of non-key attributes, must not be in a one-to-one relation.

The attributes should be mutually independent which means, none of the attributes should be functionally dependent on any combination of attributes. This mutual independence makes sure that any update on the individual attribute will not affect other attributes in a row.

Boyce-Codd Normal Form
Boyce-Codd Normal Form or BCNF is an extension to the third normal form, and is also known as 3.5 Normal Form.

A table should follow these two conditions to satisfy BCNF:

It should be in the Third Normal Form.
And, for any dependency A → B, A should be a super key. which means that A should be a key attribute if B is a key attribute.
Real World Application

Normalization is an essential concept in database design and plays a crucial role in ensuring the efficiency, integrity, and scalability of databases. Here are several key reasons why normalization is important:

Data Integrity: Normalization reduces data redundancy and eliminates anomalies such as insertion, update, and deletion anomalies. By organizing data into well-structured tables and relationships, normalization helps maintain data integrity and consistency.

Query Performance: Normalized databases typically have better query performance because they require fewer joins and provide efficient access paths to data. Well-designed normalized databases allow for faster query execution, resulting in improved application performance.

Ease of Maintenance: Normalization makes databases easier to maintain and modify over time. It simplifies database schema changes, such as adding new tables or modifying existing ones, without causing disruptions to the application or compromising data integrity.

In summary, normalization is important because it promotes data integrity and efficiency and also ensures that databases are well-structured, optimized for performance, and capable of supporting evolving business requirements.

Implementation


Let's demonstrate normalizing an unnormalized table. In our example, we have a zookeepers table. Zookeepers can be responsible for more than one exhibit, and each exhibit can contain more than one animal for them to take care of, and each exhibit can be taken care of by more than one zookeeper.

Unnormalized
CREATE TABLE zookeepers(
	first_name varchar(100),
	last_name varchar(100),
	exhibit varchar(100),
	animal_type1 varchar(100),
	animal_type2 varchar(100),
	animal_type3 varchar(100),
	salary integer,
	position_title varchar(100)
);
This table is unnormalized because it does not have a primary key and some columns are redundant.

1NF

CREATE TABLE zookeepers(
	id integer auto_increment PRIMARY KEY,
	first_name varchar(100),
	last_name varchar(100),
	exhibit varchar(100),
	animal_type varchar(100),
	salary integer,
	position_title varchar(100)
);

To put the example in 1NF, we added a PK column as well as removed redundant columns.

2NF
CREATE TABLE zookeepers(
	id integer auto_increment PRIMARY KEY,
	first_name varchar(100),
	last_name varchar(100),
	salary integer,
	position_title varchar(100)
);

CREATE TABLE exhibits(
	id integer auto_increment PRIMARY KEY, 
	exhibit_name varchar(100)
);

CREATE TABLE animals(
	id integer auto_increment PRIMARY KEY, 
	exhibit_id integer,
	species varchar(100),
	FOREIGN KEY (exhibit_id) REFERENCES exhibits(id) ON DELETE SET NULL
);

CREATE TABLE zookeepers_exhibits(
	zookeeper_id integer,
	exhibit_id integer,
	FOREIGN KEY (zookeeper_id) REFERENCES zookeepers(id) ON DELETE CASCADE,
	FOREIGN KEY (exhibit_id) REFERENCES exhibits(id) ON DELETE CASCADE,
	PRIMARY KEY (zookeeper_id, exhibit_id)
);
In order to move our table to 2NF, we needed to remove any columns that don't fully depend on or describe the key or zookeeper entity. Notice we moved exhibit and animal related columns out of the zookeepers table. Because there is a many to many relationship between zookeepers and exhibits, we have a bridge or join table that represents the pairings between the two entities. The animals table is created and contains a foreign key that references the exhibit table, since an animal would depend on / live within an exhibit.

3NF
CREATE TABLE zookeepers(
	id integer auto_increment primary key,
	position_title_id integer,
	first_name varchar(100),
	last_name varchar(100),
  	FOREIGN KEY (position_title_id) REFERENCES position_titles(id) ON DELETE SET NULL

);

CREATE TABLE position_titles(
	id integer auto_increment primary key,
	position_title_name varchar(100),
	salary integer
);

Lastly, we need to remove a transitive dependency so we can move our zookeepers table into 3NF. The two columns salary and position_title did describe the zookeeper entity, however if a change is made to the position_title column, for example if they are promoted to a Level 2 Zookeeper rather than a Level 1 Zookeeper, then a change should also be made to the salary column. This violates 3NF, so we would need to move these two columns into their own position_title table and have the zookeepers table depend on it instead.

Summary
Normalization
Normalization is the process of organizing the data and the attributes of a database. it is performed to reduce the data redundancy.
Normal Forms
1NF: A relation is in 1NF if all its attributes have an atomic value.

2NF: A relation is in 2NF if it is in 1NF and all non-key attributes are fully functionally dependent on the candidate key.

3NF: A relation is in 3NF if it is in 2NF and there is no transitive dependency.

BCNF: A relation is in BCNF if it is in 3NF and for every functional dependency, the left-hand side of the dependency is a super key.


## Multiplicity
Learning Objectives

After completing this module, associates should be able to:

To define and implement MULTIPLICITY
Description

Multiplicity vs Cardinality
Although these two terms are often used interchangeably, technically they are different. Generally, you can think of multiplicity as the possible range of associations between one entity and another. Cardinality represents a specific number of associations. For example, let's say we have an application to manage apartment buildings and we want to model two entities, a Person and a Pet. We want to have a constraint of each person having at most 2 pets. In ER Diagrams, the multiplicity could be seen as 0..2 or a range of 0 to 2 associations between Person and Pet. The cardinalities defined are 0 and 2. If there is no upper limit then we can use an asterisk (*) rather than a number. For example, the multiplicity for the relationship between a social media influencer and their followers could be 0...*.

We often look at multiplicity in simpler terms by just referring to a type of relationship entities can have with one another.

These relationships are:
one to one relationship
Definition: In a one-to-one relationship, each record in one table is associated with only one record in another table, and vice versa.
Example: A person might have only one passport, and each passport belongs to only one person.
Implementation: A one-to-one relationship can be implemented by adding a unique foreign key in one of the tables that references the primary key of the other table. Alternatively, both tables can share the same primary key.
one to many relationship
This is described from the perspective of the "one" side.
Definition: In a one-to-many relationship, a single record in the "one" table can be associated with multiple records in the "many" table.
Example: A single author can have multiple books. Here, "author" is the "one" side, and "books" are the "many" side.
Implementation: The "many" table (books) includes a foreign key that references the primary key in the "one" table (author).
many to one relationship
This is described from the perspective of the "many" side.
Definition: In a many-to-one relationship, multiple records in the "many" table can be associated with a single record in the "one" table.
Example: Many employees can belong to a single department. Here, "employees" are the "many" side, and "department" is the "one" side.
Implementation: The "many" table (employees) includes a foreign key that references the primary key in the "one" table (department).
many to many relationship
Definition: In a many-to-many relationship, multiple records in one table can be associated with multiple records in another table.
Example: A student can enroll in multiple courses, and each course can have multiple students enrolled in it.
Implementation: This type of relationship requires a junction table (also called a bridge or join table) that holds foreign keys referencing the primary keys of the two tables involved. For example, a student_courses table might store references to both student_id and course_id to track which students are in which courses.
NOTE: the difference between a one-to-many relationship and a many-to-one is strictly perspective. They are actually the same.

Real World Application

A real-world example of multiplicity can be seen in the relationship between entities within a school. Let's consider a simplified scenario where we have the different entities: School, Student, Course, Student ID Card. The different types of relationships are:

One School to Many Students:

A school can have many students enrolled in it, but each student is associated with only one school.
Many Courses to Many Students:

Many courses can have many students enrolled in them, and each student can be enrolled in many courses.
One Student ID Card to One Student:

Each student has exactly one Student ID card, and each ID card can only be associated with one student.
Implementation
One to Many Relationship
A good example of a one to many relationship could be an albums and songs table. 1 album can have many songs however 1 song cannot be on many albums (excluding "Greatest Hits" albums). This is a one-to-many relationship. As stated in the previous section, to implement a one-to-many relationship we need to have a foreign key in the "many" table that references the "one" table. In our case, the many is the songs table and the one is the albums table.

albums table

album_id	album_name
1	'Thriller'
2	'Abbey Road'
songs table

song_id	song_name	album_fk
1	'Billie Jean'	1
2	'Beat it'	1
3	'Come Together'	2
Many to One Relationship
The only difference between a one-to-many relationship and a many-to-one relationship is perspective. People generally refer to a relationship as a many-to-one because they are thinking in relation to the "many" table. However, many-to-one and One-to-many are actually the same thing. Please refer to the one-to-many example on how to implement this.

Many to Many Relationship
One example of a many-to-many relationship could be doctors table and a patients table. 1 doctor can have many patients and 1 patient can have many doctors. Since the relationship goes both ways, this is a many-to-many relationship. We achieve a many-to-many relationship utilizing a third table called a junction table (as shown below).

doctors table

doctor_id	doctor_name
1	'Dr. Smith'
2	'Dr. Adams'
3	'Dr. Keen'
patients table

patient_id	patient_name
1	'Abbey Jones'
2	'Jane Stevens'
3	'Devin Kennedy'
doctors_patients table (this is the junction table)

doctor_id_fk	patient_id_fk
1	1
1	2
3	2
2	1
2	2
One to One Relationship
For a one-to-one relationship, let's have a table for students as well as backpacks. A student posesses one backpack and alternatively a backpack only belongs to one student. Let's see how these tables would look like below.

students table

student_id	student_name
1	'Mike Jones'
2	'Steven Nguyen'
3	'Jane Jackson'
backpacks table

backpack_id	book_count	student_id_fk
1	3	2
2	3	3
3	3	1
The example above looks very similar to a one-to-many relationship. This is because it is! The only difference in the implementation of a one-to-one relationship and a one-to-many relationship is that in a one-to-one relationship, a UNIQUE constraint needs to be added on the foreign key.

Summary

Multiplicity defines the relationship between two tables
There are 4 different multiplicity relationships


## Data Modeling And ERD
Learning Objectives

After completing this module, associates should be able to:

Define Data Modeling
Interpret an Entity Relationship Diagram (ERD)
Generate an ERD
Description
Data Modeling
Data modeling is a conceptual representation of the data objects, data object relationships, and data object rules. It is used to help save costs, improve documentation, reduce time to market, and establish consistent data.

Data modeling is not strictly limited to the relational databases, but can include a variety of data models such as:

- JSON Data Model
- Key-Value Data Model
- Graph Data Model
- Wide-Column Data Model
These examples of data models are good to know of and feel free to research on them, but for the purpose of the SQL unit, we focus on the relational data model.

Entity Relationship Diagram (ERD)
Entity Relationship Diagram is a graphical representation depicting the relationships among data objects within a database. These entities, such as people, objects or concepts relate to one another within the database. Also known as ER Models, these define a set of symbols such as rectangles, diamonds and connecting lines to depict the entities relationship and their attributes. Seen in the example below:



Each table is titled with a name representing the entity. At the top of each table there is a key symbol depicting our primary key, red diamonds for foreign keys and all others denoting the remaining entity attributes.

Real World Application
Data Modeling
Data modeling is used in practice for evaluating the data objects and their relationships to other entities used to investigate the business' data requirements for processes.

For example, let's think about a driver's license. We can use data modeling to determine if we have enough available combinations for our unique identifiers. When generating ID identifiers, we need to make sure the data type and constraints we've established are enough to handle our needs, accounting for the growth of the population. Prior to implementation, we can establish the rules for our data, saving us time, money and headache in the future.

Entity Relationship Diagram
Entity Relationship diagrams allows us to visualize the entities, their attributes and relationships to one another. This can be done via third party software such as Microsoft Visio or LucidChart, but inside of most database IDEs is some form of basic ERD generation tool. As you have seen previously, the image below is the example of an ERD generated through an IDE (DBeaver):



We can see symbols at the end of relationship lines that link the entities. These symbols indicate how many instances of one entity are related to how many instances of another entity. In Crow's Foot notation, there are usually two symbols beside one another. For the first symbol, it can be a crows foot which indicates many instances, or it can be a single line which represents a single instance. For the second symbol, it can be either a single line to indiciate it is mandatory or a circle which represents that it is optional (there can be zero instances). Note that other notations use different symbols.

Using the above ERD for a commerce database, you can see how stores, customer, department and sales information is all intertwined.

Implementation

Data Modeling and ERDs can be generated through third party softwares such as Microsoft Visio, LucidChart and a variety of other web based diagram tools. For our purpose we can simply make ERDs with our database IDE (DBeaver). While in DBeaver to generate an ERD of your database simply follow these steps:

Select your database and expand in the Database Navigator Menu on left-hand side.
Expand your Databases inside of your database
Expand the name of your database
Expand the Schemas directory
Right-click on the schema you wish to generate an ERD, select View Diagram.
DBeaver, will open a new tab in the editor with an extremely basic version of an ERD for your tables. See below:


Summary

Data modeling is a process for developing the data model for storing information within a database, used in practice for evaluating the data objects and their relationships to other entities used to investigate the business' data requirements for processes

Relation Data Model is only one of a few different types, but the one we focus on for SQL
Entity Relationship Diagram (ERD) is a graphical representation depicting the relationships among data objects within a database for entities with relationships such as people, objects or concepts.

ERD can be generated with third party applications
Simple ERD can be generated through your database IDE


## Cross Join
Learning Objectives

After completing this module, associates should be able to:

Describe what a CROSS JOIN is
Write commands that make use of CROSS JOIN
Description

CROSS JOIN, commonly known as a CARTESIAN JOIN, returns all possible row combinations from each table.
If no condition is provided, the result set is obtained by multiplying each row of table1 with all rows in table2.
cross

Syntax:
SELECT *
FROM Table1
CROSS JOIN Table2;

Real World Application

CROSS JOIN is used when we need to find all the possibilities of combining multiple records where the result set includes every row from each contributing table.
Creating Test Data - CROSS JOIN can be used to generate a Cartesian product, providing a comprehensive dataset for testing.
Finding Missing Relationships - CROSS JOIN can be employed to identify missing relationships or gaps in data.
Implementation

Let's consider a real-world example involving an online store. Below, we have the two tables Customers and Products:

Table: Customers

customerid	f_name	l_name
1	Alice	May
2	Bob	Parker
3	Charlie	Chocolate
Table: Products

productid	productname	price
101	Laptop	999.99
102	Smartphone	499.99
103	Headphones	79.99
Let's generate all possible combinations of customers and products using a cross join.

-- Cross join to generate all combinations of customers and products
SELECT Customers.CustomerID, Customers.F_Name, Customers.L_Name, 
Products.ProductID, Products.ProductName, Products.Price AS ProductPrice
FROM Customers
CROSS JOIN Products;
In this example, the Customers table represents the store's customers, and the Products table represents the available products with their prices. The cross join generates all possible combinations of customers and products, allowing you to see every customer paired with every product.

OUTPUT:

customerid	f_name	l_name	productid	productname	productprice
1	Alice	May	101	Laptop	999.99
2	Bob	Parker	101	Laptop	999.99
3	Charlie	Chocolate	101	Laptop	999.99
1	Alice	May	102	Smartphone	499.99
2	Bob	Parker	102	Smartphone	499.99
3	Charlie	Chocolate	102	Smartphone	499.99
1	Alice	May	103	Headphones	79.99
2	Bob	Parker	103	Headphones	79.99
3	Charlie	Chocolate	103	Headphones	79.99
This information can be useful because now we can analyze potential sales opportunities, understand customer preferences, or generate comprehensive reports for marketing strategies.

Summary

CROSS JOIN can potentially result in a very large output.
In a CROSS JOIN, there is no ON clause specifying a matching condition because it explicitly generates all possible combinations of rows from the involved tables.


## Self-join
Learning Objectives

After completing this module, associates should be able to:

Describe the use of SELF JOIN in a database
Make use of SELF JOIN in SQL queries
Description

SELF JOIN is an SQL statement which is used to intersect or join a table in the database to itself. We use it when we need to compare data within the same table.
This type of joining of data with other data in the same table when a condition is matched is also known as something called a Unary relationship.
self

Syntax:
SELECT column_name(s)
FROM table1 T1, table1 T2
WHERE condition;

Real World Application

Real world applications include:

To query the hierarchical data and comparing records of official reports.
The main application of this join in MySQL is when we have a table reference data rows in itself.
Implementation

In the following example our database will have one table since a SELF JOIN is a type of join wherein the data is coming from a single table.

Table: Employees

employee_id	employee_name	manager_id
1	CEO	
2	Marketing Manager	1
3	Sales Manager	1
4	Marketing Specialist	2
5	Sales Representative	3
6	Marketing Intern	2
Using the records from this table, we would like to retrieve our employees and readable information for each employee’s associated manager in the company.

A SELF JOIN query will accomplish this task perfectly:

SELECT e.employee_name AS employee, m.employee_name AS manager  
FROM employees e  
JOIN employees m ON e.manager_id = m.employee_id  
ORDER BY e.employee_id;
Note that we can also executing the same logic but utilizing a query without the JOIN keyword:

SELECT e.employee_name AS employee, m.employee_name AS manager  
FROM employees e, employees m  
WHERE e.manager_id = m.employee_id  
ORDER BY e.employee_id;  
OUTPUT:

employee	manager
Marketing Manager	CEO
Sales Manager	CEO
Marketing Specialist	Marketing Manager
Sales Representative	Sales Manager
Marketing Intern	Marketing Manager
Both examples utilize a SELF JOIN. An important note however is that the CEO record is not returned from the query due to the NULL foreign key reference and the behavior of a simple SELF JOIN or a where clause. If we require the CEO data in the result set, we can re-structure our SELF JOIN.

For instance, if we perform a LEFT JOIN we can pull data with null references. As a note and to clarify any confusion this is still a SELF JOIN since we are performing a join referencing a single table.

SELECT e.employee_name AS employee, m.employee_name AS manager
FROM employees e
LEFT JOIN employees m 
ON e.manager_id = m.employee_id
ORDER BY e.employee_id;
employee	manager
CEO	NULL
Marketing Manager	CEO
Sales Manager	CEO
Marketing Specialist	Marketing Manager
Sales Representative	Sales Manager
Marketing Intern	Marketing Manager
To accomplish the same output without the use of the JOIN keyword, we’ll need a more complex version of this query:

-- Without JOIN
SELECT e.employee_name AS employee,
( SELECT employee_name
  FROM employees m
  WHERE m.employee_id = e.manager_id) AS manager
FROM employees e
ORDER BY e.employee_id;
employee	manager
CEO	NULL
Marketing Manager	CEO
Sales Manager	CEO
Marketing Specialist	Marketing Manager
Sales Representative	Sales Manager
Marketing Intern	Marketing Manager
Each example demonstrates how we can use a single table and query data on itself.

Summary

SELF JOIN is important to query ordered records by comparing rows in the same table.
We can make use of SELF JOIN considering the table as two using an alias so that the table name is not repeated two times which may lead to an error.
This type of join is useful for modeling an organized structured data form which also allows comparing the rows and produce the required output.


##  Set-operators
Learning Objectives

After completing this module, associates should be able to:

Describe the use of set operators in a database
Different types of set operators
Description

SET Operators are specific type of operators which are used to combine the result of two queries.

Operators covered under SET operators are:

UNION
UNION ALL
INTERSECT
MINUS / EXCEPT
Two important rules to perform SET operations are:

The order and number of columns must be same.
Data types must be compatible.
Real World Application

Set operators are commonly used by developers to perform operations on sets of data. In the context of databases and SQL (Structured Query Language), set operators are particularly useful. Here are some reasons why developers use set operators:

Data Retrieval and Filtering:
Union (UNION): Developers use the UNION operator to combine the results of two or more SELECT statements, eliminating duplicate rows. This is useful when working with datasets that have similar structures or when aggregating data from multiple sources.
Intersect (INTERSECT): The INTERSECT operator is used to retrieve common rows from two SELECT statements. It helps identify the shared elements between two datasets, which can be beneficial for various analysis tasks.
Except (EXCEPT or MINUS): The EXCEPT operator (MINUS in some databases) is used to retrieve rows from the first SELECT statement that are not present in the second SELECT statement. This is helpful for finding the difference between two datasets.
Data Cleaning and De-duplication:
Set operators, especially UNION, are valuable for cleaning and deduplicating data. By combining datasets and removing duplicates, developers can ensure data integrity and accuracy.
Complex Query Construction:
Set operators enable developers to construct complex queries by combining multiple sets of data. This is useful for generating reports, analytics, and insights from diverse sources.
Optimizing Queries:
Set operators can be used to optimize queries by breaking down complex problems into simpler, modular components. This can make queries more readable, maintainable, and efficient.
Logical Operations:
Set operators mimic logical operations on sets, making it easier for developers to express complex conditions. For example, UNION corresponds to a logical OR operation, INTERSECT to a logical AND operation, and EXCEPT to a logical NOT operation.
Data Analysis and Reporting:
In scenarios where developers need to analyze data from different perspectives or generate reports that involve merging or comparing datasets, set operators provide a convenient way to accomplish these tasks.
Database Query Optimization:
Set operators can be leveraged to optimize database queries, allowing developers to retrieve the required information with minimal resource usage and improved performance.
Set operators, particularly in databases, help developers manipulate and analyze sets of data efficiently, enabling them to perform tasks such as data retrieval, filtering, cleaning, and complex query construction.

Implementation

This example will use two tables, businesses and sales:

Table 1: businesses

business_id	business_name
1	Fashion Excess Cloths
2	High Fashion Shoes
Table 2: sales

sale_id	business_id	product_id	quantity	total_amount
1	1	101	3	300.00
2	1	102	2	200.00
3	2	101	5	500.00
4	2	103	4	400.00
##UNION

The SQL UNION operator merges the result sets of multiple SELECT statements into a single result set, removing duplicates. Ensure that the number and data types of columns in the SELECT statements match to avoid errors; for instance, a SELECT with 2 columns cannot be combined with another having 3 columns.

-- UNION: Get a combined list of all unique products sold across businesses
SELECT product_id
FROM (
    SELECT product_id FROM sales
    WHERE business_id IN (SELECT business_id FROM businesses)
    UNION
    SELECT product_id FROM sales
) AS union_result;
A list of unique product_id values that have been sold across all businesses. Duplicates are automatically removed by UNION .

The subquery before UNION selects the products with the product_id of 101, 102, and 103. The result of UNION will remove duplicates, so the output will be: 101, 102, 103.

OUTPUT:

product_id
102
101
103
If you wanted to include duplicate values you could use UNION ALL.

-- UNION ALL: Get a combined list of all products sold across businesses (including duplicates)
SELECT product_id
FROM (
    SELECT product_id FROM sales
    WHERE business_id IN (SELECT business_id FROM businesses)
    UNION ALL
    SELECT product_id FROM sales
) AS union_all_result;
UNION ALL differs from UNION in that it does not remove duplicate rows; it includes all rows from the combined result sets.

The subquery before UNION ALL selects products with the product_id of 101, 102, and 103.

OUTPUT:

product_id
101
102
101
103
101
102
101
103
##INTERSECT

The INTERSECT operator in SQL is used to retrieve the common rows that appear in the result sets of two SELECT statements. It returns only the distinct rows that exist in both result sets.

NOTE: MySQL does not support the INTERSECT clause. Achieve similar functionality using INNER JOIN or IN clauses. Always refer to the database documentation to ensure the correct usage of these clauses.


-- INTERSECT: Get a list of products sold in common across all businesses
SELECT product_id
FROM (
    SELECT product_id FROM sales
    WHERE business_id = 1
    INTERSECT
    SELECT product_id FROM sales
    WHERE business_id = 2
) AS intersect_result;
The subqueries selects the products with the product_id of 101. The result of INTERSECT will include product_id 101, which is common to both businesses.

OUTPUT:

product_id
101
##EXCEPT

The EXCEPT operator in SQL is used to retrieve the rows that are present in the result set of the first SELECT statement but not in the result set of the second SELECT statement. It returns the distinct rows from the first result set that do not appear in the second result set.

-- EXCEPT: Get a list of products sold in the first business but not in the second business
SELECT product_id
FROM (
    SELECT product_id FROM sales
    WHERE business_id = 1
    EXCEPT
    SELECT product_id FROM sales
    WHERE business_id = 2
) AS except_result;
The first subquery selects product_id 101 and 102. While the second subquery selects product_id 101.The result of EXCEPT will exclude product_id 102, which is sold in the first business but not in the second.

OUTPUT:

product_id
102
Note that the MINUS clause, found in some databases, is functionally equivalent to the EXCEPT clause. For example, in PostgreSQL or Oracle, you can replace EXCEPT with MINUS in your queries. Always refer to the database documentation to ensure the correct usage of these clauses.

Summary

The UNION command is used to combine the results of two or more SELECT queries into a single result set that includes all the rows from the individual SELECT queries.
UNION ALL command will return duplicate values.
The number of columns and their data types in the SELECT statements must be the same for a UNION operation to work. This ensures that corresponding columns in the combined result set align properly.
The EXCEPT operator in SQL is used to retrieve the rows that are present in the result set of the first SELECT statement but not in the result set of the second SELECT statement.
The INTERSECT operator in SQL is used to retrieve the common rows that appear in the result sets of two SELECT statements. It returns only the distinct rows that exist in both result sets.


## Views
Learning Objectives

After completing this module, associates should be able to:

Understand the purpose of a view in SQL
Create a view in SQL
Description

In SQL (Structured Query Language), a view is a virtual table based on the result of a SELECT query. Unlike a physical table, a view does not store the data itself but rather provides a way to represent the result of a query as if it were a table. Views are created for various reasons, such as simplifying complex queries, encapsulating business logic, and providing a layer of security by restricting access to certain columns or rows.

Here are key points about views in SQL:

Definition: A view is defined by a SELECT statement that specifies the columns and rows to include in the view.
Virtual Table: A view is not a physical table; it's a virtual table that is dynamically generated when queried.
Simplified Queries: Views can simplify complex queries by encapsulating the logic in a named structure. This can make it easier to understand and maintain code.
Security: Views can be used to restrict access to specific columns or rows of a table. Users can be granted permission to access a view without granting direct access to the underlying table.
Abstraction: Views provide a level of abstraction, allowing users to interact with the data in a way that is more meaningful or convenient for their needs.
Joining Tables: Views can be used to join multiple tables, making it easier to work with complex data relationships.
Updateable Views: In some databases, views can be made updateable, allowing users to perform INSERT, UPDATE, and DELETE operations through the view. However, this depends on the complexity of the view and the underlying tables.
Real World Application

The decision to use views is often driven by factors such as simplifying complex queries, enhancing security, providing a layer of abstraction, and improving overall system performance. Here are some common reasons why developers use views:

Simplifying Complex Queries: Views can encapsulate complex SQL logic, making it easier for developers to write and maintain queries. This is particularly useful when dealing with multiple tables, joins, and filtering conditions.
Abstraction and Modularity: Views provide a level of abstraction, allowing developers to work with a simplified representation of the data. This abstraction promotes modularity and code organization, making it easier to understand and maintain the database schema.
Security and Access Control: Views can be used to control access to sensitive data. By granting users access to specific views rather than direct access to underlying tables, developers can restrict which columns or rows users are allowed to see. This enhances security and ensures that users only have access to the information they need.
Performance Optimization: Views can be used to precompute and store the results of complex queries. This can improve query performance by avoiding redundant calculations and aggregations. Additionally, views can be indexed, further enhancing query performance.
Data Aggregation and Transformation: Views are valuable for aggregating and transforming data. Developers can create views that present data in a format that is more suitable for reporting or analysis. This eliminates the need to repeat complex transformations in multiple queries.
Code Reusability: Views promote code reusability by allowing developers to define a set of common queries that can be used across different parts of an application. This reduces redundancy and ensures consistency in query logic.
Hiding Complexity: Views can hide the underlying complexity of the database schema from application developers. This is particularly useful when changes are made to the database structure but the views remain unchanged, preventing the need for extensive modifications in application code.
Joining Tables: Views simplify the process of joining multiple tables. Instead of writing complex join operations in every query, developers can create a view that encapsulates the necessary joins, making subsequent queries more straightforward.
Versioning and Migration: Views can be used as an abstraction layer during database versioning and migration. Developers can update the underlying tables without affecting the queries that rely on views, minimizing the impact on the application code.
Implementation

In this example we will work with two tables. The players table will focus on the player information and the games_played table, which stores data about each game played and has a foreign key (player_id) referencing the players table to establish a relationship between players and the games they've played.

Table 1: players

player_id	player_name	player_email
1	Tanisi	cantstopmenow@example.com
2	Zelda	mamamia@example.com
3	ImmaWinner	zeldafan@example.com
Table 2: games_played

game_id	player_id	game_title	game_score
101	1	The Legend of Zelda: Ocarina of Time	95
102	1	The Legend of Zelda: Breath of the Wild	98
103	2	The Legend of Zelda: Twilight Princess	92
104	2	The Legend of Zelda: Skyward Sword	88
105	3	The Legend of Zelda: Wind Waker	90
106	3	The Legend of Zelda: A Link to the Past	94
Let's consider a scenario where you were tasked with retrieving detailed information about each player along with the games they have played. Assuming we want a view that includes detailed information about each player and the games they have played, we can create a view named player_games_view:

Syntax for Creating a View:

CREATE VIEW view_name AS
SELECT column1, column2, ...
FROM table_name
WHERE condition;
Creating a view named player_games_view:

-- Create a view to show detailed information about each player and the games they played
CREATE VIEW player_games_view
AS
SELECT p.player_id, p.player_name, p.player_email, g.game_id,g.game_title, g.game_score
FROM players p
JOIN games_played g 
ON p.player_id = g.player_id;
This view encapsulates the logic of joining the players and games_played tables, providing a simplified way to retrieve detailed information about each player and the games they have played.

To see the view we can do a basic SELECT all statement

SELECT * FROM player_games_view;
OUTPUT:

player_id	player_name	player_email	game_id	game_title	game_score
1	Tanisi	cantstopmenow@example.com	101	The Legend of Zelda: Ocarina of Time	95
1	Tanisi	cantstopmenow@example.com	102	The Legend of Zelda: Breath of the Wild	98
2	Zelda	mamamia@example.com	103	The Legend of Zelda: Twilight Princess	92
2	Zelda	mamamia@example.com	104	The Legend of Zelda: Skyward Sword	88
3	ImmaWinner	zeldafan@example.com	105	The Legend of Zelda: Wind Waker	90
3	ImmaWinner	zeldafan@example.com	106	The Legend of Zelda: A Link to the Past	94
Summary

A VIEW does not store data itself but provides a way to represent the result of a query as if it were a table.
VIEW promotes code reusability by allowing developers to define common queries that can be used across different parts of an application.
Views can restrict access to specific columns or rows, enhancing security. Users can be granted permission to access a view without direct access to underlying tables.
Views simplify complex queries by encapsulating logic, promoting modularity, and providing a level of abstraction. They help developers work with a more manageable representation of the data.


## Indexes
Learning Objectives

After completing this module, associates should be able to:

understand the purpose of indexes in SQL
Description

In SQL (Structured Query Language), an index is a database object that provides a fast and efficient way to look up and retrieve data from a table. It works similarly to an index or bookmark in a book, helping you quickly find information without having to scan the entire content. The primary purpose of an index is to improve the performance of SELECT queries by reducing the amount of data that needs to be scanned.

Here are some key points about indexes in SQL:

Structure: An index is typically a data structure that stores a sorted or hashed subset of the data in a table. It consists of key columns and corresponding pointers to the actual data rows in the table.
Types of Indexes:
Clustered Index: The rows of the table are stored in the order of the index key. Each table can have only one clustered index.
Non-Clustered Index: The index key contains a sorted order of the data, but the actual data rows are stored separately from the index.
Advantages:
Improved Query Performance: Indexes significantly speed up the retrieval of data for SELECT queries, especially when searching or sorting based on indexed columns.
Faster Joins: Indexes can enhance the performance of join operations between tables.
Unique Constraints: Unique indexes enforce the uniqueness of values in one or more columns.
Disadvantages:
Overhead on Write Operations: While SELECT queries benefit from indexes, write operations (INSERT, UPDATE, DELETE) can be slower because indexes need to be updated along with the data.
Storage Overhead: Indexes consume additional storage space.
Creating and Managing Indexes:
Indexes are created using the CREATE INDEX statement.
They can be dropped using the DROP INDEX statement.
Some databases automatically create indexes for primary keys and unique constraints.
Real World Application

Developers use indexes for several reasons to improve the performance and efficiency of database operations.

Faster Data Retrieval: Indexes allow for faster retrieval of data from a database. When a query filters or sorts data based on indexed columns, the database engine can quickly locate the relevant rows without scanning the entire table.
Optimized Query Performance: Queries that involve conditions, sorting, or joining on indexed columns generally perform better. Indexes help reduce the number of rows that need to be examined, resulting in quicker query execution times.
Enhanced JOIN Operations: Indexes improve the performance of JOIN operations, especially when joining tables on columns that are indexed. This is crucial for scenarios where data from multiple tables needs to be combined.
Unique Constraints: Indexes can enforce the uniqueness of values in one or more columns, ensuring that no duplicate values exist. This is beneficial for maintaining data integrity, especially in columns representing primary keys or unique constraints.
Accelerated Aggregations: Queries that involve aggregate functions (e.g., SUM, AVG, COUNT) on indexed columns can be processed more efficiently, leading to faster results.
Increased Concurrency: Indexes can enhance the concurrency of database operations by reducing the time it takes to read and write data. This is particularly important in applications with a high level of simultaneous database access.
Optimized ORDER BY and GROUP BY: Indexes can significantly improve the performance of queries involving sorting (ORDER BY) and grouping (GROUP BY). The database engine can leverage the sorted order provided by indexes.
Efficient WHERE Clauses: Queries with filtering conditions (WHERE clauses) on indexed columns benefit from quicker data retrieval. The database engine can quickly identify the relevant rows that satisfy the conditions.
Improved Performance for Joins: When joining multiple tables, indexes on the join columns can significantly reduce the time it takes to match and retrieve the related rows.
Minimized Disk I/O: Indexes reduce the amount of disk I/O by allowing the database engine to locate data more efficiently. This is crucial for applications where minimizing disk reads and writes is essential for performance.
It's important to note that while indexes provide performance benefits, they also come with some trade-offs, such as increased storage requirements and potential overhead on write operations. Therefore, developers should carefully consider the specific requirements of their applications and choose indexes judiciously based on the types of queries are commonly executed.

Implementation

In this example we have a Users table and an Accounts table.

Table 1: Users

userid	username	email
1	LolipopMagee	Sarah.Magee@example.com
2	ToBeOrNotToBe	harry.smith@example.com
3	JingleTrees	bob.johnson@example.com
Table 2: Accounts

accountid	userid	accountnumber	balance
101	1	A123456	1000.00
102	2	B789012	2500.50
103	3	C345678	500.25
Let's create a clustered index on the Accounts table based on the accountid column, and a non-clustered index on the Users table based on the userid column:

-- Create a Non-Clustered Index on Users table
CREATE INDEX idx_UserID ON Users (userid);

-- Create a Non-Clustered Index on Accounts table
CREATE INDEX idx_AccountID ON Accounts (accountid);

-- Cluster the Accounts table based on the idx_AccountID index
CLUSTER Accounts USING idx_AccountID;
The first statement will create a non-clustered index named idx_UserID on the Users table.
The second statement will create a non-clustered index named idx_AccountID on the Accounts table.
The third statement will use the CLUSTER command to physically reorder the Accounts table based on the idx_AccountID index.
Please note that you run these statements in order due to the index having to exist before you can cluster, and the index idx_AccountID is created successfully before attempting to cluster the table. In PostgreSQL, the CLUSTER command requires an existing index to be specified, and the index must be created before attempting to cluster the table. The order of execution is crucial. Always refer to the specific documentation of the database system being used, as the syntax and behavior of commands like CLUSTER can vary between database management systems.

If you clustered a table using the CLUSTER command, you can verify the clustering effect by checking the physical order of the table. This can be done using a simple SELECT statement without any ordering.

Example:

SELECT * FROM Accounts;
If the Accounts table was clustered based on the specified index, you may notice that the rows are physically ordered according to the clustered index.

Always ensure to review the actual execution plans, results, and any related metrics to validate that your queries are performing as expected. It's a good practice to test queries with different data scenarios to ensure robustness.

Things to take into consideration when creating indexes include:

Consideration of Query Patterns:
When creating indexes, it's essential to consider the specific columns that will be used in SQL queries. Indexes should be designed to optimize the performance of frequently executed queries.
Identify columns used in WHERE clauses, JOIN conditions, and ORDER BY clauses, as these are common candidates for indexing.
Importance of Indexes on Large Tables:
Indexes play a crucial role in optimizing query performance, especially on large tables. They help reduce the amount of data that needs to be scanned, improving the speed of data retrieval.
For large tables, well-designed indexes can significantly enhance the efficiency of queries by allowing the database engine to quickly locate and retrieve relevant rows.
Consideration for Small Tables:
On small tables, the overhead of maintaining indexes may outweigh the performance benefits. In some cases, a full table scan (reading sequentially) might be faster than using an index, especially when most or all rows are accessed.
Developers should carefully assess the size and usage patterns of tables before deciding to create indexes.
Sequential Reading vs. Index Access:
When a query needs to access a large portion of the rows in a table, a sequential read might be faster than working through an index. In such cases, the database engine may choose to perform a full table scan instead of using an index.
Balancing Write Performance and Read Performance:
It's crucial to strike a balance between read and write performance. While indexes improve read performance, they may introduce overhead during write operations (INSERT, UPDATE, DELETE). This is an important trade-off to consider.
Regular Monitoring and Optimization:
Database performance should be regularly monitored, and index strategies should be optimized based on changing usage patterns. Periodic review and adjustment of indexes can help maintain optimal performance.
Creating indexes requires thoughtful consideration of the specific queries your application executes and the characteristics of your data. Regular performance monitoring and adjustments to index strategies are essential for ensuring efficient database operations over time.

Summary

Query Optimization: Indexes are crucial for optimizing query performance, particularly for queries that involve filtering, sorting, or joining on specific columns.
Column Selection: Choose columns for indexing based on their usage in WHERE clauses, JOIN conditions, and ORDER BY clauses in frequent queries.
Large Table Performance: Indexes play a critical role in enhancing the performance of queries on large tables by reducing the amount of data that needs to be scanned.
Trade-Offs on Small Tables: On small tables, the overhead of maintaining indexes may outweigh performance benefits. Carefully evaluate the size and usage patterns of tables before creating indexes.
Balancing Read and Write Performance: Striking a balance between read and write performance is essential. Indexes improve read performance but may introduce overhead during write operations.
Regular Monitoring and Optimization: Periodically monitor and optimize index strategies based on changing usage patterns. Regular reviews help maintain optimal database performance.
Sequential Reading vs. Index Access: Consider the trade-off between sequential reading and index access, especially when a query needs to access a large portion of rows in a table.
Consideration for WHERE, JOIN, ORDER BY: Index columns commonly used in WHERE clauses, JOIN conditions, and ORDER BY clauses for effective optimization.
Unique Constraints: Indexes are useful for enforcing unique constraints on columns, ensuring data integrity.
Use with Caution on Write-Intensive Tables: Be cautious with indexes on tables with heavy write operations, as they may introduce additional overhead.


## Introduction To Http
Learning Objectives

At the end of this module, the learner should be able to do the following:

Define HTTP
Describe how HTTP functions
Understand what a resource is
Understand the anatomy of an HTTP request and response
Understand why to use HTTP
Description
What is HTTP?
HTTP stands for HyperText Transfer Protocol. If you don't know what Hypertext refers to or what a Protocol is, this definition isn't particularly helpful. Let's give a definition that is a bit easier to understand:

HTTP
A technique of transmitting data in a particular format, primarily between a server and a browser. It uses an architecture in which a client makes a connection to a server, makes a request, and waits for the response.

The data in the "particular format" mentioned is the hypertext stated in the name, with hypertext simply referring to text documents that have the special ability to link to one another. There are many kinds of hypertext documents, and ones that have the ability to show multiple kinds of media are called hypermedia documents. HTTP is capable of transmitting hypermedia documents in addition to hypertext documents.

An example of one of these hypertext documents are HTML documents, the HyperText Markup Language documents.

Another concept we have been throwing around is the server and the client, so lets talk about what we mean when we talk about these:

Server
A server is a computer or collection of computers that is the destination of the HTTP call. Commonly, this will be a powerful collection of computers that have an increased level of computation power and data storage compared to the client.

Client
A client is a computer or collection of computers that is the creator of the HTTP call. Commonly, this will be a single, personal computer.

Note that while server and client typically refer to what was mentioned, that is not what they always are. For example, you can have a client with vastly more computational power than the server, and you can have clients that are normally servers themselves.

HTTP is a protocol that is referred to as stateless, which means that once a request and response has completed, the server doesn't retain any data about the request. Care must be taken here about this definition, as it can be easily confused. Just because HTTP is stateless does NOT mean that HTTP methods cannot change data on the server, or that the server will not track HTTP calls made to it. Let's break this down in more detail:

A client makes a request to the server to calculate 5 + 5
The server takes in the data provided (5 and 5) in order to calculate
The server makes the calculation and responds with 10
The server stores in a database the user that made the request and a timestamp of when they made the call
The same client makes a request to the server, again to calculate 5 + 5
Because HTTP is stateless, the server does not still have 5 and 5 stored and must take them in again
The server makes the calculation and responds with 10
The server stores in a database the same information as before
Note what kind of information the server may store is up to the server, and may not even store any information whatsoever.

How does HTTP Work?
HTTP is broken down into a number of methods that define the behavior that the server should take when it receives the HTTP request. We'll go into more detail about the HTTP methods in a later topic. Even though each method will cause the server to respond in a different way, the general way they are structured is the same.

HTTP has two parts: the request and the response. The client creates the request, and the server creates the response. The request will hold the following information:

The method being used
A URL where the target is
What version of HTTP is being used
Optional information to help the server with the request (called headers)
For some methods, a body which contains some resource
Ex: Danni wants to upload a document to a server over the internet, so the body in this case would be the document that Danni is trying to upload.
The response will hold the following information:

What version of HTTP is being used
A status code reflecting the outcome of the request
A status message which is shorthand and less descriptive than the status code
Optional information to detail what happened with the request (called headers again)
For some methods, a body which contains some resource
Ex: This time, Danni wants to download a document from a server. In order to get that document, the browser receives it over HTTP. The body of that response is the document Danni is trying to download.
HTTP (in addition to other things) can tell the server to make changes or store some resource, and can also retrieve resources from the server. We keep using this word "resource", so let's shed some light as to what we are talking about when we say "resource". In this context, a resource is simply data. It could be a value, it could be a photo, it could be a text document, or it can commonly be a JSON. Of course, a resource isn't limited to these things, anything that is data is fair game to be a resource.

Why HTTP?
Let's take another look at the final word in the acronym of HTTP: protocol. HTTP is not the only protocol out there, but it is the most common. An example of another protocol would be SOAP, which stands for Simple Object Access Protocol. Now that we have established that there are other choices to make when it comes to the protocol we want to use to transmit data across the internet, why choose HTTP?

As mentioned earlier, HTTP is the most common. That in itself is a strong reason to use it, since everyone else is using it they have already made servers that respond to HTTP requests instead of say SOAP requests. Of course this begs the question as to why HTTP is the most common.

HTTP is very easy to use. Most programming languages have an API (Application Programming Interface) that allow us to very easily make HTTP requests and parse through the results. All we need to do is build out the request with a method, endpoint, headers and a body and the API handles the rest, in comparison to something like SOAP where there are a lot more steps that we would have to do on the programming side in order to make even a simple request, with more complicated requests being quite cumbersome to create.

When we talked about a resource, we took for granted that this resource could essentially be anything. However, this is not the case for every protocol. HTTP has the ability to transmit just about any kind of data over the internet, which is another strong reason for its use. In particular, it is able to transmit the extremely popular JSON data format.

The final reason to use HTTP is that it is extensible. This means that as long as a client and a server have the same idea of what a header means, you can change up the headers to change up how the server reacts and responds.

Real World Application
The PokeAPI
The easiest real-world application of HTTP is simply loading up a webpage on the internet, but that's not particularly interesting. Lets take a look at an example that you will be more involved with in your programming journey, using HTTP to communicate with webservices, REST APIs in particular.

Picture of a website that used HTTP to get information about the Pokemon Nidoking
REST stands for REpresentational State Transfer. At this point, we can ignore this part and just stick with the fact that a RESTful webservice uses HTTP in order to transmit data. There are a large number of these services out there, but the one we will use for our example is the PokeAPI, which contains information about the critters from the popular Pokemon franchise.

This information sits in a server somewhere, not on our computers. So we need to use HTTP in order to get the information about the Pokemon from the PokeAPI server and onto our computers and on the webpage. Using JavaScript, the browser makes a request to the PokeAPI server asking for a particular Pokemon that we input in the search bar. The server takes the request and responds with the requested Pokemon data.

Once the browser receives the response, it once again uses JavaScript to dig through the response and populate the webpage with the information we want to put on the screen. This becomes very handy, because now through the power of HTTP and the internet, it doesn't matter where in the world the client is; as long as the client can establish a connection to the server, they can get the information on it.

If you're excited to try this out for yourself, we've still got a long way to go before we can create something like this! At the very least for right now, follow the upcoming steps in order to be able to play around with the webpage pictured above. If your feeling particularly adventurous, all the code is there so you can dig through it and decipher how it works.

Implementation

This is an optional exercise where you can download and view a program that demonstrates the use of HTTP and REST APIs. Before starting, ensure that your computer is configured to open HTML files with a browser. In order to do this, follow these steps:

Press the Windows key and type in Choose default apps by file type
Scroll down to .html
Ensure that the default app is a web browser
Default App Screen

Once that is done, you are all set to follow the following steps.

Open up GitBash
You will be downloading some files from GitHub. If you want to download them somewhere in particular, navigate to that directory in your GitBash terminal.
Run the following commands:
git clone https://github.com/jnlongnecker/HTTP.git
cd HTTP/
start index.html
Your browser that you have configured to open .html files will now open the webpage. Play around with it by putting in different Pokemon names!
Summary

HTTP stands for HyperText Transfer Protocol and is used to send hypermedia over the internet
HTTP is easy to use, popular, and extensible
HTTP works by making a connection to a server, sending a request, and receiving a response
A request contains the method used, the endpoint, and optional headers and a body
A response contains the status code, status message, and optional headers and a body
A body contains a resource, which is just some data


## Http Verbs
Learning Objectives

At the end of this module, the learner should be able to do the following:

List the most commonly used HTTP verbs/methods.
Describe how to test an API using verbs/methods.
Description

HTTP Methods
HTTP defines a set of request methods to indicate the desired action to be performed for a given resource. Although they can also be nouns, these request methods are sometimes referred to as HTTP verbs.

Having a basic understanding of the different HTTP methods, or verbs, an API supports is an helpful knowledge when exploring and testing APIs.

GET
GET requests are the most common and widely used methods in APIs and websites. Simply put, the GET method is used to retrieve data from a server at the specified resource. For example, say you have an API with a /users endpoint. Making a GET request to that endpoint should return a list of all available users.

Since a GET request is only requesting data and not modifying any resources, it's considered a safe and idempotent method.

POST
In web services, POST requests are used to send data to the API server to create or update a resource. The data sent to the server is stored in the request body of the HTTP request.

The simplest example is a contact form on a website. When you fill out the inputs in a form and hit Send, that data is put in the response body of the request and sent to the server. This may be JSON, XML, or query parameters (there's plenty of other formats, but these are the most common).

It's worth noting that a POST request is non-idempotent. It mutates data on the backend server (by creating or updating a resource), as opposed to a GET request which does not change any data.

PUT
Similar to POST, PUT requests are used to send data to the API to update or create a resource. The difference is that PUT requests are idempotent. That is, calling the same PUT request multiple times will always produce the same result. In contrast, calling a POST request repeatedly may have side effects when creating the same resource multiple times.

Generally, when a PUT request creates a resource the server will respond with a 201 (Created), and if the request modifies existing resource the server will return a 200 (OK) or 204 (No Content).

PATCH
A PATCH request is one of the lesser-known HTTP methods, but I'm including it this high in the list since it is similar to POST and PUT. The difference with PATCH is that you only apply partial modifications to the resource.

The difference between PATCH and PUT, is that a PATCH request is non-idempotent (like a POST request).

To expand on partial modification, say your API has a /users/{{userid}} endpoint, and a user has a username. With a PATCH request, you may only need to send the updated username in the request body - as opposed to POST and PUT which require the full user entity.

DELETE
The DELETE method is exactly as it sounds: delete the resource at the specified URL. This method is one of the more common in RESTful APIs so it's good to know how it works.

If a new user is created with a POST request to /users, and it can be retrieved with a GET request to /users/{{userid}}, then making a DELETE request to /users/{{userid}} will completely remove that user.

HEAD
The HEAD method is almost identical to GET, except without the response body. In other words, if GET /users returns a list of users, then HEAD /users will make the same request but won't get back the list of users.

HEAD requests are useful for checking what a GET request will return before actually making a GET request -- like before downloading a large file or response body. You can learn more about HEAD requests on MDN.

It's worth pointing out that not every endpoint that supports GET will support HEAD - it completely depends on the API you're testing.

OPTIONS
An OPTIONS request should return data describing what other methods and operations the server supports at the given URL.

OPTIONS requests are more loosely defined and used than the others, making them a good candidate to test for fatal API errors. If an API isn't expecting an OPTIONS request, it's good to put a test case in place that verifies failing behavior.

TRACE
The purpose of the TRACE request is to echo the received request back to the client, allowing the client to see what changes or additions have been made by intermediate servers. It is used primarily for diagnostic purposes and is not commonly used in regular web development.

Real World Application
Why do we need HTTP methods?
Through the HTTP protocol, resources are exchanged between client devices and servers over the internet. Client devices send requests to servers for the resources needed to load a web page; the servers send responses back to the client to fulfill the requests. Requests and responses share sub-documents -- such as data on images, text, text layouts, etc. -- which are pieced together by a client web browser to display the full web page file.

The resource exchanges described above take place through HTTP methods, also known as verbs. Knowledge of methods is essential to understanding HTTP.

Implementation

In this example, let's make different kinds of requests to a fake API, https://reqres.in/. We will use the cURL program, which is already available for Mac OS or Linux.

For Windows users, for your command-line interface, I recommend GitBash which is bundled within Git for Windows. If you are just using the command prompt, you would need to install cURL manually. Instructions are here: https://developer.zendesk.com/documentation/api-basics/getting-started/installing-and-using-curl/#windows-10-version-1803-or-later

GET Request
Use the following command in your CLI: curl https://reqres.in/api/users/2

The following should be the output (formatted here for readability):

{
    "data":
    {
        "id":2,
        "email":"janet.weaver@reqres.in",
        "first_name":"Janet",
        "last_name":"Weaver",
        "avatar":"https://reqres.in/img/faces/2-image.jpg"
    },
    "support":
    {
        "url":"https://reqres.in/#support-heading",
        "text":"To keep ReqRes free, contributions towards server costs are appreciated!"
    }
}
What we get back is the response body, or the data sent back by the response. We can learn more about the response by using the -i or --include option, which includes response header information. The following command:

curl -i https://reqres.in/api/users/2
Returns:

HTTP/1.1 200 OK
Date: Thu, 18 Apr 2024 14:01:59 GMT
Content-Type: application/json; charset=utf-8
Content-Length: 280
Connection: keep-alive
... more output ommitted

{
    "data":
    {
        "id":2,
        "email":"janet.weaver@reqres.in",
        "first_name":"Janet",
        "last_name":"Weaver",
        "avatar":"https://reqres.in/img/faces/2-image.jpg"
    },
    "support":
    {
        "url":"https://reqres.in/#support-heading",
        "text":"To keep ReqRes free, contributions towards server costs are appreciated!"
    }
}
This response returns information about the response itself along with the response body.

POST Request
Use the following command in your CLI: curl -X POST -d "{\"first_name\": \"Kendra\", \"last_name\": \"Jackson\"}" -H "Content-Type: application/json" https://reqres.in/api/users/2

We include the -X option. This allows us to specify an HTTP verb to use rather than the default GET verb. We chose POST as its argument. We also included the -d option which enables us to send information in the body of the request. The information we sent is {\"first_name\": \"Kendra\", \"last_name\": \"Jackson\"}. We escape the double quotes since we are surrounding the data with double quotes and we want to prevent any confusion. Single quotes may not work depending on your CLI. Lastly, we added the -H option with the argument Content-Type: application/json. This allows us to specify the type of data we are sending in the body. The following should be the output (formatted here for readability):


{
    "first_name":"Kendra",
    "last_name":"Jackson",
    "id":"607",
    "createdAt":"2024-04-18T14:54:41.044Z"
    }
We see that a new user was created and given an id of 607. If you are following along, the id may differ for you.

PUT Request
Use the following command in your CLI: curl -X PUT -d "{\"first_name\": \"Kendra\", \"last_name\": \"Jackson\"}" -H "Content-Type: application/json" https://reqres.in/api/users/2

The following should be the output (formatted here for readability):

{
    "first_name":"Kendra",
    "last_name":"Jackson",
    "updatedAt":"2024-04-18T14:50:00.789Z"
}
We receive back the information as confirmation it was updated successfully. Note that we're updating the information for a specific resource, rather than with POST where we add information to a collection of resources. When we use a PUT request, we replace all information at that resource with what we send.

DELETE Request
Use the following command in your CLI: curl -i -X DELETE https://reqres.in/api/users/2

We are using the -i flag because the API chose not to send any information in the response body for a DELETE request. The following should be the output:

HTTP/1.1 204 No Content
Date: Thu, 18 Apr 2024 14:31:19 GMT
Content-Length: 0
Connection: keep-alive
... more output ommitted
We see a success status code of 204.

Summary

Here is a list of HTTP verbs/methods discussed in this topic:

GET
POST
PUT
HEAD
DELETE
PATCH
OPTIONS
TRACE


## Http Status Codes
Learning Objectives

At the end of this module, the learner should be able to do the following:

List the most commonly used HTTP Status Codes
Describe the different classes of HTTP Status Codes
Description

HTTP Status Codes indicate whether a specific HTTP request has been successfully completed.

Responses are grouped in five classes:

Informational responses (100–199)
Successful responses (200–299)
Redirection messages (300–399)
Client error responses (400–499)
Server error responses (500–599)
The term "HTTP status code" is actually the common term for the HTTP status line that includes both the HTTP status code and the HTTP reason phrase.

For example, the HTTP status line 500: Internal Server Error is made up of the HTTP status code of 500 and the HTTP reason phrase of Internal Server Error.

Real World Application
How are HTTP Status Codes used?
HTTP status codes (also called browser/internet error codes) are standard response codes given by web servers on the internet. The codes help identify the cause of the problem when a web page or other resource doesn't load properly.

Implementation

The following is a list of HTTP Status Codes with corresponding descriptions:

Information responses
100 Continue

This interim response indicates that the client should continue the request or ignore the response if the request is already finished.

101 Switching Protocols

This code is sent in response to an Upgrade request header from the client and indicates the protocol the server is switching to.

102 Processing (WebDAV)

This code indicates that the server has received and is processing the request, but no response is available yet.

103 Early Hints

This status code is primarily intended to be used with the Link header, letting the user agent start preloading resources while the server prepares a response.

Successful responses
200 OK

The request succeeded. The result meaning of "success" depends on the HTTP method:

GET: The resource has been fetched and transmitted in the message body.
HEAD: The representation headers are included in the response without any message body.
PUT or POST: The resource describing the result of the action is transmitted in the message body.
TRACE: The message body contains the request message as received by the server.
201 Created

The request succeeded, and a new resource was created as a result. This is typically the response sent after POST requests, or some PUT requests.

202 Accepted

The request has been received but not yet acted upon. It is noncommittal, since there is no way in HTTP to later send an asynchronous response indicating the outcome of the request. It is intended for cases where another process or server handles the request, or for batch processing.

203 Non-Authoritative Information

This response code means the returned metadata is not exactly the same as is available from the origin server, but is collected from a local or a third-party copy. This is mostly used for mirrors or backups of another resource. Except for that specific case, the 200 OK response is preferred to this status.

204 No Content

There is no content to send for this request, but the headers may be useful. The user agent may update its cached headers for this resource with the new ones.

205 Reset Content

Tells the user agent to reset the document which sent this request.

206 Partial Content

This response code is used when the Range header is sent from the client to request only part of a resource.

207 Multi-Status (WebDAV)

Conveys information about multiple resources, for situations where multiple status codes might be appropriate.

208 Already Reported (WebDAV)

Used inside a dav:propstat response element to avoid repeatedly enumerating the internal members of multiple bindings to the same collection.

226 IM Used (HTTP Delta encoding)

The server has fulfilled a GET request for the resource, and the response is a representation of the result of one or more instance-manipulations applied to the current instance.

Redirection messages
300 Multiple Choices

The request has more than one possible response. The user agent or user should choose one of them. (There is no standardized way of choosing one of the responses, but HTML links to the possibilities are recommended so the user can pick.)

301 Moved Permanently

The URL of the requested resource has been changed permanently. The new URL is given in the response.

302 Found

This response code means that the URI of requested resource has been changed temporarily. Further changes in the URI might be made in the future. Therefore, this same URI should be used by the client in future requests.

303 See Other

The server sent this response to direct the client to get the requested resource at another URI with a GET request.

304 Not Modified

This is used for caching purposes. It tells the client that the response has not been modified, so the client can continue to use the same cached version of the response.

305 Use Proxy Deprecated

Defined in a previous version of the HTTP specification to indicate that a requested response must be accessed by a proxy. It has been deprecated due to security concerns regarding in-band configuration of a proxy.

306 unused

This response code is no longer used; it is just reserved. It was used in a previous version of the HTTP/1.1 specification.

307 Temporary Redirect

The server sends this response to direct the client to get the requested resource at another URI with same method that was used in the prior request. This has the same semantics as the 302 Found HTTP response code, with the exception that the user agent must not change the HTTP method used: if a POST was used in the first request, a POST must be used in the second request.

308 Permanent Redirect

This means that the resource is now permanently located at another URI, specified by the Location: HTTP Response header. This has the same semantics as the 301 Moved Permanently HTTP response code, with the exception that the user agent must not change the HTTP method used: if a POST was used in the first request, a POST must be used in the second request.

Client error responses
400 Bad Request

The server cannot or will not process the request due to something that is perceived to be a client error (e.g., malformed request syntax, invalid request message framing, or deceptive request routing).

401 Unauthorized

Although the HTTP standard specifies "unauthorized", semantically this response means "unauthenticated". That is, the client must authenticate itself to get the requested response.

402 Payment Required Experimental

This response code is reserved for future use. The initial aim for creating this code was using it for digital payment systems, however this status code is used very rarely and no standard convention exists.

403 Forbidden

The client does not have access rights to the content; that is, it is unauthorized, so the server is refusing to give the requested resource. Unlike 401 Unauthorized, the client's identity is known to the server.

404 Not Found

The server can not find the requested resource. In the browser, this means the URL is not recognized. In an API, this can also mean that the endpoint is valid but the resource itself does not exist. Servers may also send this response instead of 403 Forbidden to hide the existence of a resource from an unauthorized client. This response code is probably the most well known due to its frequent occurrence on the web.

405 Method Not Allowed

The request method is known by the server but is not supported by the target resource. For example, an API may not allow calling DELETE to remove a resource.

406 Not Acceptable

This response is sent when the web server, after performing server-driven content negotiation, doesn't find any content that conforms to the criteria given by the user agent.

407 Proxy Authentication Required

This is similar to 401 Unauthorized but authentication is needed to be done by a proxy.

408 Request Timeout

This response is sent on an idle connection by some servers, even without any previous request by the client. It means that the server would like to shut down this unused connection. This response is used much more since some browsers, like Chrome, Firefox 27+, or IE9, use HTTP pre-connection mechanisms to speed up surfing. Also note that some servers merely shut down the connection without sending this message.

409 Conflict

This response is sent when a request conflicts with the current state of the server.

410 Gone

This response is sent when the requested content has been permanently deleted from server, with no forwarding address. Clients are expected to remove their caches and links to the resource. The HTTP specification intends this status code to be used for "limited-time, promotional services". APIs should not feel compelled to indicate resources that have been deleted with this status code.

411 Length Required

Server rejected the request because the Content-Length header field is not defined and the server requires it.

412 Precondition Failed

The client has indicated preconditions in its headers which the server does not meet.

413 Payload Too Large

Request entity is larger than limits defined by server. The server might close the connection or return an Retry-After header field.

414 URI Too Long

The URI requested by the client is longer than the server is willing to interpret.

415 Unsupported Media Type

The media format of the requested data is not supported by the server, so the server is rejecting the request.

416 Range Not Satisfiable

The range specified by the Range header field in the request cannot be fulfilled. It's possible that the range is outside the size of the target URI's data.

417 Expectation Failed

This response code means the expectation indicated by the Expect request header field cannot be met by the server.

421 Misdirected Request

The request was directed at a server that is not able to produce a response. This can be sent by a server that is not configured to produce responses for the combination of scheme and authority that are included in the request URI.

422 Unprocessable Entity (WebDAV)

The request was well-formed but was unable to be followed due to semantic errors.

423 Locked (WebDAV)

The resource that is being accessed is locked.

424 Failed Dependency (WebDAV)

The request failed due to failure of a previous request.

425 Too Early Experimental

Indicates that the server is unwilling to risk processing a request that might be replayed.

426 Upgrade Required

The server refuses to perform the request using the current protocol but might be willing to do so after the client upgrades to a different protocol. The server sends an Upgrade header in a 426 response to indicate the required protocol(s).

428 Precondition Required

The origin server requires the request to be conditional. This response is intended to prevent the 'lost update' problem, where a client GETs a resource's state, modifies it and PUTs it back to the server, when meanwhile a third party has modified the state on the server, leading to a conflict.

429 Too Many Requests

The user has sent too many requests in a given amount of time ("rate limiting").

431 Request Header Fields Too Large

The server is unwilling to process the request because its header fields are too large. The request may be resubmitted after reducing the size of the request header fields.

451 Unavailable For Legal Reasons

The user agent requested a resource that cannot legally be provided, such as a web page censored by a government.

Server error responses
500 Internal Server Error

The server has encountered a situation it does not know how to handle.

501 Not Implemented

The request method is not supported by the server and cannot be handled. The only methods that servers are required to support (and therefore that must not return this code) are GET and HEAD.

502 Bad Gateway

This error response means that the server, while working as a gateway to get a response needed to handle the request, got an invalid response.

503 Service Unavailable

The server is not ready to handle the request. Common causes are a server that is down for maintenance or that is overloaded. Note that together with this response, a user-friendly page explaining the problem should be sent. This response should be used for temporary conditions and the Retry-After HTTP header should, if possible, contain the estimated time before the recovery of the service. The webmaster must also take care about the caching-related headers that are sent along with this response, as these temporary condition responses should usually not be cached.

504 Gateway Timeout

This error response is given when the server is acting as a gateway and cannot get a response in time.

505 HTTP Version Not Supported

The HTTP version used in the request is not supported by the server.

506 Variant Also Negotiates

The server has an internal configuration error: the chosen variant resource is configured to engage in transparent content negotiation itself, and is therefore not a proper end point in the negotiation process.

507 Insufficient Storage (WebDAV)

The method could not be performed on the resource because the server is unable to store the representation needed to successfully complete the request.

508 Loop Detected (WebDAV)

The server detected an infinite loop while processing the request.

510 Not Extended

Further extensions to the request are required for the server to fulfill it.

511 Network Authentication Required

Indicates that the client needs to authenticate to gain network access.

Summary

HTTP Status Codes whether a specific HTTP request has been successfully completed.
Responses are grouped in five classes:
Informational responses (100–199)
Successful responses (200–299)
Redirection messages (300–399)
Client error responses (400–499)
Server error responses (500–599)



# APIs and Testing
## functional-interfaces
Learning Objectives

After completing this module, associates should be able to:

Describe functional programming
Describe functional interfaces
Successfully implement a Java program using Functional Interfaces
Description

Java is mainly an Object-Oriented Programming language. Functional Programming is a different way of thinking about solving programming problems. While OOP uses objects and object interactions to build programs, functional programming is more like building programs by relying on functions to transform data. Instead of having objects interact, we may write a series of function calls that process and transform the data step-by-step.

Functions in functional programming are treated as "first-class citizens," meaning they can be assigned to variables and passed as arguments into a method call. In Java, we can use lambda expressions and method references as our "first-class citizen" functions, and we can use functional interface variables and method parameters as the locations in our code where we can place these functions. Lambdas and method references are explained in more detail in other written lectures. In this lecture, we will focus more on the role functional interfaces play in functional programming.

Using functional programming in Java can be similar to utilizing object casting. The below example assumes we have an Animal supertype and Dog and Cat subtypes:

casting example

Notice with object casting, we can swap subclass objects if we have a supertype reference variable.

We can do something similar if we use functional interfaces. A functional interface is an interface that defines only one abstract method. We can use the interface as a supertype reference variable and we can assign it a function (lambda or method reference). The function must implement the functional interface’s abstract method.

In the below example, we can have a variable be of the functional interface type, and then we can assign it a lambda as its implementation:

functional interface example

The most common use of functional programming in Java, however, is to have functional interface types as parameters in methods. When the method is called, we can pass in a lambda or method reference:

function parameter example

Real World Application

Understanding functional interfaces in Java is important because they are a fundamental concept in modern Java programming, especially with the introduction of lambda expressions and the Stream API in Java 8. Here are some reasons why understanding functional interfaces is important:

Lambda Expressions: Functional interfaces enable the use of lambda expressions, which provide a concise way to express instances of single-method interfaces. Lambda expressions make code more readable and maintainable by reducing boilerplate code.
Stream API: Many methods in the Stream API, such as filter, map, and reduce, accept functional interfaces as parameters. Understanding functional interfaces allows you to leverage the power of the Stream API to perform operations on collections in a declarative and functional style.
Parallel Processing: Functional interfaces enable parallel processing of collections using features like parallel streams. Parallel streams internally use functional interfaces to split the workload and execute operations concurrently, which can lead to improved performance for certain types of tasks.
Functional Programming Paradigm: Functional interfaces are a key component of the functional programming paradigm, which promotes writing code in a declarative and composable manner. Understanding functional interfaces can help developers embrace functional programming principles and write more modular and reusable code.
In summary, understanding functional interfaces in Java is crucial for taking advantage of modern Java features, writing concise and expressive code, leveraging powerful APIs like the Stream API, designing flexible APIs, and embracing the functional programming paradigm.

Implementation

Let’s look at an example of object casting versus implementing functional programming. Let’s say we have the three files Animal.java, Cat.java, and Dog.java:

public abstract class Animal {
    
    public abstract void makeNoise();

}
public class Cat extends Animal {
    
    public void makeNoise() {
        System.out.println("Meow!");
    }

}
public class Dog extends Animal {
    
    public void makeNoise() {
        System.out.println("Woof!");
    }
}
In another file, like Main.java, we can use object casting to swap Animal type implementations wherever there is an Animal reference variable:

public class Main {
    public static void main(String[] args) {
        
    // ---- OOP ------------------------------------------ //
        Animal anim = new Dog();
        anim.makeNoise(); // output: Woof!

        // swap implementations
        anim = new Cat();
        anim.makeNoise(); // output: Meow!

    }
}
Let’s now implement functional programming. First, we’ll need a functional interface so that we can use it as a supertype variable. There are already many functional interfaces provided by Java within the java.util.Function package:https://docs.oracle.com/javase/8/docs/api/java/util/function/package-summary.html

In practice, you’ll most likely be using one of the already provided functional interfaces, but let’s create a new one:

public interface Prettifier {
    
    /**
     * This method should take in an Object, translate it to a pretty String, and return the String.
     * 
     * @param obj - the Object to prettify
     * @return - A pretty String
     */
    public abstract String prettify(Object obj);

}
Functional interfaces conventionally have names ending in “-er”, representing a general type of functionality you may want to use. In this case, we have an interface named Prettifier. The functionality the interface represents is to make a value “pretty”. We define its single abstract method with this in mind: it takes in some Object and returns a String that should be the prettified version of the value.

Let’s implement this functionality using a lambda:

public class Main {
    public static void main(String[] args) {
        
    // ---- Functional Programming ----------------------- //

      // write functionality
      Prettifier prettifierImpl = x -> "*~*~ " + x + " ~*~*";
      
      // use functionality
      Integer myInteger = 5;
      String result = prettifierImpl.prettify(myInteger);
      System.out.println(result); // output: *~*~ 5 ~*~*

      // swap implementations
      prettifierImpl = x -> "+-+- " + x + " -+-+";
      result = prettifierImpl.prettify(myInteger);
      System.out.println(result); // output: +-+- 5 -+-+

    }
}
Let us break down the above code. The statement: Prettifier prettiferImpl = x -> "*~*~" + x +" ~*~*" is where we write an implementation of the Prettifier's functionality and assign it to the Prettifier variable prettifierImpl.

The expression: x -> "*~*~ " + x + " ~*~*" has unusual syntax because it is a lambda expression. Think of lambdas as functions we can create “on the fly”. They allow us to write functionality concisely by omitting certain syntax, like the curly braces of method bodies and the parentheses of method parameters. They are also special in that they do not need to be associated with a class, like instance or static methods need to be. We can use any lambda we would like as long as it meets the requirements specified by the functional interface. In this case, our function should take in an Object and return a String, as that is what the prettify() method defines.

We then use the functional interface implementation by calling its prettify() method and passing in the value of 5 as input. We save the result and print it to the console. Notice that we can swap the implementation as needed.

One significant difference between OOP and functional programming is that we can use much less code to implement some standalone functionality. Rather than using subclasses, we can write functionality in-line where needed.

We’ll see more of the power of functional programming in lessons to come.

Summary

While OOP uses objects and object interactions to build programs, functional programming is more like building programs by relying on functions to transform data.
In Java, we can use lambda expressions and method references as our “first-class citizen” functions, and we can use functional interface variables and parameters as the locations in our code where we can place these functions.
Functional interfaces are interfaces that have only one abstract method.
The Java 8 JDK comes with many built-in functional interfaces.


## lambdas
Learning Objectives

After completing this module, associates should be able to:

Describe what lambdas do
Successfully implement a Java program using lambdas and Functional Interfaces
Description

Lambda expressions are one of the most prominent features of Java 8, and they introduce some important aspects of functional programming to Java. Lambdas are a concise way to represent an instance of a functional interface. They enable us to treat functionality as a method argument or store functions in variables, making code shorter and more readable. The most basic syntax of a lambda expression is:

(parameter list) -> expression
Let us demonstrate an example that uses a lambda. The ArrayList class defines a forEach() method. The method’s signature is below:

public void forEach(Consumer<? super E> action)
Notice its parameter. Consumer is a functional interface that defines the accept() method as its single abstract method:

void accept(T t)
A function that implements the accept() method should take in a single parameter, perform some type of operation, and then return nothing.

Below is an example of creating a list, adding elements to it, and then using the forEach() method by passing in a lambda that fulfills the requirements of the Consumer functional interface:

List<String> names = new ArrayList<>();
names.add("Alice");
names.add("Bob");
names.add("Charlie");
names.forEach(str -> System.out.println(str));
The lambda we used takes in a single String element and prints it to the console. The forEach() method uses this lambda on every element in the list.

Lambda Expression Syntax
A lambda expression is composed of three parts:

Argument List	Arrow Token	Body
(int x, int y)	->	x + y
The body can be either a single expression or a statement block. In the expression form, the body is simply evaluated and returned. In the block form, the body is evaluated like a method body, and a return statement returns control to the caller using the anonymous method. The break and continue keywords are illegal at the top level, but are permitted within loops. If the body produces a result, every control path must return something or throw an exception.

Take a look at these examples:

// Example 1
(int x, int y) -> x + y

// Example 2
() -> 42

// Example 3
(String s) -> { System.out.println(s); }

// Example 4
s -> System.out.println(s)

// Example 5
(String value1, String value2) -> {
    System.out.println(value1);
    System.out.println(value2);
};
The first expression takes two integer arguments, named x and y, and uses the expression form to return x+y. Parenthesis are required for the parameter list because there are multiple parameters. The datatypes of the parameters are explicitly mentioned, which is optional because the compiler can infer the types.
The second expression takes no arguments and uses the expression form to return an integer 42.
The third expression takes a string, uses the block form to print it to the console, and returns nothing. Because the parameter type is explicitly mentioned, parenthesis are required.
The fourth expression is exactly the same as the one before it. However, the parameter's datatype was not written, which allows us to omit the parenthesis. The body is also a single statement, so we do not need to create a statement block.
The fifth expression has two parameters, which means parenthesis are required. The lambda's body contains more than one statement, therefore the curly braces must be present.
Real World Application

Lambdas in Java bring several important benefits to the language and its ecosystem:

Conciseness and Readability: Lambdas allow you to express instances of single-method interfaces (functional interfaces) concisely. This leads to more readable code, especially for small, self-contained functions. Reduced Boilerplate Code: By using lambdas, you can avoid writing verbose anonymous inner classes for simple operations. This reduces boilerplate code and makes the codebase more compact and expressive.
Improved API Design: Lambdas enable the design of APIs that accept behavior as parameters, promoting flexibility and extensibility. This facilitates writing APIs that are more adaptable to different use cases and allows clients to customize behavior easily.
Functional Programming Style: Lambdas enable Java to embrace functional programming principles, such as higher-order functions, immutability, and referential transparency. This leads to code that is more modular, composable, and easier to reason about.
Enhanced Collections Framework: Lambdas, together with the Stream API introduced in Java 8, provide a powerful and expressive way to work with collections. Operations like filtering, mapping, and reducing can be performed using a functional and declarative style, leading to more elegant and efficient code.
In summary, lambdas play a crucial role in modern Java development by promoting conciseness, reducing boilerplate code, enabling functional programming practices, and more.

Implementation

Below is an example of a Java program that does the following:

creates a functional interface
creates a method that has a parameter of that functional interface type
calls the method and passes in a lambda as an argument
 // == Functional Interface ====================================

interface BinaryCalculator {
    public int binaryOperation(int value1, int value2);
}

// == A Class with a main() method =============================


public class UsingLambdasAsParameters {

    // method that has a parameter of the functional interface type
    public static void printBinaryResult(int a, int b, BinaryCalculator func) {
        // perform operation, print result
        int result = func.binaryOperation(a, b);
        System.out.println(result);
    }

    // calling the method and passing in lambdas
    public static void main(String[] args) {
        printBinaryResult(3, 4, (a, b) -> a + b);
        printBinaryResult(3, 4, (a, b) -> a * b);
    }

    
}
Output:

7
12
Exercise (optional)
Given an array of numbers where every number is represented as string, successfully execute a Java program that sorts these numbers in ascending and descending order, using lambdas.

Summary

A lambda expression in Java is a concise way to represent an instance of a functional interface.
A lambda expression is composed of a parameter list, an arrow token, and a body. The syntax can differ in conciseness depending on how you choose to write it.


## method-reference-syntax
Learning Objectives

After completing this module, associates should be able to:

Describe method reference syntax
Successfully implement a Java program that uses method reference syntax.
Description

In Java, method references are a feature introduced in Java 8 that provides a shorthand way of referring to methods without having to explicitly call them. A method reference is a compact and readable way to refer to a method in cases where a lambda expression simply calls an existing method. Instead of writing out a full lambda expression, you can just use the method's name along with a special syntax.

There are four kinds of method references:

A reference to a static method
A reference to an instance method of a particular object
A reference to an instance method of an arbitrary object of a particular type
A reference to a constructor
Below is a code snippet that compares the syntax of a lambda and the corresponding method reference:

// lambda that returns a character's index in a named String object by calling the indexOf() method
char -> myString.indexOf(char)

// method reference that does the same thing
myString::indexOf
Notice the more concise syntax. Method references always utilize the :: operator, with one operand on each side. The lefthand operand tells the JVM where to find the method, and the righthand side tells the JVM what method to use. The above example is a reference to an instance method of a particular object.

Reference to a Static Method
A method reference to a static method in Java is a shorthand way of referring to a static method of a class. For a reference to a static method, the lefthand side of the operator is the class that contains the method we want, and the righthand side is the name of the static method itself.

method-reference-syntax-1

For example, let’s say we need an implementation for the BiFunction functional interface or a method that takes in two arguments and returns a result. We can refer to the Math class’s static method, max:

Math::max
More on the max method: https://docs.oracle.com/javase/8/docs/api/java/lang/Math.html#max-long-long-

Reference to an Instance Method of a Particular Object
A method reference to an instance method of a particular object in Java is a shorthand way of referring to an instance method of a named object. A named object is one that has a reference variable assigned to it. For a reference to an instance method of a particular object, the lefthand side of the operator is the object’s reference variable, and the righthand side is the name of the instance method itself that we want to use.

method-reference-syntax-2

For example, let’s say we need an implementation for the Supplier functional interface or a method that doesn’t take in any arguments and returns a value. We can refer to the String class’s instance method, toUppercase:

// our named object
String myString = "hello";

// referencing an instance method using our named object
myString::toUpperCase
More on the toUpperCase method: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#toUpperCase()

Reference to an Instance Method of an Arbitrary Object of a Particular Type
A method reference to an instance method of an arbitrary object of a particular type in Java is a shorthand way of referring to an instance method of an unnamed object. An unnamed object is one created during an operation and isn’t assigned a reference variable. It is typically used right away and then discarded. For this type of reference, the lefthand side of the operator is the object’s type, and the righthand side is the name of the instance method itself that we want to use.

method-reference-syntax-1

Notice that this syntax is exactly the same as a reference to a static method, but what’s happening “behind the scenes” is different. The JVM will need to infer the correct action to take by using the code’s surrounding context.

For example, let’s say we need an implementation for the Predicate functional interface or a method that takes in a single argument and returns a boolean value. We can refer to the String class’s instance method, startsWith:

String::startsWith
More on the startsWith method: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#startsWith(java.lang.String)

Reference to a Constructor
A method reference to a constructor in Java is a shorthand way of referring to a class’s constructor. For this type of reference, the lefthand side of the operator is the relevant class name, and the righthand side is the new keyword.

method-reference-syntax-3

For example, let’s say we need another implementation of the Supplier functional interface. Since a no-args constructor takes on no arguments and returns an object, we can use a constructor as the implementation:

String::new
Resources
Math class: https://docs.oracle.com/javase/8/docs/api/java/lang/Math.html
Function package: https://docs.oracle.com/javase/8/docs/api/?java/util/function/package-summary.html
String class: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html
Real World Application

Method references in Java provide a concise way to refer to methods or constructors without explicitly defining a lambda expression. They are important for several reasons:

Readability and Conciseness: Method references allow you to express code more concisely by referring to existing methods or constructors directly. This leads to clearer and more readable code, especially when working with functional interfaces.
Functional Programming Paradigm: Method references align with the functional programming paradigm by treating methods as first-class citizens. They allow you to pass behavior as parameters to methods, enabling functional-style programming in Java.
Integration with Existing APIs: Many existing APIs in Java, such as the Stream API and the Comparator interface, support method references. Using method references allows you to seamlessly integrate with these APIs and take advantage of their functionality.
Overall, method references are an important feature in Java that enhances readability, promotes code reuse, facilitates maintainability, aligns with the functional programming paradigm, improves performance in some cases, and integrates well with existing APIs. They provide a powerful and expressive way to work with methods and constructors in Java.

Implementation

Below are examples of using method references.

Reference to a Static Method
import java.util.function.*;

public class Examples {

    // a static method to reference
    public static int triple(int num) {
        return num * 3;
    }

    public static void main(String[] args) {
        
        // using a lambda to implement the Function functional interface
        Function<Integer, Integer> computation = num -> triple(num);
    }
}
The above example uses a lambda to provide implementation to a functional interface. Below demonstrates how to instead use a method reference:

import java.util.function.*;

public class Examples {

    // a static method to reference
    public static int triple(int num) {
        return num * 3;
    }

    public static void main(String[] args) {
       
        // reference to a static method 
        Function<Integer, Integer> computation = Examples::triple;

        // use the implementation, print out the result
        int result = computation.apply(-3);
        System.out.println("Result #1: " + result);

        // another example of referencing a static method
        computation = Math::abs;

        // printing out the new result
        result = computation.apply(-3);
        System.out.println("Result #2: " + result);
    }
}
In the above code, we provided a static method reference to provide implementation to the Function functional interface reference variable. We can then use that variable to perform the functionality.

Output:

-9
3
Reference to an Instance Method of a Particular Object
import java.util.function.*;

public class Examples {

    public static void main(String[] args) {
        // provide an object to use as a reference
        String myString = "hello world";

        // reference to a particular object's instance method
        Predicate<String> evaluation = myString::startsWith;

        // use the Predicate's test() method and print the result
        boolean result = evaluation.test("he");
        System.out.println("Result #1: " + result);

        evaluation = myString::equalsIgnoreCase;
        result = evaluation.test("HeLlo WoRlD");
        System.out.println("Result #2: " + result);

    }
}
In the above code, we provided an instance method reference to provide implementation to the Predicate functional interface reference variable. We can then use that variable to perform the functionality.

Output:

Result #1: true
Result #2: true
Reference to an Instance Method of an Arbitrary Object of a Particular Type
The below example is similar to the one above, however instead of creating a named object, we’ll create and use an object only when we need it:

import java.util.*;
import java.util.function.*;

public class Examples {

    public static void main(String[] args) {
  
        // reference to an arbitrary object of the String type
        BiPredicate<String, String> evaluation = String::startsWith;

        // use the BiPredicate's test() method and print the results
        boolean result = evaluation.test("hello world", "he");
        System.out.println("Result #1: " + result);

        // another example
        evaluation = String::equalsIgnoreCase;
        result = evaluation.test("goodbye!", "gOoDbYe!");
        System.out.println("Result #2: " + result);

    }
}
In the above code, we provided an instance method reference to provide implementation to the BiPredicate functional interface reference variable. We can then use that variable to perform the functionality. Notice that the objects hello world and goodbye! and provided only when they’re needed (when we need to call their instance methods), and they are not assigned to reference variables for future use.

Output:

Result #1: true
Result #2: true
Reference to a Constructor
import java.util.function.*;

public class Dog {
    
    String name;
    int age;

    @Override
    public String toString() {
        return "Dog [name: " + name + " age: " + age + "]";
    }
}

class Main {
    public static void main(String[] args) {
        // reference to a constructor
        Supplier<Dog> dogGetter = Dog::new;

        // use supplier's get() method to retrieve and use an object
        Dog myDog = dogGetter.get();
        myDog.name = "Charlie";
        myDog.age = 3;
        System.out.println(myDog);

    }
}
In the above code, we provided a constructor reference to provide implementation to the Supplier functional interface reference variable. We can then use that variable to perform the functionality.

Output:

Dog [name: Charlie age: 3]
Exercise (Optional)
Complete the commented line below with a method reference call to "addition."
Compile, test, and run the program.
public class StaticMethodReference2 {

    public static void main(String[] args) {

        BiFunction<Integer, Integer, Integer> customFI = // Place method reference call to "addition" here!!!
        int sum = customFI.apply(10, 20);
        System.out.println("Addition : " + sum);

    }

    public static int addition(int a, int b) {
        return a + b;
    }

}
Summary

In Java, method references are a feature introduced in Java 8 that provide a shorthand way of referring to methods without having to explicitly call them.
A method reference is a compact and readable way to refer to a method in cases where a lambda expression simply calls an existing method.
There are four kinds of method references:
Static methods
Instance methods of particular objects
Instance methods of an arbitrary object of a particular type
Constructor


## Introduction To Rest
Learning Objectives

After completing this module, associates should be able to:

Understand and explain REST
Description

Representational State Transfer (REST) is an architectural style that defines a set of constraints to be used for creating web services. A REST API is a way of accessing web services in a simple and flexible way without having any processing.

REST technology is generally preferred to the more robust Simple Object Access Protocol (SOAP) technology because REST uses less bandwidth, and is simple and flexible making it more suitable for internet usage. It’s used to fetch or give some information from a web service. All communication done via REST API uses only HTTP requests.

In HTTP there are five methods that are commonly used in a REST-based Architecture: POST, GET, PUT, PATCH, and DELETE. These correspond to create, read, update, and delete (or CRUD) operations respectively.

REST Constraints
There are some key constraints to think about when considering whether a RESTful API is the right type of API for your needs:

Client-Server: This constraint operates on the concept that the client and the server should be separate from each other and allowed to evolve individually.
Stateless: REST APIs are stateless, meaning that calls can be made independently of one another, and each call contains all of the data necessary to complete itself successfully.
Cache: Because a stateless API can increase request overhead by handling large loads of incoming and outbound calls, a REST API should be designed to encourage the storage of cacheable data.
Uniform Interface: The key to decoupling the client from server is having a uniform interface that allows independent evolution of the application without having the application’s services, or models and actions, tightly coupled to the API layer itself.
Layered System: REST APIs have different layers of their architecture working together to build a hierarchy that helps create a more scalable and modular application.
Code on Demand: Code on Demand allows for code or applets to be transmitted via the API for use within the application.
Using REST
You access data via URLs.
For example, /users/Ada lets you access data about a person named Ada.
You use HTTP methods to access or change data.
For example, you’d view Ada’s data by issuing a GET request to /people/Ada, and you’d modify Ada’s data by issuing a POST request to /people/Ada. You can use the other HTTP methods (like PUT or DELETE) to interact with the data as well.
You can represent data however you want.
For example that GET request might return a JSON string that represents the user data. The POST request might then take a JSON string. Or it could take a binary string, or XML, or a list of properties. It’s up to you.
Each request should be standalone.
In other words, you should not store session information on the server! Everything needed to fulfill a request must be included in the request itself!
Real World Application

Understanding REST (Representational State Transfer) is crucial for modern web development for several reasons:

Standardization: REST has become the de facto standard for designing web APIs. Understanding REST allows developers to adhere to industry best practices and conventions when designing and implementing APIs, promoting consistency and interoperability across different systems.
Statelessness: RESTful APIs are stateless, meaning each request from a client contains all the information necessary for the server to fulfill it. This architectural constraint simplifies server implementation, improves scalability, and enables better fault tolerance and load balancing.
Uniform Interface: REST emphasizes a uniform interface between clients and servers, typically based on standard HTTP methods (GET, POST, PUT, DELETE) and representations (such as JSON or XML). This simplifies client-server interactions and enables the use of generic components like web browsers and proxies.
Flexibility: Data is not tied to resources or methods, so REST can handle multiple types of calls, return different data formats and even change structurally with the correct implementation of hypermedia. REST is not constrained to XML, but instead can return XML, JSON, YAML or any other format depending on what the client requests.
Interoperability: RESTful APIs can be consumed by clients written in any programming language and running on any platform, as long as they adhere to the standard HTTP protocol. This interoperability promotes collaboration and integration between different systems and technologies.
In summary, understanding REST is essential for designing, implementing, and consuming web APIs that are scalable, maintainable, interoperable, and aligned with industry best practices. It provides a framework for building distributed systems that communicate effectively over the internet while promoting flexibility and interoperability.

Implementation

REST stands for representational state transfer, which is a set of rules that you can follow to build a web app that provides access to data in a way that’s reusable by multiple applications, or many users of the same application.

Let's consider an example of a website that provides access to book data. Programming languages have many different libraries and frameworks to help you develop a REST API. When you access a REST API's data, you don't need to know how the REST API works, you just need to know the interface it provides. Let's say the book website provides the following API:

The API will allow clients to perform CRUD (Create, Read, Update, Delete) operations on the book resources using its interface, which is by sending HTTP request to the relevant URLS:

GET request to www.notarealbooksite.com/api/books: Retrieve a list of all books.
GET request to www.notarealbooksite.com/api/books/{id}: Retrieve details of a specific book identified by its ID.
POST request to www.notarealbooksite.com/api/books: Create a new book by providing the book details in the request body.
PUT request to www.notarealbooksite.com/api/books/{id}: Update an existing book identified by its ID with the provided details.
DELETE request to /books/{id}: Delete a book identified by its ID.
You can send these requests from a web browser or an application you develop that will use the data. An example request / response exchange could look like the following:

Making the request on a terminal:

curl -X GET https://www.notarealbooksite.com/api/books

Data received from response body:

[
    {
        "id": 1,
        "title": "To Kill a Mockingbird",
        "author": "Harper Lee",
        "genre": "Fiction"
    },
    {
        "id": 2,
        "title": "1984",
        "author": "George Orwell",
        "genre": "Dystopian"
    }
]

Further details about REST URL design is explained in another lesson.

Summary

Representational State Transfer (REST) is an architectural style that defines a set of constraints to be used for creating web services.
REST API is a way of accessing web services in a simple and flexible way without having any processing.
REST is used to fetch or give some information from a web service. All communication done via REST API uses only HTTP requests.


## Introduction To Javalin
Learning Objectives

After completing this module, associates should be able to:

Give an overview of Javalin.
List advantages of using Javalin.
Description
What is Javalin?
Javalin is a very lightweight web framework for Java 8 (and later) and Kotlin. It supports modern features such as HTTP/2, WebSocket, and asynchronous requests. Javalin is servlet-based, and its main goals are simplicity, a great developer experience, and first-class interoperability between Java and Kotlin. Kotlin is a programming language initially designed for the JVM and Android that combines object-oriented and functional programming features.

Many developers would say Javalin is a library rather than a framework. This is because in Javalin, unlike in most frameworks, you never extend anything; it sets no requirements for your application structure and there are no annotations, no reflection, and no other magic — just code.

Resources
Official documentation: https://javalin.io/
Real World Application
Why use Javalin?
Simple
Unlike other Java and Kotlin web frameworks, Javalin has very few concepts that you need to learn. You never extend classes and you rarely implement interfaces.
Lightweight
Javalin is just a few thousand lines of code on top of Jetty, and its performance is equivalent to raw Jetty code.
Interoperable
Other Java and Kotlin web frameworks usually offer one version for each language. Javalin is being made with inter-operability in mind, apps are built the same way in both Java and Kotlin.
Flexible
Javalin is designed to be simple and blocking, as this is the easiest programming model to reason about. But, if you set a Future as a result, Javalin switches into asynchronous mode.
OpenAPI
Many lightweight Java and Kotlin web frameworks don't support OpenAPI, but Javalin does (including Swagger UI and ReDoc).
Jetty
Javalin runs on top of Jetty, one of the most used and stable web-servers on the JVM. You can configure the Jetty server fully, including SSL and HTTP2 and everything else that Jetty offers.
Implementation
Installing Javalin
Follow the instructions on this page to install Javalin: Download Javalin

In Javalin, there are no requirements for your application structure. In other words, we don't have any of that extra "fluff" like annotations, reflection, and the like, just the pure code. As you can see, the “Hello World” example is just four lines and an import statement:

import io.javalin.Javalin;

public static void main(String[] args) {
    Javalin app = Javalin.create().start(7000);
    app.get("/", ctx -> ctx.result("Hello World"));
}
This snippet creates a new Javalin instance and starts it on port 7000. It then attaches a Handler that is triggered by GET requests to the root path. You can build and package this application as a JAR file. If you use Maven, just add this to your build:

<dependency>
    <groupid>io.javalin</groupid>
    <artifactid>javalin</artifactid>
    <version>2.5.0</version>
</dependency>
You will also need a logger and JSON parsing library:

<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-simple</artifactId>
    <version>2.0.7</version>
</dependency>
<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-databind</artifactId>
    <version>2.16.0-rc1</version>
    <type>jar</type>
</dependency>
Note that the version of these dependencies will not be the latest version. Check Maven Central for the most up to date version of dependencies: https://central.sonatype.com/

Run the output JAR file like any other Java program. The syntax for running a jar is as follows: java -jar myjar.jar.

Summary

Javalin is a very lightweight web framework for Java 8 (and later) and Kotlin.
It supports modern features such as HTTP/2, WebSocket, and asynchronous requests.
Javalin is servlet-based, and its main goals are simplicity, a great developer experience, and first-class interoperability between Java and Kotlin.
Javalin never extends classes and rarely implements interfaces


## Configuration
Learning Objectives

After completing this module, associates should be able to:

Discuss Javalin configuration
Successfully modify a Javalin configuration
Description

Some Javalin configuration information:

Javalin runs on an embedded Jetty. The architecture for adding other embedded servers is in place, and pull requests are welcome.
Javalin can be used to start and stop the server.
If you don’t need any custom configuration, you can quick-start a server in Javalin
You can customize the embedded server in Javalin
You can configure your embedded jetty-server and Javalin will attach its own handlers to the end of the chain.
Real World Application

Advantages of Remote Server Configuration:

Easy to turn off and turn on the server
Easy to Monitor Resource Usage
Easy to Monitor Security
Lower Maintenance Cost
Implementation
Server setup
Javalin runs on an embedded Jetty. The architecture for adding other embedded servers is in place, and pull requests are welcome.

Starting and stopping
To start and stop the server, use the appropriately named start() and stop methods.

Javalin app = Javalin.create()
    .start() // start server (sync/blocking)
    .stop() // stop server (sync/blocking)
Quick-start
If you don’t need any custom configuration, you can use the Javalin.start(port) method.

Javalin app = Javalin.start(7000);
This creates a new server which listens on the specified port (here, 7000), and starts it.

Configuration
The following snippet shows all the configuration currently available in Javalin:

Javalin.create() // create has to be called first
    .contextPath("/context-path") // set a context path (default is "/")
    .dontIgnoreTrailingSlashes() // treat '/test' and '/test/' as different URLs
    .defaultContentType(string) // set a default content-type for responses
    .defaultCharacterEncoding(string) // set a default character-encoding for responses
    .disableStartupBanner() // remove the javalin startup banner from logs
    .embeddedServer( ... ) // see section below
    .enableCorsForOrigin("origin") // enables cors for the specified origin(s)
    .enableDynamicGzip() // gzip response (if client accepts gzip and response is more than 1500 bytes)
    .enableRouteOverview("/path") // render a HTML page showing all mapped routes
    .enableStandardRequestLogging() // does requestLogLevel(LogLevel.STANDARD)
    .enableStaticFiles("/public") // enable static files (opt. second param Location.CLASSPATH/Location.EXTERNAL)
    .maxBodySizeForRequestCache(long) // set max body size for request cache
    .port(port) // set the port
    .start(); // start has to be called last
Any argument to contextPath() will be normalized to the form /path (slash, path, no-slash)

Custom server
If you need to customize the embedded server, you can call the app.embeddedServer() method:

app.embeddedServer(new EmbeddedJettyFactory(() -> {
    Server server = new Server();
    // do whatever you want here
    return server;
}));
Custom jetty handlers
You can configure your embedded jetty-server with a handler-chain (example), and Javalin will attach it’s own handlers to the end of this chain.

StatisticsHandler statisticsHandler = new StatisticsHandler();

Javalin.create()
    .embeddedServer(new EmbeddedJettyFactory(() -> {
        Server server = new Server();
        server.setHandler(statisticsHandler);
        return server;
    }))
    .start();
Examples of different configurations
HTTP Configuration
Javalin.create(config -> {
    config.http.generateEtags = booleanValue;     // if javalin should generate etags for dynamic responses (not static files)
    config.http.prefer405over404 = booleanValue;  // return 405 instead of 404 if path is mapped to different HTTP method
    config.http.maxRequestSize = longValue;       // the max size of request body that can be accessed without using using an InputStream
    config.http.defaultContentType = stringValue; // the default content type
    config.http.asyncTimeout = longValue;         // timeout in milliseconds for async requests (0 means no timeout)
});
Routing Configuration
    Javalin.create(config -> {
    config.routing.contextPath = stringValue; // the context path (ex '/blog' if you are hosting an app on a subpath, like 'mydomain.com/blog')
    config.routing.ignoreTrailingSlashes = booleanValue; // treat '/path' and '/path/' as the same path
    config.routing.treatMultipleSlashesAsSingleSlash = booleanValue; // treat '/path//subpath' and '/path/subpath' as the same path
});
Real example with HttpClient
Below we will show you how we could use Javalin to create an application that generates random names assuming that we have a random name generator API already created that we can call to.

private static CompletableFuture<HttpResponse<String>> getRandomName() {
    HttpRequest request = HttpRequest.newBuilder()
        .uri(URI.create("https://nameGenerator.person/name"))
        .build();
    return httpClient.sendAsync(request, ofString());
}
The above code makes a call to the API and returns a CompleteableFuture<String> which will either resolve in a random name or an error. Below, we will implement the Javalin code that will asynchronously return a randomly generated name to the client.

app.get("/random-name", ctx -> {
    ctx.future(() -> {
        return getRandomName()
            .thenAccept(response -> ctx.html(response.body()).status(response.statusCode()))
            .exceptionally(throwable -> {
                ctx.status(500).result("Could not get a random name" + throwable.getMessage());
                return null;
            })
    });
});
Here we are using a get method to set the response that the client will get or set the error message the client will get if the method fails.

Summary

Javalin runs on an embedded Jetty.
Javalin can be used to start and stop the server.
If you don’t need any custom configuration, you can quick-start a server in Javalin
You can customize the embedded server in Javalin
You can configure your embedded jetty-server and Javalin will attach its own handlers to the end of the chain.


## Handlers
Learning Objectives

After completing this module, associates should be able to:

Discuss handlers in Javalin
Successfully implement handlers in a program.
Description

The below information can be found in Javalin's documentation: https://javalin.io/documentation#handlers

Handlers
Javalin has three main handler types: before-handlers, endpoint-handlers, and after-handlers. There are also exception-handlers and error-handlers, but they will be discussed in another topic. The before-, endpoint- and after-handlers require three parts:

An HTTP verb, like GET or POST
A path, ex: /, /hello-world, /hello/{name}
A handler implementation, ex ctx -> { ... }, MyClass implements Handler, etc
The Handler interface has a void return type. You use a method like ctx.result(result), ctx.json(obj), or ctx.future(future) to set the response which will be returned to the user.

Before handlers
Before-handlers are matched before every request (including static files).

app.before(ctx -> {
    // runs before all requests
});
app.before("/path/*", ctx -> {
    // runs before request to /path/*
});
Endpoint handlers
Endpoint handlers are the main handler type, and they define your API. For example, you can add a GET handler to serve data to a client, or a POST handler to receive some data. Common methods are supported directly on the Javalin class (GET, POST, PUT, PATCH, DELETE, HEAD, OPTIONS), uncommon operations (TRACE, CONNECT) are supported via Javalin#addHandler.

Endpoint-handlers are matched in the order they are defined.

app.get("/output", ctx -> {
    // some code
    ctx.json(object);
});

app.post("/input", ctx -> {
    // some code
    ctx.status(201);
});
Handler paths can include path-parameters. These are available via ctx.pathParam("key"):

app.get("/hello/{name}", ctx -> { // the {} syntax does not allow slashes ('/') as part of the parameter
    ctx.result("Hello: " + ctx.pathParam("name"));
});
Handler paths can also include wildcard parameters:

app.get("/path/*", ctx -> { // will match anything starting with /path/
    ctx.result("You are here because " + ctx.path() + " matches " + ctx.matchedPath());
});
However, you cannot extract the value of a wildcard. Use a slash accepting path-parameter (<param-name>) if you need this behavior.

After handlers
After-handlers run after every request (even if an exception occurred). You might know after-handlers as filters, interceptors, or middleware from other libraries.

app.after(ctx -> {
    // run after all requests
});
app.after("/path/*", ctx -> {
    // runs after request to /path/*
});
Context
The Context object provides you with everything you need to handle a http-request. It contains the underlying servlet-request and servlet-response, and a bunch of getters and setters.

Request Method	Description
body()	allows access to request body as string
pathParam("name")	path parameter by name as string
attribute("name", value)	set an attribute on the request
attribute("name")	get an attribute on the request
path()	request path
method()	allows you to use request methods (GET, POST, etc)
queryParam("name")	query param by name as string
Response Method	Description
redirect("/path", code)	redirect to the given path with the given status code
status(code)	set the response status code
status()	get the response status code
Real World Application

Typical uses for HTTP handlers include the following:

RSS feeds: To create an RSS feed for a Web site, you can create a handler that emits RSS-formatted XML. You can then bind a file name extension such as .rss to the custom handler.

Image server: If you want a Web application to serve images in a variety of sizes, you can write a custom handler to resize images and then send them to the user as the handler's response.

Implementation

Let's look into a more realistic use case for Javalin:

First, we need to create a model of the object we're working with. We start by creating a package called user under the root project.

Then, we add a new User class:

public class User {
    public final int id;
    public final String name;

    // constructors
}
Also, we need to set up our data access object (DAO). We'll use an in-memory object to store our users in this example.

We create a new class in the user package called UserDao.java:

class UserDao {

    private List<User> users = Arrays.asList(
      new User(0, "Steve Rogers"),
      new User(1, "Tony Stark"),
      new User(2, "Carol Danvers")
    );

    private static UserDao userDao = null;

    private UserDao() {
    }

    static UserDao instance() {
        if (userDao == null) {
            userDao = new UserDao();
        }
        return userDao;
    }

    Optional<User> getUserById(int id) {
        return users.stream()
          .filter(u -> u.id == id)
          .findAny();
    }

    Iterable<String> getAllUsernames() {
        return users.stream()
          .map(user -> user.name)
          .collect(Collectors.toList());
    }
}
Implementing our DAO as a singleton makes it easier to use in the example. We could also declare it as a static member of our main class or use dependency injection from a library if we wanted to.

Finally, we want to create our controller class. Javalin allows us to be very flexible when we declare our route handlers, so this is only one way of defining them.

We create a new class called UserController.java in the user package:

public class UserController {
    public static Handler fetchAllUsernames = ctx -> {
        UserDao dao = UserDao.instance();
        Iterable<String> allUsers = dao.getAllUsernames();
        ctx.json(allUsers);
    };

    public static Handler fetchById = ctx -> {
        int id = Integer.parseInt(Objects.requireNonNull(ctx.pathParam("id")));
        UserDao dao = UserDao.instance();
        Optional<User> user = dao.getUserById(id);

        // Return the user object in the response
        if (user.isPresent()) {
            ctx.json( user.get() );
        } else {
            ctx.html("Not Found");
        }

        /*
        Alternative to if-else statement above:
        user.ifPresent( u -> ctx.json(u) )

        if (!user.isPresent()) {
            ctx.html("Not Found");
        }
        */
    };
}
By declaring the handlers as static, we ensure that the controller itself holds no state. But, in more complex applications, we may want to store state between requests, in which case we'd need to remove the static modifier.

Also note that unit testing is harder with static methods, so if we want that level of testing we will need to use non-static methods.

Adding Routes
We now have multiple ways of fetching data from our model. The last step is to expose this data via REST endpoints. We need to register two new routes in our main application.

Let's add them to our main application class:

app.get("/users", UserController.fetchAllUsernames);
app.get("/users/{id}", UserController.fetchById);
After compiling and running the application, we can make a request to each of these new endpoints. Calling http://localhost:7000/users will list all users and calling http://localhost:7000/users/0 will get the single User JSON object with the id 0. We now have a microservice that allows us to retrieve User data.

Extending Routes
Retrieving data is a vital task of most microservices.

However, we also need to be able to store data in our datastore. Javalin provides the full set of path handlers that are required to build services.

We saw an example of GET above, but PATCH, POST, DELETE, and PUT are possible as well.

Also, if we include Jackson as a dependency, we can parse JSON request bodies automatically into our model classes.

app.post("/",  ctx -> {
  User user = ctx.bodyAsClass(User.class);
});
The above code would allow us to grab the JSON User object from the request body and translate it into the User model object.

Summary

Javalin has three main handler types:
before-handlers
endpoint-handlers
after-handlers
The before-, endpoint- and after-handlers require three parts:
An HTTP verb, like GET or POST
A path, ex: /, /hello-world, /hello/{name}
A handler implementation, ex ctx -> { ... }, MyClass implements Handler, etc
The Handler interface has a void return type. You use a method like ctx.result(result), ctx.json(obj), or ctx.future(future) to set the response which will be returned to the user.


## Exception Handling
Learning Objectives

After completing this module, associates should be able to:

Discuss exception handling in Javalin
Successfully implement Javalin exception handling in a program.
Description

The below information can be found in Javalin's documentation: https://javalin.io/documentation#exception-mapping

Exception Mapping
All handlers (before, endpoint, after) can throw Exception (and any subclass of Exception) The app.exception() method gives you a way of handling these exceptions:

app.exception(NullPointerException.class, (e, ctx) -> {
    // handle nullpointers here
});

app.exception(Exception.class, (e, ctx) -> {
    // handle general exceptions here
    // will not trigger if more specific exception-mapper found
});
Error Mapping
Error mapping is similar to exception mapping, but it operates on HTTP status codes instead of Exceptions:

app.error(404, ctx -> {
    ctx.result("Generic 404 message")
});
Exception and Error Mapping
Sometimes we want to use Exception and Error Mapping together:

app.exception(FileNotFoundException.class, (e, ctx) -> {
    ctx.status(404);
}).error(404, ctx -> {
    ctx.result("Generic 404 message")
});
Real World Application
Why handle Java exceptions?
Java exception handling is important because it helps maintain the normal, desired flow of the program even when unexpected events occur. If Java exceptions are not handled, programs may crash or requests may fail. This can be very frustrating for customers and if it happens repeatedly, you could lose those customers.

The worst situation is if your application crashes while the user is doing any important work, especially if their data is lost. To make the user interface robust, it is important to handle Java exceptions to prevent the application from unexpectedly crashing and losing data. There can be many causes for a sudden crash of the system, such as incorrect or unexpected data input. For example, if we try to add two users with duplicate IDs to the database, we should throw an exception since the action would affect database integrity.

Developers can predict many of the Java exceptions that a piece of code is capable of throwing.

The best course of action is to explicitly handle those exceptions to recover from them gracefully. Programming languages provide ways to handle exceptions starting from specific ones and moving toward the more generic ones. Tracking exceptions centrally offers visibility to your development team on the quality of the code and what causes these errors so they can fix them faster.

Implementation

In this example, we will demonstrate how to use error and exception handlers within a note-saving application. For simplicity, we will only include code that is necessary for explaining our topic.

Note.java and NoteController.java
We will need a Note.java class to describe the domain entity:


public class Note {

    // fields
    private long id;
    private String content;
    private String priority;

    // ... other class members, like constructors and getters/setters
}
And we have a NoteController.java class that includes endpoint and handler setup:

public class NoteController {

    // ... service layer dependency configuration code omitted
    
    // method that starts Javalin and sets up endpoints
    public void setup() {
        // create Javalin object
        Javalin app = Javalin.create().start(8080);

        // endpoints
        app.get("/notes", this::getAllNotesHandler);
        app.get("/notes/{id}", this::getNoteByIdHandler);

        app.error(404, this::NotFoundHandler);
        app.exception(Exception.class, this::genericExceptionHandler);
    }

     // handlers
    private void getAllNotesHandler(Context ctx) {
        // ... statements omitted
    }

    private void getNoteByIdHandler(Context ctx) {
        // ... statements omitted
    }

    private void NotFoundHandler(Context ctx) {
        ctx.result("Resource not found.");
    }

    private void genericExceptionHandler(Exception e, Context ctx) {
        ctx.result("Oops, something went wrong: " + e.getClass() + "\n" + e.getMessage());
    }

}
As you can see, we have two endpoints, one for a GET request to /notes and another for a GET request to /notes/{id}. We also include error and exception mapping using app.error() and app.exception(). We can see that the syntax for these methods is very similar to defining an endpoint.

Error Mapping
The app.error() call requires that we specify a status code to map to and a handler whose action will be performed. In our example, we added an app.error() call to handle any client requests that would return an 404 status code, meaning the client attempted to access a resource that we did not define in our application. The associated handler simply specifies that the response should return the text Resource not found.

Exception Mapping
The app.exception() call requires that we specify an exception class and a handler whose action will be performed. If any handler throws the specified exception, then this mapping's handler will run. In our example, if any generic Exception is thrown, then the response should return the text Oops, something went wrong: along with information about the Exception itself, like its class and message.

Summary

Exception Mapping
All handlers (before, endpoint, after) can throw Exception (and any subclass of Exception)
The app.exception() method gives you a way of handling these exceptions
Error Mapping
Error mapping is similar to exception mapping, but it operates on HTTP status codes instead of Exceptions


## JSON
Learning Objectives

After completing this module, associates should be able to:

Describe what is JSON
Understand the use cases and formatting of JSON
Description

JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate.

JSON Object is a set of key and value pair enclosed within curly braces.

A key is a string enclosed in quotation marks.
A value can be a string, number, boolean expression, array, or object.
A key value pair follows a specific syntax, with the key followed by a colon followed by the value. The key/value pairs are separated by commas.
Note: The order of key-value pairs in a JSON object does not affect its interpretation; they can be listed in any order.

For example:

{
    "key1": "value1",
    "key2": "value2"
}
Real World Application

Some of the applications of JSON are listed below:

Used to transmit data between the server and web application using JSON.
JSON format helps transmit and serialize all types of structured data.
Web services and Restful APIs use the JSON format to get public data.
Implementation

In this example we'll make a GET request to a fake API, https://reqres.in/, so that we can see an example of the JSON format. We will use the cURL program, which is already available for Mac OS or Linux.

For Windows users, for your command-line interface, I recommend GitBash which is bundled within Git for Windows. It will include cURL. If you are just using the command prompt, you would need to install cURL manually. Instructions are here: https://developer.zendesk.com/documentation/api-basics/getting-started/installing-and-using-curl/#windows-10-version-1803-or-later

GET Request
Use the following command in your CLI: curl https://reqres.in/api/users/

The following should be the output (formatted here for readability):

{
   "page":1,
   "per_page":6,
   "total":12,
   "total_pages":2,
   "data":
      [
         {
            "id":1,
            "email":"george.bluth@reqres.in",
            "first_name":"George",
            "last_name":"Bluth",
            "avatar":"https://reqres.in/img/faces/1-image.jpg"
         },
         {
            "id":2,
            "email":"janet.weaver@reqres.in",
            "first_name":"Janet",
            "last_name":"Weaver",
            "avatar":"https://reqres.in/img/faces/2-image.jpg"
         },
         {
            "id":3,
            "email":"emma.wong@reqres.in",
            "first_name":"Emma",
            "last_name":"Wong",
            "avatar":"https://reqres.in/img/faces/3-image.jpg"
         },
         {
            "id":4,
            "email":"eve.holt@reqres.in",
            "first_name":"Eve",
            "last_name":"Holt",
            "avatar":"https://reqres.in/img/faces/4-image.jpg"
         },
         {
            "id":5,
            "email":"charles.morris@reqres.in",
            "first_name":"Charles",
            "last_name":"Morris",
            "avatar":"https://reqres.in/img/faces/5-image.jpg"
         },
         {
            "id":6,
            "email":"tracey.ramos@reqres.in",
            "first_name":"Tracey",
            "last_name":"Ramos",
            "avatar":"https://reqres.in/img/faces/6-image.jpg"
         }
      ],
   "support":
      {
         "url":"https://reqres.in/#support-heading",
         "text":"To keep ReqRes free, contributions towards server costs are appreciated!"
      }
}
Summary

JSON (JavaScript Object Notation) is a lightweight data-interchange format.
It is easy for humans to read and write.
It is easy for machines to parse and generate.


## Exposing And Consuming Restful Api Endpoints
Learning Objectives

After completing this module, associates should be able to:

Discuss RESTful API Endpoints
Description

Every developer who wants to build modern, robust web applications must understand how to consume APIs to fetch data into their applications. In this topic, you will learn how to consume RESTful APIs.

What is a REST API?
If you've ever spent any time programming or researching programming, you've likely come across the term "API." API stands for Application Programming Interface. It is a medium that allows different applications to communicate programmatically with one another and return a response in real time.

Roy Fielding defined REST in 2000 as an architectural style and methodology commonly used in the development of internet services, such as distributed hypermedia systems. It is an acronym that stands for "REpresentational State Transfer."

When a request is made via a REST API, it sends a representation of the resource's current state to the requester or endpoint. This state representation can take the form of JSON (JavaScript Object Notation), XML, or HTML. JSON is the most widely used file format because it is language-independent and can be read by both humans and machines.

Below is an example of JSON:

[
   {
      "userId": 1,
      "id": 1,
      "title": "sunt excepturi",
      "body": "quia et suscipit\nsuscipit recusandae consequuntur "
   },
   {
      "userId": 1,
      "id": 2,
      "title": "qui est esse",
      "body": "est rerum tempore vitae\nsequi sint nihil"
   }
]
Exposing Endpoints
You could let the users create, read, update or delete information using the HTTP protocol through your API, which adheres to a set of rules. RESTful APIs use HTTP verbs to perform some actions on information and they adhere to some principles that say how resources should be identified/represented and how they should be manipulated through those representations.

A REST resource is a piece of identifiable information that can be accessed through an API. Note that resources are not necessarily domain entities, although they usually appear to be so. Exposing endpoints means that you enable information to be accessed through some means. An endpoint is a combination of an action to perform and its associated resource.

For example, in a RESTful API for managing a collection of books, exposing endpoints might involve defining resources such as /books which can be used to retrieve all books or to create a new book, or /books/{id} which can be used to retrieve, update, and delete a specific book by its ID. After defining those resources, you'll need to define routes or handlers. These are typically methods or functions that wait for an HTTP request to be routed to a specific endpoint. If a request is made, the method or function will execute. The result of the execution should be a response that is sent back to the client or application that made the request.

Consuming Endpoints
Consuming endpoints involves sending requests to the exposed endpoints of another application or service to access its functionalities or resources. In other words, it's the process of making HTTP requests to interact with an API and retrieve data or perform actions.

To consume endpoints, you use HTTP client libraries or tools in your application to send requests to the specified URLs and handle the responses returned by the server. You typically specify the HTTP method (GET, POST, PUT, DELETE), along with any necessary request parameters or data, and process the response received from the server.

In a client application (e.g., a web application, mobile app, or desktop application), consuming endpoints might involve sending GET requests to retrieve a list of books from a remote server, sending POST requests to create a new book, sending PUT requests to update an existing book, and sending DELETE requests to delete a book.

Implementation

Let's pretend we're building a REST API for a blog application.

Exposing Endpoints
Step 1: Identify Your Resources
The first thing to do when building a REST API is to identify which resources will be defined by our application. These resources will allow users to interact with one or all blog posts:

/posts: Represents a collection of blog posts.
/posts/{id}: Represents a specific blog post.
Next, let's define some endpoints we could expose to a client application.

Step 2: Identify Your Endpoints
These endpoints will allow the client to perform various actions such as creating, reading, updating, and deleting blog posts:

GET to /posts: Retrieve a list of all blog posts.
GET to /posts/{id}: Retrieve details of a specific blog post by its ID.
POST to /posts: Create a new blog post.
PUT to /posts/{id}: Update an existing blog post by its ID.
DELETE to /posts/{id}: Delete a blog post by its ID.
We could also identify resources and endpoints for other entities in our application, like users or comments.

Step 3: Implementation
The implementation of a REST API will vary depending on the programming language and tool you are using. You'll most likely be using a web framework like Express for Node.js, Flask for Python, or the Spring Framework for Java.

Examples of routes/handlers for the GET to /posts endpoint:

// JavaScript example using Express

app.get('/posts', (req, res) => {
  res.json(posts); 
});

Python example using Flask

@app.route('/posts', methods=['GET'])
def get_posts():
    return jsonify(posts)
// Java example using Spring Boot

@GetMapping("/posts")
public List<Post> getPosts() {
    return posts;
}
Consuming Endpoints
The implementation of consuming endpoints from a REST API will also vary depending on the programming language and tool you are using. For now, we can use a simple command-line interface program, cURL. Let's say we want to send a GET request to the /books resources of our API. We can use the following command:

curl https://notarealbooksite.com/books
The result should be the response body and contains JSON data representing a list of books:

{
  "data":
    {
      "id": 1,
      "title": "To Kill a Mockingbird",
      "author": "Harper Lee",
      "genre": "Fiction",
      "year": 1960
    },
    {
      "id": 2,
      "title": "1984",
      "author": "George Orwell",
      "genre": "Dystopian",
      "year": 1949
    },
    {
      "id": 3,
      "title": "The Great Gatsby",
      "author": "F. Scott Fitzgerald",
      "genre": "Classic",
      "year": 1925
    },
    {
      "id": 4,
      "title": "To Kill a Mockingbird",
      "author": "Harper Lee",
      "genre": "Fiction",
      "year": 1960
    }
}

Summary

API stands for Application Programming Interface. It is a medium that allows different applications to communicate programmatically with one another and return a response in real time.
When a request is made via a REST API, it sends a representation of the resource's current state to the requester or endpoint. This state representation can take the form of JSON (JavaScript Object Notation), XML, or HTML.
Exposing means that you enable objects to be accessed through some means. You could let the users create, read, update or delete objects using the HTTP protocol through some predefined way to interact with your objects (an API).
Consuming endpoints involves sending requests to the exposed endpoints of another application or service to access its functionalities or resources. In other words, it's the process of making HTTP requests to interact with an API and retrieve data or perform actions.


## Rest Resources And Url Construction
Learning Objectives

After completing this module, associates should be able to:

Discuss REST Resources and URLs
Description
What is a Resource?
In REST, the primary data representation is called resource. Having a consistent and robust REST resource naming strategy will prove one of the best design decisions in the long term.

The key abstraction of information in REST is a resource. Any information that can be named can be a resource: a document or image, a temporal service (e.g. “today’s weather in Los Angeles”), a collection of other resources, a non-virtual object (e.g., a person), and so on.

In other words, any concept that might be the target of an author’s hypertext reference must fit within the definition of a resource.

A resource is a conceptual mapping to a set of entities, not the entity that corresponds to the mapping at any particular point in time.

Singleton and Collection Resources
A resource can be a singleton or a collection.

For example, customers is a collection resource and customer is a singleton resource (in a banking domain).

We can identify the customers collection resource using the URI /customers. We can identify a single customer resource using the URI /customers/{customerId}.

Collection and Sub-collection Resources
A resource may contain sub-collection resources also.

For example, sub-collection resource accounts of a particular customer can be identified using the URN /customers/{customerId}/accounts (in a banking domain).

Similarly, a singleton resource account inside the sub-collection resource accounts can be identified as follows: /customers/{customerId}/accounts/{accountId}.

URI
REST APIs use Uniform Resource Identifiers (URIs) to address resources. REST API designers should create URIs that convey a REST API’s resource model to the potential clients of the API. When resources are named well, an API is intuitive and easy to use. If done poorly, that same API can be challenging to use and understand.

The constraint of a uniform interface is partially addressed by the combination of URIs and HTTP verbs and using them in line with the standards and conventions.

RESTful URIs should refer to a resource that is a thing (noun) instead of referring to an action (verb) because nouns have properties that verbs do not have, like attributes. Some examples of a resource are:

Below is an example of defintion resources for an application that has user and network device domain entities:

http://api.example.com/device-management/managed-devices 
http://api.example.com/device-management/managed-devices/{device-id} 
http://api.example.com/user-management/users
http://api.example.com/user-management/users/{id}
Resource Archetypes
For more clarity, let’s divide the resource archetypes into four categories (document, collection, store, and controller). Then it would be best if you always targeted to put a resource into one archetype and then use its naming convention consistently.

For uniformity’s sake, resist the temptation to design resources that are hybrids of more than one archetype.

Document
A document resource is a singular concept that is akin to an object instance or database record.

In REST, you can view it as a single resource inside a resource collection. A document’s state representation typically includes both fields with values and links to other related resources.

Use “singular” name to denote document resource archetype.

http://api.example.com/device-management/managed-devices/{device-id}
http://api.example.com/user-management/users/{id}
http://api.example.com/user-management/users/admin
Collection
A collection resource is a server-managed directory of resources.

Clients may propose new resources to be added to a collection. However, it is up to the collection resource to choose to create a new resource or not.

A collection resource chooses what it wants to contain and also decides the URIs of each contained resource.

Use the “plural” name to denote the collection resource archetype.

http://api.example.com/device-management/managed-devices
http://api.example.com/user-management/users
http://api.example.com/user-management/users/{id}/accounts
Store
A store is a client-managed resource repository. A store resource lets an API client put resources in, get them back out, and decide when to delete them.

A store never generates new URIs. Instead, each stored resource has a URI. The URI was chosen by a client when the resource initially put it into the store.

Use “plural” name to denote store resource archetype.

http://api.example.com/song-management/users/{id}/playlists
Controller
A controller resource models a procedural concept. Controller resources are like executable functions, with parameters and return values, inputs, and outputs.

Use “verb” to denote controller archetype.

http://api.example.com/cart-management/users/{id}/cart/checkout http://api.example.com/song-management/users/{id}/playlist/play
Best Practices
Use consistent resource naming conventions and URI formatting for minimum ambiguity and maximum readability and maintainability. You may implement the below design practices to achieve consistency:

Use a forward slash (/) to indicate hierarchical relationships
The forward-slash (/) character is used in the path portion of the URI to indicate a hierarchical relationship between resources. e.g.

http://api.example.com/device-management
http://api.example.com/device-management/managed-devices
http://api.example.com/device-management/managed-devices/{id}
http://api.example.com/device-management/managed-devices/{id}/scripts
http://api.example.com/device-management/managed-devices/{id}/scripts/{id}
Do not use trailing forward slash (/) in URIs
As the last character within a URI’s path, a forward slash (/) adds no semantic value and may confuse. It’s better to drop it from the URI.

http://api.example.com/device-management/managed-devices/ 
http://api.example.com/device-management/managed-devices  /* This is a much better version */
Use hyphens (-) to improve the readability of URIs
To make your URIs easy for people to scan and interpret, use the hyphen (-) character to improve the readability of names in long path segments.

http://api.example.com/devicemanagement/manageddevices/
http://api.example.com/device-management/managed-devices 	/* This is a much better version */
Do not use underscores ( _ )
It’s possible to use an underscore in place of a hyphen to be used as a separator – But depending on the application’s font, it is possible that the underscore (_) character can either get partially obscured or completely hidden in some browsers or screens.

To avoid this confusion, use hyphens (-) instead of underscores ( _ ).

http://api.example.com/inventory_management/managed_entities/{id}/install_script_location 
http://api.example.com/inventory-management/managed-entities/{id}/install-script-location  /* This is a much better version */
Use lowercase letters in URIs
When convenient, lowercase letters should be consistently preferred in URI paths.

http://api.example.org/my-folder/my-doc       //1
HTTP://API.EXAMPLE.ORG/my-folder/my-doc       //2
http://api.example.org/My-Folder/my-doc       //3
In the above examples, 1 and 2 are the same but 3 is not as it uses My-Folder in capital letters.

Do not use file extensions
File extensions look bad and do not add any advantage. Removing them decreases the length of URIs as well. There is no reason to keep them.

Apart from the above reason, if you want to highlight the media type of API using file extension, then you should rely on the media type, as communicated through the Content-Type header, to determine how to process the body’s content.

http://api.example.com/device-management/managed-devices.xml
http://api.example.com/device-management/managed-devices 	/* This is the correct URI */
Use a query component to filter URI collection
Often, you will encounter requirements where you will need a collection of resources sorted, filtered, or limited based on some specific resource attribute.

For this requirement, do not create new APIs – instead, enable sorting, filtering, and pagination capabilities in resource collection API and pass the input parameters as query parameters.

http://api.example.com/device-management/managed-devices
http://api.example.com/device-management/managed-devices?region=USA
http://api.example.com/device-management/managed-devices?region=USA&brand=XYZ
http://api.example.com/device-management/managed-devices?region=USA&brand=XYZ&sort=installation-date
Implementation
Components of a URL
A URL can have many forms. The most general however follows a three-components system as proposed below:

Protocol: HTTP is the protocol here
Hostname: Name of the machine on which the resource lives.
File Name: The pathname to the file on the machine.
Port Number: Port number to which to connect (typically optional).
Good RESTful URL examples
List of magazines:
GET /api/v1/magazines.json HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
Filtering and sorting are server-side operations on resources:
GET /api/v1/magazines.json?year=2011&sort=desc HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
A single magazine in JSON format:
GET /api/v1/magazines/1234.json HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
All articles in (or belonging to) this magazine:
GET /api/v1/magazines/1234/articles.json HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
All articles in this magazine in XML format:
GET /api/v1/magazines/1234/articles.xml HTTP/1.1
Host: www.example.gov.au
Accept: application/xml, text/javascript
Specify query parameters in a comma separated list:
GET /api/v1/magazines/1234.json?fields=title,subtitle,date HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
Add a new article to a particular magazine:
POST /api/v1/magazines/1234/articles.json HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
Bad RESTful URL examples
Non-plural noun:
GET /magazine HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
GET /magazine/1234 HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
Verb in URL:
GET /magazine/1234/create HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
Filter outside of query string:

GET /magazines/2011/desc HTTP/1.1
Host: www.example.gov.au
Accept: application/json, text/javascript
Summary

In REST, the primary data representation is called resource.
Any information that can be named can be a resource:
A document or image
A temporal service (e.g. “today’s weather in Los Angeles”)
A collection of other resources
A non-virtual object (e.g., a person), and so on.
There are four resource archetypes: document, collection, store, and controller
It is best practice to follow resource naming conventions


## Authorization Vs Authentication
Learning Objectives

After completing this module, associates should be able to:

Compare and contrast authorization vs. authentication.
Description

The distinction between authentication and authorization is important in understanding how RESTful APIs work and why connection attempts are either accepted or denied:

Authentication is the verification of the credentials of the connection attempt. This process consists of sending the credentials from the remote access client to the remote access server in an either plaintext or encrypted form by using an authentication protocol.
Authorization is the verification that the connection attempt is allowed. Authorization occurs after successful authentication.
In other words: Authentication is stating that you are who are you are and Authorization is asking if you have access to a certain resource.

Real World Application

There are several common ways to authenticate a user's credentials in web applications:

Username and Password: This is the most common form of authentication. Users provide their username (or email) and password, which are then verified against stored credentials in a database. Passwords are typically hashed and stored securely to protect against security breaches.
Token-Based Authentication:
JSON Web Tokens (JWT): Users authenticate by exchanging their credentials for a JWT, which is a digitally signed token containing user claims (such as user ID, role, etc.). The JWT is then sent with subsequent requests as an authorization header.
OAuth 2.0: Users authenticate using OAuth providers (such as Google, Facebook, or GitHub), which issue access tokens that grant permission to access protected resources. OAuth 2.0 provides a flexible authentication framework for both web and mobile applications.
Two-Factor Authentication (2FA): Users authenticate using two different factors, typically something they know (e.g., password) and something they have (e.g., a mobile device or hardware token). This adds an extra layer of security by requiring additional verification beyond just the password.
Single Sign-On (SSO): Users authenticate once to access multiple related but independent software systems. SSO systems manage authentication centrally, allowing users to access all connected systems with a single set of credentials.
After authenticating a user, there are several common ways to authorize the user to access specific resources or perform certain actions within a web application:

Role-Based Access Control (RBAC): Assign users to roles (e.g., admin, manager, user) and define permissions associated with each role. Users are then authorized based on their assigned role, with access restricted to resources and actions permitted for that role.
Claims-Based Authorization: Use security tokens (such as JWT) containing user claims (e.g., roles, permissions) to authorize access to resources. The server validates the claims in the token and grants access based on the user's claims.
Permission-Based Access Control: Define explicit permissions for each user or user group to access specific resources or perform certain actions. Permissions are typically managed in a permissions database or access control list (ACL), allowing fine-grained control over who can access what.
Each authentication and authorization method has its own advantages, disadvantages, and use cases. The choice of a method depends on factors such as security requirements, user experience, scalability, and integration with existing systems. It's essential to choose a method that balances security with usability and meets the specific needs of your application.

Implementation

Let's consider an example of how authentication and authorization might be implemented for a user trying to access a book website:
Authentication:

When a user visits the book website, they are prompted to log in.
The user enters their username and password into a login form and submits it.
The server verifies the user's credentials against stored data (e.g., database) to authenticate the user.
If the credentials are valid, the server could generate a session token or issues a JWT (JSON Web Token) to the user, indicating that they are authenticated.
The session token or JWT is stored in the client's browser (e.g., as a cookie or in local storage) for subsequent requests.
Authorization:

Once authenticated, the user attempts to access various resources on the website, such as viewing a list of books or adding a book to their reading list.
Each time the user makes a request to access a resource, the server checks their authorization to determine if they have permission to access the requested resource or perform the requested action.
Authorization can be based on various factors, such as the user's role, permissions, or specific attributes.
For example, if the user is an admin, they might have permission to add, edit, or delete books, while regular users might only have permission to view books and add them to their reading list.
The server evaluates the user's permissions and determines whether to allow or deny access to the requested resource or action.
If the user is authorized, the server fulfills the request and returns the appropriate response (e.g., list of books, confirmation of book addition).
If the user is not authorized, the server returns an error or redirects the user to a login page or access-denied page.
Summary

The distinction between authentication and authorization is important in understanding how RESTful APIs are working and why connection attempts are either accepted or denied:
Authentication is the verification of the credentials of the connection attempt. This process consists of sending the credentials from the remote access client to the remote access server in an either plaintext or encrypted form by using an authentication protocol.
Authorization is the verification that the connection attempt is allowed. Authorization occurs after successful authentication.


## Java Introduction to Logback
Learning Objectives

After completing this module, associates should be able to:

Describe logback and why it is needed
Description

Logback is one of the most widely used logging frameworks in the Java Community. It's a replacement for its predecessor, Log4j. Logback offers a faster implementation, provides more options for configuration, and more flexibility in archiving old log files.

Logback Architecture
The Logback architecture is comprised of three classes: Logger, Appender, and Layout.

A Logger is a context for log messages. This is the class that applications interact with to create log messages.
Appenders place log messages in their final destinations. A Logger can have more than one Appender. We generally think of Appenders as being attached to text files, but Logback is much more potent than that.
Layout prepares messages for outputting. Logback supports the creation of custom classes for formatting messages, as well as robust configuration options for the existing ones.
Real World Application
Reasons to prefer logback over log4j 1.x
Faster implementation
Logback internals have been re-written to perform about ten times faster on certain critical execution paths. Not only are logback components faster, they have a smaller memory footprint as well.

Extensive battery of tests
Logback comes with a very extensive battery of tests developed over the course of several years and untold hours of work. While log4j 1.x is also tested, logback takes testing to a completely different level. This is the single most important reason to prefer logback over log4j 1.x. You want your logging framework to be rock solid and dependable even under adverse conditions.

Automatic reloading of configuration files
Logback-classic can automatically reload its configuration file upon modification. The scanning process is fast, contention-free, and dynamically scales to millions of invocations per second spread over hundreds of threads. It also plays well within application servers and more generally within the JEE environment as it does not involve the creation of a separate thread for scanning.

Graceful recovery from I/O failures
Logback's FileAppender and all its subclasses, including RollingFileAppender, can gracefully recover from I/O failures. Thus, if a file server fails temporarily, you no longer need to restart your application just to get logging working again. As soon as the file server comes back up, the relevant logback appender will transparently and quickly recover from the previous error condition.

Automatic removal of old log archives
By setting the maxHistory property of TimeBasedRollingPolicy or SizeAndTimeBasedFNATP, you can control the maximum number of archived files. If your rolling policy calls for monthly rollover and you wish to keep one year's worth of logs, simply set the maxHistory property to 12. Archived log files older than 12 months will be automatically removed.

Automatic compression of archived log files
RollingFileAppender can automatically compress archived log files during rollover. Compression always occurs asynchronously so that even for large log files, your application is not blocked for the duration of the compression.

Implementation
Setup
Maven Dependency
Logback uses the Simple Logging Facade for Java (SLF4J) as its native interface. Before we can start logging messages, we need to add Logback and SLF4J to our pom.xml:

<dependency>
    <groupId>ch.qos.logback</groupId>
    <artifactId>logback-core</artifactId>
    <version>1.2.6</version>
</dependency>

<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-api</artifactId>
    <version>1.7.30</version>
    <scope>test</scope>
</dependency>
Maven Central has the latest version of the Logback Core and the most recent version of slf4j-api.

Classpath
Logback also requires logback-classic.jar on the classpath for runtime.

We'll add this to pom.xml as a test dependency:

freestar
<dependency>
    <groupId>ch.qos.logback</groupId>
    <artifactId>logback-classic</artifactId>
    <version>1.2.6</version>
</dependency>
Basic Example and Configuration
Let's start with a quick example of using Logback in an application.

First, we need a configuration file. We'll create a text file named logback.xml:

<configuration>

  <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
    <encoder>
      <pattern>%date %level [%thread] %logger{10} [%file:%line] - %msg%n</pattern>
    </encoder>
  </appender>

  <appender name="FILE" class="ch.qos.logback.core.FileAppender">
    <file>app.log</file>
    <encoder>
      <pattern>%date %level [%thread] %logger{10} [%file:%line] - %msg%n</pattern>
    </encoder>
  </appender>

  <root level="debug">
    <appender-ref ref="CONSOLE" />
    <appender-ref ref="FILE" />
  </root>

</configuration>
This file is typically located within the src/main/resources folder of a Maven project. In this configuration file, we created an Appender that appends to a file that we named app.log. We also chose a specific pattern for how the log ouput will look. We then configured the root logger to be given the appender we configured so that when we use our root logger in the application, our logs will go to the app.log file.

Next, to use our logger, we need a simple class with a main method:

public class Example {

    private static final Logger logger 
      = LoggerFactory.getLogger(Example.class);

    public static void main(String[] args) {
        logger.info("Example log from {}", Example.class.getSimpleName());
    }
}
This class creates a Logger and calls info() to generate a log message.

When we run Example, we see our message logged to the console.

Summary

Logback is one of the most widely used logging frameworks in the Java community.
It's a replacement for its predecessor, Log4j.
Logback offers a faster implementation, provides more options for configuration, and more flexibility in archiving old log files.
The Logback architecture is comprised of three classes: Logger, Appender, and Layout.
A Logger is a context for log messages. This is the class that applications interact with to create log messages.
Appenders place log messages in their final destinations. A Logger can have more than one Appender. We generally think of Appenders as being attached to text files, but Logback is much more potent than that.
Layout prepares messages for outputting. Logback supports the creation of custom classes for formatting messages, as well as robust configuration options for the existing ones.


## Java Logback Logging Levels
Learning Objectives

After completing this module, associates should be able to:

Describe the different logging levels in logback and how they should be used.
Successfully implement the different logback logging levels in a Java program.
Description

In most logging frameworks like Logback, you will encounter all or some of the following log levels:

TRACE
DEBUG
INFO
WARN
ERROR
The names of some of those give you a hint on what they are about. However, let’s discuss each of them in greater detail.

TRACE
The most fine-grained information only used in rare cases where you need the full visibility of what is happening in your application and inside the third-party libraries that you use. You can expect the TRACE logging level to be very verbose. For example, you can use it to annotate each step in the algorithm or each individual query with parameters in your code.

DEBUG
Less granular compared to the TRACE level, but it is more than you will need in everyday use. The DEBUG log level should be used for information that may be needed for diagnosing issues and troubleshooting or when running application in the test environment for the purpose of making sure everything is running correctly.

INFO
The standard log level indicating that something happened, the application entered a certain state, etc. For example, a controller of your authorization API may include an INFO log level with information on which user requested authorization if the authorization was successful or not. The information logged using the INFO log level should be purely informative and not looking into them on a regular basis shouldn’t result in missing any important information.

WARN
The log level that indicates that something unexpected happened in the application, a problem, or a situation that might disturb one of the processes. But that doesn’t mean that the application failed. The WARN level should be used in situations that are unexpected, but the code can continue the work. For example, a parsing error occurred that resulted in a certain document not being processed.

ERROR
The log level that should be used when the application hits an issue preventing one or more functionalities from properly functioning. The ERROR log level can be used when one of the payment systems is not available, but there is still the option to check out the basket in the e-commerce application or when your social media logging option is not working for some reason.

Real World Application

Logging levels offer several benefits in application development and operations:

Granular Control: Logging levels provide a mechanism for developers and administrators to control the verbosity of log messages produced by an application. By categorizing log messages into different levels based on their severity or importance (e.g., DEBUG, INFO, WARN, ERROR), users can customize the amount of detail captured in log files or displayed in log streams, allowing for fine-grained control over the volume and type of logging output.
Flexible Configuration: Logging frameworks typically allow users to configure the logging levels for different components or modules of an application independently. This flexibility enables users to adjust the logging behavior dynamically at runtime without modifying the application code, facilitating troubleshooting, debugging, and monitoring in various environments (e.g., development, testing, production).
Prioritization of Issues: Logging levels assist in prioritizing issues and focusing attention on critical problems or anomalies that require immediate attention. By categorizing log messages according to their severity, developers and administrators can prioritize their response efforts, focusing on addressing high-priority issues (e.g., errors, warnings) that impact system stability, performance, or security.
In summary, logging levels offer a structured approach to logging that promotes granular control and flexibility. By leveraging logging levels effectively, organizations can enhance the manageability and reliability of their software systems.

Implementation

Below is an example of using different logging levels in an application.

Include the following files, logback.xml, in the src/main/resources folder of your Maven project:

<configuration>

  <appender name="FILE" class="ch.qos.logback.core.FileAppender">
    <file>app.log</file>

    <encoder>
      <pattern>%date %level [%thread] %logger{10} [%file:%line] -%kvp- %msg%n</pattern>
    </encoder>
  </appender>

  <root level="debug">
    <appender-ref ref="FILE" />
  </root>
</configuration>
Main.java:

public class Main {

    //initalize logger object
    private static final Logger logger = LoggerFactory.getLogger(Main.class);

    public static void main(String[] args) {
        
        //adding a trace log to our log file
        logger.trace("Example TRACE log from {}", Main.class.getSimpleName());

        //adding a debug log to our log file
        logger.debug("Example DEBUG log from {}", Main.class.getSimpleName());

        //adding a info log to our log file
        logger.info("Example INFO log from {}", Main.class.getSimpleName());

        //adding a warn log to our log file
        logger.warn("Example WARN log from {}", Main.class.getSimpleName());

        //adding a error log to our log file
        logger.error("Example ERROR log from {}", Main.class.getSimpleName());
    }
}
Summary

In most logging frameworks like Logback, you will encounter all or some of the following log levels:
TRACE
DEBUG
INFO
WARN
ERROR


## Intro to TDD
Learning Objectives

After completing this module, associates should be able to:

Define TDD and Unit Testing
Successfully execute a Java program that demonstrates TDD and Unit Testing
Description
TDD
When developing software, it is important to ensure that most if not all of the code being written is tested to verify the functionality of the code. One way to ensure this is to follow a process called test-driven development, or TDD.

TDD Process
The TDD process consists of writing unit tests first, before the implemented application code has been written. Then, the implemented application code can be written to make the test pass and the process can be completed for each piece of functionality required. Thus, the process is:

Design interface/function
Write tests that use that function in many different ways
Refactor method signatures to ensure that the functions are intuitive in usage
Implement functions
Run tests to verify functionality
Following the TDD process can be useful for ensuring that a valid unit tests always exists for any class or method that is written. Later, when refactoring code, the unit tests give us confidence that we can change the source code without breaking existing functionality. If we mess up somewhere, when the unit tests are run we can pinpoint exactly where the problem lies. This makes debugging much easier.

Unit Testing
Unit testing is the testing of individual software components in isolation from the rest of the system, meaning that we test only certain portions of the final product, rather than the whole product itself. This is done by writing unit tests which execute the code we want to inspect. When the code under test deviates from an expected outcome or behavior, the test will fail. If a test passes, it means the application performs as expected (unless there is a problem with the test itself). In Java, the most common unit testing framework is called JUnit.

Real World Application

TDD has several important benefits:

Improved Code Quality: TDD encourages developers to think about the design and behavior of their code before writing it. By writing tests first, developers focus on defining clear specifications and expectations for the functionality they are implementing. This leads to code that is more modular, maintainable, and less prone to bugs.
Faster Feedback Loop: TDD promotes a rapid feedback loop where developers receive immediate feedback on the correctness of their code. Running tests frequently helps catch errors and regressions early in the development process, reducing the time and effort required for debugging and troubleshooting later.
Simplified Refactoring: TDD provides a safety net for refactoring code. Because tests are written before the code implementation, developers can refactor with confidence, knowing that any changes they make will be validated by the existing tests. This encourages continuous improvement and evolution of the codebase without introducing new bugs or breaking existing functionality.
Documentation and Specification: Tests serve as living documentation for the codebase, providing clear examples of how the code should behave under different conditions. Tests act as executable specifications that describe the intended behavior of the system, making it easier for new developers to understand and contribute to the codebase.
Overall, TDD is an effective practice for improving code quality. By adopting TDD, teams can build software that is more reliable, maintainable, and adaptable to changing requirements and business needs.

Implementation

Below is an example of Test Driven Development that does not use a Unit Testing library or framework.

Step 1: Write the initial tests
In order to know what tests to write, consider what your expectations are. In our example, we want to test a method that returns true if a number is prime and false otherwise. A prime number is a number greater than 1 that can only be divded by itself and 1 without any remainders.

Two expectations that we can have are:

the implementation returns true for a prime number, like 7.
the implementation returns false for a non-prime, like 9.
Let's write the relevant tests.

public static void isPrime_ReturnsTrue_WithPrimeInput() {
   // 1. Set up any information needed
   int testValue = 7;

   // 2. Run the method that is being tested
   boolean actual = isPrime(testValue);
   boolean expected = true;
   
   // 3. Compare actual versus expected results
   if (actual == expected) {
      System.out.println("The test isPrime_ReturnsTrue_WithPrimeInput passed!");
   } else {
      System.out.println("The test isPrime_ReturnsTrue_WithPrimeInput failed!");
      System.out.println("Expected: " + expected + ", Actual: " + actual);
   }
   
}

public static void isPrime_ReturnsFalse_WithNonPrimeInput() {
   // 1. Set up any information needed
   int testValue = 9;

   // 2. Run the method that is being tested
   boolean actual = isPrime(testValue);
   boolean expected =  false;
   
   // 3. Compare actual versus expected results
   if (actual == expected) {
      System.out.println("The test isPrime_ReturnsFalse_WithNonPrimeInput passed!");
   } else {
      System.out.println("The test isPrime_ReturnsFalse_WithNonPrimeInput failed!");
      System.out.println("Expected: " + expected + ", Actual: " + actual);
   }
}
Step 2: Write the implementation
Now we can write the implementation for the method being tested.

public static boolean isPrime(int number) {
   if (number <= 1) {
      return false;
   }
   for (int i = 2; i <= Math.sqrt(number); i++) {
      if (number % i == 0) {
            return false;
      }
   }
   return true;
}
Step 3: Run tests
Run your tests by calling them in your main() method. If a test fails, double check your implementation logic.

It’s easy to get sidetracked and miss out on code optimizations. It’s the developer’s responsibility to take a step back and analyze the code to see if any improvements can be added or not. A few helpful tips to stay on track are:

Make a change, run the test(s) and get the feedback
If you get stuck, go back to your last predictable place when you knew exactly what was happening
Simplify the logic as much as possible
Use the IDE tools to check language syntax
Summary

When developing software, it is important to ensure that most if not all of the code being written is tested to verify the functionality of the code. One way to ensure this is to follow a process called test-driven development, or TDD.
The TDD process consists of writing unit tests first, before the implemented application code has been written. Then, the implemented application code can be written to make the test pass and the process can be completed for each piece of functionality required.
Unit testing is the testing of individual software components in isolation from the rest of the system. This is done by writing unit tests which execute the code we want to inspect.


## Intro to Junit
Learning Objectives

After completing this module, associates should be able to:

Describe how JUnit works for testing Java programs
Successfully execute a Java program that demonstrates JUnit's functionality.
Description

Unit Testing can be done via two approaches, manual testing or automated testing. In both the approaches the workflow remains common:

Creating a test case
Reviewing it
Rework if corrections needed
Execute the test case
Analyze the test results
Automated Testing is preferred over manual testing for the following reasons:

Manual testing requires a great deal of human effort.
Manual testing also requires more resources to be allocated.
Schedule constraints may put time allocated for testing at a premium.
Developers can use a testing framework for creating automated unit test cases. The following are common features that testing frameworks provide:

In order to verify if the code is working as expected, test cases with specific checkpoints or verification criteria are created.
The tests are executed by the framework.
The framework will report a summarized result on the passed test cases and failed ones.
What Is JUnit?
JUnit is an open-source framework that is used for writing and executing unit tests in Java programming language. It is one of the best-known testing frameworks.

Some characteristics of JUnit:

Annotations are used to support, identify, and execute test method features.
Assertions are used to check actual output versus expected output.
It provides a Test Runner for executing the tests.
JUnit provides a basic built-in template so that you may write small, simple test cases in no time.
JUnit tests help you to write independent modules, thereby bettering the coverage of the test and the quality of the application.
Examples Of JUnit Test cases
Given below are the two examples of a very basic Hello World program to get an understanding of how a JUnit test class looks like or how different does it look when compared with a usual Java class file.

Example 1
Here is a JUnit testcase HelloWorldJUnit.java that verifies that the string “Hello world” matches the string “hello world” which fails on execution, as the match is case sensitive. Hence, the two strings don’t match and the test fails.

The code for HelloWorldJUnit.java

package demo.tests;
import static org.junit.Assert.*;
import org.junit.Test;
public class HelloWorldJUnit {
    @Test
    public void test() {
        assertEquals("Hello world","hello world");  
    }
}
Example 2
Here, we will see how a typical Java class interacts with a JUnit test case. We create a Java class file HelloWorld_Java.java with a constructor that allows us to pass a String value and a method getText() to fetch the string value.

In the JUnit Test class HelloWorldJUnit.java has a test that creates a HelloWorld_Java object and passes a String into the constructor call. The assertEquals() from JUnit verifies if the expected and actual string values match.

The code for HelloWorld_Java.java:


public class HelloWorld_Java {
    
    private String s;
    public HelloWorld_Java(String s){
        this.s = s;
    }

    public String getText(){
        return this.s;
    }
}
The code for HelloWorldJUnit.java:

public class HelloWorldJUnit{
    
    @Test
    public void test() {
        HelloWorld_Java hw=new HelloWorld_Java("Hello World");
        assertEquals(hw.getText(),"Hello World");   
    }
}
Since the strings match, the code will pass the JUnit test.

Real World Application
Unit Testing
Generally, software goes under four level of testing: Unit Testing, Integration Testing, System Testing, and Acceptance Testing but sometimes due to time constraints, software testers do minimal unit testing.

Some crucial reasons for unit testing are listed below:

Unit testing helps testers and developers understand the root causes of errors.
Unit testing helps with documentation.
Unit testing fixes defects early in the development phase.
Unit testing helps with code reusability.
Here are some best practices for using JUnit in the real world:

Always test Core Methods:
It's not always possible to get 100% code coverage, so don't aim to write unit tests for each method. Instead, write unit tests for a method that is likely to have bugs during maintenance.
Always test core method and core classes which are used heavily by different parts of a program.
Run the JUnit test as part of the Build Process
You should Integrate Junit tests with your build script so that with every compile your tests run automatically.
Always Test for boundary Conditions
Develop test cases based on usage and boundary conditions.
Align Tests with Business Requirements
Make sure your JUnit test is aligned with your business requirement specifications.
Tests for Non-Functional Requirements
Write a test for the non-functional requirements as well.
For example, while writing a Thread-safe class, it's important to write tests that try to break thread safety.
Test for Ordering
If a function or method depends on the order of events then make sure your JUnit test covers ordering requirements.
Test both correct ordering and incorrect ordering.
Use Tools
You should always use available tools provided by your framework.
Implementation
Example
In this example we are implementing a Temperature Converter that either converts a Fahrenheit temperature to Celsius or vice versa.

First we design the interface:
package tdd;

// 1. design our interface
public interface TemperatureConverter {


    double farenheitToCelcius(double temp);
    double celciusToFarenheit(double temp);

    double convertTemp(double temp, String from, String to);

}
Then we write the tests:
package tdd;

import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;
import org.opentest4j.AssertionFailedError;

// 1. So long as the Test method finishes without throwing an exception as test is considered passed
// 2. Assertions are just methods that throw an exception if the assertion is false
// 3. All Test methods are VOID. That is a JUnit thing.

// TDD is more than just seeing if your code works
// they are instrumental in DESIGNING your applications

public class TemperatureTests {

    TemperatureConverter converter = new TempConverterImpl();

    @Test
    void celcius_to_farenheit(){ // only acceptable use case of underscores in Java
        double result = this.converter.celciusToFarenheit(100);
        Assertions.assertEquals(212, result, 0.1);// by how much it can be off
    }

    @Test
    void farenheit_to_celcius(){
        double result = this.converter.farenheitToCelcius(212);
        Assertions.assertEquals(100,result, 0.1);
    }

    @Test // negative test. makes sures something fails appropriately
    void absolute_Zero_raises_exception(){
        Assertions.assertThrows(RuntimeException.class, ()->{
            this.converter.celciusToFarenheit(-400);
        });
    }

    @Test
    void kelvin_celcius(){
        // do we care if it is case sensitive?
        // what do we do if it misspelled?
        // should we allow the letter k,s,c to indicate temperature?
        double result = this.converter.convertTemp(100,"kelvin", "celcius");
        Assertions.assertEquals(-173.15,result);
    }

    @Test
    void case_insensitive(){
        double result = this.converter.convertTemp(100,"KeLVin", "cElcIus");
        Assertions.assertEquals(-173.15,result);
    }

}
Then we write the code:
package tdd;

public class TempConverterImpl implements TemperatureConverter{

    @Override
    public double farenheitToCelcius(double temp) {
        double celcius = (temp-32)*5/9;
        return celcius;
    }

    @Override
    public double celciusToFarenheit(double temp) {
        if(temp <= -273.15){
            throw  new RuntimeException("Input was below absolute zero");
        }
        double f = (9d/5d*temp) + 32;
        return f;
    }

    @Override
    public double convertTemp(double temp, String from, String to) {
        return 0;
    }
}
(Temperature Converter courtesy of Adam Ranieri; https://github.com/adamranieri/2203Java/tree/main/Day4/src/tdd)

Exercise: A Banking account
For this exercise, you will create two files: Account.java and AccountTest.java.

The objective of this assignment is to write a class to demonstrate bank account functionality, but then create a corresponding JUnit test case to ensure its correctness.
The Account.java class has to have the following methods and member variables.
A double member variable to hold the current account balance.
public Account() {...} The default constructor should initialize the balance to 0.0.
public void deposit(double amount) {...} A deposit method to add money to the account.
public double withdraw(double amount) {...} A withdraw method that withdraws the given amount from the account. If the amount given can be withdrawn, it should return that amount. If not, it should return 0.0.
public double getBalance() {...} A method to get the current balance in the account.
Summary

JUnit is an open-source framework that is used for writing and executing unit tests in Java programming language. It is one of the best-known unit testing frameworks.
Annotations are used to support, identify, and execute test method features.
Assertions are used to check actual output versus expected output.
It provides Test Runner for executing the tests.
JUnit provides a basic built-in template so that you may write small, simple test cases in no time.
JUnit tests help you to write independent modules, thereby bettering the coverage of the test and the quality of the application.


## Junit Annotations and Assertions
Learning Objectives

After completing this module, associates should be able to:

Define the terms annotations and assertions with respect to JUnit
Successfully execute a Java program that demonstrates annotations and assertions by testing code
Description
What’s new in JUnit 5?
JUnit 5 is a complete rewrite of the framework and requires Java 8, though it can test Java code that is written in older Java versions. It allows using lambdas in assertions, a feature you might already know from the popular assertion library AssertJ. Unlike its predecessor, JUnit 5 is not an all-in-one library but instead provides a set of well-structured modules.

JUnit 5 tests look a lot like JUnit 4 tests: just create a class and add test methods annotated with @Test. However, JUnit 5 provides a completely new set of annotations which resides in a different package than their JUnit 4 counterparts. In addition, the assertion methods moved from org.junit.Assert to junit.jupiter.api.Assertions.

JUnit 5 Assertions
The available assertion methods in JUnit 5 are similiar to JUnit 4. The Assertions class provides assertTrue, assertEquals, assertNull, assertSame and their negative equivalents. What’s new is the overloaded versions of these methods that expect a lambda expression to supply the assertion message or boolean condition. On top of that, there is a new feature called Grouped Assertions which allows the execution of a group of assertions and have failures reported together. It was previously bad practice to put multiple assertions into one test to avoid assertions not being executed after a previous failure. Now we can group the respective assertions together.

Another improvement over JUnit 4 is the way to assert on expected exceptions. Instead of putting the expected exception type into the @Test annotation or wrapping your test code into a try-catch, you can use assertThrows and equalsThrows.

Testing Best Practices
When it comes to testing code, a few best practices to follow include:

Utilize dependency injection
Write testable code
Use a mocking library like Mockito for dependencies
Measure your code coverage with a tool like Jacoco
Externalize test data when possible (i.e. read in the test data from an external file or generate it dynamically)
Generally, you still want to use only 1 assert statement per test - this ensures you can pinpoint the defect when debugging
Write deterministic tests (they shouldn't fail sometimes and pass other times - otherwise known as "flaky" tests)
Real World Application

The following code is an example of using annotations and assertions in JUnit. Here, we have a class called NumberUtility that will accept an integer as an input and determine if it is prime, returning a Boolean value depending on if the integer is prime. We will use JUnit annotations and assertions to test the program.

First, this the NumberUtility program:

package com.revature.utility;

public class NumberUtility {

	// true if a number is prime
	// false if a number is not prime
	// Prime: a number that can be divided only by
	// itself and 1
	// 2, 3, 5, 7, 11, 13
	public boolean isPrime(int number) {
		if (number <= 1) { // edge case
			return false;
		}
		
		for (int i = 2; i < number; i++) {
			if (number % i == 0) {
				return false;
			}
		}
		
		return true;
	}
	
}
Following this is a class called NumberUtilityTest which uses assertions and annotations and assertions to test the NumberUtility class:

package com.revature.utility;

import org.junit.jupiter.api.AfterAll;
import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

// This class will contain various test methods
// for the NumberUtility class
public class NumberUtilityTest {

	
	public static NumberUtility nu;
	// @Test: used to signify that a method is a test case
	// @BeforeAll: used to signify that a method is going to run
	// before all of the test case methods
	// @BeforeEach: used to signify that a method is going to run
	// before each test case
	// @AfterEach: used to signify that a method is going to run
	// after each test case
	// @AfterAll: used to signify that a method is going to run
	// after all of the test case methods
	
	@BeforeAll
	public static void setup() {
		nu = new NumberUtility();
		System.out.println("Runs once before all test methods in the class");
	}
	
//	@BeforeEach
//	public void init() {
//		System.out.println("Runs before each test method");
//	}
//	
//	@AfterEach
//	public void tearDown() {
//		System.out.println("Runs after each test method");
//	}
//	
//	@AfterAll
//	public static void done() {
//		System.out.println("Runs once after all test methods in the class");
//	}
	
	
	// We will write some unit tests inside of this class
	
	@Test
	public void testIsPrime_1_shouldBeFalse() {
		
		/*
		 * AAA
		 * 
		 * Arrange: set up the required objects/configurations
		 * Act: use (invoke) whatever you want to test
		 * Assert: check to see if the output is what you expected
		 * 
		 */
		
		// Arrange
//		NumberUtility nu = new NumberUtility();
		
		// Act
		boolean actual = nu.isPrime(1); // actual output
		
		// Assert
		boolean expected = false;
		Assertions.assertEquals(expected, actual);
	}
	
	@Test
	public void testIsPrime_13_shouldBeTrue() {
		// Arrange
//		NumberUtility nu = new NumberUtility();
		
		// Act
		boolean actual = nu.isPrime(13);
		
		// Assert
		Assertions.assertTrue(actual);
	}
	
	@Test
	public void testIsPrime_negative1000_shouldBeFalse() {
		// Arrange
//		NumberUtility nu = new NumberUtility();
		
		// Act
		boolean actual = nu.isPrime(-1000);
		
		// Assert
		Assertions.assertFalse(actual);
	}
	
}
Implementation

For the following coding examples showcase how to create unit tests using JUnit 5.

@Test Annotation
class JUnit5TestAnnotationTest{
    @Test
    void myTestAnnotationTest() {
        assertEquals(4, 2 + 2);
    }
}
@ParameterizedTest
@ParameterizedTest
@ValueSource(strings = { "Jack", "Jane", "Michael" })
void genderByNameTest(String name) {
    assertTrue(isMan(name));
}
@RepeatedTest
@RepeatedTest(10)
void repeatedTest() {
   // Repeats 10 times.
}

@RepeatedTest(3)
void repeatedTestWithRepetitionInfo(RepetitionInfo repetitionInfo) {
    assertEquals(3, repetitionInfo.getTotalRepetitions());
}

@RepeatedTest(value = 1, name = "{displayName} {currentRepetition}/{totalRepetitions}")

@DisplayName
@DisplayName("My Test Class Name")
class DisplayNameTest {
    @Test
    @DisplayName("My Test Name")
    void testWithDisplayName() {
    }
}
@BeforeEach & @AfterEach
class MyTestClass{
    @BeforeEach
    void setup() {
        log.info("@BeforeEach executes before each test method in this class.");
    }
    @Test
    void myTestMethod() {
    }
    @AfterEach
    void tearDown() {
        log.info("@AfterEach executes after each test method in this class.");
    }
}
@BeforeAll & @AfterAll
class MyTestClass{
    @BeforeAll
    static void initAll() {
          log.info("@BeforeAll executes once before all test methods in this class.");
    }
    @Test
    void myTestMethod() {
    }
    @AfterAll
    static void tearDownAll() {
          log.info("@AfterAll executes once after all test methods in this class.");
    }
}
@Nested
The nested annotation denotes that the annotated class is nested. It is a non-static test class. @BeforeAll and @AfterAll methods cannot be used directly in a @Nested test class.

@DisplayName("A stack")
class TestingAStackDemo {
    Stack<Object> stack;
    @Test
    @DisplayName("is instantiated with new Stack()")
    void isInstantiatedWithNew() {
        new Stack<>();
    }
    @Nested
    @DisplayName("when new")
    class WhenNew {
        @BeforeEach
        void createNewStack() {
            stack = new Stack<>();
        }
        @Test
        @DisplayName("is empty")
        void isEmpty() {
            assertTrue(stack.isEmpty());
        }
        @Test
        @DisplayName("throws EmptyStackException when popped")
        void throwsExceptionWhenPopped() {
            assertThrows(EmptyStackException.class, () -> stack.pop());
        }
        @Test
        @DisplayName("throws EmptyStackException when peeked")
        void throwsExceptionWhenPeeked() {
            assertThrows(EmptyStackException.class, () -> stack.peek());
        }
        @Nested
        @DisplayName("after pushing an element")
        class AfterPushing {
            String anElement = "an element";
            @BeforeEach
            void pushAnElement() {
                stack.push(anElement);
            }
            @Test
            @DisplayName("it is no longer empty")
            void isNotEmpty() {
                assertFalse(stack.isEmpty());
            }
            @Test
            @DisplayName("returns the element when popped and is empty")
            void returnElementWhenPopped() {
                assertEquals(anElement, stack.pop());
                assertTrue(stack.isEmpty());
            }
            @Test
            @DisplayName("returns the element when peeked but remains not empty")
            void returnElementWhenPeeked() {
                assertEquals(anElement, stack.peek());
                assertFalse(stack.isEmpty());
            }
        }
    }

Exercises (Optional)
If not done previously, do the first two steps. If you have already done Steps 1 and 2, proceed to Step 3

Step 1
Create a MathOps class and create the following method:
add : Accept two int parameters, and return their sum.
Create the TestMathOps class and demonstrate the usage of the annotations and assert methods. Run the test and show the output.
Step 2
Create test methods for subtract and divide methods before writing them in MathOps.

Step 3
Create the ParameterizedMathTest class and explain the RunWith and Parameter annotations. Build the class and its test method.


## Intro to Mockito
Learning Objectives

After completing this module, associates should be able to:

Describe the need for mocking with respect to unit testing.
Successfully implement mocking to test a Java program.
Description
What is Mockito?
Mockito is a popular open source framework for mocking objects (mock objects) and methods (method stubs) in software tests. Using Mockito greatly simplifies the development of tests for classes with external dependencies.

A mock object is a dummy implementation for an interface or a class. It allows to define the output of certain method calls. They typically record the interaction with the system and so that you can perform behavior testing. Behavior testing is where you verify the interactions between the objects in your code rather than just the output or state changes of those objects. There are also spies, which are partial mocks and by default have the actual implementation of the methods rather than dummy implementations.

Recent versions of Mockito can also mock static methods and final classes. Also, private methods are not visible for tests, so they can not be mocked.

During testing, a stub is a piece of code that takes place of another component.

Mocks and Stubs are Java classes and methods that are used in place of external dependencies. These classes are created before running tests to allow a developer to dictate expected behaviors from integrations, and focus on local methods, typically for unit tests.

More specifically:

A stub is an entity that has its return values pre-programmed. It is injected into the test class so that you have complete control over the input being tested.
A mock is a class whose interactions with the test class can be evaluated after the test is over. For example, you can ask it whether one of its methods was called or how many times it was called. Classes with unintended consequences that must be investigated, like those that send emails or communicate data to another external service, are common candidates for mocks.
a spy is a class that uses the real implementation of methods unless we choose to stub them. This is different from mocks in that, if we choose to not stub a mock's method, it will still not have an implementation and the return value is a default fake value, like 0 or null.
Real World Application

From the Mockito website:

Mockito is a mocking framework that tastes really good. It lets you write beautiful tests with a clean & simple API. Mockito doesn’t give you hangover because the tests are very readable and they produce clean verification errors.

Massive StackOverflow community voted Mockito the best mocking framework for java. Even though StackOverflow shuns questions that likely raise emotional debates the fact is Mockito has the most votes.

Top 10 Java library across all libraries, not only the testing tools. In late 2013 there was an analysis made of 30.000 GitHub projects. Although Mockito reached number 9 in the main report, mockito-core and mockito-all are the same tool and therefore the factual position of Mockito is number 4, surpassing famous tools like Guava or Spring. Treat this study as an indicator of a big impact that Mockito makes every day on unit tests written in Java.

Dan North, the originator of Behavior-Driven Development wrote this back in 2008:

“We decided during the main conference that we should use JUnit 4 and Mockito because we think they are the future of TDD and mocking in Java”
Given the current popularity of Mockito, Dan was spot on with his prediction.

Use your own judgement in choosing a testing framework. The Mockito team always respects your choice. Keep writing great tests every day!

Implementation
Adding Mockito to a project
Using the Mockito libraries should be done with a modern dependency system like Maven or Gradle. All modern IDEs (Eclipse, Visual Studio Code, IntelliJ) support both Maven and Gradle.

The following contains detailed descriptions for your environment, pick the one which is relevant for you. The latest version of Mockito can be found via https://search.maven.org/artifact/org.mockito/mockito-core.

The Pet.java Class
Below is a class we will be using in this example, Pet.java:

public class Pet {
    
    private int id;
    private String name;
    private int age;
    private String species;
    private int vetId;

    // ... constructors, getters/setters omitted

    public String talk() {
        return "animal noise";
    }

    public String bow() {
        return "the pet bows";
    }
    
    public void doTrick() {
        talk();
        bow();
        talk();
    }
}
Please note imports and other code is excluded from these examples for simplicity.

Creating Mocks
Using the mock() method:

public class PetTest {
    
    Pet petMock = mock(Pet.class);

    // ... tests omitted
}
Using the @Mock annotation and the openMocks() method:

public class PetTest {
    
    @Mock
    Pet petMock;

    @BeforeEach
    public void setUpTests() {
        MockitoAnnotations.openMocks(this);
    }

    // ... tests omitted
}
Using the @Mock annotation and the @ExtendWith() annotation:

@ExtendWith(MockitoExtension.class)  
public class PetTest {
    
    @Mock
    Pet petMock;

    // ... tests omitted
}
Please note that the @ExtendWith() annotation is included in a separate dependency: https://mvnrepository.com/artifact/org.mockito/mockito-junit-jupiter

Using Stubs
Stubbed methods return dummy values by default. Now that we've mocked a pet object, when we get the returned value of its stubbed method, talk(), we can assert that the value is null.

Checking that the stubbed method returns null:

 @Test
public void talk_returnsNull() {
    String actual = petMock.talk();
    String expected = null;

    assertEquals(expected, actual); // test passes
}
To change the behavior of a stubbed method, you can use the when() method. It takes in the method whose behavior you want to modify as an argument. You can then chain an OngoingStubbing call to do the modification. Here's an example of using the thenReturn() method:

    @Test
    public void bow_returnsSomethingDifferent() {
        when(petMock.bow()).thenReturn("this other String!");
    }
Sometimes in testing you would want to test behavior if an exeption is thrown. To throw exceptions in your tests, you can use thenThrow():

    @Test
    public void bow_throwExceptionJustBecause() {
        when(petMock.bow()).thenThrow(Exception.class);
    }
Stubbing void methods requires the use of a different syntax because the when() method does not like void methods, but the results are the same.

@Test
public void doTrick_throwsExceptionJustBecause() {
    // when(petMock.doTrick()).thenThrow(Exception.class); // compiler error!
    doThrow(Exception.class).when(petMock).doTrick(); // this works!
}
You can find more information about this family of methods in the documentation: https://javadoc.io/doc/org.mockito/mockito-core/1.10.19/org/mockito/Mockito.html#do_family_methods_stubs

For more flexibility with stubbing, you can look into argument matchers: https://javadoc.io/static/org.mockito/mockito-core/1.10.19/org/mockito/Matchers.html

Creating Spies
You would create a spy just as you would your mock, with any of the three methods mentioned. The only difference is you are replacing the word Mock or mock with Spy or spy in your syntax. An example for creating a spy:

public class PetTest {
    
    Pet petSpy = spy(Pet.class);

    // ... other code omitted
}
Behavior Testing
Checking that the spy's not stubbed doTrick() method does call talk() at least once:

@Test
    public void doTrick_callsTalkAtLeastOnce() {
        petSpy.doTrick();

        verify(petSpy, atLeastOnce()).talk(); // test passes
    }
You can use the Mocikto class's verify() method to test the interactions between objects and their method calls. The first argument is the object to test, the second is the Verification Mode and then you specify the method you want to verify.

You can also verify the order of the calls or that an interaction never happened.

Dependency Injection
Mockito provides an annotation for injecting mock objects into dependent objects. Let's add to our example code to demonstrate this annotation. We will now have a PetService interface and an implemention class, PetServiceImpl. This layer of our application depends on a DAO layer. We can see the dependency in the PetServiceImpl class. Our goal in this section is to test the service layer, not the DAO layer.

public interface PetService {

    public Pet getPetById(int id);

    public List<Pet> getAllPetsByVetId(int id);
    
    // ... other methods omitted
}

public class PetServiceImpl implements PetService {

    // dependency
    PetDAO petDAO;

    public PetServiceImpl(PetDAO petDAO) {
        this.petDAO = petDAO;
    }

    @Override
    public Pet getPetById(int id) {
            return petDAO.getPetById(id);
    }

    @Override
    public List<Pet> getAllPetsByVetId(int id) {
        List<Pet> allPetsReturned = petDAO.getAllPets();
		List<Pet> allPetsById = new ArrayList<>();
		
		for (Pet pet : allPetsReturned) {
            if (pet.getVetId() == id) {
                allPetsById.add(pet);
            }
        }
	
		return allPetsById;
    }

    // ... other methods omitted

}
Let's write out test class:

public class PetServiceTest {

    // specify which object to inject into
    @InjectMocks
    private PetService petService = new PetServiceImpl(null);

    // specify what to inject
    @Mock
    private PetDAO petDAOMock;

    // create a variable that keeps track of mocked objects
    private AutoCloseable openMocks;
    
    @BeforeEach
    public void setUpTests() {
        // does actual injecting
        openMocks = MockitoAnnotations.openMocks(this);
    }

    // ... tests omitted

    @AfterEach
    public void tearDownTests() throws Exception {
        // release mocks
        openMocks.close();
    }
}
Notice how we use the @InjectMocks annotation to specify which object needs its dependency injected. We then created a mock, and then in the @BeforeEach setup method, we do the actual injecting. Note that the @InjectMocks annotation only works with fields annotated with @Mock or @Spy. MockitoAnnotations.openMocks() returns an instance of AutoCloseable. This allows us to keep track of our mocks and release their resources when we are finished with them.

Let's do an example of a service method test:

@Test
public void getPetById_succeeds() {
    // arrange: create dummy info for stubbed method to return
    Pet pet = new Pet(1, "Dobby", 3, "Dog", 1);
    when(petDAOMock.getPetById(1)).thenReturn(pet);

    // act: use actual implementation of object you are trying to test
    Pet petReturned = petService.getPetById(1);

    // assert: service layer method should return whatever DAO layer method did
    assertEquals(pet, petReturned);
}
This next example is a little more complex:

@Test
public void test_getAllPetsByVetId_returned_list_is_of_correct_size() {
    // arrange
    List<Pet> testList = new ArrayList<>();
    Collections.addAll(testList, new Pet(1, "Candy", 1, "Dog", 1),
                            new Pet(2, "Torchy", 3, "Cat", 2),
                            new Pet(3, "Egg", 8, "Bird", 3),
                            new Pet(4, "Whiskers", 3, "Cat", 2),
                            new Pet(5, "Joe", 10, "Dog", 1));
    
    when(petDAOMock.getAllPets()).thenReturn(testList);

    // act
    List<Pet> listReturned = petService.getAllPetsByVetId(2);

    // assert
    assertTrue(listReturned.size() == 2);
}
This example should also drive home the point that we want to test the service layer, not the DAO layer. We control the data we want the DAO layer to return so that the test results not vary. If we chose not to mock the DAO layer, we may get a different set of pets because the database data may change. What we want to test is whether our getAllPetsByVetId method works as intended with whatever information it receives from the DAO layer.

For more information about the @InjectMocks annotation, you can look at the relevant documentation: https://javadoc.io/doc/org.mockito/mockito-core/3.6.28/org/mockito/InjectMocks.html

Summary

Mockito is a popular open source framework for mocking objects in software test. Using Mockito greatly simplifies the development of tests for classes with external dependencies.

A mock object is a dummy implementation for an interface or a class. It allows to define the output of certain method calls. They typically record the interaction with the system and tests can validate that.

Recent versions of Mockito can also mock static methods and final classes. Also, private methods are not visible for tests, so they can not be mocked.

Mockito records the interaction with mock and allows you to check if the mock object was used correct, e.g. if a certain method has been called on the mock. This allows you to implement behavior testing instead of only testing the result of method calls.


## Mocking the DAO
Learning Objectives

After completing this module, associates should be able to:

Describe mocking the DAO
Successfully implement a Java program that mocks the DAO.
Description
The DAO Layer and the Service Layer
The DAO layer is responsible for interacting with the underlying data storage mechanism, such as a database or a web service. It encapsulates data access logic and provides an abstraction over the data persistence mechanism. DAOs handle CRUD (Create, Read, Update, Delete) operations and database transactions.

The service layer contains the business logic of the application. It orchestrates the interactions between different components, including DAOs, to fulfill application requirements. Services implement use cases, enforce business rules, perform validation, and coordinate transactions across multiple DAOs.

The service layer depends on the DAO layer to access and manipulate data. Services contain DAO instances that can be used to call their respective CRUD operation methods.

Testing the Service Layer
When testing the service layer, DAOs are often mocked or stubbed to isolate the service logic from database interactions. Mocking DAOs allows for unit testing of service methods with controlled data scenarios. DAOs are tested separately to ensure correct database interactions and query execution, but these are integration tests, not unit tests.

We can use Mockito to mock the DAO instance. The @InjectMocks annotation can be used to automatically inject mocked dependencies into the fields of the target object being tested. It simplifies the process of setting up tests by automatically creating mocks for the dependencies of the target object and injecting them during test initialization.

Real World Application

Understanding how to mock dependencies and test the service layer is crucial for several reasons:

Isolation of Unit Tests: By mocking dependencies, you can isolate the code under test from its collaborators, such as databases, external APIs, or third-party services. This allows you to focus on testing the logic specific to the service layer without the need for complex setup or external dependencies.
Flexibility in Test Scenarios: Mocking dependencies gives you control over the behavior and responses of external components, allowing you to simulate various scenarios and edge cases that may be difficult to reproduce in a real-world environment. This enables thorough testing of error handling, edge cases, and exceptional conditions within the service layer.
Enhances Code Quality and Maintainability: Writing tests for the service layer with mocked dependencies encourages modular, loosely coupled design principles, leading to cleaner, more maintainable code. It also provides documentation of the service layer's behavior and interactions, aiding in understanding and debugging.
In summary, understanding how to mock dependencies and test the service layer is essential for effective and comprehensive unit testing of your application's business logic and behavior.

Implementation
Example Introduction
The use case under test is finding a birthday celebration that actually happened. Let's assume such birthdays were stored in a database with their year, but we don't know which was the latest, as celebrations did not happen every year. Thus the business logic is searching back from a given year to find the most recent celebration.

It is this logic that I want to test, and nothing else. I am not interested in testing database access, further I don't want to rely on data stored in a database.

Technically I know that the service uses a DAO, and I know the DAO class so that I can mock it. I also have access to the service implementation class, so that I can construct it. Nevertheless the DAO is a private hidden field in the service. The challenge is to write a unit test, using Mockito, that asserts that the service actually searches backwards for a birthday celebration.

Getting Set Up
The dependencies in the pom.xml:

<project xmlns="http://maven.apache.org/POM/4.0.0"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>fri</groupId>
    <artifactId>mockitoTest3</artifactId>
    <version>0.0.1-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.12</version>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <version>3.1.0</version>
        </dependency>

    </dependencies>

</project>
Java Source Code
The below code is the service interface. The comment on the findBirthdayCelebration() method describes the business logic. (Mind that methods in interfaces don't need to be declared as public, because everything in a Java interface implicitly is public.)

import java.util.Date;

public interface BirthdayCelebrationService
{
    /**
     * @param startYear the year where to start searching backward.
     * @return the most recent year when your birthday celebration actually happened.
     */
    Date findBirthdayCelebration(int startYear);
}
The service implementation contains the business logic that I want to test:

import java.util.Date;
import fri.mockitoTest.service.BirthdayCelebrationService;

public class BirthdayCelebrationServiceImpl implements BirthdayCelebrationService
{
    private static final int MAXIMUM_YEARS_BACK = 100;
    
    private BirthdayCelebrationDao dao = new BirthdayCelebrationDao();
    
    public Date findBirthdayCelebration(int startYear) {
        Date found = null;
        int year = startYear;
        for (int count = 0; found == null && count < MAXIMUM_YEARS_BACK; count++)   {
            found = dao.findBirthdayCelebration(year);
            year--;
        }
        return found;
    }
}
Finally here is the DAO. This normally encapsulates persistence access (database):

import java.util.Date;

public class BirthdayCelebrationDao
{
    public Date findBirthdayCelebration(int year) {
        // TODO: read from database for given year
        return null;
    }
}
As the DAO will be mocked and this is just a demo example, I did not implement anything here.

The Unit Test
import static org.junit.Assert.*;
import static org.mockito.Mockito.*;
import java.util.Calendar;
import java.util.Date;
import org.junit.Before;
import org.junit.Test;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;
import fri.mockitoTest.service.BirthdayCelebrationService;
import fri.mockitoTest.service.impl.BirthdayCelebrationDao;
import fri.mockitoTest.service.impl.BirthdayCelebrationServiceImpl;

/**
 * How to mock a DAO in an existing Service, so that we can
 * test service functionality without needing a real DAO.
 */
public class BirthdayCelebrationServiceTest
{
    @Mock
    private BirthdayCelebrationDao dao;
    
    @InjectMocks
    private BirthdayCelebrationService service = new BirthdayCelebrationServiceImpl();
    
    @Before
    public void initMocks() {
        MockitoAnnotations.initMocks(this);
    }
    
    @Test
    public void whenNoBirthdayCelebrationThenSkipToPreviousYear_Success()  {
        // set up test data
        final int LATEST_BIRTHDAY_CELEBRATION_YEAR = 2017;
        final Date expectedBirthdayCelebration = newBirthdayDate(LATEST_BIRTHDAY_CELEBRATION_YEAR);
        final int START_YEAR = LATEST_BIRTHDAY_CELEBRATION_YEAR + 2;
        
        // set up behavior
        for (int year = START_YEAR; year > LATEST_BIRTHDAY_CELEBRATION_YEAR; year--)
            when(dao.findBirthdayCelebration(year)).thenReturn(null);
        when(dao.findBirthdayCelebration(LATEST_BIRTHDAY_CELEBRATION_YEAR)).thenReturn(expectedBirthdayCelebration);
        
        // take action
        final Date birthdayCelebration = service.findBirthdayCelebration(START_YEAR);
        
        // assert result
        assertEquals(expectedBirthdayCelebration, birthdayCelebration);
    }

    private Date newBirthdayDate(int year) {
        final Calendar calendar = Calendar.getInstance();
        calendar.set(Calendar.MONTH, Calendar.MAY);
        calendar.set(Calendar.DAY_OF_MONTH, 12);
        calendar.set(Calendar.YEAR, year);
        return calendar.getTime();
    }
}
The static imports from JUnit and Mockito in lines 1 and 2 are for using static methods from these libraries like assertEquals(....) and when(....) without having to always write their class prefix. This makes the code much more readable.

Every unit test consists of at least three parts:

Build test data
Carry out the action to test
Assert the action result (to success or failure)
Exercises (Optional)
Exercise 1: Payment Processor Testing

You are tasked with testing a PaymentService class that depends on an external PaymentGateway for processing payments. The PaymentGateway interface has a method processPayment(double amount) that returns a boolean indicating whether the payment was successful.

Create a unit test for the PaymentService class, specifically focusing on a method named makePayment(double amount). Use Mockito to mock the PaymentGateway dependency. Your test should verify that the PaymentService correctly calls the processPayment method on the PaymentGateway when makePayment is invoked.

Additionally, test for both possible outcomes of the processPayment method: success (true) and failure (false), and ensure that PaymentService handles these outcomes appropriately.

Note, we are focused on Unit testing, so you do not require integration with different systems or a full application to create the tests.

Use the following skeleton code to help you get started:

Copy code
public interface PaymentGateway {
    boolean processPayment(double amount);
}

public class PaymentService {
    private PaymentGateway paymentGateway;

    public PaymentService(PaymentGateway paymentGateway) {
        this.paymentGateway = paymentGateway;
    }

    public boolean makePayment(double amount) {
        // Implementation here
    }
}

Hints:

You will need to create a mock of the PaymentGateway class and inject a mock PaymentGateway into PaymentService. Use mock(PaymentGateway.class) to create the mock.
Use verify() to check if processPayment was called with the expected amount.
You should simulate successful and unsuccessful payment scenarios. Different outcomes of processPayment can be simulated using when(...).thenReturn(...).
Exercise 2: Email Notification Testing

You are working on a notification system that sends out emails to users. The system uses a EmailSender class, which has a method sendEmail(String recipient, String message) that returns a String indicating the status of the email sending operation ("Success" or "Failure").

Your task is to write a unit test for a method named notifyUser(User user, String message) in the NotificationService class. This method uses EmailSender to send an email to the user. Use Mockito's when-then feature to simulate the behavior of the EmailSender depending on different scenarios (e.g., email sending succeeds or fails).

Note, we are focused on Unit testing, so you do not require integration with different systems or a full application to create the tests.

Use the following skeleton code to help you get started:

Copy code
public class EmailSender {
    public String sendEmail(String recipient, String message) {
        // Implementation here
    }
}

public class NotificationService {
    private EmailSender emailSender;

    public NotificationService(EmailSender emailSender) {
        this.emailSender = emailSender;
    }

    public boolean notifyUser(User user, String message) {
        // Implementation here
    }
}

public class User {
    private String email;

    // Constructor, getters and setters
}

Hints:

You will need to mock the EmailSender. To do so, use mock(EmailSender.class).
You should simulate different outcomes for the sendEmail method. Utilize when(...).thenReturn(...) to define the behavior of sendEmail for different inputs.
Assert that notifyUser reacts properly to both success and failure statuses from the EmailSender.
Summary

The service layer depends on the DAO layer to access and manipulate data.
When testing the service layer, DAOs are often mocked or stubbed to isolate the service logic from database interactions.
Mockito's @InjectMocks annotation can be used to automatically inject mocked dependencies into the fields of the target object being tested.


# Java Collections and Algorithms
## Overview Of Collections Hierarchy
Learning Objectives

After completing this module, associates should be able to:

Describe the Collections Hierarchy
Successfully execute a Java program that demonstrates the need for a Collections Hierarchy
Description
Collections Framework
The Collections Framework in Java is a set of classes and interfaces that implement commonly used data structures. A collection is a single object that acts as a container for other objects. The Collections API is organized in a class hierarchy shown in the simplified version below:

Collection API class hierarchy

The important interfaces in the Collections API are:

Iterable - guarantees the collection can be iterated over
List - an ordered collection
Set - a collection does not contain duplicates
Queue - a collection that operates on a first-in-first-out (FIFO) basis
Map - contains key/value pairs. Does not extend Iterable
Collection Interface
The basic interface of the collections framework is the Collection interface, which is the root interface of all collections in the API.
It is placed at the top of the collection hierarchy in Java. It provides the basic operations for adding and removing elements in the collection.
List, Queue, and Set are three interfaces that extend the Collection interface. 
An important note is that the Map interface does not extend the Collection interface, though it is considered part of the Java Collection API.
Another important note is that the Collection interface extends the Iterable interface. The Iterable interface has only one method called iterator(). The function of the iterator method is to return the iterator object. Using this iterator object, we can iterate over the elements of the collection.
List Interface
This interface represents a collection of elements whose elements are arranged in a sequential order.
A list retains the order in which we add elements using indexes.
This same sequence is used to retrieve elements.
With lists, a developer can insert elements into the list at any location.
The list allows storing duplicate elements in Java.
ArrayList, vector, and LinkedList are three concrete subclasses that implement the list interface.
### Set Interface

This interface represents a collection of elements that contains unique elements.
The Set interface does not maintain any order while storing elements.
When retrieving elements, elements are not guaranteed to be retrieved in the same order as they were added to the collection.
A set does not allow any duplicate elements.
The HashSet, LinkedHashSet, and TreeSet classes implement the Set interface. TreeSet also implements the SortedSet interface which extends the Set interface.
A set can be iterated by using an iterator, but it cannot be iterated using a list iterator.
### SortedSet Interface

This interface extends a set whose iterator traverses its elements according to their natural ordering.
TreeSet implements the SortedSet interface.
### Queue Interface

A queue is an ordered collection in which new elements are added at one end (the rear) and removed from the other end (the front). Just like a queue in a supermarket.
This interface represents a special type of list whose elements are removed only from the head.
LinkedList, PriorityQueue, ArrayQueue, PriorityBlockingQueue, and LinkedBlockingQueue are the concrete subclasses that implement the Queue interface.
### Deque Interface

A deque (double-ended queue) is a sub-interface of the Queue interface.
This interface was added to the collection framework in Java SE 6.
The Deque interface extends the queue interface and uses its method to implement deque.
A deque is a linear collection of elements in which elements can be inserted and removed from either end.
LinkedList and ArrayDeque classes implement the Deque interface.
### Map Interface

The Map interface does not inherit from the Collection interface. It represents an object that stores and retrieves elements in the form of key/value pairs, and their location within the map is determined by a key.
Maps utilize hashing techniques to manage key-value pairs.
Maps do not allow for duplicate keys; however, duplicate values are allowed.
The HashMap, HashTable, LinkedHashMap, and TreeMap classes implement the Map interface.
### SortedMap Interface

This interface represents a map whose elements are stored in their natural ordering. It extends the Map interface, which in turn is implemented by TreeMap classes.
### Methods of the Collection Interface:

The Collection interface consists of a total of fifteen methods for manipulating elements in the collection. They are as follows:

add(): This method is used to add or insert an element in the collection. The general syntax for the add() method is as follows:
add(Object element): boolean
This method will return true when the elements are successfully added. Otherwise, it will return false if the element is already present and the collection doesn’t allow duplicates.
addAll(): This method adds a collection of elements to the collection. It returns true if the elements are added; otherwise, it returns false. The general syntax for this method is as follows:
addAll(Collection c): boolean
clear(): This method clears or removes all the elements from the collection. The general form of this method is as follows:
clear(): void
contains(): It checks whether an element is present or not in a collection. That is, it is used to search for an element. The general for contains() method is as follows:
contains(Object element): boolean
This method returns true if the element is present; otherwise, it returns false.
containsAll(): This method checks whether a specified collection of elements is present or not. It returns true if the calling collection contains all specified elements; otherwise, it returns false. The general syntax is as follows:
containsAll(Collection c): boolean
equals(): It checks for equality with another object. The general form is as follows:
equals(Object element): boolean
hashCode(): It returns the hash code number for the collection. Its return type is an integer. The general form for this method is:
hashCode(): int
isEmpty(): It returns true if a collection is empty. That is, this method returns true if the collection contains no elements.
isEmpty(): boolean
iterator(): It returns an iterator. The general form is given below:
iterator(): Iterator
remove(): It removes a specified element from the collection. The general syntax is given below:
remove(Object element): boolean
This method returns true if the element was removed. Otherwise, it returns false.
removeAll(): The removeAll() method removes all elements from the collection. It returns true if all elements are removed; otherwise, it returns false.
removeAll(Collection c): boolean
retainAll(): This method is used to remove all elements from the collection except the specified collection. It returns true if all the elements are removed; otherwise, it returns false.
retainAll(Collection c): boolean
size(): The size() method returns the total number of elements in the collection. Its return type is an integer. The general syntax is given below:
size(): int
toArray(): It returns the elements of a collection in the form of an array. The array elements are copies of the collection elements. The general syntax is given below:
toArray(Object array[]): Object[]
### Collections Class in Java

The Collections class provides useful static operations that can be performed on collections. These operations include finding the min and max values, sorting, reversing, and more. To find out more about this class, review the Java documentation: https://docs.oracle.com/javase/8/docs/api/java/util/Collections.html

Real World Application

The Collection Framework in Java plays a crucial role in organizing, manipulating, and managing data efficiently. Below are some reasons this part of the Java API is useful:

Organized Data Structures: It provides a comprehensive set of interfaces (e.g., List, Set, Queue, Map) and classes that organize data efficiently. These data structures offer various functionalities like insertion, deletion, searching, and iteration, making them suitable for different use cases.
Reusable and Standardized Components: The Collection Framework offers a set of reusable and standardized components, ensuring consistency and interoperability across different Java applications. Developers can rely on these components to implement common data manipulation tasks without reinventing the wheel.
Type-Safety and Generics: With the introduction of generics in Java 5, the Collection Framework became type-safe, enabling developers to work with collections of specific types without the risk of runtime errors. Generics ensure compile-time type checking, enhancing code reliability and maintainability.
Flexibility and Extensibility: The Collection Framework supports a wide range of data structures and provides flexibility for developers to create custom implementations if needed. This extensibility allows developers to tailor collections to suit specialized requirements or performance optimizations.
Support for Functional Programming: With the introduction of functional interfaces and lambda expressions in Java 8, the Collection Framework gained support for functional programming paradigms. Operations such as filtering, mapping, and reducing can be performed on collections using the Stream API, leading to concise and expressive code.
Implementation

Below is an example of using two different collections, an ArrayList and a HashMap. More details on these collections can be found in their respective written lessons.

import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;

public class Main {
    public static void main(String[] args) {
        // Example of ArrayList
        ArrayList<String> names = new ArrayList<>();

        // Adding elements to the ArrayList
        names.add("Alice");
        names.add("Bob");
        names.add("Charlie");
        names.add("David");

        // Iterating over the elements of the ArrayList
        System.out.println("Names:");
        for (String name : names) {
            System.out.println(name);
        }

        // Example of HashMap
        HashMap<String, Integer> ages = new HashMap<>();

        // Adding key-value pairs to the HashMap
        ages.put("Alice", 30);
        ages.put("Bob", 25);
        ages.put("Charlie", 35);
        ages.put("David", 40);

        // Iterating over the entries of the HashMap
        System.out.println("\nAges:");
        for (Map.Entry<String, Integer> entry : ages.entrySet()) {
            String name = entry.getKey();
            int age = entry.getValue();
            System.out.println(name + ": " + age);
        }
    }
}

Summary

The Collections framework in Java is a set of classes and interfaces that implement commonly used data structures. A collection is a single object which acts as a container for other objects.
The important interfaces in the Collections API are:
Iterable - guarantees the collection can be iterated over
List - an ordered collection
Set - a collection does not contain duplicates
Queue - a collection that operates on a first-in-first-out (FIFO) basis
Map - contains key/value pairs. Does not extend Iterable.


## Generics
Learning Objectives

After completing this module, associates should be able to:

Describe generics with respect to collections
Successfully execute a Java program that demonstrates generics.
Description
Generics
Generics are constructs introduced in Java 5 which enforce compile time safety by allowing you to use parameterized types. They are covered here because the are frequently and heavily used with collections. Generics can be declared on a class (generic types), method parameters (generic methods), or return types.

Before Java 5, you had to write something like this and hope other developers understood to only put Strings inside:

List names = new ArrayList();
names.add("Alice"); // good use
names.add(new Object()); // uh oh - we want to prevent this from happening
With generics, you can restrict a class to only accept objects of a given type and the compiler will prevent you from using any other type:

List<String> names = new ArrayList<>(); // using a List of Strings only
names.add("Alice"); // nice!
names.add(new Object()); // now we get a compilation error to stop this - generics saves the day!
Generic Classes
To make a class (or interface) generic, use the angle brackets when declaring it, and use an arbitrary "generic type" which is determined by the invoking code. The generic type can then be reused throughout the class to enforce type safety.

public class MyGenericClass<T> {
  private T instance;
  
  // simple generic setter method
  public void setObject(T object) {
    this.instance = object;
  }
}
Naming Convention for Generics
Technically, type parameters can be named anything you want. The convention is to use single, uppercase letters to make it obvious that they are not real class names.

E => Element
K => Map Key
V => Map Value
N => Number
T => Generic data type
S, U, V, and so on => For multiple generic data types
Real World Application

Consider this scenario where we are implementing a radio station on the Web. One part of this large project is to create a class called MetaInformationObject which is the superclass for PLS and M3U playlists:

public abstract class MetaInformationObject {
    // ... code omitted
}

public class M3UInfo extends MetaInformationObject {
    // ... code omitted
}

public class PLSInfo extends MetaInformationObject {
    // ... code omitted
}
We allow the web "DJ" to change playlists via a selection dialog box. This means that our dialog class, SelectMultipleStreamDialog will have an object which can be of any type that extends MetaInformationObject:

public class SelectMultipleStreamDialog <T extends MetaInformationObject> {

    private T selectedStream;

    public T getSelectedStream() {
        return selectedStream;
    }

    // ... other code omitted
}

We now have a flexible dialog box that can store different kinds of streams, rather than just one type. Now we can supply any subtype implementation of MetaInformationObject to code that expects a MetaInformationObject object.

Implementation

Recall that Generics are a feature set that you can employ to define overall functionality within a class and then restrict the type at compile time to whichever type the developer defines. This is also known as type safety.

Generics are commonly employed in Collection interfaces and classes as well as utility classes (a class whose purpose is to house popular functions used in an application).

In this example, we'll update a class to utilize generics to enforce type-safety at compile-time.

Company XYZ has a special Container that they have developed. A container supports actions for adding an object, viewing what's inside the container, and removing the item from the container. The class is setup like the following:

package com.yourname.model;

public class Container {

    private Object obj;

    public void setObject(Object obj) { this.obj = obj; }

    public Object getObject() { return this.obj; }

    public Object removeObject(){ Object temp = this.obj; obj = null; return temp; }

}
Company XYZ has an issue though. They want to restrict the type of items that go in the Container to be of a single type. For example, they don't want to mix Integer with String values. How can you accomplish this?

With Generics of course! (Drum roll)

Potential Project Usage
How can you make this class use generics?

Since we want the entire class and its single property to be constrained to a particular type, we'll first parameterize the class. Do you recall how to do this?

We'll need to choose a letter and enclose it within brackets after the class name.

//Example
public class Container <A> {
  …
}
Choosing the letter is important as there are industry conventions that you should follow. Out of the available letters (E, K, V, N, T, S, U), which should we choose?

T, S or U make the most sense in this context, since E is for classes that store multiple objects (like Collections), we're not specific to numbers and we're not using key/value pairs.

So the class should look like this:

public class Container<T> {
  …
}
That's a great start, but it doesn't really mean anything until we apply this parameterized aspect to fields or methods. Since Company XYZ wants to restrict the type of data that the Container can have, then we should change the Object instance variable to be of type T instead of Object.

Doing so will give us a field like this:

public class Container<T> {

  private T obj;
   …
}
If you were to stop here, it will cause a compiler error in your code. The setObject(...) method reports an error because we're trying to assign a variable of type Object to our variable of type T. Although T is an Object, technically since all classes derive from Object, generics enforces this variable to be of the same type!



So we need to change the parameter of this method to be of type T as well. The code should look like the following:

      ...
    
      public void setObject(T obj) {
    
        this.obj = obj;
    
      }
    
      ...
    }
At this stage, we shouldn't see any compiler errors. Although our code is free of compiler errors we should continue to parameterize the other methods so that they make use of our T type throughout. Otherwise, when the other methods such as getObject() are called, they would return something of type Object as opposed to what T will resolve to.

We will make use of T instead of Object. The code should look like the following:

public class Container<T> {

  private T obj;

  public void setObject(T obj) {
    this.obj = obj;
  }

  public T getObject() {
    return this.obj;
  }

  public T removeObject() {
    T temp = this.obj;
    obj = null;

    return temp;
  }
}
Moving onto the TestContainer class we update it to this.

public class TestContainer {

  public static void main(String[] args) {
    Container container = new Container();
    container.setObject("Hello");

    System.out.println(container.getObject());
  }
}
If you were to write this code, the IDE would show a warning about the code that creates a Container object and executes some of its methods.



Depending on which line of code we hover over, we'll see a message about the class being called as a raw type and the class or the method needing to be parameterized.

Since the setObject() method is being called with a String variable, let's parameterize Container for use with Strings.

Now we need to update the code to parameterize Container.

The code should now look like the following:

    Container<String> container = new Container<>();
Notice that we used the diamond operator <> as a shortcut on the assignment statement. Java can infer the type that the new Container object will use by referring to the reference variable declaration, which specifies the type. All compiler warnings should disappear and you can now run your class with type-safety assurance.

Exercises (Optional)
Write a generic method to count the number of elements in a collection that have a specific property (for example, odd integers, prime numbers, palindromes).

Write a generic method to exchange the positions of two different elements in an array.

Summary

Generics are constructs introduced in Java 5 which enforce compile time safety by allowing you to use parameterized types.
They are covered here because the are frequently and heavily used with collections.
Generics can be declared on a class (generic types), method parameters (generic methods), or return types.
With generics, you can restrict a class to only accept objects of a given type and the compiler will prevent you from using any other type.
To make a class (or interface) generic, use the angle brackets when declaring it, and use an arbitrary "generic type" which is determined by the invoking code. The generic type can then be reused throughout the class to enforce type safety.
Naming Convention for Generics
Technically, type parameters can be named anything you want. The convention is to use single, uppercase letters to make it obvious that they are not real class names.
E => Element
K => Map Key
V => Map Value
N => Number
T => Generic data type
S, U, V, and so on => For multiple generic data types


## Set Interface
Learning Objectives

After completing this module, associates should be able to:

Describe the Set interface.
Successfully execute a Java program that demonstrates the Set interface
Description

The Set interface contains only methods inherited from Collection and adds the restriction that duplicate elements are prohibited.

Set also adds a stronger contract on the behavior of the equals and hashCode operations, allowing Set instances to be compared meaningfully even if their implementation types differ.

HashSet
A HashSet implements Set and is backed by a HashMap. It:

Guarantees no ordering when iterating
Allows one null value
Allows fast insertion and traversal
Does not maintain order in which you insert elements
TreeSet
A TreeSet is a Set whose elements maintain sorted order when inserted. Internally, it is backed by a Sorted Tree. Insertion and removal of elements is slow, because the elements must maintain sorted order. It cannot contain any null values, since null cannot be compared to any object.

The methods declared by Set are summarized as follows:

add( ): Adds an object to the collection.

clear( ): Removes all objects from the collection.

contains( ): Returns true if a specified object is an element within the collection.

isEmpty( ): Returns true if the collection has no elements.

iterator( ) : Returns an Iterator object for the collection, which may be used to retrieve an object.

remove( ): Removes a specified object from the collection.

size( ): Returns the number of elements in the collection.

Set has its implementation in various classes like HashSet, TreeSet, LinkedHashSet. Following is an example to explain Set functionality:

import java.util.*;
public class SetDemo {

  public static void main(String args[]) { 
      int count[] = {34, 22,10,60,30,22};
      Set<Integer> set = new HashSet<Integer>();
      try {
         for(int i = 0; i < 5; i++) {
            set.add(count[i]);
         }
         System.out.println(set);

         TreeSet sortedSet = new TreeSet<Integer>(set);
         System.out.println("The sorted list is:");
         System.out.println(sortedSet);

         System.out.println("The First element of the set is: "+ (Integer)sortedSet.first());
         System.out.println("The last element of the set is: "+ (Integer)sortedSet.last());
      }
      catch(Exception e) {}
   }
} 
Output:

[34, 22, 10, 60, 30]
The sorted list is:
[10, 22, 30, 34, 60]
The First element of the set is: 10
The last element of the set is: 60
Real World Application

A Set is a type of collection that does not allow duplicate elements. That means an element can only exist once in a Set. It models the set abstraction in mathematics.

Characteristics of a Set collection:
The following characteristics differentiate a Set collection from others in the Java Collections framework:

Duplicate elements are not allowed.
Elements are not stored in order. That means you cannot expect elements sorted in any order when iterating over elements of a Set.
Why and When Use Sets?
Based on the characteristics, consider using a Set collection when:

You want to store elements distinctly without duplication, or unique elements.
You don’t care about the order of elements.
You can use a Set to store unique integer numbers
You can use a Set to store cards randomly in a card game
You can use a Set to store numbers in random order, for example, in board gaming which often requires the use of random numbers.
Implementation
Operations on the Set Interface
The set interface allows the users to perform basic set operations. Let’s use two sets of numbers to understand these basic operations: [1, 3, 2, 4, 8, 9, 0] and [1, 3, 7, 5, 4, 0, 7, 5]. Then the possible operations on the sets are:

Intersection: This operation returns all the common elements from the given two sets. For the above two sets, the intersection would be:
Intersection = [0, 1, 3, 4]
Union: This operation adds all the elements in one set with the other. For the above two sets, the union would be:
Union = [0, 1, 2, 3, 4, 5, 7, 8, 9]
Difference: This operation removes all the values present in one set from the other set. For the above two sets, the difference would be:
Difference = [2, 8, 9]
Now let us implement the following operations as defined above as follows:

// Importing all utility classes
import java.util.*;
  
// Main class 
public class SetExample {
    
    // Main driver method 
    public static void main(String args[])
    {
        // Creating an object of Set class 
        // Declaring object of Integer type 
        Set<Integer> a = new HashSet<Integer>();
        
        // Adding all elements to List 
        a.addAll(Arrays.asList(
            new Integer[] { 1, 3, 2, 4, 8, 9, 0 }));
        
      // Again declaring object of Set class
      // with reference to HashSet
        Set<Integer> b = new HashSet<Integer>();
          
      b.addAll(Arrays.asList(
            new Integer[] { 1, 3, 7, 5, 4, 0, 7, 5 }));
  
          
        // To find union
        Set<Integer> union = new HashSet<Integer>(a);
        union.addAll(b);
        System.out.print("Union of the two Sets");
        System.out.println(union); // [0, 1, 2, 3, 4, 5, 6, 8, 9]
  
        // To find intersection
        Set<Integer> intersection = new HashSet<Integer>(a);
        intersection.retainAll(b);
        System.out.print("Intersection of the two Sest");
        System.out.println(intersection); // [0, 1, 3, 4]
  
        // To find the symmetric difference
        Set<Integer> difference = new HashSet<Integer>(a);
        difference.removeAll(b);
        System.out.print("Difference of the two Sets");
        System.out.println(difference); // [2, 8, 9]
    }
}
Summary

The Set interface contains only methods inherited from Collection and adds the restriction that duplicate elements are prohibited.
Set also adds a stronger contract on the behavior of the equals and hashCode operations, allowing Set instances to be compared meaningfully even if their implementation types differ.
A HashSet implements Set and is backed by a HashMap.
A TreeSet is a Set whose elements maintain sorted order when inserted. Internally, it is backed by a Sorted Tree.


## HashSet and TreeSet
Learning Objectives

After completing this module, associates should be able to:

Describe the HashSet and TreeSet collections
Successfully execute a Java program that demonstrates HashSet and TreeSet
Description
HashSet
A HashSet implements Set and is backed by a HashMap. It:

Guarantees no ordering when iterating
Allows one null value
Allows fast insertion and traversal
Does not maintain order in which you insert elements
TreeSet
A TreeSet is a Set whose elements maintain sorted order when inserted. Internally, it is backed by a Sorted Tree. Insertion and removal of elements is slow, because the elements must maintain sorted order. It cannot contain any null values, since null cannot be compared to any object.

Hash set and tree set both belong to the collections framework. HashSet is the implementation of the Set interface whereas Tree set implements sorted set. Tree set is backed by a TreeMap while HashSet is backed by a hashmap.

Category	Hash Set	Tree Set
Implementation	Hash set is implemented using HashTable	The tree set is implemented using a tree structure.
Null Object	HashSet allows a null object	The tree set does not allow the null object. It throws the null pointer exception.
Methods	Hash set use equals method to compare two objects	Tree set use compare method for comparing two objects.
Heterogeneous objects	Hash set doesn't allow heterogeneous objects	TreeSet does not allow heterogeneous objects as it requires elements to be comparable to each other
Ordering	HashSet does not maintain any order	TreeSet maintains an object in sorted order
Real World Application
HashSets
Hash Tables are store data in key-value pairs. It only stores data that has a key associated with it. Inserting and Searching operations are easily manageable while using Hash Tables.

Some applications of a hashtable are:

Data stored in databases is generally of the key-value format which is done through hash tables.
Every time we type something to be searched in google chrome or other browsers, it generates the desired output based on the principle of hashing.
Message Digest, a function of cryptography also uses hashing for creating output in such a manner that reaching the original input from that generated output is almost next to impossible.
In our computers we have various files stored in it, each file has two very crucial pieces of information that is, the filename and file path, in order to make a connection between the filename to its corresponding file path hash tables are used.
Social network “feeds”.
Password hashing.
Used for fast data lookup - symbol table for compilers, database indexing, caches, Unique data representation.
To store a set of fixed keywords that are referenced very frequently.
TreeSets
Some applications of trees are:

XML Parser uses tree algorithms.
The decision-based algorithm is used in machine learning which works upon the algorithm of the tree.
Databases also use tree data structures for indexing.
Domain Name Server(DNS) also uses tree structures.
File explorer/my computer of mobile/any computer
BST used in computer Graphics
Posting questions on websites like Quora, the comments are a child of questions.
Parsers(XML parser).
Code Compression(zip).
DOM in Html.
Evaluate an expression (i.e., parse).
Integral to compilers/automata theory.
To store the possible moves in a chess game.
To store the genealogy information of biological species.
Used by JVM (Java Virtual Machine) to store Java objects.
Implementation

Here we will be discussing HashSet and TreeSet with 2 examples of each.

HashSet Example 1
// Java Program to Demonstrate Working of HashSet Class
 
// Importing required HashSet class from java.util package
import java.util.HashSet;
 
// Main class
class Rev {
 
    // Main driver method
    public static void main(String[] args)
    {
        // Creating a HashSet object of string type
        HashSet<String> hset = new HashSet<String>();
 
        // Adding elements to HashSet
        // using add() method
        hset.add("People");
        hset.add("for");
        hset.add("practice");
        hset.add("contribute");
 
        // Duplicate removed
        hset.add("People");
 
        // Printing HashSet elements using for each loop
 
        // Display message only
        System.out.println("HashSet contains: ");
 
        // Iterating over hashSet using for-each loop
        for (String temp : hset) {
 
            // Printing all elements inside above hashSet
            System.out.println(temp);
        }
    }
}
Output:

HashSet contains: 
practice
People
for
contribute
HashSet Example 2:
// Java Program to Illustrate Working of HashSet Class
// From another Collection
 
// Importing utility classes
import java.util.*;
 
class Rev {
 
    // Main driver method
    public static void main(String[] args)
    {
 
        ArrayList<String> ll = new ArrayList<String>();
 
        // Adding elements to ArrayList
        ll.add("Computer");
        ll.add("Science");
 
        // Creating HashSet object of string type
        HashSet<String> hs = new HashSet(ll);
        hs.add("Portal");
        hs.add("Rev");
 
        // Iterating via iterators
        Iterator<String> iter = hs.iterator();
 
        // Condition holds true till there is single element
        // in th List
        while (iter.hasNext()) {
 
            // Printing all elements inside objects
            System.out.println(iter.next());
        }
    }
}
Output:

Rev
Computer
Science
Portal
TreeSet Example 1
// Java program to demonstrate working of
// TreeSet Class
 
// Importing required classes
import java.util.TreeSet;
 
// Class
class TreeSetDemo {
 
    // Main driver method
    public static void main(String[] args)
    {
        // Create an empty TreeSet
        TreeSet<String> tset = new TreeSet<String>();
 
        // Adding elements to HashSet
        // using add() method
        tset.add("People");
        tset.add("for");
        tset.add("practice");
        tset.add("contribute");
 
        // Duplicate elements being removed
        tset.add("People");
 
        // Displaying TreeSet elements
        System.out.println("TreeSet contains: ");
       
        for (String temp : tset) {
            System.out.println(temp);
        }
    }
}
Output:

TreeSet contains: 
People
contribute
for
practice
TreeSet Example 2:
// Java Program to Illustrate Working of TreeSet
// From another Collection
 
// Importing utility classes
import java.util.*;
 
// Main class
class Rev {
 
    // Main driver method
    public static void main(String[] args)
    {
 
        // Creating list of string
        ArrayList<String> ll = new ArrayList<String>();
 
        // Adding elements to ArrayList
        // using add() method
        ll.add("Computer");
        ll.add("Science");
 
        // Creating TreeSet object of string type
        TreeSet<String> ts = new TreeSet(ll);
 
        // Adding elements to above TreeSet
        ts.add("Portal");
        ts.add("Rev");
 
        // Iterating via iterators
        Iterator<String> iter = ts.iterator();
 
        // Condition that holds true till
        // there is single element in th List
        while (iter.hasNext()) {
 
            // Printing all elements inside objects
            System.out.println(iter.next());
        }
    }
}
Output:

Computer
Portal
Rev
Science
Summary

A HashSet implements Set and is backed by a HashMap. It:
Guarantees no ordering when iterating
Allows one null value
Allows fast insertion and traversal
Does not maintain order in which you insert elements
A TreeSet is a Set whose elements maintain sorted order when inserted.
Internally, it is backed by a Sorted Tree.
Insertion and removal of elements is slow, because the elements must maintain sorted order.
It cannot contain any null values, since null cannot be compared to any object.


## Queue Interface
Learning Objectives

After completing this module, associates should be able to:

Describe the Queue interface
Successfully execute a Java program that demonstrates the Queue interface
Description
Queue Interface
To visualize a queue, let's use a quick analogy. Imagine we've just opened our first business – a hot dog stand. We want to serve our new potential clients in the most efficient way possible for our small business; one at a time. We ask them to form an orderly line in front of our stand, with new customers joining at the rear. Those who get in line first will be served first, and this will allow us to distribute our tasty hot dogs in a fair way.

Queues in Java work in a similar way. Queues place objects on a “waiting list”, typically based on First-In-First-Out (FIFO) and are useful for storing objects prior to processing them. After we declare our Queue, we can add new elements to the back (tail), and remove them from the front (head).

Some queue-specific methods are:

offer() – Inserts a new element onto the Queue
poll() – Removes an element from the front of the Queue
peek() – Inspects the element at the front of the Queue, without removing it
One such exception to the FIFO rule is the PriorityQueue. When new elements are inserted into the PriorityQueue, they are ordered based on their natural ordering, or by a defined Comparator provided when we construct the PriorityQueue. Priority queues are discussed in detail in a separate topic.

Sub-Interfaces
Generally, the Queue interface is inherited by 3 main sub-interfaces. Blocking Queues, Transfer Queues, and Deques.

Together, these 3 interfaces are implemented by the vast majority of Java's available Queues. Let's take a quick look at what these interfaces have been set out to do.

Blocking Queues
The BlockingQueue interface supports additional operations which force threads to wait on the Queue depending on the current state. A thread may wait on the Queue to be non-empty when attempting a retrieval, or for it to become empty when adding a new element.

Standard Blocking Queues include LinkedBlockingQueue, SynchronousQueue, and ArrayBlockingQueue.

Transfer Queues
The TransferQueue interface extends the BlockingQueue interface but is tailored toward the producer-consumer pattern. It controls the flow of information from producer to consumer, creating backpressure in the system.

Java includes one implementation of the TransferQueue interface, LinkedTransferQueue.

Deques
Deque (pronounced "deck") is short for Double-Ended Queue and is analogous to a deck of cards – elements may be taken from both the start and end of the Deque. Much like the traditional Queue, the Deque provides methods to add, retrieve and peek at elements held at both the top and bottom. Deques can be used to implement a stack, with Last-In-First-Out (LIFO) behavior.

Some Deque implementations include LinkedList and ArrayDeque.

Commonly Used Implementing Classes
LinkedList: Implements both the List and Deque interfaces.
ArrayDeque: Resizable-array implementation of Deque interface that is likely to be faster than Stack when used as a stack, and faster than LinkedList when used as a queue.
ArrayBlockingQueue: Array-backed, implements BlockingQueue interface which supports operations that wait on the queue to contain an element or for space to become available in the queue. Solution to the producer-consumer problem, where a producer thread inputs elements into the array while a consumer thread removes them for processing
PriorityQueue: Elements in the queue are ordered by priority based on their natural ordering (or a Comparator)
Thread Safety
Adding items to Queues is particularly useful in multi-threaded environments. A Queue can be shared amongst threads, and be used to block progress until space is available – helping us overcome some common multi-threaded problems.

For example, writing to a single disk from multiple threads creates resource contention and can lead to slow writing times. Creating a single writer thread with a BlockingQueue can alleviate this issue and lead to vastly improved write speeds.

Luckily, Java offers ConcurrentLinkedQueue, ArrayBlockingQueue, and ConcurrentLinkedDeque which are thread-safe and perfect for multi-threaded programs.

Real World Application

Queues are used when things don’t have to be processed immediately, but have to be processed in First In First Out order. This property of Queue makes it also useful in following kind of scenarios.

When a resource is shared among multiple consumers. Examples include CPU scheduling, Disk Scheduling.
When data is transferred asynchronously (data not necessarily received at same rate as sent) between two processes. Examples include IO Buffers, pipes, file IO, etc.
In Operating systems:
Semaphores
FCFS ( first come first serve) scheduling, example: FIFO queue
Spooling in printers
Buffer for devices like keyboards
In Networks:
Queues in routers/ switches
Mail Queues
Some other applications of Queue:

Waiting lists for a single shared resource like CPU, Disk, Printer.
Buffers on MP3 players and portable CD players.
How operating systems handle interruptions/exceptions.
Add a song at the end of a playlist.
Implementation
Creating Queue Objects:
Since Queue is an interface, objects cannot be created of the type Queue. We always need a class which implements this interface in order to create an object. And also, after the introduction of Generics in Java 1.5, it is possible to restrict the type of object that can be stored in the Queue. The following is an example of creating a queue:

// Obj is the type of the object to be stored in Queue 
Queue<Obj> queue = new PriorityQueue<Obj> ();  
Example Queue:

// Java program to demonstrate a Queue
 
import java.util.LinkedList;
import java.util.Queue;
 
public class QueueExample {
 
    public static void main(String[] args)
    {
        Queue<Integer> q
            = new LinkedList<>();
 
        // Adds elements {0, 1, 2, 3, 4} to
        // the queue
        for (int i = 0; i < 5; i++)
            q.add(i);
 
        // Display contents of the queue.
        System.out.println("Elements of queue "
                           + q);
 
        // To remove the head of queue.
        int removedele = q.remove();
        System.out.println("removed element-"
                           + removedele);
 
        System.out.println(q);
 
        // To view the head of queue
        int head = q.peek();
        System.out.println("head of queue-"
                           + head);
 
        // Rest all methods of collection
        // interface like size and contains
        // can be used with this
        // implementation.
        int size = q.size();
        System.out.println("Size of queue-"
                           + size);
    }
}
Creating Deque Objects
Since Deque is an interface, objects cannot be created of the type Deque. We always need a class that implements this interface in order to create an object. And also, after the introduction of Generics in Java 1.5, it is possible to restrict the type of object that can be stored in the Deque.

// Java program to demonstrate the working
// of a Deque in Java
 
import java.util.*;
 
public class DequeExample {
    public static void main(String[] args)
    {
        Deque<String> deque
            = new LinkedList<String>();
 
        // We can add elements to the queue
        // in various ways
 
        // Add at the last
        deque.add("Element 1 (Tail)");
 
        // Add at the first
        deque.addFirst("Element 2 (Head)");
 
        // Add at the last
        deque.addLast("Element 3 (Tail)");
 
        // Add at the first
        deque.push("Element 4 (Head)");
 
        // Add at the last
        deque.offer("Element 5 (Tail)");
 
        // Add at the first
        deque.offerFirst("Element 6 (Head)");
 
        System.out.println(deque + "\n");
 
        // We can remove the first element
        // or the last element.
        deque.removeFirst();
        deque.removeLast();
        System.out.println("Deque after removing "
                           + "first and last: "
                           + deque);
    }
}
Exercises (Optional)
Write a Java program that uses the Queue interface to create a line of twelve customers, then assigns delicatessen ticket numbers, starting at 1 to those customers, in the proper order.

Summary

The Queue places objects on a “waiting list”, typically based on First-In-First-Out (FIFO)
Useful for storing objects prior to processing
Elements are added to the tail of the queue
Elements can be removed from the front of the queue
The Deque Interface extends the Queue interface
Short for “double-ended queue”
Pronounced “deck”
Supports element insertion and removal from both ends of the queue
Can be used to implement a stack, with Last-In-First-Out (LIFO) behavior
Queue implementations include:
LinkedList
ArrayDeque
PriorityQueue
ArrayBlockingQueue


## Map Interface
Learning Objectives

After completing this module, associates should be able to:

Describe the Map interface
Successfully execute a Java program that demonstrates the Map interface
Description
Map Interface
Map does not implement the Collection interface, however it is considered to be part of the Collections framework. A Map is useful if you have to search, update or delete elements on the basis of a key. It is used to identify a value by a key, and each entry in a map is a key-value pair. A Map contains unique keys.

Because it does not implement Iterable, Maps cannot be iterated over directly. Instead, one must either:

use the entrySet() method to iterate over the set of Map.Entry
use the keySet() method to iterate over the set of keys
use the values() method to return a Collection of values which can then be iterated over
There are two interfaces for implementing Map in Java: Map and SortedMap, and three classes: HashMap, LinkedHashMap, and TreeMap.

HashMap
HashMap is a Map which:

Stores elements in key-value pairs
Insertion/Retrieval of element by key is fast
Tradeoff is that it does not maintain the order of insertion
Permits one null key and null values
TreeMap
TreeMap is a Map whose:

Keys are stored in a Sorted Tree structure
Main benefit is that keys are always in a sorted order
Insertion/Retrieval are slow
Cannot contain null keys as null cannot be compared for sorting
HashTable
HashTable is an older, thread-safe implementation of a HashMap. It does not allow null keys or null values.

Real World Application

Maps are perfect to use for key-value association mapping such as dictionaries. The maps are used to perform lookups by keys or when someone wants to retrieve and update elements by keys. Some common scenarios are as follows:

A map of error codes and their descriptions.
A map of zip codes and cities.
A map of managers and employees. Each manager (key) is associated with a list of employees (value) he manages.
A map of classes and students. Each class (key) is associated with a list of students (value).
If you are working on a banking project, you can map a customer to their various accounts.
Implementation
Creating Map Objects
Since Map is an interface, objects cannot be created of the type Map. We always need a class that implements this interface in order to create an object. And also, after the introduction of Generics in Java 1.5, it is possible to restrict the type of object that can be stored in the Map.

Example:
// Java Program to Demonstrate
// Working of Map interface

// Importing required classes
import java.util.*;

// Main class
class Rev {

	// Main driver method
	public static void main(String args[])
	{
		// Creating an empty HashMap
		Map<String, Integer> hm
			= new HashMap<String, Integer>();

		// Inserting pairs in above Map
		// using put() method
		hm.put("a", new Integer(100));
		hm.put("b", new Integer(200));
		hm.put("c", new Integer(300));
		hm.put("d", new Integer(400));

		// Traversing through Map using for-each loop
		for (Map.Entry<String, Integer> me :
			hm.entrySet()) {

			// Printing keys
			System.out.print(me.getKey() + ":");
			System.out.println(me.getValue());
		}
	}
}

Exercise (Optional)
Successfully execute a Java program that uses the Map interface to parse a String input and count the frequency of each letter of the alphabet in the String.

Summary

Map Interface: Map does not implement the Collection interface, however it is considered to be part of the Collections framework.
It is used to identify a value by a key, and each entry in a map is a key-value pair.
Because it does not implement Iterable, Maps cannot be iterated over directly. Instead, one must either:
Use the entrySet() method to iterate over the set of Map.Entry
Use the keySet() method to iterate over the set of keys
Use the values() method to return a Collection of values which can then be iterated over
Implemenations of Map include:
HashMap
TreeMap
HashTable, which is an older, thread-safe implementation of a HashMap. It does not allow null keys or null values.


## Map Interface
Learning Objectives

After completing this module, associates should be able to:

Describe the Map interface
Successfully execute a Java program that demonstrates the Map interface
Description
Map Interface
Map does not implement the Collection interface, however it is considered to be part of the Collections framework. A Map is useful if you have to search, update or delete elements on the basis of a key. It is used to identify a value by a key, and each entry in a map is a key-value pair. A Map contains unique keys.

Because it does not implement Iterable, Maps cannot be iterated over directly. Instead, one must either:

use the entrySet() method to iterate over the set of Map.Entry
use the keySet() method to iterate over the set of keys
use the values() method to return a Collection of values which can then be iterated over
There are two interfaces for implementing Map in Java: Map and SortedMap, and three classes: HashMap, LinkedHashMap, and TreeMap.

HashMap
HashMap is a Map which:

Stores elements in key-value pairs
Insertion/Retrieval of element by key is fast
Tradeoff is that it does not maintain the order of insertion
Permits one null key and null values
TreeMap
TreeMap is a Map whose:

Keys are stored in a Sorted Tree structure
Main benefit is that keys are always in a sorted order
Insertion/Retrieval are slow
Cannot contain null keys as null cannot be compared for sorting
HashTable
HashTable is an older, thread-safe implementation of a HashMap. It does not allow null keys or null values.

Real World Application

Maps are perfect to use for key-value association mapping such as dictionaries. The maps are used to perform lookups by keys or when someone wants to retrieve and update elements by keys. Some common scenarios are as follows:

A map of error codes and their descriptions.
A map of zip codes and cities.
A map of managers and employees. Each manager (key) is associated with a list of employees (value) he manages.
A map of classes and students. Each class (key) is associated with a list of students (value).
If you are working on a banking project, you can map a customer to their various accounts.
Implementation
Creating Map Objects
Since Map is an interface, objects cannot be created of the type Map. We always need a class that implements this interface in order to create an object. And also, after the introduction of Generics in Java 1.5, it is possible to restrict the type of object that can be stored in the Map.

Example:
// Java Program to Demonstrate
// Working of Map interface

// Importing required classes
import java.util.*;

// Main class
class Rev {

	// Main driver method
	public static void main(String args[])
	{
		// Creating an empty HashMap
		Map<String, Integer> hm
			= new HashMap<String, Integer>();

		// Inserting pairs in above Map
		// using put() method
		hm.put("a", new Integer(100));
		hm.put("b", new Integer(200));
		hm.put("c", new Integer(300));
		hm.put("d", new Integer(400));

		// Traversing through Map using for-each loop
		for (Map.Entry<String, Integer> me :
			hm.entrySet()) {

			// Printing keys
			System.out.print(me.getKey() + ":");
			System.out.println(me.getValue());
		}
	}
}

Exercise (Optional)
Successfully execute a Java program that uses the Map interface to parse a String input and count the frequency of each letter of the alphabet in the String.

Summary

Map Interface: Map does not implement the Collection interface, however it is considered to be part of the Collections framework.
It is used to identify a value by a key, and each entry in a map is a key-value pair.
Because it does not implement Iterable, Maps cannot be iterated over directly. Instead, one must either:
Use the entrySet() method to iterate over the set of Map.Entry
Use the keySet() method to iterate over the set of keys
Use the values() method to return a Collection of values which can then be iterated over
Implemenations of Map include:
HashMap
TreeMap
HashTable, which is an older, thread-safe implementation of a HashMap. It does not allow null keys or null values.


## HashMap and HashTable
Learning Objectives

After completing this module, associates should be able to:

Describe the similarities differences between a HashMap and a HashTable.
Successfully execute a Java program that demonstrates HashMap and HashTable.
Description
HashTable
The Hashtable class implements a hash table, which maps keys to values.
Any non-null object can be used as a key or as a value.
To successfully store and retrieve objects from a hashtable, the objects used as keys must implement the hashCode method and the equals method.
Key features:
Hash is synchronized.
Hashtable stores key/value pair in a hash table.
In Hashtable we specify an object that is used as a key, and the value we want to associate to that key. The key is then hashed, and the resulting hash code is used as the index at which the value is stored within the table.
HashMap
Now we will discuss HashMap which is a hashtable-based implementation. It extends the AbstractMap class and implements the Map interface. A HashMap works on the principle of hashing.
This Map implementation usually acts as a bucketed hash table, but when buckets get too large, they get transformed into nodes of TreeNodes, each structured similarly to those in java.util.TreeMap.
On the other hand, TreeMap extends AbstractMap class and implements NavigableMap interface. A TreeMap stores map elements in a Red-Black tree, which is a Self-Balancing Binary Search Tree.
Order
HashMap doesn't provide any guarantee over the way the elements are arranged in the Map.

It means, we can't assume any order while iterating over keys and values of a HashMap:

@Test
public void whenInsertObjectsHashMap_thenRandomOrder() {
    Map<Integer, String> hashmap = new HashMap<>();
    hashmap.put(3, "TreeMap");
    hashmap.put(2, "vs");
    hashmap.put(1, "HashMap");
    
    assertThat(hashmap.keySet(), containsInAnyOrder(1, 2, 3));
}
Real World Application
HashMap
Here are some ways that HashMaps are used:

To populate a dropdown list in your form (key as a dropdown option value, value as dropdown option displayText)
To Implement a phone book (Name - Number)
To implement a Dictionary Application (Word - Meaning, Description)
Priority Queues
Dijkstra's Algorithm
Topological Sort
Hashtable
Here are some ways that Hashtables are used:

Implementation of programming languages
File systems
Pattern search
Distributed key-value storage
Cryptography
Implementation
HashMap
HashMap is similar to HashTable, but it is unsynchronized. It allows to store the null keys as well, but there should be only one null key object and there can be any number of null values. This class makes no guarantees as to the order of the map. To use this class and its methods, you need to import java.util.HashMap package or its superclass.


// Java program to illustrate HashMap class of java.util
// package
  
// Importing HashMap class
import java.util.HashMap;
  
// Main class
public class Rev {
  
    // Main driver method
    public static void main(String[] args)
    {
        // Create an empty hash map by declaring object
        // of string and integer type
        HashMap<String, Integer> map = new HashMap<>();
  
        // Adding elements to the Map
        // using standard put() method
        map.put("vishal", 10);
        map.put("sachin", 30);
        map.put("vaibhav", 20);
  
        // Print size and content of the Map
        System.out.println("Size of map is:- "
                           + map.size());
  
        // Printing elements in object of Map
        System.out.println(map);
  
        // Checking if a key is present and if
        // present, print value by passing
        // random element
        if (map.containsKey("vishal")) {
  
            // Mapping
            Integer a = map.get("vishal");
  
            // Printing value fr the corresponding key
            System.out.println("value for key"
                               + " \"vishal\" is:- " + a);
        }
    }
}
Output

Size of map is:- 3
{vaibhav=20, vishal=10, sachin=30}
value for key "vishal" is:- 10
HashTable
This creates an empty hashtable with the default load factor of 0.75 and an initial capacity is 11.

// Java program to demonstrate
// adding elements to Hashtable
  
import java.io.*;
import java.util.*;
  
class AddElementsToHashtable {
    public static void main(String args[])
    {
        // No need to mention the
        // Generic type twice
        Hashtable<Integer, String> ht1 = new Hashtable<>();
  
        // Initialization of a Hashtable
        // using Generics
        Hashtable<Integer, String> ht2
            = new Hashtable<Integer, String>();
  
        // Inserting the Elements
        // using put() method
        ht1.put(1, "one");
        ht1.put(2, "two");
        ht1.put(3, "three");
  
        ht2.put(4, "four");
        ht2.put(5, "five");
        ht2.put(6, "six");
  
        // Print mappings to the console
        System.out.println("Mappings of ht1 : " + ht1);
        System.out.println("Mappings of ht2 : " + ht2);
    }
}
Performing Various Operations on Hashtable
Adding Elements: In order to add an element to the hashtable, we can use the put() method. However, the insertion order is not retained in the hashtable. Internally, for every element, a separate hash is generated and the elements are indexed based on this hash to make it more efficient.
// Java program to demonstrate
// adding elements to Hashtable
  
import java.io.*;
import java.util.*;
  
class AddElementsToHashtable {
    public static void main(String args[])
    {
        // No need to mention the
        // Generic type twice
        Hashtable<Integer, String> ht1 = new Hashtable<>();
  
        // Initialization of a Hashtable
        // using Generics
        Hashtable<Integer, String> ht2
            = new Hashtable<Integer, String>();
  
        // Inserting the Elements
          // using put() method
        ht1.put(1, "Alpha");
        ht1.put(2, "Beta");
        ht1.put(3, "Gamma");
  
        ht2.put(1, "Alpha");
        ht2.put(2, "Beta");
        ht2.put(3, "Gamma");
          
          // Print mappings to the console
        System.out.println("Mappings of ht1 : " + ht1);
        System.out.println("Mappings of ht2 : " + ht2);
    }
}
Changing Elements: After adding the elements if we wish to change the element, it can be done by again adding the element with the put() method. Since the elements in the hashtable are indexed using the keys, the value of the key can be changed by simply inserting the updated value for the key for which we wish to change.
// Java program to demonstrate
// updating Hashtable
  
import java.io.*;
import java.util.*;
class UpdatesOnHashtable {
    public static void main(String args[])
    {
  
        // Initialization of a Hashtable
        Hashtable<Integer, String> ht
            = new Hashtable<Integer, String>();
  
        // Inserting the Elements
          // using put method
        ht.put(1, "Delta");
        ht.put(2, "Delta");
        ht.put(3, "Delta");
          
          // print initial map to the console
        System.out.println("Initial Map " + ht);
          
          // Update the value at key 2
        ht.put(2, "Epsilon");
          
          // print the updated map
        System.out.println("Updated Map " + ht);
    }
}
Removing Elements: In order to remove an element from the Map, we can use the remove() method. This method takes the key value and removes the mapping for a key from this map if it is present in the map.
// Java program to demonstrate
// the removing mappings from Hashtable
  
import java.io.*;
import java.util.*;
class RemovingMappingsFromHashtable {
  
    public static void main(String args[])
    {
        // Initialization of a Hashtable
        Map<Integer, String> ht
            = new Hashtable<Integer, String>();
  
        // Inserting the Elements
          // using put method
        ht.put(1, "Mu");
        ht.put(2, "Nu");
        ht.put(3, "Pi");
        ht.put(4, "Nu");
  
        // Initial HashMap
        System.out.println("Initial map : " + ht);
  
          // Remove the map entry with key 4
        ht.remove(4);
  
        // Final Hashtable
        System.out.println("Updated map : " + ht);
    }
}
Exercise (Optional)
Successfully execute a Java program which emulates operations with your shopping cart while shopping at a grocery store

You need to use following HashMap Operations:

put: If you want the item, you need to put the item into shopping cart.
get: Once you reach the checkout line, you need to get the items out of your shopping cart for the cashier to tally.
remove: If you don’t want an item in your cart, you need to remove it.
Summary

Hashing is a way to store data into some data structure in such a way that the basic operations on that data i.e. the insertion, deletion, and searching can be performed in O(1) time.
In Hashtable we specify an object that is used as a key, and the value we want to associate to that key.
The key is then hashed, and the resulting hash code is used as the index at which the value is stored within the table.
The HashMap is a hashtable-based implementation. It extends the AbstractMap class and implements the Map interface. A HashMap works on the principle of hashing.
This Map implementation usually acts as a bucketed hash table, but when buckets get too large, they get transformed into nodes of TreeNodes, each structured similarly to those in java.util.TreeMap.


## Iterators
Learning Objectives

After completing this module, associates should be able to:

Describe the differences between Iterator and Iterable.
List the different ways to iterate.
Successfully execute a Java program that demonstrates all of the different ways to iterate.
Description
Iterable and Iterator Interface
The Iterable interface defines a data structure which can be directly traversed using the .iterator() method, which returns an Iterator. This can be useful for fine-grained control over iteration. The Iterator interface contains methods for traversal, including:

hasNext()
next()
remove()
For example:

List<String> names = new ArrayList<>();
// add names...
Iterator<String> it = names.iterator();
while (it.hasNext()) {
  String name = it.next();
  doSomething(name);
}
Enhanced For Loop
Any object which implements the Iterable interface can also be iterated over using a special kind of for-loop: the "enhanced" for loop. The syntax is as follows:

List<String> names = new ArrayList<>();
// add names...
for (String name : names) {
  System.out.println(name);
}
The downside of the enhanced for loop is that the index of iteration is not tracked, so fine-grain control over execution is not possible. However, this simplified syntax is beneficial for many simple iterations.

Both Iterator and Iterable are interfaces in Java that look very similar but both are two different things. In short, if any class implements the Iterable interface, it gains the ability to iterate over an object of that class using an Iterator.

Now let’s discuss the major differences between the two in detail:

Iterable
An Iterable represents a collection that can be traversed. Implementing the Iterable interface allows an object to make use of the for-each loop. It does that by internally calling the iterator() method on the object. For example, the following code only works as a List interface extends the Collection interface, and the Collection interface extends the Iterable interface.

List<String> persons = new ArrayList<>(Arrays.asList("A", "B", "C"));
 
for (String person: persons) {
    System.out.println(person);
}
Please note that we can also call the forEach() method on an iterable starting from Java 8, which performs the given action for each element of the Iterable. It is worth noting that the default implementation of forEach() also uses for-each internally.

On the other hand, Iterator is an interface that allows us to iterate over some other object, which is a collection of some kind. To iterate over an Iterator, we can use hasNext() + next() methods in a while loop, as shown below:

Iterator<Integer> iterator = Arrays.asList(1, 2, 3, 4, 5).iterator();
while (iterator.hasNext()) {
    System.out.println(iterator.next());
}
Starting from Java 8, we can also easily iterate over an Iterator by using the forEachRemaining() method.

Iterator<Integer> iterator = Arrays.asList(1, 2, 3, 4, 5).iterator();
iterator.forEachRemaining(System.out::println);
One can also use an Iterator inside a for-each loop by converting the Iterator into an Iterable by using a lambda:

for (Integer i: (Iterable<Integer>) () -> iterator) {
    System.out.println(i);
}
Any class that implements the Iterable interface needs to override the iterator() method provided by the Iterable interface. The iterator() method returns an Iterator, which then can be used to iterate over an object of that class.
Any class implementing the Iterator interface needs to override the hasNext() and next() methods provided by the Iterator interface. The hasNext() method returns true if the iteration has more elements, and the next() method returns the next element in the iteration.
The Iterator instance stores the iteration state. That means it provides utility methods to get the current element, check if the next element exists, and move forward to the next element if present. In other words, an Iterator remembers the current position in a collection and returns the next item in sequence if present. The Iterable, on the other hand, doesn’t maintain any such iteration state
The contract for Iterable is that it should produce a new instance of an Iterator every time the iterator() method is called. This is because the Iterator instance maintains the iteration state, and things won’t work as if the implementation returns the same Iterator twice.
For an Iterable, we can move forward only in the forward direction, but some of the Iterator subinterfaces, like ListIterator, allow us to move back and forth over a List.
Also, Iterable doesn’t provide any method to modify its elements, nor can we modify them using the for-each loop. But Iterator allows removing elements from the underlying collection during the iteration with the remove() method.
Real World Application

Java iterators play a crucial role in modern Java programming for several reasons:

Standardized Interface: Java iterators provide a standardized way to traverse elements in a collection, regardless of the underlying data structure. This abstraction allows developers to write code that operates on collections without needing to know the specific implementation details.
Safe Iteration: Iterators provide safe and fail-fast traversal of collections, ensuring that modifications to the underlying collection during iteration are detected and handled properly. This helps prevent concurrent modification exceptions and ensures data integrity during traversal.
Compatibility with Enhanced for-loop: Iterators seamlessly integrate with the enhanced for-loop syntax introduced in Java 5, providing a convenient and concise way to iterate over collections without the need for explicit iterator instantiation and traversal logic.
Support for Custom Collections: Iterators can be implemented for custom collection classes, allowing developers to define custom iteration logic tailored to the specific requirements of their collections. This enables the seamless integration of custom collections into existing codebases and libraries.
Implementation
Iterate an Iterable using enhanced for loop
Objects of classes implementing the Collection interface can be iterated using a for-each loop, since the Collection interface extends the Iterable interface.

// Java Program to demonstrate iterate 
// an iterable using for-each loop
  
import java.io.*;
import java.util.*;
  
class IterateUsingEnhancedForLoop {
    public static void main (String[] args) {
          
      // create a list
      List<String> list = new ArrayList<String>();
  
      // add elements
      list.add("people");
      list.add("for");
      list.add("people");
  
      // Iterate through the list
      for( String element : list ){
          System.out.println( element );
      }
    }
}
Iterate an Iterable using forEach loop
The forEach() method a lambda expression as a parameter. This lambda expression is called for each element of the collection. In the below example, for each element of the list, the function prints the element to the console.

// Java Program to demonstrate iterate
// an Iterable using forEach method
  
import java.io.*;
import java.util.*;
  
class IterateUsingforEach {
    public static void main(String[] args)
    {
          // create a list
        List<String> list = new ArrayList<>();
  
          // add elements to the list
        list.add("people");
        list.add("for");
        list.add("people");
  
          // Iterate through the list
        list.forEach(
            (element) -> { System.out.println(element); });
    }
}
Iterate an Iterable using Iterator
We can iterate the elements of Java Iterable by obtaining the Iterator from it using the iterator() method.

The methods used while traversing the collections using Iterator to perform the operations are:

hasNext(): It returns false if we have reached the end of the collection, otherwise returns true.
next(): Returns the next element in a collection.
remove(): Removes the last element returned by the iterator from the collection.
forEachRemaining(): Performs the given action for each remaining element in a collection, in sequential order.

// Java Program to demonstrate iterate
// an Iterable using an Iterator
  
import java.io.*;
import java.util.*;
  
class IterateUsingIterator {
    public static void main(String[] args)
    {
        List<String> list = new ArrayList<>();
  
        list.add("people");
        list.add("for");
        list.add("people");
  
        Iterator<String> iterator = list.iterator();
  
        while (iterator.hasNext()) {
            String element = iterator.next();
            System.out.println(element);
        }
    }
}
Exercises (Optional)
Write a single Java program that demonstrates the three different ways above to iterate.

Summary

The Iterable interface defines a data structure which can be directly traversed using the .iterator() method, which returns an Iterator. This can be useful for fine-grained control over iteration.
Any object which implements the Iterable interface can also be iterated over using a special kind of for-loop: the "enhanced" for loop.
The downside of the enhanced for loop is that the index of iteration is not tracked, so fine-grain control over execution is not possible. However, this simplified syntax is beneficial for many simple iterations.


## ArrayList and LinkedList
Learning Objectives

After completing this module, associates should be able to:

Describe the Arraylist and Linkedlist interfaces
Successfully execute a Java program that demonstrates both Arraylist and Linkedlist
Description

The ArrayList and LinkedList classes implement the List interface. This means you can perform list-specific operations, as well as the common Collection interface operations. Additionally, both classes include their own operations.

ArrayList
An ArrayList is a data structure which contains an array within it, but can resize dynamically. Once it reaches maximum capacity it will increase its size by copying its elements to a new larger, internal array. The main benefits of using an ArrayList in these cases are the ability to dynamically add and remove items, access items by index, and iterate over the contents easily.

LinkedList
A LinkedList is a data structure that is internally composed of nodes that contain data and a reference to the next node. A Doubly-Linked List is composed of nodes that contain references to the next and previous node. Java’s implementation of a LinkedList implements both the List interface and the Deque interface, meaning it can act as either a list or a queue. The main advantages of a LinkedList are efficient insertion and deletion operations and the ability for it to be used as a foundation for stacks, queues, and other complex data structures.

ArrayList	LinkedList
ArrayList internally uses a dynamic array to store the elements.	LinkedList internally uses a doubly linked list to store the elements.
Manipulation with ArrayList is slow because it internally uses an array. If any element is removed from the array, all the other elements are shifted in memory.	Manipulation with LinkedList is faster than ArrayList because of the use of nodes where you just need to change the nearest references of a node.
An ArrayList class can act as a list only because it implements List only.	LinkedList class can act as a list and queue both because it implements List and Deque interfaces.
ArrayList is better for storing and accessing data.	LinkedList is better for manipulating data.
The memory location for the elements of an ArrayList is contiguous.	The location for the elements of a linked list is not contiguous.
Resources
ArrayList class Java 8 Documentation: https://docs.oracle.com/javase/8/docs/api/java/util/ArrayList.html
LinkedList class Java 8 Documentation: https://docs.oracle.com/javase/8/docs/api/java/util/LinkedList.html
Deque Interface Java 8 Documentation: https://docs.oracle.com/javase/8/docs/api/java/util/Deque.html
List Interface Java 8 Documentation: https://docs.oracle.com/javase/8/docs/api/java/util/List.html
Real World Application

An ArrayList can be used whenever you need a resizable, ordered collection of elements. Here are some real-world examples of how an ArrayList can be used in different applications:

Caching popular searches:
Search engines can cache popular queries in an ArrayList for fast lookups.
High score tables:
Games can use an ArrayList to maintain a list of top scores, inserting new scores in the right position.
Storing user search history:
An ArrayList can store the list of recent search keywords entered by a user on a website. New keywords are easily added to the end.
A LinkedList can be used whenever you need a stack, queue, or other ordered collection of elements. Here are some real-world examples of how lists are used in different applications:

Undo/Redo operations in text editors:
A stack stores all text changes, pops them off to undo changes, and pushes them to redo.
Live support/chat:
Customer support requests can be put in a queue to be handled by the next available agent.
Tab management in browsers:
Open browser tabs can be stored in a deque, allowing users to switch between adjacent tabs from either end.
Implementation
Creating an ArrayList and LinkedList
// creating our collections
ArrayList<Integer> myArrayList = new ArrayList<>();
LinkedList<Integer> myLinkedList = new LinkedList<>();

// adding elements to our collections
Collections.addAll(myArrayList,
                      1, 2, 3, 4, 5);

Collections.addAll(myLinkedList,
      1, 2, 3, 4, 5);

// printing our collections
System.out.println(myArrayList); // [1, 2, 3, 4, 5]
System.out.println(myLinkedList); // [1, 2, 3, 4, 5]

// demonstrating that ArrayLists can resize, unlike arrays
System.out.println(myArrayList.size()); // 5
Collections.addAll(myArrayList,
      6, 7, 8, 9);
System.out.println(myArrayList.size()); // 9
In the above code, we creating an ArrayList and a LinkedList, populate the lists, and print them out. Rather than using the add() method that each collection defines, we use the Collections class’s addAll() method to add several elements to a specified collection. This is simply for convenience.

We also demonstrate that, unlike an array, an ArrayList and other collections are dynamic and can change their size.

Manipulating an ArrayList and LinkedList
// using indexes
System.out.println(myArrayList.get(3)); // 4
System.out.println(myLinkedList.indexOf(1)); // 0

// LinkedList-specific operations
  // deque operations
myLinkedList.addFirst(0);
myLinkedList.addLast(6);
System.out.println(myLinkedList); // [0, 1, 2, 3, 4, 5, 6]

  // Queue operations
System.out.println(myLinkedList.peek()); // 0
myLinkedList.offer(7);
myLinkedList.poll();
System.out.println(myLinkedList); // [1, 2, 3, 4, 5, 6, 7]

  // Stack operations
LinkedList<Integer> myStack = new LinkedList<>();
myStack.push(1);
myStack.push(2);
myStack.push(3);
myStack.push(4);
myStack.push(5);
System.out.println(myStack); // [5, 4, 3, 2, 1]

myStack.push(6);
System.out.println(myStack.peek()); // 6
System.out.println(myStack); // [6, 5, 4, 3, 2, 1]

myStack.pop();
System.out.println(myStack); // [5, 4, 3, 2, 1]
Both an ArrayList and LinkedList implement List. This means we can use list-specific operations, such as retrieve a value at a specific index or retrieve the index of a specific value. Because LinkedList also implements the Deque Interface, we can also perform deque, or queue, or stack operations.

Exercise (Optional)
Create a text-based Java program that simulates a music playlist application where the user can do the following:
add a song to the list
remove a song from the list
re-arrange the songs on the list
mark a song as their favorite and move it to the top of the list
mark a song as their least favorite and move it to the bottom of the list
Summary

An ArrayList is a concrete class which implements List.
It is a data structure which contains an array within it, but can resize dynamically.
It is useful for storing ordered elements and accessing elements using an index.
A LinkedList implements both the List and Queue interfaces, so it has all methods in both interfaces.
The data structure is composed of nodes that reference one another
It is useful for manipulating a collection of ordered elements and for creating stacks and queues.


## Priority Queue
Learning Objectives

After completing this module, associates should be able to:

Describe the priority queue
Successfully execute a Java program that demonstrates the priority queue
Description

The Java PriorityQueue class is a queue data structure implementation that processes the queue items based on their priorities. Note that PriorityQueue is different from other standard queues which implement the FIFO (First-In-First-Out) algorithm.

By default, the priority is determined by objects’ natural ordering. Default priority can be overridden by a Comparator provided at queue construction time.

A priority queue does not permit null elements. A priority queue relying on natural ordering also does not permit insertion of non-comparable objects (doing so may result in a ClassCastException).

The head of this queue is the least element with respect to the specified ordering. If multiple elements are tied for least value, the head is one of those elements -- ties are broken arbitrarily. The queue retrieval operations poll, remove, peek, and element access the element at the head of the queue.

A priority queue is unbounded, but has an internal capacity governing the size of an array used to store the elements on the queue. It is always at least as large as the queue size. As elements are added to a priority queue, its capacity grows automatically. The details of the growth policy are not specified.

This class and its iterator implement all of the optional methods of the Collection and Iterator interfaces. The Iterator provided in the iterator() method is not guaranteed to traverse the elements of the priority queue in any particular order. If you need ordered traversal, consider using Arrays.sort(pq.toArray()).

Note that this implementation is not synchronized. Multiple threads should not access a PriorityQueue instance concurrently if any of the threads modifies the queue. Instead, use the thread-safe PriorityBlockingQueue class.

PriorityQueue Features
Let’s note down a few important features of the PriorityQueue.

PriorityQueue is an unbounded queue that grows dynamically.
The default initial capacity is '11' which can be overridden using the initialCapacity parameter in the appropriate constructor.
It does not allow NULL objects.
The queue items must be Comparable to determine their priorities.
By default, the items in the priority queue are ordered in natural order.
A Comparator can be used for custom ordering of objects in the queue.
PriorityQueue relying on natural ordering does not permit insertion of non-comparable objects (doing so may result in ClassCastException).
The queue retrieval operations poll, remove, peek, and element access the element at the head of the queue.
The head of the PriorityQueue is the least element based on the natural ordering or the Comparator based ordering.
If multiple objects are present of the same priority then the queue can poll any one of them randomly.
PriorityQueue is not thread safe. Use PriorityBlockingQueue in concurrent environments.
It provides O(log(n)) time performance for add() and poll() methods.
The Iterator provided by the method iterator() is not guaranteed to traverse the elements of the priority queue in any particular order. If you need ordered traversal, consider using Arrays.sort(pq.toArray()).
PriorityQueue Constructors
The PriorityQueue class provides 6 different ways to construct a priority queue in Java.

PriorityQueue() : constructs empty queue with the default initial capacity (11) that orders its elements according to their natural ordering.
PriorityQueue(Collection c) : constructs empty queue containing the elements in the specified collection.
PriorityQueue(int initialCapacity) : constructs empty queue with the specified initial capacity that orders its elements according to their natural ordering.
PriorityQueue(int initialCapacity, Comparator comparator) : constructs empty queue with the specified initial capacity that orders its elements according to the specified comparator.
PriorityQueue(PriorityQueue c) : Creates a priority queue containing the elements in the specified priority queue. The elements are ordered according to their natural ordering.
PriorityQueue(SortedSet c) : Creates a priority queue containing the elements in the specified sorted set. The elements are ordered according to their natural ordering.
PriorityQueue Methods
Below is a list of important methods that the PriorityQueue class provides.

Adding Items
boolean add(object) : Inserts the specified element into this priority queue. If the queue is full, it throws an exception.
boolean offer(object) : Inserts the specified element into this priority queue. If the queue is full, it returns false.
Accessing Items
Object element() : Retrieves, but does not remove, the head of this queue, or throws NoSuchElementException if this queue is empty.
Object peek() : Retrieves, but does not remove, the head of this queue, or returns null if this queue is empty.
Removing Items
boolean remove(object) : Removes a single instance of the specified element from this queue, if it is present.
Object poll() : Retrieves and removes the head of this queue, or returns null if this queue is empty.
void clear() : Removes all of the elements from this priority queue.
Other Methods
Comparator comparator() : Returns the comparator used to order the elements in this queue, or null if this queue is sorted according to the natural ordering of its elements.
boolean contains(Object o) : Returns true if this queue contains the specified element.
Iterator iterator() : Returns an iterator over the elements in this queue.
int size() : Returns the number of elements in this queue.
Object[] toArray() : Returns an array containing all of the elements in this queue.
Real World Application

A Priority Queue is different from a normal queue, because instead of being “first-in-first-out”, values come out in order by priority. It is an abstract data type that captures the idea of a container whose elements have “priorities” attached to them. An element of highest priority always appears at the front of the queue. If that element is removed, the next highest priority element advances to the front.

A priority queue is typically implemented using a Heap data structure.

Applications:

Dijkstra’s Shortest Path Algorithm using priority queue: When the graph is stored in the form of adjacency list or matrix, priority queues can be used to extract minimum efficiently when implementing Dijkstra’s algorithm.
Prim’s algorithm: It is used to implement Prim’s Algorithm to store keys of nodes and extract minimum key node at every step.
Data compression : It is used in Huffman codes which is used to compresses data.
Artificial Intelligence : A* Search Algorithm : The A* search algorithm finds the shortest path between two vertices of a weighted graph, trying out the most promising routes first. The priority queue (also known as the fringe) is used to keep track of unexplored routes, the one for which a lower bound on the total path length is smallest is given highest priority.
Heap Sort : Heap sort is typically implemented using Heap which is an implementation of Priority Queue.
Operating systems: It is also use in Operating Systems for load balancing (load balancing on server), interrupt handling.
Implementation

The example below explains the following basic operations of the priority queue.

boolean add(E element): This method inserts the specified element into this priority queue.
public peek(): This method retrieves, but does not remove, the head of this queue, or returns null if this queue is empty.
public poll(): This method retrieves and removes the head of this queue, or returns null if this queue is empty.

// Java program to demonstrate the
// working of PriorityQueue
import java.util.*;
  
class PriorityQueueDemo {
    
      // Main Method
    public static void main(String args[])
    {
        // Creating empty priority queue
        PriorityQueue<Integer> pQueue = new PriorityQueue<Integer>();
  
        // Adding items to the pQueue using add()
        pQueue.add(10);
        pQueue.add(20);
        pQueue.add(15);
  
        // Printing the top element of PriorityQueue
        System.out.println(pQueue.peek());
  
        // Printing the top element and removing it
        // from the PriorityQueue container
        System.out.println(pQueue.poll());
  
        // Printing the top element again
        System.out.println(pQueue.peek());
    }
}
Operations on PriorityQueue
Let’s see how to perform a few frequently used operations on the Priority Queue class.

Adding Elements
In order to add an element in a priority queue, we can use the add() method. The insertion order is not retained in the PriorityQueue. The elements are stored based on the priority order which is ascending by default.

import java.util.*;
import java.io.*;
    
public class PriorityQueueDemo {
    
    public static void main(String args[])
    {
        PriorityQueue<Integer> pq = new PriorityQueue<>();
        for(int i=0;i<3;i++){
            pq.add(i);
            pq.add(1);
        }
        while (!pq.isEmpty()) {
            System.out.print(pq.poll() + " ");
        }
    }
}
Output

[0, 1, 1, 1, 1, 2]
We will not get sorted elements by printing PriorityQueue.

import java.util.*;
import java.io.*;
    
public class PriorityQueueDemo {
    
    public static void main(String args[])
    {
        PriorityQueue<String> pq = new PriorityQueue<>();
    
        pq.add("Gators");
        pq.add("For");
        pq.add("Gators");
    
        System.out.println(pq);
    }
}
Output

[For, Gators, Gators]
Removing Elements
In order to remove an element from a priority queue, we can use the remove() method. If there are multiple such objects, then the first occurrence of the object is removed. Apart from that, the poll() method is also used to remove the head and return it.

// Java program to remove elements
// from a PriorityQueue
  
import java.util.*;
import java.io.*;
  
public class PriorityQueueDemo {
  
    public static void main(String args[])
    {
        PriorityQueue<String> pq = new PriorityQueue<>();
  
        pq.add("Gators");
        pq.add("For");
        pq.add("Gators");
  
        System.out.println("Initial PriorityQueue " + pq);
  
          // using the method
        pq.remove("Gators");
  
        System.out.println("After Remove - " + pq);
  
        System.out.println("Poll Method - " + pq.poll());
  
        System.out.println("Final PriorityQueue - " + pq);
    }
}
Output

Initial PriorityQueue [For, Gators, Gators]
After Remove - [For, Gators]
Poll Method - For
Final PriorityQueue - [Gators]
Accessing the elements
Since Queue follows the First In First Out principle, we can access only the head of the queue. To access elements from a priority queue, we can use the peek() method.

// Java program to access elements
// from a PriorityQueue
import java.util.*;
  
class PriorityQueueDemo {
    
      // Main Method
    public static void main(String[] args)
    {
  
        // Creating a priority queue
        PriorityQueue<String> pq = new PriorityQueue<>();
        pq.add("Gators");
        pq.add("For");
        pq.add("Gators");
        System.out.println("PriorityQueue: " + pq);
  
        // Using the peek() method
        String element = pq.peek();
        System.out.println("Accessed Element: " + element);
    }
}
Output

PriorityQueue: [For, Gators, Gators]
Accessed Element: For
Iterating the PriorityQueue
There are multiple ways to iterate through the PriorityQueue. The most famous way is converting the queue to the array and traversing using the for loop. However, the queue also has a built-in iterator which can be used to iterate through the queue.

// Java program to iterate elements
// to a PriorityQueue
  
import java.util.*;
  
public class PriorityQueueDemo {
  
      // Main Method
    public static void main(String args[])
    {
        PriorityQueue<String> pq = new PriorityQueue<>();
  
        pq.add("Gators");
        pq.add("For");
        pq.add("Gators");
  
        Iterator iterator = pq.iterator();
  
        while (iterator.hasNext()) {
            System.out.print(iterator.next() + " ");
        }
    }
}
Output

For Gators Gators
Exercises (Optional)
Write a program that takes five random United States, adds them to a Priority Queue in a random order, and then outputs the states in alphabetical order.

Summary

The Java PriorityQueue class is a queue data structure implementation that processes the queue items based on their priorities. Note that PriorityQueue is different from other standard queues which implement the FIFO (First-In-First-Out) algorithm.
The elements of the priority queue are ordered according to their natural ordering, or by a Comparator provided at queue construction time, depending on which constructor is used.
The head of this queue is the least element with respect to the specified ordering.
If multiple elements are tied for least value, the head is one of those elements -- ties are broken arbitrarily.
A priority queue is unbounded, but has an internal capacity governing the size of an array used to store the elements on the queue. It is always at least as large as the queue size. As elements are added to a priority queue, its capacity grows automatically. The details of the growth policy are not specified.
This class and its iterator implement all of the optional methods of the Collection and Iterator interfaces.
Note that this implementation is not synchronized. Multiple threads should not access a PriorityQueue instance concurrently if any of the threads modifies the queue. Instead, use the thread-safe PriorityBlockingQueue class.


## Stacks and Vector
Learning Objectives

After completing this module, associates should be able to:

Describe the Stack and Vector collections
Successfully execute a Java program that demonstrates stack and vector
Description
Vector
The Vector class implements a growable array of objects. Like an array, it contains components that can be accessed using an integer index. However, the size of a Vector can grow or shrink as needed to accommodate adding and removing items after the Vector has been created.

Each vector tries to optimize storage management by maintaining a capacity and a capacityIncrement. The capacity is always at least as large as the vector size; it is usually larger because as components are added to the vector, the vector's storage increases in chunks the size of capacityIncrement. An application can increase the capacity of a vector before inserting a large number of components; this reduces the amount of incremental reallocation.

The iterators returned by this class's iterator and listIterator methods are fail-fast: if the vector is structurally modified at any time after the iterator is created, in any way except through the iterator's own remove or add methods, the iterator will throw a ConcurrentModificationException. Thus, in the face of concurrent modification, the iterator fails quickly and cleanly, rather than risking arbitrary, non-deterministic behavior at an undetermined time in the future. The Enumerations returned by the elements method are not fail-fast.

Note that the fail-fast behavior of an iterator cannot be guaranteed as it is, generally speaking, impossible to make any hard guarantees in the presence of unsynchronized concurrent modification. Fail-fast iterators throw ConcurrentModificationException on a best-effort basis. Therefore, it would be wrong to write a program that depended on this exception for its correctness: the fail-fast behavior of iterators should be used only to detect bugs.

As of the Java 2 platform v1.2, this class was retrofitted to implement the List interface, making it a member of the Java Collections Framework. Unlike the new collection implementations, Vector is synchronized. If a thread-safe implementation is not needed, it is recommended to use ArrayList in place of Vector.

Stack
The Stack class represents a last-in-first-out (LIFO) stack of objects. It extends class Vector with five operations that allow a vector to be treated as a stack. The usual push and pop operations are provided, as well as a method to peek at the top item on the stack, a method to test for whether the stack is empty, and a method to search the stack for an item and discover how far it is from the top. When a stack is first created, it contains no items.

A more complete and consistent set of LIFO stack operations is provided by the Deque interface and its implementations, which should be used in preference to this class.

In the following example, we have created an instance of the Stack class. After that, we have invoked the empty() method two times. The first time it returns true because we have not pushed any element into the stack. After that, we have pushed elements into the stack. Again we have invoked the empty() method that returns false because the stack is not empty.


import java.util.Stack;  
public class StackEmptyMethodExample  
{  
public static void main(String[] args)   
{  
//creating an instance of Stack class  
Stack<Integer> stk= new Stack<>();  
// checking stack is empty or not  
boolean result = stk.empty();  
System.out.println("Is the stack empty? " + result);  
// pushing elements into stack  
stk.push(78);  
stk.push(113);  
stk.push(90);  
stk.push(120);  
//prints elements of the stack  
System.out.println("Elements in Stack: " + stk);  
result = stk.empty();  
System.out.println("Is the stack empty? " + result);  
}  
} 
Output:


Is the stack empty? true
Elements in Stack: [78, 113, 90, 120]
Is the stack empty? false
Stack Class push() and pop() Methods
The push() method inserts an item onto the top of the stack. It works the same as the method addElement(item) method of the Vector class. It passes a parameter item to be pushed into the stack. The pop() method removes an object at the top of the stack and returns the same object. It throws EmptyStackException if the stack is empty.

Let's implement the stack in a Java program and perform push and pop operations.

import java.util.*;  
public class StackPushPopExample   
{  
public static void main(String args[])   
{  
//creating an object of Stack class  
Stack <Integer> stk = new Stack<>();  
System.out.println("stack: " + stk);  
//pushing elements into the stack  
pushelmnt(stk, 20);  
pushelmnt(stk, 13);  
pushelmnt(stk, 89);  
pushelmnt(stk, 90);  
pushelmnt(stk, 11);  
pushelmnt(stk, 45);  
pushelmnt(stk, 18);  
//popping elements from the stack  
popelmnt(stk);  
popelmnt(stk);  
//throws exception if the stack is empty  
try   
{  
popelmnt(stk);  
}   
catch (EmptyStackException e)   
{  
System.out.println("empty stack");  
}  
}  
//performing push operation  
static void pushelmnt(Stack stk, int x)   
{  
//invoking push() method      
stk.push(new Integer(x));  
System.out.println("push -> " + x);  
//prints modified stack  
System.out.println("stack: " + stk);  
}  
//performing pop operation  
static void popelmnt(Stack stk)   
{  
System.out.print("pop -> ");  
//invoking pop() method   
Integer x = (Integer) stk.pop();  
System.out.println(x);  
//prints modified stack  
System.out.println("stack: " + stk);  
}  
}  
Output:

stack: []
push -> 20
stack: [20]
push -> 13
stack: [20, 13]
push -> 89
stack: [20, 13, 89]
push -> 90
stack: [20, 13, 89, 90]
push -> 11
stack: [20, 13, 89, 90, 11]
push -> 45
stack: [20, 13, 89, 90, 11, 45]
push -> 18
stack: [20, 13, 89, 90, 11, 45, 18]
pop -> 18
stack: [20, 13, 89, 90, 11, 45]
pop -> 45
stack: [20, 13, 89, 90, 11]
pop -> 11
stack: [20, 13, 89, 90]
Real World Application

Here are some different applications of the stack data structure in computer science:

Memory management
Function calls, particularly recursive function calls
String reversal
Parenthesis checking
Syntax parsing
Matching HTML tags in web development
Arithmetic expression evaluation
Implementation

Below is an example of implementing a stack:

public class Stack {

  private int lastItem = 0;
  private int[] items = new int[0];

  // add items to the stack
  public void push(int newItem) {

  }

  // remove items from the stack
  public int pop() {
    return 0;
  }

  // view last item
  public int peek() {
    return 0;
  }
}
We'll use this basic structure to keep track of the last item we've added and an array to manage all of the items.

Notice we've initialized our items array to be 0 items long. You'll see why we did this shortly.

Implementing Push
The first thing we'll do is implement the push() functionality. Push is used to add an item to the stack and update the lastItem to the one we're pushing.

To add an item to the array we'll simply add the new item (defined by the parameter) to the end of the items array. But...how do you know where the end of the array is? Since we initialized it with a length of zero, what we can do is use the length property to find the end. In addition to that, though, we must increase the length of our array so that we can actually fit some items within it. If we increase it by 1, that'll suffice to fit in our new 1 item.

So our code can look like this:

public void push(int newItem){

  int index = items.length; // Get current length to store as index

  // Create a new array with an increased size
  int[] newArray = new int[items.length + 1];

  // Copy all elements from the old array to the new array
  System.arraycopy(items, 0, newArray, 0, items.length);

  // Add the new item at the correct position
  newArray[index] = newItem;

  // Update the reference to the new array
  items = newArray;

  // Update lastItem reference
  lastItem = newItem;

  // Debugging output
  System.out.println("Updated stack: " + Arrays.toString(items));
}
Implementing Pop
Adding items to the Stack is great, but we'll also need to be able to remove items so that we can do a little bit of testing of our code. You'll find that our current approach to a Stack may be challenging to support.

For instance we have a reference to the last item, but we can't simply return that? We need to remove it entirely from the array. Arrays are fixed-sized objects in Java, so that means we'll have to copy over all of the elements to a new array and get rid of the old one.

Adding the following code to our class:

public int pop(){

  int tempItem = lastItem; //store the last item into a variable
  
   //copy a new array but remove last item
   int[] newArray = new int[items.length-1];

   for(int i = 0; i < newArray.length; i++){
     newArray[i] = items[i];
  }

   items = newArray; //update the items array
   lastItem = items[items.length-1]; //update lastItem

   //return our ‘popped' element
   return tempItem;
}
Ok. So now we can test our code a bit.

Testing your Stack
We will create a new class in our project called TestStack.

Adding the following contents to TestStack:

public class TestStack {

  public static void main(String[] args) {
    Stack stack = new Stack();

    stack.push(1);
    stack.push(2);

    int value = stack.pop();

    System.out.println(value);
    System.out.println(value);
  }
}
Essentially, we'll try adding two values to the stack and them popping them off. What happens when we run the code?



The console prints out 2 twice?!?!

Why is that?

Just looking at our code you may already have an inclination. However, imagine that you really weren't sure (and maybe you're not). Then you should start by examining how each of our methods accomplish their tasks.

Thus, let's re-examine the push() method first. One simple approach to debugging our issue is to use System.out.println(...) statements to verify our assumed values.

We will update our push method to the code below which adds println() statements after each time we alter any data.

public void push(int newItem) {

   int index = items.length; // Get current length to store as index
    System.out.println("Current array length (index for new item): " + index);
    
    // Create a new array with an increased size
    int[] newArray = new int[items.length + 1];
    System.out.println("Created a new array with size: " + newArray.length);
    
    // Copy all elements from the old array to the new array
    System.arraycopy(items, 0, newArray, 0, items.length);
    System.out.println("Copied old array elements to new array: " + Arrays.toString(newArray));
    
    // Add the new item at the correct position
    newArray[index] = newItem;
    System.out.println("Added new item (" + newItem + ") at index " + index + ": " + Arrays.toString(newArray));

    // Update the reference to the new array
    items = newArray;
    System.out.println("Updated items array reference: " + Arrays.toString(items));

    // Update lastItem reference
    lastItem = newItem;
    System.out.println("Updated lastItem to: " + lastItem);
}
When we re-run the TestStack class.

What do we see on the console?



The first line is fine, but then the second line has this interesting output that starts with a left bracket "[". Recall that arrays are objects and printing an object defaults to its memory location. So what is display on the console is actually a memory address.

A simple shortcut that we'll show you is to use the Arrays class and call the Arrays.toString(...) method. This methods will convert the contents of an array to a String.

Updateing our code to use Arrays.toString(...):

public void push(int newItem) {

  int index = items.length; // get current length to store as index
  System.out.println("The array length is: " + index);

  items = new int[items.length + 1]; // increase the array by 1
  System.out.println("items after extending the length by 1: " + Arrays.toString(items));

  items[index] = newItem; // store the value in the index
  System.out.println("items after storing the new value: "+ Arrays.toString(items));

  lastItem = newItem; // update lastItem
  System.out.println("The last item: " + lastItem);
}
When we re-run the TestStack class we'll see output like the following:



Highlighted in red we have a particular issue.

The value printed is [0, 0] and then [0, 2]. This indicates that our array is not saving any previous values! On further inspection of the code, we can see that instead of copying the values of the array when we extend it by one element we merely create a new array which will default to have all 0's as elements.

Summary

The Vector class implements a growable array of objects. Like an array, it contains components that can be accessed using an integer index. However, the size of a Vector can grow or shrink as needed to accommodate adding and removing items after the Vector has been created.
As of the Java 2 platform v1.2, this class was retrofitted to implement the List interface, making it a member of the Java Collections Framework.
Unlike the new collection implementations, Vector is synchronized.
If a thread-safe implementation is not needed, it is recommended to use ArrayList in place of Vector.
The Stack class represents a last-in-first-out (LIFO) stack of objects. It extends class Vector with five operations that allow a vector to be treated as a stack. The usual push and pop operations are provided, as well as a method to peek at the top item on the stack, a method to test for whether the stack is empty, and a method to search the stack for an item and discover how far it is from the top.
When a stack is first created, it contains no items.
A more complete and consistent set of LIFO stack operations is provided by the Deque interface and its implementations, which should be used in preference to this class.


## Comparable Interface
Learning Objectives

After completing this module, associates should be able to:

Describe the comparable interface
Contrast the comparable and comparator interfaces
Successfully execute a Java program that demonstrates the comparable interface
Description

Comparable is an interface which defines the natural ordering for a class. A class must implement Comparable if it is to be sorted by the compareTo() method.

public interface Comparable<T> {
	public int compareTo(T o);
}
The compareTo() method returns an int which is:

Zero, if the two objects are equal
Negative, if this object is smaller than that
Positive, if this object is greater than that
We can sort the elements of:

String objects
Wrapper class objects
User-defined class objects
Collections class
Collections class provides static methods for sorting the elements of collections. If collection elements are of Set or Map, we can use TreeSet or TreeMap. However, we cannot sort the elements of List. Collections class provides methods for sorting the elements of List type elements.

Real World Application

The comparable interface is often used to implement sorting routines in Java.

Some of the best examples of real-world implementation of sorting are:

Bubble sorting is used in programming guide (cable box/satellite box) software to sort channels based on numerical order, alphabetical order, or in some cases, audience viewing time.
Databases use external merge sort to sort sets of data that are too large to be fully loaded into memory.
Online sports scoreboards are organized by the quick sort algorithm in real time.
Implementation
Java Comparable Example
Let's see the example of the Comparable interface that sorts the list elements on the basis of age.

class Student implements Comparable<Student> {
    int rollno;
    String name;
    int age;

    Student(int rollno, String name, int age) {
        this.rollno = rollno;
        this.name = name;
        this.age = age;
    }

    public int compareTo(Student st) {
        if (age == st.age)
            return 0;
        else if (age > st.age)
            return 1;
        else
            return -1;
    }
}
import java.util.*;

public class TestSort1 {
    public static void main(String args[]) {
        ArrayList<Student> al = new ArrayList<Student>();
        al.add(new Student(101, "Vijay", 23));
        al.add(new Student(106, "Ajay", 27));
        al.add(new Student(105, "Jai", 21));

        Collections.sort(al);
        for (Student st : al) {
            System.out.println(st.rollno + " " + st.name + " " + st.age);
        }
    }
}
105 Jai 21
101 Vijay 23
106 Ajay 27
Java Comparable Example: reverse order
Let's see the same example of the Comparable interface that sorts the list elements on the basis of age in reverse order.

class Student implements Comparable<Student> {
    int rollno;
    String name;
    int age;

    Student(int rollno, String name, int age) {
        this.rollno = rollno;
        this.name = name;
        this.age = age;
    }

    public int compareTo(Student st) {
        if (age == st.age)
            return 0;
        else if (age < st.age)
            return 1;
        else
            return -1;
    }
}   

import java.util.*;

public class TestSort2 {
    public static void main(String args[]) {
        ArrayList<Student> al = new ArrayList<Student>();
        al.add(new Student(101, "Vijay", 23));
        al.add(new Student(106, "Ajay", 27));
        al.add(new Student(105, "Jai", 21));

        Collections.sort(al);
        for (Student st : al) {
            System.out.println(st.rollno + " " + st.name + " " + st.age);
        }
    }
}

106 Ajay 27
101 Vijay 23
105 Jai 21
Summary

Comparable is an interface which defines the natural ordering for a class. A class must implement Comparable if it is to be sorted by the compareTo() method.
The compareTo() method returns an int which is:
Zero, if the two objects are equal
Negative, if this object is smaller than that
Positive, if this object is greater than that
We can sort the elements of:
String objects
Wrapper class objects
User-defined class objects


## Comparator Interface
Learning Objectives

After completing this module, associates should be able to:

Describe the comparator interface
Successfully execute a Java program that demonstrates the comparator interface
Description

Comparator is an interface which allows you to define a total ordering on some collection of objects. A class that is to be sorted by compare() does not have to implement Comparator.

public interface Comparator<T> {
	public int compare(T firstObject, T secondObject);
}
A comparison function, which imposes a total ordering on some collection of objects. Comparators can be passed to a sort method (such as Collections.sort or Arrays.sort) to allow precise control over the sort order. Comparators can also be used to control the order of certain data structures (such as sorted sets or sorted maps), or to provide an ordering for collections of objects that don't have a natural ordering. The ordering imposed by a comparator c on a set of elements S is said to be consistent with equals if and only if c.compare(e1, e2)==0 has the same boolean value as e1.equals(e2) for every e1 and e2 in S.

Caution should be exercised when using a comparator capable of imposing an ordering inconsistent with equals to order a sorted set (or sorted map). Suppose a sorted set (or sorted map) with an explicit comparator c is used with elements (or keys) drawn from a set S. If the ordering imposed by c on S is inconsistent with equals, the sorted set (or sorted map) will behave "strangely." In particular the sorted set (or sorted map) will violate the general contract for set (or map), which is defined in terms of equals.

For example, suppose one adds two elements a and b such that (a.equals(b) && c.compare(a, b) != 0) to an empty TreeSet with comparator c. The second add operation will return true (and the size of the tree set will increase) because a and b are not equivalent from the tree set's perspective, even though this is contrary to the specification of the Set.add method.

Note: It is generally a good idea for comparators to also implement java.io.Serializable, as they may be used as ordering methods in serializable data structures (like TreeSet, TreeMap). In order for the data structure to serialize successfully, the comparator (if provided) must implement Serializable.

For the mathematically inclined, the relation that defines the imposed ordering that a given comparator c imposes on a given set of objects S is:

   {(x, y) such that c.compare(x, y) <= 0}.
The quotient for this total order is: {(x, y) such that c.compare(x, y) == 0}.

It follows immediately from the contract for compare that the quotient is an equivalence relation on S, and that the imposed ordering is a total order on S. When we say that the ordering imposed by c on S is consistent with equals, we mean that the quotient for the ordering is the equivalence relation defined by the objects' equals(Object) method(s): {(x, y) such that x.equals(y)}. Unlike Comparable, a comparator may optionally permit comparison of null arguments, while maintaining the requirements for an equivalence relation.

This interface is a member of the Java Collections Framework.

Real World Application

Java Comparator interface imposes a total ordering on the objects which may not have a desired natural ordering.

For example, for a List of Employee objects, the natural order may be ordered by employee’s id. But in real-life applications, we may want to sort the list of employees by their first name, date of birth or simply any other such criteria. In such conditions, we need to use Comparator interface.

We can use the Comparator interface in the following situations.

Sorting the array or list of objects, but NOT in natural order.
Sorting the array or list of objects where we can not modify the source code to implement Comparable interface.
Using group by sort on list or array of objects on multiple different fields.
Implementation

Suppose we have an Array/ArrayList of our own class type, containing fields like rollNo, name, address, DOB, etc, and we need to sort the array based on rollNo or name?

Method 1: One obvious approach is to write our own sort() function using one of the standard algorithms. This solution requires rewriting the whole sorting code for different criteria like RollNo and Name.
Method 2: Use the Comparator interface to order the objects of our user-defined class. This interface is present in java.util package and contains 2 methods compare(Object obj1, Object obj2) and equals(Object element). Using a comparator, we can sort the elements based on data members. For instance, it may be on rollNo, name, age, or anything else.
How do the sort() method of Collections class work?
Internally the sort() method calls the compare() method of the classes it is sorting. To compare two elements, it asks “Which is greater?” The compare() method returns -1, 0, or 1 to say if it is less than, equal, or greater to the other. It uses this result to then determine if they should be swapped for their sort.

Below is an example.

// Java Program to Demonstrate Working of
// Comparator Interface
 
// Importing required classes
import java.io.*;
import java.lang.*;
import java.util.*;
 
// Class 1
// A class to represent a Student
class Student {
 
    // Attributes of a student
    int rollno;
    String name, address;
 
    // Constructor
    public Student(int rollno, String name, String address)
    {
 
        // This keyword refers to current instance itself
        this.rollno = rollno;
        this.name = name;
        this.address = address;
    }
 
    // Method of Student class
    // To print student details in main()
    public String toString()
    {
 
        // Returning attributes of Student
        return this.rollno + " " + this.name + " "
            + this.address;
    }
}
 
// Class 2
// Helper class implementing Comparator interface
class Sortbyroll implements Comparator<Student> {
 
    // Method
    // Sorting in ascending order of roll number
    public int compare(Student a, Student b)
    {
 
        return a.rollno - b.rollno;
    }
}
 
// Class 3
// Helper class implementing Comparator interface
class Sortbyname implements Comparator<Student> {
 
    // Method
    // Sorting in ascending order of name
    public int compare(Student a, Student b)
    {
 
        return a.name.compareTo(b.name);
    }
}
 
// Class 4
// Main class
class GFG {
 
    // Main driver method
    public static void main(String[] args)
    {
 
        // Creating an empty ArrayList of Student type
        ArrayList<Student> ar = new ArrayList<Student>();
 
        // Adding entries in above List
        // using add() method
        ar.add(new Student(111, "Mayank", "london"));
        ar.add(new Student(131, "Anshul", "nyc"));
        ar.add(new Student(121, "Solanki", "jaipur"));
        ar.add(new Student(101, "Aggarwal", "Hongkong"));
 
        // Display message on console for better readability
        System.out.println("Unsorted");
 
        // Iterating over entries to print them
        for (int i = 0; i < ar.size(); i++)
            System.out.println(ar.get(i));
 
        // Sorting student entries by roll number
        Collections.sort(ar, new Sortbyroll());
 
        // Display message on console for better readability
        System.out.println("\nSorted by rollno");
 
        // Again iterating over entries to print them
        for (int i = 0; i < ar.size(); i++)
            System.out.println(ar.get(i));
 
        // Sorting student entries by name
        Collections.sort(ar, new Sortbyname());
 
        // Display message on console for better readability
        System.out.println("\nSorted by name");
 
        // // Again iterating over entries to print them
        for (int i = 0; i < ar.size(); i++)
            System.out.println(ar.get(i));
    }
}
Output

Unsorted
111 Mayank london
131 Anshul nyc
121 Solanki jaipur
101 Aggarwal Hongkong

Sorted by rollno
101 Aggarwal Hongkong
111 Mayank london
121 Solanki jaipur
131 Anshul nyc

Sorted by name
101 Aggarwal Hongkong
131 Anshul nyc
111 Mayank london
121 Solanki jaipur
By changing the return value inside the compare method, you can sort in any order that you wish to. For example, for descending order we can just change the positions of ‘a’ and ‘b’ in the above compare method.

Sort collection by more than one field
In the previous example, we have discussed how to sort the list of objects on the basis of a single field using the Comparable and Comparator interfaces. But, what if we have a requirement to sort ArrayList objects in accordance with more than one field like firstly, sort according to the student name and secondly, sort according to student age?

Below is an example.

// Java Program to Demonstrate Working of
// Comparator Interface Via More than One Field
 
// Importing required classes
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.Iterator;
import java.util.List;
 
// Class 1
// Helper class representing a Student
class Student {
 
    // Attributes of student
    String Name;
    int Age;
 
    // Parameterized constructor
    public Student(String Name, Integer Age)
    {
 
        // This keyword refers to current instance itself
        this.Name = Name;
        this.Age = Age;
    }
 
    // Getter setter methods
    public String getName() { return Name; }
 
    public void setName(String Name) { this.Name = Name; }
 
    public Integer getAge() { return Age; }
 
    public void setAge(Integer Age) { this.Age = Age; }
 
    // Method
    // Overriding toString() method
    @Override public String toString()
    {
        return "Customer{"
            + "Name=" + Name + ", Age=" + Age + '}';
    }
 
    // Class 2
    // Helper class implementing Comparator interface
    static class CustomerSortingComparator
        implements Comparator<Student> {
 
        // Method 1
        // To compare customers
        @Override
        public int compare(Student customer1,
                           Student customer2)
        {
 
            // Comparing customers
            int NameCompare = customer1.getName().compareTo(
                customer2.getName());
 
            int AgeCompare = customer1.getAge().compareTo(
                customer2.getAge());
 
            // 2nd level comparison
            return (NameCompare == 0) ? AgeCompare
                                      : NameCompare;
        }
    }
 
    // Method 2
    // Main driver method
    public static void main(String[] args)
    {
 
        // Create an empty ArrayList
        // to store Student
        List<Student> al = new ArrayList<>();
 
        // Create customer objects
        // using constructor initialization
        Student obj1 = new Student("Ajay", 27);
        Student obj2 = new Student("Sneha", 23);
        Student obj3 = new Student("Simran", 37);
        Student obj4 = new Student("Ajay", 22);
        Student obj5 = new Student("Ajay", 29);
        Student obj6 = new Student("Sneha", 22);
 
        // Adding customer objects to ArrayList
        // using add() method
        al.add(obj1);
        al.add(obj2);
        al.add(obj3);
        al.add(obj4);
        al.add(obj5);
        al.add(obj6);
 
        // Iterating using Iterator
        // before Sorting ArrayList
        Iterator<Student> custIterator = al.iterator();
 
        // Display message
        System.out.println("Before Sorting:\n");
 
        // Holds true till there is single element
        // remaining in List
        while (custIterator.hasNext()) {
 
            // Iterating using next() method
            System.out.println(custIterator.next());
        }
 
        // Sorting using sort method of Collections class
        Collections.sort(al,
                         new CustomerSortingComparator());
 
        // Display message only
        System.out.println("\n\nAfter Sorting:\n");
 
        // Iterating using enhanced for-loop
        // after Sorting ArrayList
        for (Student customer : al) {
            System.out.println(customer);
        }
    }
}
Output

Before Sorting:

Customer{Name=Ajay, Age=27}
Customer{Name=Sneha, Age=23}
Customer{Name=Simran, Age=37}
Customer{Name=Ajay, Age=22}
Customer{Name=Ajay, Age=29}
Customer{Name=Sneha, Age=22}


After Sorting:

Customer{Name=Ajay, Age=22}
Customer{Name=Ajay, Age=27}
Customer{Name=Ajay, Age=29}
Customer{Name=Simran, Age=37}
Customer{Name=Sneha, Age=22}
Customer{Name=Sneha, Age=23}
Exercise (Optional)
Use the Comparator interface to modify both programs above to sort by name in reverse order.

Summary

A comparison function, which imposes a total ordering on some collection of objects.

Comparators can be passed to a sort method (such as Collections.sort or Arrays.sort) to allow precise control over the sort order.
Comparators can also be used to control the order of certain data structures (such as sorted sets or sorted maps), or to provide an ordering for collections of objects that don't have a natural ordering.
The ordering imposed by a comparator c on a set of elements S is said to be consistent with equals if and only if c.compare(e1, e2)==0 has the same boolean value as e1.equals(e2) for every e1 and e2 in S.
Caution should be exercised when using a comparator capable of imposing an ordering inconsistent with equals to order a sorted set (or sorted map).
This interface is a member of the Java Collections Framework.


## Creational: Factory
Learning Objectives

After completing this module, associates should be able to:

Describe the Factory Design pattern
Describe scenarios where a Factory Design pattern may be helpful to implement
Description

Factory is a design pattern which creates objects in which the precise type may not be known until runtime. There are several reasons to use the factory pattern:

If you don't know the exact types needed before running the code.
If you want to hide the creational logic, which prevents end user creating things that they shouldn't.
If you need an easy way to extend internal components.
Depending on implementation, the factory pattern can be used to reuse objects, instead of making new ones, which saves space.
Some extra benefits of using the factory pattern:

Single responsibility is upheld by putting all of the construction code in a single function.
Open/Closed principle is upheld by allowing new subclasses to easily implement and be added, without negatively affecting any of the already written classes.
Abstracts the actual creation of the objects away from the user.
Real World Application

The Factory design pattern offers flexibility, encapsulation, and separation of concerns within an application, so it can be widely useable across a variety of situations and projects. Many libraries and frameworks will implement a factory design pattern to ensure proper creation of required objects, but allows for the implementor using the library or framework to specify their own use cases. Other examples of Factory Design patterns are as follows:

Object Creation with Complex Initialization: When an object requires complex initialization or configuration, the Factory Design Pattern can be employed. The factory encapsulates the creation logic and hides the complexity, providing a clean interface for creating objects.

Dependency Injection: In Dependency Injection frameworks, the Factory Design Pattern is often used to create and manage instances of classes with complex dependencies. The factory can handle the resolution of dependencies and provide the appropriate instances to the requesting components.

Database Access: In database applications, the Factory Design Pattern can be used to create database connections or data access objects. The factory can handle the creation of the database-specific objects based on the configuration or other criteria.

Logging: In logging frameworks, the Factory Design Pattern can be used to create logger objects. The factory can determine the appropriate logger implementation based on the specified configuration, such as logging level, destination, or formatting.

GUI Component Creation: In graphical user interface (GUI) development, the Factory Design Pattern can be employed to create different types of GUI components based on user preferences or application settings. The factory can generate the appropriate components, such as buttons, panels, or dialogs, based on the desired style or behavior.

Note that this is not an exhaustive list of when and where to apply the Factory design pattern, but hopefully this list illustrates how useful it can be!

Implementation

To make a factory:

Create the abstract data type
Create several classes that inherit the abstract data type (the objects whose instantiation details may not be known until runtime)
Set up a static method whose return type is the abstract data type in (1), which will return the concrete instance
Below you can see an example of a Factory implementation

    // Abstract Data Type
    public interface Dessert {}

    // Classes that inherit the abstract data type
    public class Cake implements Dessert {}

    public class Cookie implements Dessert {}

    public class IceCream implements Dessert {}

    // Good practice to throw an exception if the desired concrete instance is not found
    public class DessertNotFoundException extends RuntimeException {}

    // Factory class that returns the concrete instance
    public class DessertFactory {
        public static Dessert getDessert(String dessertType) {
            switch(dessertType) {
                case "cake":
                    return new Cake();
                case "cookie":
                    return new Cookie();
                case "ice cream":
                    return new IceCream();
                default:
                    throw new DessertNotFoundException(dessertType + " not found!");
            }
        }

        public class Demo {
            public static void main(String[] args) {
                Dessert d1 = DessertFactory.getDessert("cake");
                Dessert d2 = DessertFactory.getDessert("cookie");
                Dessert d3 = DessertFactory.getDessert("ice cream");
                Dessert d4 = DessertFactory.getDessert("candy");    // Throws DessertNotFoundException
            }
        }
    }
Summary

The Factory pattern creates objects with unknown types until runtime, providing flexibility.
Reasons to use the factory pattern include handling unknown types, hiding creational logic, enabling easy extension, and reusing objects.
Benefits of the factory design pattern include adhering to a single responsibility and open/close principle, abstracting object creation from the user and encapsulating the object.
Some situations where using a factory can be useful include Dependency injection frameworks, database access, logging frameworks, and GUI component creation in graphical user interface development.


## Creational: Singleton
Learning Objectives

After completing this module, associates should be able to:

Describe the Singleton Design pattern
Describe scenarios where using the Singleton Design pattern may be useful
Description
Singleton Pattern
A Singleton is a design pattern which allows the creation of an object in memory only once in an application and can be shared across multiple classes. It can be useful for services in an application, or other resources like a connection or thread pool.

There are many benefits to using a Singleton:

There will only be 1 instance, which allows coordination across a system.
There is a clear way to fetch the correct instance, for example, a getInstance() method
The programmer has complete control over instantiation.
It is a global access point
The singleton is not created until it is used, often referred to as lazy instantiation.
However, there are also drawbacks to using a Singleton:

Harder to work with in multithreaded environments.
Different components can be given too much control over other components.
Overall, there are many situations to use a Singleton, for instance:

Where other creational patterns need to limit the number of objects.
Facade (another common design pattern) objects are often singletons.
State objects may break if there are duplicates.
Objects used by many different pieces, such as game boards or memory.
Real World Application

The primary benefit of a Singleton is the management of data or functionality through a single object in memory. There are many situations in real world applications which may benefit from the implementation of a Singleton, for instance:

Logging: In logging frameworks, the Singleton Design Pattern can be used to ensure that there is only one instance of the logger throughout the application. This allows different parts of the application to access and use the same logger instance for logging purposes.

Database Connections: In database applications, the Singleton Design Pattern can be used to manage and share a single database connection instance. This helps in preventing multiple connections to the database, ensuring efficient resource utilization, and providing a centralized access point for database operations.

Caching: In caching scenarios, the Singleton Design Pattern can be used to maintain a single instance of a cache manager or cache store. This ensures that all parts of the application access the same cache instance and allows for efficient storage and retrieval of frequently accessed data.

Configuration Settings: In applications that require global access to configuration settings, the Singleton Design Pattern can be used to provide a single instance that holds and manages the configuration data. This allows different parts of the application to access and utilize the configuration settings consistently.

Thread Pool or Task Manager: In concurrent programming, the Singleton Design Pattern can be used to create a thread pool or task manager with a fixed number of worker threads. This ensures that there is a single instance managing the concurrent tasks or threads, providing control and coordination over the execution.

Keep in mind, this is not an exhaustive list of when and where to apply the Singleton design pattern. Proper use and implementation can vary from application to application.

Implementation

To make a class into a Singleton in Java:

private static variable of the class' type
private constructor - to prevent arbitrary object creation
public static getInstance() method, which will either instantiate the object or return the instance in memory
Below you can see an example of a Singleton class

    public class Singleton {
        // Private static variable of the class' type
        private static Singleton instance;
        private int number;
        // Private Constructor
        private Singleton() {
            number = 0;
        }

        // Public static getInstance method
        public static Singleton getInstance() {
            if (instance == null) 
                instance = new Singleton();
            return instance;
        }
        //this method is called from an instance of the class.
        //However because there is only 1 instance, whenever this is called it will affect all pointers to the instance.
        public void printer(){
            this.number++;
            system.out.println("printer has been called " + this.number + " times.");

        }
    }

    class Main{
        public static void main(String[] args){
            Singleton mySingle = Singleton.getInstance(); //When this is called the first time the singleton does not exist, so it gets created.
            mySingle.printer(); // output: printer has been called 1 times
            Singleton yourSingle = Singleton.getInstance(); //This time when getInstance is called there already exists an instance, so it just returns the reference to the singleton.
            yourSingle.printer(); // output: printer has been called 2 times
            System.out.println(mySingle == yourSingle); //This will output true, because mySingle points to the same object as yourSingle
        }
    }
Summary

The Singleton design pattern allows for the creation of a single instance of an object in memory that can be shared across multiple classes.
Benefits of using a Singleton include coordination across the system, clear instance retrieval, control over instantiation, and global access point.
Situations where Singleton pattern can be useful include logging frameworks, database connections, caching, configuration settings, and thread pool/task manager in concurrent programming.


## What is an Algorithm
Learning Objectives

After completing this module, you should be able to:

Describe what an algorithm is
Describe common applications for algorithms
Describe common techniques used to create algorithms and their differences
Description

An algorithm is a process or set of rules/steps to be followed in calculations or some other problem-solving operation.

When crafting algorithms, the amount or size of input will often affect the overall runtime of the algorithm.

As programmers, we measure the effectiveness of algorithms based on their Time-complexity (how many steps are required for an algorithm), and Space-complexity (how much memory is required for an algorithm). Both complexity measurements are represented using Big-O notation, which measures the efficiency of an algorithm as inputs approach infinity.

Additionally, some problems occur frequently in programming, such as sorting data sets or searching for specific values, and many common techniques exist which can be used to solve these problems.

Real World Application

Developers use algorithms in nearly every facet of development. Some common examples and ones you may be familiar with already include:

Searching Algorithms: Algorithms which retrieve specified data from some larger collection. These are used in search engines to find the most relevant information on the Internet based on a query. For example, Google's search algorithm sorts through billions of web pages to find the most relevant results.
Sorting Algorithms: Algorithms which re-order elements of some larger collection based on an established sorting rule. These algorithms are essential in data processing for organizing data. Examples include sorting names in alphabetical order or arranging numbers from smallest to largest.
Pathfinding Algorithms: Algorithms which are used to map routes based on 2d or 3d environments or mazes. Used in GPS systems to find the shortest path between two points. These algorithms typically calculate the most efficient route for a journey.
Machine Learning Algorithms: These algorithms are used to make predictions or decisions based on data. They are fundamental in applications like recommendation systems (like Netflix or YouTube), facial recognition software, and autonomous vehicles.
Cryptography Algorithms: Algorithms which are essential for securing data transmission. They are used to encrypt and decrypt data, ensuring secure communication in digital platforms.
Implementation

Unlike most other topics in computer science, the ability to effectively create or use algorithms is not as simple as memorizing syntax or learning about specific solutions. Instead, creating efficient algorithms requires practice and understanding how to apply various tools to certain scenarios. A common, loose-guideline for solving problems is as follows:

Fully Understand the Problem
The first step is to understand what is being asked, and what components are available.
What is the input and desired output?
Break Down the Problem into discrete ‘sub-problems’
Before working on solutions, it is helpful to break larger problems into smaller, more manageable problems or steps which should be solved in sequence or repeated based on some conditions.
In many ways, this step is a further distillation of step 1.
Create a Plan
Creating a plan involves working out the specific steps prior to writing any code.
You don’t need to have working code to plan out what you want your code to do.
Pseudocode (see below) is commonly used to plan out algorithms.
Implement a Solution
Here is where you actually write your code.
With proper planning, implementation should primarily focus on writing the correct syntax, and not so much on what steps your code should be doing.
Test your Solution
Although a solution may seem to be correct through your planning or at first glance, it is important to double-check your work to ensure that the problem is solved fully.
Make sure to check for edge cases as well.
Refine/Refactor your Solution
No one gets it right on the first try! If (and when) a solution does not function fully, it is best to move through each of the previous five steps once again with new knowledge and a new perspective on the problem.
Pseudocode
Pseudocode is a simplified, half-code, half-plain language description used to represent an algorithm. It is not actual code written in a specific programming language but instead a tool to illustrate the basic principles and logic of an algorithm without getting bogged down in syntax. Pseudocode helps in planning and visualizing the steps of an algorithm clearly, making it easier to later implement in a programming language.

Key Characteristics of Pseudocode:
Language-Independent: It doesn't adhere to the syntax rules of any specific programming language. This makes it accessible to individuals with varying programming backgrounds.
Focus on Logic: Pseudocode emphasizes the logic behind an algorithm, not the implementation details or specific programming constructs.
Readable and Understandable: It is written in a way that is easily readable and understandable by humans, often resembling plain English or a combination of English and programming language elements.
Structured Format: While it doesn’t have strict syntax rules, pseudocode is often written in a structured format to make it easier to translate into actual code later.
Common Techniques and Rules for Writing Pseudocode:
Use Clear and Concise Statements: Pseudocode should be brief and to the point, avoiding overly complex descriptions.
Use Standard Programming Structures: Incorporate standard structures like loops (for, while), conditionals (if, else), and methods (function, procedure).
Indentation for Blocks: Use indentation to represent block structures (like loops or if-else conditions) to enhance readability.
Use Descriptive Names for Variables: Variable names should be self-explanatory, indicating the purpose or type of data they store.
Specify Inputs and Outputs: Clearly indicate what inputs the algorithm takes and what outputs it produces.
Commenting and Documentation: Include comments to explain complex logic or assumptions in the algorithm.
Sequence of Steps: Present steps in a logical order necessary for the algorithm's execution.
Avoid Specific Language Features: Steer clear of using features or functions specific to a particular programming language.
Let's illustrate this with a simple example of pseudocode for a sorting algorithm:

Procedure BubbleSort(Array A)
    For i from 1 to length(A)
        For j from 0 to length(A) - i - 1
            If A[j] > A[j + 1]
                Swap A[j] and A[j + 1]
            End If
        End For
    End For
End Procedure
This pseudocode describes a basic bubble sort algorithm. It uses standard programming constructs like loops and conditionals, is structured and indented for clarity, and employs descriptive names for variables and procedures. It provides a clear, step-by-step description of the algorithm without delving into the syntax specifics of any programming language.

Summary

An algorithm is a series of rules or steps to be followed to solve a problem
The complexity of an Algorithm is measured using worst-case scenarios (as inputs approach infinity) and are represented using Big-O notation.
Common applications of algorithms include, searching, sorting, pathfinding, machine learning, and Cryptography, among others.
Pseudocode is a simple, half-code, plain and descriptive language used to plan and represent algorithms.


## Recursive Algorithm
Learning Objectives

After completing this module, you should be able to:

describe recursive algorithms
Description
Recursive Algorithms
A recursive algorithm is one that calls itself directly or indirectly in order to continuously repeat the same set of steps until a solution is reached. Certain problems can actually be solved much more easily using recursion. Let's take a look at the psuedocode of a function that returns the summation up to a given input n, using a while loop:

INPUT n is an integer

FUNCTION sum(n)

    SET sum to 0

    WHILE n is greater than 0
        compute sum as sum + n
        decrease the value of n by 1
    END LOOP

    RETURN sum

END FUNCTION
The above solution loops descending through all numbers between n and 0 and adds them all to a sum variable. For instance, if given the number 5, this function will return 15 (5 + 4 + 3 + 2 + 1). Lets take a look at the same result, but using recursion:

INPUT n is an integer

FUNCTION sumRecursion(n)

    IF n equals 0 THEN 
        RETURN 0
    ELSE 
        RETURN n + CALL sumRecursion(n - 1)

END FUNCTION
With the second solution, you'll see that in the return statement, we return the current value of n, plus whatever is returned from calling this same function but with n-1 as an argument. If we look at the execution of the function and what is eventually returned, it would look something like this:

sumRecursion(5) | we call the function with an input of 5 which returns 5 + sumRecursion(4)
sumRecursion(4) | we call the function with an input of 4 which returns 4 + sumRecursion(3)
sumRecursion(3) | we call the function with an input of 3 which returns 3 + sumRecursion(2)
sumRecursion(2) | we call the function with an input of 2 which returns 2 + sumRecursion(1)
sumRecursion(1) | we call the function with an input of 1 which returns 1 + sumRecursion(0)
sumRecursion(0) | we call the function with an input of 0 which returns 0 and DOES NOT call itself again, because we've reached the bottom
So at the end of it all, this results in the summation, 5 + 4 + 3 + 2 + 1 + 0 = 15

Real World Application

Recursive algorithms find applications in various real-world scenarios where problems can be broken down into smaller subproblems of the same type. Here's a real-world application for recursive algorithms:

File System Traversal:
Recursive algorithms are commonly used for traversing file systems, especially in situations where directories can contain subdirectories, and those subdirectories may contain further subdirectories, and so on. By applying a recursive algorithm to traverse a file system, it becomes possible to explore the entire directory structure, regardless of its depth. For example, when performing tasks like backing up files, searching for specific files, or performing disk space analysis, recursive algorithms can efficiently traverse through directories and subdirectories, handling each level of nesting recursively.

Implementation
How to write a Recursive Function
We can break a recursive function down into a few main parts. First, let's understand/review what the fibonacci sequence is for our example. It is a sequence of numbers where each entry is the sum of the previous 2 (starting with 0 and 1).

The first few numbers of the fibonacci sequence are:

0, 1, 1, 2, 3, 5, 8, 13, 21
Base Case
With recursive functions, we are usually going to keep calling our own function (recursing) until we reach some condition called the base case. The base case in recursion is the condition that defines the simplest scenario or stopping point for the recursive algorithm. In the case of fibonacci sequence, the base cases are 0 and 1. So we can write a function that retrieves the nth number in the Fibonacci sequence, starting with these base cases:

INPUT n is an integer

FUNCTION fibonacci(n) 

    IF n is equal to 0 THEN
        RETURN 0

    IF n is equal to 1 THEN
        RETURN 1

END FUNCTION

In this case, we directly return 0 and 1 for the 1st and 2nd terms of the sequence, respectively, because these are the base cases (the function should and would always return the same values when n=1 or n=2 are passed in as arguments). Once we have the base case, we can work on the recursive part.

Note: If you do not include a base case in a recursive function, then the function will run infinitely and will cause a stack overflow.

Recursion
    RETURN the sum of CALLING fibonacci(n - 1) and CALLING finbonacci(n - 2)
This is all we have to write because we just want the sum of the previous 2 numbers.

Putting it all together:

INPUT n is an integer

FUNCTION fibonacci(n) 

    IF n is equal to 0 THEN
        RETURN 0

    IF n is equal to 1 THEN
        RETURN 1

    RETURN the sum of CALLING fibonacci(n - 1) and CALLING finbonacci(n - 2)

END FUNCTION

To demonstrate this function in action, let's use fibonacci(3):

fibonacci(3) will return fibonacci(2) + fibonacci(1)
fibonacci(2) will return fibonacci(1) + fibonacci(0)
fibonacci(1) will return 1
fibonacci(0) will return 0
Without these base cases, the recursive calls would be infinite.

In the end, the solution will be 1 + 0 + 1 = 2.

Summary

A recursive algorithm is one that calls itself directly or indirectly in order to continuously repeat the same set of steps until a solution is reached.
With recursive functions, the functions keep executing and calling themselves (recursing) until they reach some condition called the base case.
The base case in recursion is the condition that defines the simplest scenario or stopping point for the recursive algorithm.


## Greedy Algorithm
Learning Objectives

After completing this module, you should be able to:

describe greedy algorithms
Description
Greedy Algorithms
A Greedy Algorithm is a paradigm where in a given solution, at any specific stage, the algorithm will always choose the most immediate and optimal solution to satisfy the current stage, even if it is not the most optimal for the solution as whole. As such, greedy algorithms are not always the most optimal paradigm to use; however, it is often the most lightweight and cost effective in terms of Time complexity and can be used incredibly effectively when used on sorted data.

Real World Application

Real-world examples of this application include:

Telecommunication Networks: Building a telecommunication network infrastructure to connect different regions or cities while minimizing the cost of laying cables or setting up towers.
Transportation Networks: Planning transportation routes such as highways or railways to connect cities or towns efficiently while minimizing construction costs.
Pipeline Networks: Designing pipeline networks to transport resources (such as oil or gas) from extraction sites to processing plants or distribution centers while minimizing the total pipeline length and construction costs.
In each of these cases, a greedy algorithm helps in making locally optimal decisions at each step, leading to a globally optimal solution for connecting the network components.

Implementation

Lets assume for example that we have the given tree and we want to write a greedy algorithm to determine the maximum total that the tree can provide as we traverse to the bottom of the tree.

blank-tree image

With a greedy algorithm, we ignore the state of the solution overall and instead look at the immediate best solution. In this case, we start at the root node, 50, and have an immediate decision to make. We can either go down to the left node, 17, or we can go down to the right node, which is 23. Well, since our algorithm ONLY cares about what is right in front of it, it will always choose 23. Next we have a choice between 14 and 6, so our optimal choice is 14, leading to a combined total of 50 + 23 + 14 = 87. See the diagram below:

greedy-solution image

However, you may have noticed that while our greedy solution thought it had the most optimal solution, it did not account for the sub branches of the less optimal immediate choice. If you look at the solution below, you can see that there is a more optimal solution overall. If we decide to sacrifice our first choice and go down the left path to 17, and then once again take the lesser choice of 3, we then find a node with a value of 127, leading to a combined total of 50 + 17 + 3 + 127 = 197. For this type of a solution, you may want to refer to backtracking algorithms that always find the most optimal overall solution.

optimal-solution image

Summary

A Greedy Algorithm is a paradigm where in a given solution, at any specific stage, the algorithm will always choose the most immediate and optimal solution to satisfy the current stage, even if it is not the most optimal for the solution as whole.


## Divide and Conquer Algorithm
Learning Objectives

After completing this module, you should be able to:

describe divide and conquer algorithms
Description
Divide and Conquer Algorithms
A Divide and Conquer algorithm, or DAC for short, can be thought of as a technique that separates a given problem out into tangible sub-problems that can be more easily solved. There are typically three parts to a DAC algorithm:

Divide: In this step, we break the problem down into smaller sub-problems until they can be solved.
Conquer: This step is where we actually solve the problem, typically through recursion.
Combine: Combine the results from all of the subproblems to get our final solution for the entire problem.
Real World Application

Divide and conquer algorithms are widely used in various real-world scenarios where sorting large datasets efficiently is crucial:

Data Processing: In data analytics, a DAC algorithm is used to sort large volumes of data efficiently before performing further analysis or processing. For example, sorting records in a database, sorting log files, or sorting results in search engines.
External Sorting: When dealing with datasets that are too large to fit into memory, a DAC algorithm is used for external sorting. External sorting involves sorting data that is stored on external storage devices (e.g., hard drives) where only a portion of the data can be loaded into memory at a time.
File Systems: File systems often need to maintain files in sorted order for efficient retrieval.A DAC algorithm can be used during file system operations such as indexing files or organizing directory structures.
Network Routing: In network routing algorithms, a DAC algorithm can be used to sort routing tables efficiently, helping routers make optimal routing decisions.
Implementation

Lets take a look at an example array that we want to sort ascending with a Divide and Conquer approach:

[8, 4, 1, 7, 5, 3]
Using the steps of divide, conquer, and combine, lets start to solve this.

Divide
At this step we will divide the input. We can divide the array into two parts, a left and a right:

[8, 4, 1] [7, 5, 3]
Even though we've divided the problem, it's still not as easily solvable as it could be. Lets divide again:

[8, 4] [1] [7, 5] [3]
Now we're a bit closer, but some of our "problems" still have two pieces of data that we really want to be individual. One last division gets us 6 different arrays to work with:

[8] [4] [1] [7] [5] [3]
Conquer and Combine
Remember what we're trying to solve for here is that each of our nodes get put in ascending order, so for that, we can move piece by piece through the sub-problems and solve for ascending order. Keep in mind, our algorithm is also combining our results back together in stages, based on how the problems were split, so first we combine 4 and 8, then 5 and 7, and as we continue through, we stitch the rest of the results together.

[4, 8] [1] [5, 7] [3]

[1, 4, 8] [3, 5, 7]

[1, 3, 4, 5, 7, 8]
To look at how this might be achieved in code, take a look at the pseudocode example below:

INPUT arr is an array of integers

FUNCTION dac(arr)

    SET midIndex to the length of arr divided by 2

    IF the arr's length is 1 or less THEN
        RETURN arr

    SET leftArr to be the first half of arr
    SET rightArr to be the second half of arr

    CALL merge(dac(leftArr),dac(rightArr))

END FUNCTION


FUNCTION merge(leftArr, rightArr)

    SET sortedArr to be an empty array

    WHILE both arrays have elements to iterate over
        IF the current element of leftArr is less than or equal to the current element of the rightArr THEN
            add leftArr's element to sortedArr
        ELSE
            add rightArr's element to sortedArr
    END WHILE

    WHILE leftArr still has elements to iterate over
        add leftArr's remaining element to sortedArr
    END WHILE

    WHILE rightArr still has elements to iterate over
        add rightArr's remaining element to sortedArr
    END WHILE

END FUNCTION
Summary

A divide-and-conquer algorithm is a technique that separates a given problem out into tangible sub-problems that can be more easily solved.
There are typically three parts to a DAC algorithm: Divide, conquer, and combine.


## Backtracking Algorithm
Learning Objectives

After completing this module, you should be able to:

describe backtracking algorithms
Description
Backtracking Algorithms
A Backtracking Algorithm, similar to Brute Force, is a paradigm that uses a recursive approach to test every possible solution and then compare the results to find the most optimal. These types of algorithms are often quite expensive in terms of Time complexity, because it needs to check every possibility that can exist. The term Backtracking means that if the algorithm determines that the current path it is traversing is never going to be optimal, it will backtrack to a point where another decision, or path exists. Each time we backtrack and iterate, we can describe the next solution as a new Permutation.

Real World Application

Sudoku puzzles are a popular recreational activity in newspapers, puzzle books, and mobile applications. While Sudoku solving may seem like a trivial game, it is an excellent example of how backtracking algorithms can efficiently solve complex constraint satisfaction problems. Backtracking algorithms are also used in various other applications such as solving crossword puzzles, generating mazes, and solving certain optimization problems in computer science and operations research.

Implementation

Let's talk more about Backtracking through the lens of a common problem that this paradigm is used to solve, the N-Queen Problem. In the N queen problem, we are tasked with placing N queens on an NxN chessboard such that no two queens attack eachother. In chess, queens can move any amount in any direction and even diagonally (left, right, up, down, upLeft, downLeft, upRight, downRight), therefore, in order to solve this problem, none of the queens should be able to make a legal move and come into contact with any other queen on that turn. For this example, lets take a look at the problem using 4 as N. So we have a 4x4 chessboard and need to determine whether or not we can place 4 queens legally without any of them being under attack from another queen. Lets begin solving the problem logically and then move to an actual programmatic solution.

Start in the left most column
If all queens are placed, return true (solution found)
Try all rows in the current column and do the following for every row
If the queen can be safely placed in this row, then mark this index [row, column] as part of the solution and recursively check if placing a queen here leads to a solution (call this function again and jump to start)
If placing the queen in [row, column] leads to a solution(no more queens), return true.
If placing the queen doesn't lead to a solution, unmark this row and jump to step 1 for another row.
If all rows have been tried and nothing worked, return false to trigger backtracking and move to the next column.
We will denote the chessboard as a matrix with a 0 representing an empty square and a 1 representing a queen. Here is the empty board to start with:

[0, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]

Lets follow the steps we mapped out to try and solve this. Starting in the left most column we start at index [0, 0]. We don't return on step 2 because not all queens are placed. Instead, we move to step three and move through the rows.

The next question asks if a queen can be placed safely in this row, at [0, 0] and that answer is yes so we can mark [0, 0] as part of the solution and recursively jump back to the start on the next column by calling the function. The board now looks like this:

[1, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]

We're now at [0, 1]. Lets work through our steps:

Not all queens are placed, so we don't return true. Instead, we work through each row.

[0, 1] -> we can't place here because we'd be under direct attack from [0, 0] so we move to the next row.
[1, 1] -> We can't place here either because we'd be under diagonal attack from [0, 0] so we move next.
[2, 1] -> finally we found a safe square! Lets mark the index [2, 1] as part of the solution and move on to the next column. The board would look like this:
[1, 0, 0, 0]
[0, 0, 0, 0]
[0, 1, 0, 0]
[0, 0, 0, 0]

Woo, progress! We're now at [0, 2] and two queens still need to be placed. Lets start through our steps:

We don't return true yet because we still have queens to place, so we move on to trying the rows.

[0, 2] -> not safe because of attack from [0, 0].
[1, 2] -> not safe because of diagonal attack from [2, 1].
[2, 2] -> not safe because of direct attack from [2, 1].
[3, 2] -> not safe becuase of diagonal attack from [2, 1].
Oh no, we reached the end of the entire column without placing a queen. This isn't looking good, but luckily our algorithm can handle this. Because we know that nothing will ever be placeable in this column due to our current placement of queens, we can backtrack to the last queen we placed and remove it and try again. That said, we will remove [2, 1] from the solution and continue trying those rows in that column for the next solution. Lets examine our new board state after backtracking:

[1, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]

Because we backtracked to after [2, 1] got checked, we're actually at the location [3, 1], so we need to check it.

[0, 1] -> already checked
[1, 1] -> already checked
[2, 1] -> already checked (we returned from this one)
[3, 1] -> safe! So lets place the queen and move on to the next column. The board state now looks like this:
[1, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]
[0, 1, 0, 0]

We're now at location [0, 2] so we try our steps. We don't return true because we still have queens, so we try the rows.

[0, 2] -> not safe because of attack from [0, 0].
[1, 2] -> safe! Lets mark it and move to the next column. Here's the board state:
[1, 0, 0, 0]
[0, 0, 1, 0]
[0, 0, 0, 0]
[0, 1, 0, 0]

We're now at [0, 3] and we run through our steps again. We still don't return true, so lets try the rows.

[0, 3] -> not safe because of attack from [0, 0].
[1, 3] -> not safe because of attack from [1, 2].
[2, 3] -> not safe because of diagonal attack from [1, 2].
[3, 3] -> not safe because of attack from [3, 1].
Oh no, we reached the end and don't have a queen placed, that means we need to backtrack and remove the last queen and keep trying forward. So we jump to the placement of [1, 2] and remove it. Here is the board state:

[1, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]
[0, 1, 0, 0]

Similar to the last time we backtracked, we are now at the index following [1, 2]. We're at location [2, 2] so lets try our steps.

[0, 2] -> already checked
[1, 2] -> already checked (we returned from this)
[2, 2] -> not safe because of diagonal attack from [3, 1].
[3, 2] -> not safe because of attack from [3, 1].
Once again, we reached the end without a solution so we need to backtrack to the last queen, [3, 1]. Once we jump back and remove it, the board looks like this:

[1, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]

Since we jumped back to after the last row was tried in this column, that actually means that we need to backtrack even further, to our first ever queen placement at [0, 0] and remove it. Here is the board state:

[0, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]

Well, we're back to square 1 (literally), but that's fine because our algorithm just eliminated a TON of impossible solutions. That said, we're now at [1, 0] and need to try our steps.

[0, 0] -> already checked (we returned from this)
[1, 0] -> safe! Place the queen and go to the next column. Here is the board state:
[0, 0, 0, 0]
[1, 0, 0, 0]
[0, 0, 0, 0]
[0, 0, 0, 0]

We're now at [0, 1] and we try our steps.

[0, 1] -> not safe due to diagonal attack from [1, 0].
[1, 1] -> not safe due to attack from [1, 0].
[2, 1] -> not safe due to diagonal attack from [0, 1].
[3, 1] safe! Place the queen and move to the next column. Here is the board state:
[0, 0, 0, 0]
[1, 0, 0, 0]
[0, 0, 0, 0]
[0, 1, 0, 0]

We're now at [0, 2] and we try our steps.

[0, 2] -> safe! Place the queen and move on to the next column. Here is the board state:
[0, 0, 1, 0]
[1, 0, 0, 0]
[0, 0, 0, 0]
[0, 1, 0, 0]

We're now at [0, 3] and we try our steps.

[0, 3] -> not safe due to attack from [0, 2].
[1, 3] -> no safe due to diagonal attack from [0, 2].
[2, 3] -> safe! Place the queen and move to the next column. Here is the board state:
[0, 0, 1, 0]
[1, 0, 0, 0]
[0, 0, 0, 1]
[0, 1, 0, 0]

Wait a minute... we've placed all of our queens! Remember back to our steps where we check for this. (If all queens are placed, return true). This will navigate back up the call stack, returning true each time until we finally return true for the entire solution!

Summary

A Backtracking Algorithm, similar to Brute Force, is a paradigm that uses a recursive approach to test every possible solution and then compare the results to find the most optimal.
These types of algorithms are often quite expensive in terms of Time complexity, because it needs to check every possibility that can exist.


## Time Complexity
Learning Objectives

After completing this module, associates should be able to:

Describe time complexity and Big O notation
Analyze the time complexity of algorithms
Description

So far we have mostly emphasized the correctness of programs. In practice, another issue is also important: efficiency. When analyzing a program in terms of efficiency, we want to look at questions such as, "How long does it take for the program to run?" and "Is there another approach that will get the answer more quickly?"

Efficiency will always be less important than correctness; if you don't care whether a program works correctly, you can make it run very quickly indeed, but no one will think it's much of an achievement! On the other hand, a program that gives a correct answer after ten thousand years isn't very useful either, so efficiency is often an important issue.

The term "efficiency" can refer to efficient use of almost any resource, including time, computer memory, disk space, or network bandwidth. In this topic, however, we will deal exclusively with time efficiency, and the major question that we want to ask about a program is, how long does it take to perform its task?

It really makes little sense to classify an individual program as being "efficient" or "inefficient." It makes more sense to compare two (correct) programs that perform the same task and ask which one of the two is "more efficient," that is, which one performs the task more quickly. However, even here there are difficulties. The running time of a program is not well-defined. The run time can be different depending on the number and speed of the processors in the computer on which it is run as well as many other factors.

In spite of these difficulties, there is a field of computer science dedicated to analyzing the efficiency of programs known as Analysis of Algorithms. The focus is on algorithms, rather than on programs as such, to avoid having to deal with multiple implementations of the same algorithm written in different languages, compiled with different compilers, and running on different computers.

Analysis of Algorithms is a mathematical field that abstracts away from these down-and-dirty details. Still, even though it is a theoretical field, every working programmer should be aware of some of its techniques and results.

One of the main techniques of analysis of algorithms is asymptotic analysis. The term "asymptotic" here means basically "the tendency in the long run, as the size of the input is increased." An asymptotic analysis of an algorithm's run time looks at the question of how the run time depends on the size of the problem. The analysis is asymptotic because it only considers what happens to the run time as the size of the problem increases without limit; it is not concerned with what happens for problems of small size or, in fact, for problems of any fixed finite size. An asymptotic analysis is only a first approximation, but in practice it often gives important and useful information.

Central to asymptotic analysis is Big-Oh notation. Using this notation, we might say, for example, that an algorithm has a running time that is O(n^2) or O(n) or O(log(n)). These notations are read "Big-Oh of n squared," "Big-Oh of n," and "Big-Oh of log n" (where log is a logarithm function). The "n" in this notation refers to the size of the problem. Before you can even begin an asymptotic analysis, you need some way to measure problem size. Usually, this is not a big issue. For example, if the problem is to sort a list of items, then the problem size can be taken to be the number of items in the list. When the input to an algorithm is an integer, as in the case of an algorithm that checks whether a given positive integer is prime, the usual measure of the size of a problem is the number of bits in the input integer rather than the integer itself. More generally, the number of bits in the input to a problem is often a good measure of the size of the problem.

Common Big O notations:

O(1) : Constant time
O(N) : Linear time
O(N^2) : Quadratic time
O(log(n)) : Logarithmic time
Real World Application
Real-Life Big-O
Many “operations” in real life can help us with finding what the order is. When analyzing algorithms/operations, we often consider the worst-case scenario, such as, "What’s the worst that can happen to our algorithm?" and "when does our algorithm need the most instructions to complete the execution"? Big O notation will always assume the upper limit where the algorithm will perform the maximum number of iterations.

Let’s assume I am standing in the front of a class of students and one of them has my bag. Here are few scenarios and ways in which I can find my bag and their corresponding order of notation.

O(n) — Linear Time:
Scenario: Only one student in my class who hid my bag knows about it.

Approach: I will have to ask each student individually in the class if they have my bag. If they don’t, I move on to ask the next student.

Worst-Case Scenario: In the worst case scenario, I will have to ask n questions.

O(1) — Constant Time
Scenario: Student who hid my bag name is known to me.

Approach : Since I know Joe has my bag, I’ll directly ask him to give it to me.

O(n²) — Quadratic Time:
Scenario: In the entire class, only one student knows on which student desk my bag is hidden.

Approach: I will start questioning each student individually and ask him about all the others students too. If I don’t get the answer from the first student, I will move on to the next one.

Worst-Case Scenario: In the worst case scenario, I will have to ask n² questions, questioning each student about other students as well.

O(log n) — Logarithmic time:
Scenario: Here, all the students know who has my bag but will only tell me if I guessed the right name.

Approach: I will divide the class in half, then ask: “Is my bag on the left side, or the right side of the class?” Then I take that group and divide it into two and ask again, and so on.

Worst Case Scenario: In the worst case, I will have to ask log n questions.

Implementation

The following examples describe algorithms as well as their complexity. Note, the actual syntax and keywords used in the following examples are not specific to any particular language. Instead make sure to focus on the steps of the algorithm and the complexity.

Make sure to read through the algorithm and code prior to reviewing the complexity. You can even try to solve the complexity on your own as well!

Problem 1
We can use the following algorithm to see if a numerical value exists in an array. The algorithm will find the first place where the number exists and return its index.

public int findFirstIndexOfNumber(int number, int[] array) {
  for (int i = 0; i < array.length; i++) {
    if (array[i] == number) {
      return i;
    }
  }
  return -1;
}
This algorithm needs to loop through the array until it finds the first occurrence of the number we are looking for. It doesn't matter how big the data set is, though - at most we need to loop through it once. The algorithm is O(N) linear time.

Problem 2
Now for a wrinkle on the last problem. Instead of finding just the first index where a number exists, our algorithm needs to find every index where the number exists. Here's the algorithm:

public int[] findEachIndexOfNumber(int number, int[] array) {
  List<Integer> indexes = new ArrayList<Integer>();
  for (int i = 0; i < array.length; i++) {
    if (array[i] == number) {
      indexes.add(null);
    }
  }
  return indexes.stream().mapToInt(i -> i).toArray();
}
While this algorithm is a bit more intensive than the previous one because it always has to search all elements in the array, the upper bound is still the same - looping through each element once. The runtime is O(N) linear time.

Problem 3
The following function checks to see if the last item in a data set is higher or lower than the first item in a data set - and returns Higher, Lower, or Neither.

public static void main(String[] args) {
  int[] array = {36, 14, 1, 7, 21};
  String result = higherOrLower(array);
  System.out.println(result);
}

public static String higherOrLower(int[] array) {
  if (array[array.length -1 ] > array[0]) {
    return "Higher";
  } else if (array[array.length -1 ] < array[0]) {
    return "Lower";
  } else {
    return "Neither";
  }
}
It doesn't matter how large the data set is. The algorithm just needs the first and last element in the array to calculate the answer. This algorithm is O(1) constant time.

Problem 4
We can use the following function to determine the sum of an array of sequential numbers:

public static void main(String[] args) {
  int[] array = {1,2,3,4,5,6,7,8};
  int result = determineSumOfSequentialArray(array);
  System.out.println(result);
}

public static int determineSumOfSequentialArray(int[] array) {
   int sum = 0;
   for (int i = 0; i < array.length; i++) {
       sum += array[i];
   }
   return sum;
}
determineSumOfSequentialArray() has a single loop. As the data set grows, the time it takes to loop through the data set will also grow in a linear fashion. This algorithm is O(N).

Problem 5
We can also find the sum of an array of sequential numbers that begins with one in another way as well:

int[] array = {1,2,3,4,5,6,7,8};

public static int determineSumOfSequentialArray(int[] array) {
  return array.length * (array.length + 1)/2;
}
No matter how big the size of the data set, our algorithm is simply doing a single computation. Because arrays have an inherent length property, there are no extra operations for larger arrays. This algorithm is O(1) (constant time). This is a much more efficient way to sum sequential arrays! Please note this algorithm works because we are examining an array where each element is increasing by 1 in sequential order.

Problem 6
We can use the following recursive function to search an array of sorted numerical values to find a specific number in that array (or return -1 if the value isn't in the array):

public int recursiveBinarySearch(int target, int[] array, int beginIndex, int endIndex) {
  int middleIndex = (int) Math.floor((beginIndex + endIndex)/2);
  if (array[middleIndex] == target) {
    return middleIndex;
  } else if (beginIndex >= endIndex) {
    return -1;
  } else if (array[middleIndex] < target) {
    beginIndex = middleIndex + 1;
    return recursiveBinarySearch(target, array, beginIndex, endIndex);
  } else if (array[middleIndex] > target) {
    endIndex = middleIndex - 1;
    return recursiveBinarySearch(target, array, beginIndex, endIndex);
  }
  
  return -1;
}
The key here is to determine that each time the function is called, the algorithm looks at the middle index of the array, determines whether or not the number it's searching for is higher or lower, and then throws out the other half of the array. Halving a data set is a good indicator that this algorithm has a runtime complexity of O(log N). The space complexity, which we should consider because it's a recursive function and uses a call stack, is also O(log N). By the way, this is a binary search algorithm - which we'll learn more about in a future lesson.

Bonus Problems (Optional)
As an additional challenge try to solve the Big(O) Complexity of the following algorithms:

Here, we are searching for the largest element of an array.

Bonus Problem 1
public int maxElement(int[] array) {
  int max = array[0];
  for (int i = 1; i < array.length; i++) {
    if (array[i] > max) {
      max = array[i];
    }
  }

  return max;
}

Bonus Problem 2
The following function prints all pairs of data in a given array.

public void printArrayPairs(int[] array) {
  for (int i = 0; i < array.length; i++) {
    for (int j = 0; j < array.length; j++) {
      System.out.println(array[i] + " : " + array[j]);
    }
  }
}
Bonus Problem 3
In this algorithm we find both the sum and product of the first five elements in a given array.

public void performMathOperations(int[] array) {
  int sum = 0;
  int product = 1;
  int limit = 5;

  if (array.length < limit) {
    System.out.println("array too small");
    return;
  }

  for (int i = 0; i < limit; i++) {
    sum += array[i];
  }

  for (int j = 0; j < limit; j++) {
    product *= array[j];
  }

  System.out.println("Sum:" + sum);
  System.out.println("Product:" + product);
}

Bonus Problem 4
For the final problem, we multiply each number by 100; however, it is achieved (quite inefficiently) by adding each element to itself nintey-nine times.

public void inefficientMultiplication(int[] array) {
  for (int i = 0; i < array.length; i++) {
    int element = array[i];
    for (int j = 1; j < 100; j++) {
      array[i] += element;
    }
  }
}
Answers to Bonus Problems
In the first problem, we loop through the collection and compare each element to the value max. This algorithm only iterates through any given collection once, so as the input array size grows so does our number of operations. This means our algorithms has Linear O(N) complexity.
In the second problem, we have a nested loop and each loop iterates over the array. With this structure, as the input array grows, we must iterate over those additional elements an additional time for each element. This means the algorithm grows exponentially or in other words the Algorithm as Quadratic O(N^2) complexity.
In the third problem, although we have two loops which iterate over a collection, we limit the number of operations to at most, the first 5 elements. This means that this algorithm, ultimately has constant O(1) complexity.
In the fourth problem, we have another nested loop, but don't be confused! The second loop will, at most, iteration ninety-nine times. Although a nested loop typically indicates a quadratic complexity, in this instance, the inner loop actually does not factor into the overall BigO complexity of the algorithm. Instead as inputs approach infinity, the size of the Array ultimately determines the complexity. This means that this algorithm has a Linear Complexity O(N).
Summary

So far we have mostly emphasized the correctness of programs. In practice, another issue is also important: efficiency.
There is a field of computer science dedicated to analyzing the efficiency of programs known as Analysis of Algorithms.
The focus is on algorithms, rather than on programs as such, to avoid having to deal with multiple implementations of the same algorithm written in different languages, compiled with different compilers, and running on different computers.
One of the main techniques of analysis of algorithms is asymptotic analysis.
The term "asymptotic" here means basically "the tendency in the long run, as the size of the input is increased."
An asymptotic analysis is only a first approximation, but in practice it often gives important and useful information.
Central to asymptotic analysis is Big-Oh notation. Using this notation, we might say, for example, that an algorithm has a running time that is O(n^2) or O(n) or O(log(n)).
Common Big O notations:
O(1) : Constant time
O(N) : Linear time
O(N^2) : Quadratic time
O(log(n)) : Logarithmic time


## Linear Search
Learning Objectives

After completing this module, associates should be able to:

Describe how the Linear Search works
Successfully execute a Java program that implements a Linear Search
Description

Linear search is a very simple search algorithm. In this type of search, a sequential search is done for all items one by one. Every item is checked and if a match is found then that particular item is returned, otherwise the search continues till the end of the data collection.

Algorithm
Get the length of the array.
Get the element to be searched and store it in a variable named value.
Compare each element of the array with the variable value.
In case of a match print a message saying element found.
Else, print a message saying element not found.
Although linear search is generally not the most efficient method of searching an array, it is the best method for searching an unsorted array.

Real World Application

Suppose you use the Uber application as a rider and you request a ride to go from one place to another. Your driver just arrived at the parking lot of your place. The only thing you know about the ride is the license plate number. How do you find your Uber ride?

The obvious thing is to go to the parking lot and search all the cars one by one. This method of searching is called linear search, where we search for all possible options (brute force search) until we get our desired result. We follow linear search in our daily life such as while finding a specific book, medicine or movie in stores.

Implementation

Suppose you have this problem:

Given an array arr[] of n elements, write a function to search a given element x in arr[].

Examples of correct output from the given input:

Input : arr[] = {10, 20, 80, 30, 60, 50, 
                     110, 100, 130, 170}
          x = 110;
Output : 6
Element x is present at index 6

Input : arr[] = {10, 20, 80, 30, 60, 50, 
                     110, 100, 130, 170}
           x = 175;
Output : -1
Element x is not present in arr[].
A simple approach is to do a linear search, i.e

Start from the leftmost element of arr[] and one by one compare x with each element of arr[] If x matches with an element, return the index. If x doesn’t match with any of the elements, return -1.

Example:

// Java code for linearly searching x in arr[]. If x
// is present then return its location, otherwise
// return -1
 
class Rev
{
    public static int search(int arr[], int x)
    {
        int n = arr.length;
        for (int i = 0; i < n; i++)
        {
            if (arr[i] == x)
                return i;
        }
        return -1;
    }
 
    // Driver code
    public static void main(String args[])
    {
        int arr[] = { 2, 3, 4, 10, 40 };
        int x = 10;
 
        // Function call
        int result = search(arr, x);
        if (result == -1)
            System.out.print(
                "Element is not present in array");
        else
            System.out.print("Element is present at index "
                             + result);
    }
}
Output

Element is present at index 3
The time complexity of the above algorithm is O(n).

Linear search is rarely used practically because other search algorithms such as the binary search algorithm and hash tables allow significantly faster searching compared to Linear search.

Exercise: (Optional)
Comment on the time complexity of the attempt below to rewrite the Linear Search. Is it the same, or is it an improvement over the Linear Search program above? Justify your response.

// Java program for linear search

import java.io.*;

class Rev
{

  public static void search(int arr[], int search_Element)
  {
    int left = 0;
    int length = arr.length;
    int right = length - 1;
    int position = -1;

    // run loop from 0 to right
    for (left = 0; left <= right;)
    {
      
      // if search_element is found with left variable
      if (arr[left] == search_Element)
      {
        position = left;
        System.out.println(
          "Element found in Array at "
          + (position + 1) + " Position with "
          + (left + 1) + " Attempt");
        break;
      }
    
      // if search_element is found with right variable
      if (arr[right] == search_Element)
      {
        position = right;
        System.out.println(
          "Element found in Array at "
          + (position + 1) + " Position with "
          + (length - right) + " Attempt");
        break;
      }
      
      left++;
      right--;
    }

    // if element not found
    if (position == -1)
      System.out.println("Not found in Array with "
              + left + " Attempt");
  }

  
  // Driver code
  public static void main(String[] args)
  {
    int arr[] = { 1, 2, 3, 4, 5 };
    int search_element = 5;
  
    // Function call
    search(arr,search_element);
  }
}

Summary

In a Linear Search, a sequential search is done for all items one by one.
Every item is checked and if a match is found then that particular item is returned, otherwise the search continues till the end of the data collection.
Although linear search is generally not the most efficient method of searching an array, it is the best method for searching an unsorted array.


## Binary Search
Learning Objectives

After completing this module, associates should be able to:

Describe how the Binary Search works
Successfully execute a Java program that implements a Binary Search
Description

If the elements in the array are known to be in increasing or decreasing order, then a much faster search algorithm than Linear Search can be used. An array in which the elements are in order is said to be sorted. Of course, it takes some work to sort an array, but if the array is to be searched many times, then the work done in sorting it can really pay off.

Binary search is a method for searching for a given item in a sorted array. Although the implementation is not trivial, the basic idea is simple: If you are searching for an item in a sorted list, then it is possible to eliminate half of the items in the list by inspecting a single item. For example, suppose that you are looking for the number 42 in a sorted array of 1000 integers. Let's assume that the array is sorted into increasing order. Suppose you check item number 500 in the array, and find that the item is 93. Since 42 is less than 93, and since the elements in the array are in increasing order, we can conclude that if 42 occurs in the array at all, then it must occur somewhere before location 500. All the locations numbered 500 or above contain values that are greater than or equal to 93. These locations can be eliminated as possible locations of the number 42.

Once we know that 42 can only be in the first half of the array, the obvious next step is to check location 250. If the number at that location is, say, -21, then you can eliminate locations before 250 and limit further search to locations between 251 and 499. The next test will limit the search to about 125 locations, and the one after that to about 62. After just 10 steps, there is only one location left. This is a whole lot better than looking through every element in the array. If there were a million items, it would still take only 20 steps for binary search to search the array! (Mathematically, the number of steps is approximately equal to the logarithm, in the base 2, of the number of items in the array.)

Time and Space Complexities
The time complexity of the binary search algorithm is O(log n).
The best-case time complexity would be O(1) when the central index would directly match the desired value. The worst-case scenario could be the values at either extremity of the list or values not in the list.
The space complexity of the binary search algorithm depends on the implementation of the algorithm. There are two ways of implementing it:
Iterative method: In the iterative method, the iterations can be controlled through the looping conditions.
In the iterative method, the space complexity would be O(1).
Recursive method:
There is no loop in the recursive method.
Rather than passing the new values to the next iteration of the loop, it passes them to the next recursion.
In the recursive method, the maximum and minimum are used as the boundary conditions.
The space complexity for the recursive method would be O(log n).
Real World Application

Some real life applications of Binary Search:

Dictionary
We go to the middle page and compare the words on that page with "Voracious".
If "Voracious" is alphabetically smaller than the word on the middle page then we can ignore all pages on the right side.
If it is alphabetically larger than the word on the middle page then we ignore all pages on the left side.
We keep repeating this process until we find the word.
Debugging a linear piece of code
If a code has many steps mostly executed in a sequence and there's a bug, we can isolate the bug by finding the earliest step where the code produces results which are different from the expected ones.
Figuring out resource requirements for a large system
Try running load tests on the system and binary search for the minimum amount of CPUs required to handle a predicted load.
Semiconductor test programs used for measuring digital timing or analog levels make extensive use of binary search.
Binary search can be also used to find numerical solutions to an equation.
Implementation

In order to make binary search into a Java subroutine that searches an array, A, for an item, N, we just have to keep track of the range of locations that could possibly contain N. At each step, as we eliminate possibilities, we reduce the size of this range. The basic operation is to look at the item in the middle of the range. If this item is greater than N, then the second half of the range can be eliminated. If it is less than N, then the first half of the range can be eliminated. If the number in the middle just happens to be N exactly, then the search is finished. If the size of the range decreases to zero, then the number N does not occur in the array. Here is a subroutine that implements this idea:

/**
 * Searches the array A for the integer N.
 * Precondition:  A must be sorted into increasing order.
 * Postcondition: If N is in the array, then the return value, i,
 *    satisfies A[i] == N.  If N is not in the array, then the
 *    return value is -1.
 */
static int binarySearch(int[] A, int N) {
      
    int lowestPossibleLoc = 0;
    int highestPossibleLoc = A.length - 1;
    
    while (highestPossibleLoc >= lowestPossibleLoc) {
       int middle = (lowestPossibleLoc + highestPossibleLoc) / 2;
       if (A[middle] == N) {
                 // N has been found at this index!
          return middle;
       }
       else if (A[middle] > N) {
                 // eliminate locations >= middle
          highestPossibleLoc = middle - 1;
       }
       else {
                 // eliminate locations <= middle
          lowestPossibleLoc = middle + 1;   
       }
    }
    
    // At this point, highestPossibleLoc < lowestPossibleLoc,
    // which means that N is known to be not in the array.  Return
    // a -1 to indicate that N could not be found in the array.
 
    return -1;
  
}
Exercises (Optional)
Successfully execute a complete Java program that implements the Binary Search subroutine above.
Use Binary Search to successfully execute a complete Java program that, given a sorted integer array, finds the index of a given number’s first and last occurrences.
If the element is not present in the array, report that as well.
Summary

If the elements in the array are known to be in increasing or decreasing order, then a much faster search algorithm than Linear Search can be used.
Binary search is a method for searching for a given item in a sorted array.
If you are searching for an item in a sorted list, then it is possible to eliminate half of the items in the list by inspecting a single item.
After eliminating half of the initial list, the same process can be repeated to continually cut the list in half until the routine completes by either finding the item, or concluding that the item is not in the list.
The time complexity of the binary search algorithm is O(log n).
The space complexity of the binary search algorithm depends on the implementation of the algorithm.
In the iterative method, the space complexity would be O(1).
The space complexity for the recursive method would be O(log n).


## Bubble Sort
Learning Objectives

After completing this module, associates should be able to:

Describe and implement Bubble Sort
Description
Bubble Sort
Bubble Sort is generally the first sorting algorithm that new developers are introduced to. This is because it's the easiest to implement. It is a simple sorting algorithm that repeatedly steps through the array to be sorted, compares adjacent elements and swaps them if they are in the wrong order.

Visualization
Initial Array: [64, 34, 25, 12]

Iteration 1
NOTE: The numbers in bold are indicating a swap

Swaps	index 0	index 1	index 2	index 3
1st swap	34	64	25	12
2nd swap	34	25	64	12
3rd swap	34	25	12	64
Result of Iteration 1 [34, 25, 12, 64]

Now this array isnt yet sorted however it is a little more sorted than it was before. If we do this same process over and over again, the array will eventually be sorted.

Iteration 2
Swaps	index 0	index 1	index 2	index 3
1st swap	25	34	12	64
2nd swap	25	12	34	64
Result of Iteration 2 [25, 12, 34, 64]

Iteration 3
Swaps	index 0	index 1	index 2	index 3
1st swap	12	25	34	64
Result of Iteration 3 [12, 25, 34, 64] <---- Officially Sorted!

NOTE: This algorithm generally is not used in practice due to it being an inefficient algorithm. The time complexity of bubble sort is O(n^2).

Real World Application

While bubble sort is not commonly used in real-world applications due to its inefficiency for large datasets, it can still be found in certain scenarios where simplicity and ease of implementation are prioritized over performance. Here are a few potential real-world applications where bubble sort might be used:

Small Datasets: In cases where the dataset is small and performance is not critical, such as sorting a short list of names, bubble sort may be sufficient. Its simplicity makes it easy to understand and implement in such scenarios.
Embedded Systems: In resource-constrained environments such as microcontrollers or embedded systems with limited memory and processing power, bubble sort may be chosen for its simplicity and low memory footprint. It can be suitable for sorting small arrays or lists in such environments.
Implementation

Below is an Implementation of Bubble Sort:

public class Main {
  public static void main(String[] args) {
    int[] arr = {64, 34, 25, 12};

    // Loop through each element of the array
    for (int i = 0; i < arr.length - 1; i++) {
      // Last i elements are already sorted
      for (int j = 0; j < arr.length - i - 1; j++) {
        // Compare adjacent elements and swap if needed
        if (arr[j] > arr[j + 1]) {
          int temp = arr[j];
          arr[j] = arr[j + 1];
          arr[j + 1] = temp;
        }
      }
    }
  }
}
In this code, we first define an array arr with some unsorted elements. We then use two loops, one nested inside another, to iterate through the array and compare adjacent elements. If the current element is greater than the next element, we swap them. We repeat this process until the array is fully sorted.

Summary

The algorithm uses two loops, one nested inside another, to iterate through an array and compares adjacent elements. If the current element is greater than the next element, the elements are swapped. This process is repeated until the array is fully sorted.
The time complexity of bubble sort is O(n^2)


## Merge Sort
Learning Objectives

After completing this module, associates should be able to:

Describe and implement Merge Sort
Description
Merge Sort
Merge sort is a sorting algorithm that is more efficient than bubble sort. This sorting algorithm utilizes a divide-and-conquer approach. A divide-and-conquer algorithm divides a problem into smaller subproblems, solves the subproblems recursively, and then combines their solutions to solve the original problem. The time complexity of this sort is O(nlog(n))

Visualization
mergesort

Real World Application

Merge sort, known for its efficiency, finds applications in various real-world scenarios, especially where large datasets need to be sorted quickly and reliably. Here's a real-world use case for Merge Sort:

Sorting large datasets stored on disk in a database management system (DBMS) during query processing or building indexes:
In databases, especially in scenarios where the dataset is too large to fit into memory, external sorting techniques like Merge Sort are employed. When sorting large datasets that exceed available memory, the data is divided into smaller chunks that can fit into memory, each chunk is sorted individually using an efficient algorithm like Merge Sort, and then the sorted chunks are merged to produce the final sorted result. Merge Sort's ability to efficiently merge already sorted arrays makes it well-suited for this task. External sorting is commonly used in database systems for operations like sorting query results, indexing, and creating sorted data files for efficient searching.

Implementation

Below is an implementation of Merge Sort:

public class App {
    public static void main(String[] args) {
      int[] arr = {64, 34, 25, 12, 22, 11, 90};
  
      // Call the merge sort method
      mergeSort(arr, 0, arr.length - 1);
  
      // Print the sorted array
      for (int i = 0; i < arr.length; i++) {
        System.out.print(arr[i] + " ");
      }
    }
  
    public static void mergeSort(int[] arr, int left, int right) {
      if (left < right) {
        int middle = (left + right) / 2;
  
        // Recursively sort the left and right sub-arrays
        mergeSort(arr, left, middle);
        mergeSort(arr, middle + 1, right);
  
        // Merge the sorted sub-arrays
        merge(arr, left, middle, right);
      }
    }
  
    public static void merge(int[] arr, int left, int middle, int right) {
      // Find the sizes of the left and right sub-arrays
      int n1 = middle - left + 1;
      int n2 = right - middle;
  
      // Create temporary arrays to hold the left and right sub-arrays
      int[] L = new int[n1];
      int[] R = new int[n2];
  
      // Copy the elements of the left and right sub-arrays into the temporary arrays
      for (int i = 0; i < n1; i++) {
        L[i] = arr[left + i];
      }
      for (int j = 0; j < n2; j++) {
        R[j] = arr[middle + 1 + j];
      }
  
      // Merge the temporary arrays back into the original array
      int i = 0, j = 0;
      int k = left;
      while (i < n1 && j < n2) {
        if (L[i] <= R[j]) {
          arr[k] = L[i];
          i++;
        } else {
          arr[k] = R[j];
          j++;
        }
        k++;
      }
  
      // Copy any remaining elements from the left sub-array
      while (i < n1) {
        arr[k] = L[i];
        i++;
        k++;
      }
  
      // Copy any remaining elements from the right sub-array
      while (j < n2) {
        arr[k] = R[j];
        j++;
        k++;
      }
    }
  }
In this code, we first define an array arr with some unsorted elements. We then call the mergeSort method, which recursively sorts the array using a divide-and-conquer approach. The mergeSort method splits the array into two halves, sorts each half recursively using the mergeSort method, and then merges the two sorted halves using the merge method. The merge method creates two temporary arrays to hold the left and right sub-arrays, and then merges them back into the original array in sorted order.

Summary

Merge sort is a divide-and-conquer algorithm
Merge sort uses a merge method that takes two sub-arrays and combines them



# SQL/Java Advanced
## DCL
Learning Objectives

After completing this module, associates should be able to:

Describe the DCL sublanguage
Identify the command set of DCL
Execute DCL statements on a RDBMS
Description

The Data Control Language of SQL is used to control rights, and permissions of users on database objects.

Commands
GRANT
REVOKE
Grant
The GRANT command is used to give users access privileges to the database or specific objects

Revoke
The REVOKE command removes users' access privileges to the database or specific objects.

Real World Application

The DCL sublanguage is used to control acces to databases and objects. In large scale applications, databases could be providing data to numerous applications. In practice, each one of those applications would represent a distinct user. Each one of those applications could possibly have a different set of access privileges to the different databases on the server and different objects in the databases.

In general, these privileges are granted before the database is accessed by the user, so the DBA would be responsible to setup user access prior to the user attempting to login to the database.

Implementation

The DCL sublanguage provides a means of defining access control to a database and its objects. Privileges can be granted or revoked on users.

Privileges
A database can have numerous privileges which can be permitted to users. Below is a summary of more common privileges.

Privilege	Description
SELECT	allows SELECT statements on tables
INSERT	allows INSERT statements on tables
DELETE	allows DELETE statements on tables
UPDATE	allows UPDATE statements on tables
INDEX	allows creating indexes on tables
CREATE	allows CREATE table statements
ALTER	allows ALTER table statements
DROP	allows DROP table statements
ALL	allow all permissions except GRANT
GRANT	allows GRANT statements
GRANT
The GRANT command is used to add permissions to a user. To GRANT permissions, the user executing the statement must have the GRANT permission.

GRANT PRIVILEGES ON object TO user;
To GRANT a simple privilege like SELECT on a table named 'posts' to a user named 'john.user'.

GRANT SELECT ON posts TO 'john.user';
To GRANT all of the CRUD abilities on a table named 'posts' to a user named 'community.manager'.

GRANT SELECT,  INSERT, UPDATE, DELETE ON posts TO 'community.manager';
Revoke
The REVOKE command is used to remove permissions from a user.

REVOKE PRIVILEGES ON object FROM user;
To revoke the SELECT privilege on the 'posts' table from 'john.user'

REVOKE SELECT ON posts FROM 'john.user';
To remove the CRUD privileges on 'posts' from 'community.manager'.

revoke SELECT, INSERT, UPDATE, DELETE ON posts FROM 'community.manager';
Summary

The DCL sublanguage is used to GRANT or REVOKE access privileges to databases and database objects.


## TCL
Learning Objectives

After completing this module, associates should be able to:

Define TCL
Define and implement TCL commands
Description

An SQL transaction is a sequence of one or more SQL operations (such as queries, inserts, updates, or deletes) that are treated as a single unit of work. Transactions are managed using Transaction Control Language (TCL) commands.

Commands
START TRANSACTION / BEGIN: Starts a new transaction.

COMMIT: This command is used to save the data permanently. Whenever we perform any of the DML command like INSERT, DELETE or UPDATE, these can be rolled back if the data is not stored permanently. So in order to be at the safer side COMMIT command is used.

ROLLBACK: This command is used to get the data or restore the data to the last savepoint or last committed state. If due to some reasons the data inserted, deleted or updated is not correct, you can rollback the data to a particular savepoint or if savepoint is not done, then to the last committed state.

SAVEPOINT: This command is used to save the data at a particular point temporarily, so that whenever needed can be rolled back to that particular point.

Real World Application

Here's why TCL is important:

Data Integrity: TCL commands like COMMIT, ROLLBACK, and SAVEPOINT are essential for ensuring data integrity in database transactions. They allow you to control when changes made within a transaction are permanently saved (committed) or discarded (rolled back), thus maintaining the consistency and reliability of the database.
ACID Properties: TCL plays a key role in enforcing the ACID (Atomicity, Consistency, Isolation, Durability) properties of transactions. By providing mechanisms to start, commit, or rollback transactions, TCL ensures that database operations are atomic (either all succeed or none), consistent (maintains database integrity), isolated (transactions do not interfere with each other), and durable (committed changes are permanent).
Error Handling: TCL provides mechanisms for error handling and recovery in database transactions. In case of errors or exceptions during transaction execution, TCL allows you to rollback the transaction to a previous consistent state using ROLLBACK, ensuring that no incomplete or erroneous changes are persisted in the database.
Overall, TCL is essential for maintaining data integrity, enforcing transactional properties, and handling errors in relational database systems.

Implementation

Consider an example of a transaction where you tried to add a new bank account ACC3 and set the funds to 10000. In that same transaction, you tried to create another account, ACC4, and set its funds to 900000 when it was supposed to be 9000. If the entire transaction is rolled back, the step of creating ACC3 with 10000 initial funds would also be rolled back. To avoid this, a save point can be added soon after creating ACC3 and we can roll back to before the statement of that creates account ACC4.

START TRANSACTION;
INSERT INTO bankaccounts VALUES("ACC3", 10000);
SAVEPOINT sv;
INSERT INTO bankaccounts VALUES("ACC4", 900000);
ROLLBACK TO sv;
INSERT INTO bankaccounts VALUES("ACC4", 90000);
COMMIT;
Summary

TCL stands for Transaction Control Language.
A single unit of work in a database is formed after the consecutive execution of commands is known as a transaction.
TCL commands help the user manage the transactions that take place in a database.
COMMIT, ROLLBACK and SAVEPOINT are the most commonly used TCL commands in SQL.


## What Is A Transaction
Learning Objectives

After completing this module, associates should be able to:

To define and create transactions
To understand transaction commands
Description

SQL transactions are a fundamental concept in database management systems (DBMS) that allow you to group multiple SQL operations into a single, atomic unit of work. Transactions ensure the integrity, consistency, and reliability of database operations, especially in multi-user and concurrent environments. Transactions are typically managed using Transaction Control Language (TCL) commands.

Commands
BEGIN or START TRANSACTION: Starts a new transaction.

COMMIT: Commits/saves the changes of a successful transaction.

SAVEPOINT: Sets a point within a transaction to which you can later rollback if needed, without rolling back the entire transaction.

ROLLBACK: Used to undo the changes done by the transaction.

Real World Application

An example for a transaction would be a funds transfer from one bank account to the other. Let's say that you want to debit $100 from Account A and credit that $100 to another account, Account B. If these two updates are not grouped together and only one of the statements is executed, i.e. the amount is debited from Account A but not credited into Account B, then where does the $100 go? To avoid situations like this, both updates should be grouped into a single transaction.

Implementation
SQL Transactions
Consider an example where there’s a bankaccounts table and we are doing an intra-bank transfer, i.e. person holding account ACC1 is trying to transfer $100 to account ACC2.

Step 1: Create Table bankaccount with columns account_no and funds.

CREATE TABLE bankaccounts(
    account_no varchar(20) PRIMARY KEY NOT NULL,
    funds decimal(8,2)
);
Step 2: Add two accounts named ACC1 and ACC2 with an initial fund balance of $1000 each.

INSERT INTO bankaccounts VALUES("ACC1", 1000);
INSERT INTO bankaccounts VALUES("ACC2", 1000);
Step 3: Now, let’s see the transaction of transfering 100$ from ACC1 to ACC2.

START TRANSACTION or BEGIN; 
UPDATE bankaccounts SET funds=funds-100 WHERE account_no='ACC1';
UPDATE bankaccounts SET funds=funds+100 WHERE account_no='ACC2';
COMMIT; 
Summary

In SQL transaction is grouping of statements into a single unit. A transaction satisfies all the ACID properties.

Commands
BEGIN / START TRANSACTION
COMMIT
SAVEPOINT
ROLLBACK


## ACID Properties
Learning Objectives

After completing this module, associates should be able to:

Define Atomicity
Define Consistency
Define Isolation
Define Durability
Description

The ACID properties are a set of four properties that guarantee the reliability and consistency of database transactions. They can be remembered using the acronym ACID: Atomicity, Consistency, Isolation, and Durability.

Atomicity
A transaction is considered to be atomic if it cannot be further broken down into individual operations, and all of the operations that occur within a transaction either succeed or fail as a single unit. If a single operation fails during a transaction, then everything is considered to have failed and must be rolled back.

Consistency
One of the advantages of using a transaction is that, even if the transaction is a success or a failure, the database is consistent, and the data integrity is maintained.

Isolation
Every transaction is isolated from other transactions. Therefore, a transaction shouldn't affect other transactions running at the same time. Stated another way, data modifications made by one transaction should be isolated from the data modifications made by other transactions. So, while a transaction can see data in the state it was in before another concurrent transaction modified it, as well as after the second transaction has completed, it cannot see any intermediate states.

Durability
Data modifications that take place within a successful transaction may be safely considered to be stored in the database regardless of whatever else may occur. As each transaction is completed, a row is entered in the database transaction log. Thus, in the event of a system failure that requires the database to be restored from a backup, you can use this transaction log to get the database back to the state it was in after a successful transaction.

Real World Application

Database transactions are a backbone of using a database for small and large-scale applications. It may be difficult at first to see how each of the properties of these transactions are applied to this larger context, so please review these examples:

Atomicity
A bank transaction of debiting $100 from ACC1 and crediting it to ACC2 should be atomic, if either of the statements in this transaction fails, the transaction of transferring funds from ACC1 to ACC2 should fail.

Consistency
Consider two bank accounts ACC1 and ACC2 with funds of 10000 and 9000, which is is total of 19000, after a transaction of transferring $50 from ACC2 to ACC1 the funds in ACC1 is $10050 and ACC2 are $8950, which means the totals funds in AC1 and ACC2 adds up to $19000, So the funds in ACC1 and ACC2 are consistent before and after the Transaction.

Isolation
Consider that the user A withdraws $100 and user B withdraws $250 from user ACC1 account, which has a balance of $1,000. Since both A and B draw from ACC1 account, one of the users is required to wait until the other user transaction is completed, avoiding inconsistent data.

Durability
Any real-life changes made to any database like transferring funds, crediting or debiting funds, updating employee details and updating grades of a student, etc are durable. once these transactions are successfully executed, the changes made to the database are permanent even if an unexpected system failure or an error occurs.

Implementation

For the following examples consider a banking application which allows funds to transfer between accounts.

Atomicity
Consider transferring 50$ from an account (ACC1) to another account (ACC2) where we check if ACC1 has sufficient funds.

START TRANSACTION;
UPDATE TABLE bankaccounts SET funds = funds-50 WHERE account_no= ACC1 AND funds>50;
UPDATE TABLE bankaccounts SET funds = funds+50 WHERE account_no= ACC2;
COMMIT;
In the above example debiting 50$ from ACC1 and crediting 50$ to ACC2 is a single transaction. If the entire transaction was not atomic, then it would be possible that our application debits ACC1 but does not credit ACC2 due to some error.

Consistency
Building off of this example, let's examine ACC1 and ACC2

SELECT * FROM bankaccounts;
RESULT:

account_no	funds
ACC1	850
ACC2	1,150
From the above table we can conclude that the sum of funds in ACC1 and ACC2 is 2000$.

Consider a transaction of transfering 150$ funds from ACC2 to ACC1.

START TRANSACTION;
UPDATE TABLE bankaccounts SET funds = funds-150 WHERE account_no= ACC2;
UPDATE TABLE bankaccounts SET funds = funds+150 WHERE account_no= ACC1;
COMMIT;
SELECT * FROM bankaccounts;
RESULT:

account_no	funds
ACC1	1,000
ACC2	1,000
From the above table it can be observed that the total funds in ACC1 and ACC2 are 2000$, from this we can conclude that the database is consistent before and after the transaction.

Isolation
Expanding on this code, we take the following steps:

Step 1: create a new bank account named ACC3

INSERT INTO bankaccounts VALUES("ACC3", 1000);
Step 2 (transaction 1): crediting 1000$ funds to ACC3

START TRANSACTION or BEGIN;
UPDATE bankaccounts SET funds=funds+1000 WHERE account_no='ACC3';
COMMIT;
Step 3 (transaction 2): transferring 100$ from ACC2 to ACC1.

START TRANSACTION or BEGIN;
UPDATE bankaccounts SET funds=funds-100 WHERE account_no='ACC2';
UPDATE bankaccounts SET funds=funds+100 WHERE account_no='ACC1';
COMMIT;
In the above example, transaction 1 and transaction 2 are concurrent, as we are affecting different accounts with each update statement.

Durability
Let's look at a database before and after a successful transaction:

SELECT * FROM banckaccounts;
account_no	funds
ACC1	1,100
ACC2	900
ACC3	1,000
START TRANSACTION or BEGIN;
UPDATE bankaccounts SET funds=funds+100 WHERE account_no='ACC3';
COMMIT;
SELECT * FROM banckaccounts;
account_no	funds
ACC1	1,100
ACC2	900
ACC3	1,100
From the above example, it can be observed that the results of the transaction, which is crediting 100$ to ACC3 are successfully updated into the database.

Summary

Atomicity is combining all statements into a single unit.
if a database crashes for some reason, atomicity makes sure that the entire transaction is rolled back, and partial results are not updated in the database.
Consistency is important to maintain data integrity, a database should consistent before and after a transaction.
All inconsistent data is removed, and all transactions that might cause inconsistency are aborted and an error is created or transcribed into an error log.
Isolation is a property that guarantees the individuality of each transaction and prevents them from being affected from other transactions.
It ensures that transactions are securely and independently processed at the same time without interference, but it does not ensure the order of transactions.
Durability ensures that the data changes caused by the successful execution of a transaction become permanent in the database.
The durability of the data is so permanent that even if the system fails or leads to a crash, the database still survives, and the changes of a transaction are updated in the database.
The ACID property of DBMS plays a vital role in maintaining the consistency and availability of data in the database.


## Transaction Commit Rollback Isolation Levels
Learning Objectives

After completing this module, associates should be able to:

Define the Isolation levels for Transactions
Understand the importance of transactional integrity.
Explain the Transaction phenomena
Description

Transaction Isolation Levels
Isolation levels are the degree to which a transaction must be isolated from the data modifications from any other transaction in the database system. The higher the degree of isolation, the more concurrency anomalies are prevented. However, this also means that the higher the degree of isolation, the less concurrent transactions become and this can impact performance.

Read Uncommitted
Read Uncommitted is the lowest isolation level
One transaction may read not yet committed changes made by other transactions (transactions are not isolated from each other in any way)
Transaction phenomena allowed: dirty read, non-repeatable read, phantom read
Transaction phenomena prevented: none
Read Committed
Guarantees that any data queried has already been committed
Transaction holds a read or write lock on the current record, preventing other transactions from reading, updating, or deleting that record.
Transaction phenomena allowed: non-repeatable read, phantom read
Transaction phenomena prevented: dirty read
Repeatable Read
Prevents other transactions from updating or deleting any data it queries
The transaction holds read locks on all records it references and writes locks on referenced records for update and delete actions
Transaction phenomena allowed: phantom read
Transaction phenomena prevented: dirty read, non-repeatable read
Serializable
Serializable is the highest isolation level.
Guaranteed to be serializable
Defined to be an execution of operations in which concurrently executing transactions appears to be serially executing.
Transaction phenomena allowed: none
Transaction phenomena prevented: dirty read, non-repeatable read, phantom read
Transaction Phenomena
Transaction phenomena, or concurrency anomalies, are unwanted or unexpected occurrences that can occur due to concurrent transactions being executed.

Dirty Read
Dirty Read is a situation when a transaction reads data from another transaction that has not yet been committed.
Example:
Transaction 1 updates a record and leaves it uncommitted.
Transaction 2 queries the updated record.
If transaction 1 rolls back the change, transaction 2 will have queried data that never existed.
Non Repeatable read
Non Repeatable read occurs when a transaction reads the same record twice and gets a different value each time.
Example:
Transaction 1 queries data.
Due to concurrency, Transaction 2 updates the same data and commit
If transaction 1 re-queries the same data, it will retrieve a different value.
Phantom Read
Phantom Read occurs when two same queries are executed, but the records retrieved by the two, are different.
Example:
Transaction 1 retrieves a set of records that satisfy a queries conditions.
Transaction 2 generates some new records that match the queries conditions for transaction T1.
If transaction T1 re-queries the records, it gets a different set of records this time.
Isolation Level	Dirty Reads	Non-repeatable Reads	Phantoms
Read Uncommitted	May Occur	May Occur	May Occur
Read Committed	Don't Occur	May Occur	May Occur
Repeatble Read	Don't Occur	Don't Occur	May Occur
Serializable	Don't Occur	Don't Occur	Don't Occur
Real World Application
Advantages
Improve concurrency by allowing multiple transactions to run concurrently without the risk of interfering with one another.

Provide control over the level of data consistency required by a particular application.

Reduce phenomena such as dirty reads, non-repeatable reads, and phantom reads.

Provides flexibility in designing applications that require different levels of data consistency.

Disadvantages
Increase overhead because the database management system must perform additional checks and acquire more locks.

Some isolation levels can decrease concurrency by requiring transactions to acquire more locks, which can lead to blocking.

Can limit the portability of applications across different systems as not all RDBMS support isolation levels.

Adds complexity to the design of database applications, making them more difficult to implement and maintain.

Implementation

Let's do examples of each isolation level and possible anomalies that might occur in a concurrent system. We'll use the following table:

create table users (
	id INTEGER PRIMARY KEY,
	name VARCHAR(50),
	age INTEGER
);

INSERT INTO users VALUES(1, 'Sandy', 40), 
                        (2, 'Jared', 32),
                        (3, 'Paco', 18),
                        (4, 'Derrick', 25);
Read Uncommitted
Transaction 1
BEGIN;
SELECT age FROM users WHERE id = 2; -- 32
-- other operations... transaction 2's UPDATE statement executes, even if it's COMMIT statement hasn't executed
SELECT age FROM users WHERE id = 2; -- 21
COMMIT;
Transaction 2
BEGIN;
UPDATE users SET age = 21 WHERE id = 2;
-- other operations...
COMMIT;
A dirty read occurred. The second transaction's UPDATE statement had yet to be committed. This can cause an inconsistency if the update was rolled back.

Read Committed
Transaction 1
BEGIN;
SELECT age FROM users WHERE id = 2; -- 32
-- other operations... transaction 2's UPDATE statement and COMMIT statement execute
SELECT age FROM users WHERE id = 2; -- 21
COMMIT;
Transaction 2
BEGIN;
UPDATE users SET age = 21 WHERE id = 2;
-- other operations...
COMMIT;
A non-repeatable read occurred. Although the update was committed successfully and the update was intended, the same query resulted in different results in the same transaction. If we were making a decision in our transaction that depended on there being a consistent value for the user's age, then we might not get the results we expect.

Repeatable Read
Transaction 1
BEGIN;
SELECT age FROM users WHERE id = 2; -- 32
-- other operations... transaction 2 can start execution...
SELECT age FROM users WHERE id = 2; -- 32
COMMIT; -- ...however the UPDATE statement cannot execute until this transaction finishes due to the lock on record
Transaction 2
BEGIN;
UPDATE users SET age = 21 WHERE id = 2;
-- other operations...
COMMIT;
In this example, the first transaction has a lock on the record that has an id of 2. It isn't until the first transaction ends that the second transaction can execute its UPDATE statement and the rest of its transaction. Notice that transaction 2 can still execute concurrently and is only blocked when it tries to interact with the same record as the first transaction.

An issue with this isolation level is that it does not prevent phantom reads, which is where the number of records returned from a query may differ. For example:

Transaction 1
BEGIN;
SELECT * from users WHERE age BETWEEN 10 AND 30; -- this should return records with ids of 2, 3, and 4
-- other operations... transaction 2 executes
SELECT * from users WHERE age BETWEEN 10 AND 30;-- this should return records with ids of 2, 3, 4, and 5
COMMIT;
Transaction 2
BEGIN;
INSERT INTO users VALUES (5, 'Jenny', 20);
-- other operations...
COMMIT;
During the execution of transaction 1, the second transaction executed completely and inserted a record that will change the amount of records returned from the query in transaction 1.

Serializable
Transaction 1
BEGIN;
SELECT * from users WHERE age BETWEEN 10 AND 30; -- this should return records with ids of 2, 3, and 4
-- other operations... 
SELECT * from users WHERE age BETWEEN 10 AND 30;-- this should return records with ids of 2, 3, and 4
COMMIT; -- Only after transaction 1 finishes does transaction 2 begin
Transaction 2
BEGIN;
INSERT INTO users VALUES (5, 'Jenny', 20);
-- other operations...
COMMIT;
In this isolation level, transactions are completely isolated from each other, ensuring that transactions execute as if they were serialized one after the other. This level provides the highest degree of isolation and prevents all concurrency anomalies. However, it may lead to increased contention and decreased concurrency due to locking.

Summary
Transaction Isolation Level
Read Uncommitted is the lowest isolation level where one transaction may read not yet committed changes made by other transactions, preventing isolation from each other.

Read Committed guarantees that any data queried is committed at the moment it is read and transaction holds a read or write lock on the current record, preventing other transactions from reading, updating, or deleting that record.

Repeatable Read is the most restrictive isolation level that holds read locks on all records it references and writes locks on referenced records for update and delete actions

Serializable is the highest isolation level to be an execution of operations in which concurrently executing transactions appears to be serially executing.

Transaction Phenomena
Dirty Read is a situation when a transaction reads data from another transaction that has not yet been committed.

Non Repeatable read occurs when a transaction reads the same record twice and gets a different value each time.

Phantom Read occurs when two of the same queries are executed, but the records retrieved by the two, are different.


## Aggregate Functions
Learning Objectives

After completing this module, associates should be able to:

Understand the purpose of aggregate functions in SQL
Explore different types of aggregate functions and their uses
Description

Aggregate functions in MySQL provide the output as a single value after performing different operations on a set of values.

The 5 most commonly used aggregate functions are given as follows:

Aggregate Function	Descriptions
count()	Returns the number of rows, including rows with NULL values in a group.
sum()	Returns the total summed values in a set.
avg()	Returns the average value of an expression.
min()	Returns the minimum (lowest) value in a set.
max()	Returns the maximum (highest) value in a set.
Real World Application

Real world applications include:

Bank Management system: Finding the count of all accounts in a bank.
Employee management system: finding the average salary of all employees.
Student report: Finding the minimum and maximum marks scored in a specific course.
Implementation
Examples
Counting the amount of records in a table:

SELECT COUNT(*) FROM EMP;    
Counting the values of a particular column:

SELECT COUNT(EMP_NAME) FROM EMP;   
Finding the sum of all values from a specific field:

SELECT SUM(duration) AS "Total duration" FROM EMP;
Calculating the average of the values from specified columnL

SELECT AVG(salary) FROM EMP;
Finding the minimum salary:

SELECT MIN(salary) FROM EMP;    
Finding the maximum salary:

SELECT MAX(salary) FROM EMP; 
Summary

MySQL aggregate functions retrieve a single value after performing a calculation on a set of values.
In general, aggregate functions ignore null values.
Often, aggregate functions are accompanied by the GROUP BY clause of the SELECT statement.


## Scalar Functions
Learning Objectives

After completing this module, associates should be able to:

Able to understand the purpose of scalar functions in SQL
Explore different types of scalar functions and their uses
Description

Scalar functions are pre-defined functions in SQL. They operate on a single value and returns a single value. Seven common scalar functions are given as follows:

Scalar Function	Description
UCASE()	Converts the value of a field to uppercase.
LCASE()	Converts the value of a field to lowercase.
MID()	Returns the substring from the text field.
LEN()	Returns the length of the value in a text field.
ROUND()	Used to round a numeric field to the number of decimals specified.
NOW()	Returns the current system date and time.
FORMAT()	Used to format how a field is to be displayed.
Real World Application

Scalar Functions can be used to maintain records which are case-sensitive and format-sensitive.

Real world applications include:

Bank Management system: Maintaining proper bank balance in correct format.
Employee management system: Maintaining proper ID verification which can be case-sensitive.
Captcha Manager: To provide verification to enter websites needs case-sensitive inputs.
Implementation
Examples
Converting a string to uppercase:

SELECT UCASE ("Hello World") AS UpperCase_String;  
UpperCase_String
HELLO WORLD
Converting a string into lowercase:

SELECT LCASE ("Hello World") AS LowerCase_String;  
LowerCase_String
hello world
Selecting a substring of a string:

SELECT MID ("Hello World", 4, 8) AS Substring;
Substring
lo World
Selecting the length of a string:

SELECT LENGTH ("Hello World") AS String_Length;
String_Length
11
Rounding a numeric value to the number of decimals specified:

SELECT ROUND (1560.44444, 2) AS Round_Value; 
Round_Value
1560.44
Selecting the current day and time:

SELECT NOW () AS CurrentDateTime;  
CurrentDateTime
2022-07-17 08:44:36
Formatting a numeric value:

SELECT FORMAT (1234.1234, 2) AS Format_Number;  
Format_Number
1234.12
Summary

Scalar functions operate on a single value and return a single value.


## Sequence
Learning Objectives

After completing this module, associates should be able to:

Describe the use of sequence in a database
Create a sequence
Description

A database sequence is an object in a relational database management system (RDBMS) that generates a sequence of numeric values according to a specified pattern. Sequences are often used to generate unique identifiers for primary keys in database tables, especially when the primary keys need to be generated independently of the data being inserted into the table.

Popular database systems, such as Oracle, PostgreSQL, and SQL Server, support sequences as a built-in feature. Note that each database system may have its own syntax and options for creating and using sequences.

Real World Application

Real world applications include:

creating primary keys for database tables
Priority based ticket booking
Sequence of students waiting to receive their awards
Implementation
Syntax to create a sequence
In PostgreSQL:

CREATE SEQUENCE sequence_name
    [ INCREMENT BY increment ]
    [ START WITH start ]
    [ MAXVALUE max_value | NO MAXVALUE ]
    [ MINVALUE min_value | NO MINVALUE ]
    [ CYCLE | NO CYCLE ];

sequence_name: Name of the sequence.
START WITH: Starting value from where the sequence starts.
INCREMENT BY: Value by which sequence will increment itself. Increment_value can be positive or negative.
MAXVALUE: Maximum value of the sequence.
MINVALUE: Minimum value of the sequence.
CYCLE: When sequence reaches its set_limit it starts from beginning.
NOCYCLE: An exception will be thrown if the sequence exceeds its max_value.
Example 1:

CREATE SEQUENCE example_1
AS INT
START WITH 10
INCREMENT BY 10; 
Example 2:

CREATE SEQUENCE example_2
start with 100
increment by -1
minvalue 1
maxvalue 100
cycle;
Summary

A sequence is a list of integers produced in an ascending order.
Each database system may have its own syntax and options for creating and using sequences.


## Trigger
Learning Objectives

After completing this module, associates should be able to:

Describe the use of triggers in a database
Create/Delete a trigger
Description

A trigger is a named SQL unit that is stored in the database and runs in response to an event that occurs in the database or when there is a modification to the database. You can specify the event, whether the trigger fires before or after the event, and whether the trigger runs for each event or for each row affected by the event.

For example, you can create a trigger that runs every time an INSERT statement affects the EMPLOYEES table.

Real World Application

Real world applications include:

A decision which enforces business rules
Validating input data
Performing cascade operations when related records are changed
Implementation

Syntax to create a trigger:

create trigger [trigger_name] 
    [before | after]  
    {insert | update | delete}  
    on [table_name]  
    [for each row]  
    [trigger_body] 
We can define 6 types of triggers for each table:

AFTER INSERT activated after data is inserted into the table.

AFTER UPDATE: activated after data in the table is modified.

AFTER DELETE: activated after data is deleted/removed from the table.

BEFORE INSERT: activated before data is inserted into the table.

BEFORE UPDATE: activated before data in the table is modified.

BEFORE DELETE: activated before data is deleted/removed from the table.

Example:

create trigger student_grade 
before INSERT 
on 
Student 
for each row 
set Student.total = Student.subj1 + Student.subj2 + Student.subj3, 
Student.perc = Student.total * 50 / 100;
Syntax to delete a trigger

DROP TRIGGER [IF EXISTS] trigger_name
Summary

A trigger is a named SQL unit that is stored in the database and runs in response to an event that occurs in the database
You can specify the event, whether the trigger fires before or after the event, and whether the trigger runs for each event or for each row affected by the event


## What Is A User Defined Function

Learning Objectives

After completing this module, associates should be able to:

Describe the purpose of function in a database
Create, execute, and delete a function
Description

A function in SQL serves a similar purpose to functions in dedicated programming and scripting languages. They perform some set of defined operations on given data and return a result. Functions in SQL are similar to procedures except functions return a value.

When creating a function, we give a definition of what our function has to do. We will have to call that function to perform the defined task as and when required. When a program calls a function, program control is switched to the respective function.

Characteristics of a function include:
Functions are separate blocks that are mainly used for calculations or bundling common operations into a single call.
Functions with DML commands can be called from other blocks, but functions without DML commands are typically called using a SELECT query.
Arguments (input values) can be passed into functions when called by defining function parameters during function creation.
These parameters should be included in the calling statements.
A function uses the RETURN command to return the value or raise an exception.
A return command is mandatory to create SQL functions, as it will always return the value, in calling statements it always accompanies with assignment operator to populate the variables.
Many SQL dialects and RDBMS' provide built-in functions for convenience, however, the creation of custom functions may be helpful to support a developer's specific database operations.

Real World Application

Real world applications include:

Development of a password strength function
Maintaining student or bank account details
Accessing and modifying data in directory
Implementation

Functions are created using the DML command CREATE. For the following examples, we are showcasing MySQL, but many SQL dialects support the creation of functions.

Syntax for creating a function
The following showcases the typical structure of a function using the CREATE command.


CREATE FUNCTION function_name [ (parameter datatype [, parameter datatype]) ]
RETURNS return_datatype
BEGIN

   -- declaration_section

   -- executable_section

END;
Notice that BEGIN and END are used to declare the operations for the function. Additionally, the RETURNS keyword is used to define the type of data returned from the function. Similar to other functions, optional parameter lists can be declared for the function to be later referenced within the function's operations.

Example:
The SQL statement below creates the function get_balance
``` SQL
CREATE FUNCTION get_balance(acc_no INT) 
RETURNS INT 
LANGUAGE plpgsql
AS $$
DECLARE 
   acc_bal INT;
BEGIN
   SELECT order_total 
   INTO acc_bal 
   FROM sample2.orders 
   WHERE customer_id = acc_no 
   LIMIT 1;



   RETURN acc_bal;
END;
$$;
```
In this example the get_balance function is used to find the value of the order_total column for a specified customer_id (based on the input, acc_no argument) from a table called orders. DELIMITER // is used to change the delimitor temporarily, so that semicolons within the procedure do not end the CREATE PROCEDURE statement prematurely. DELIMITER ; resets the delimiter back to the default, semicolon.

Syntax for executing a function
The below query showcases how to execute the function. Here, we are locating the order_total from the account 101.

SELECT get_balance(101);
Output:

4700
Syntax for deleting a function
The SQL statement below showcases the syntax to DROP a previously created function. An optional IF EXISTS statement can be added to the DROP command to check for the existence of the function prior to deletion. This can be helpful to avoid errors.

DROP FUNCTION [ IF EXISTS ] function_name;
Here, we see the syntax to drop the previously created get_balance function.

DROP FUNCTION IF EXISTS get_balance;
Summary

SQL functions perform some set of defined operations on given data and return a result
They can have parameters
A function uses the RETURN command to return the value or raise an exception and the RETURN command is mandatory
Many SQL dialects and RDBMS' provide built-in functions for convenience, however, the creation of custom functions may be helpful to support a developer's specific database operations


## What Is A Stored Procedure
Learning Objectives

After completing this module, associates should be able to:

Describe the purpose of stored procedure in a database
Describe how to Create or Alter a Stored Procedure
Description

A stored procedure is similar to functions in other programming languages, in which a set of statements are named and stored to executed at a future point in time as a batch. These stored procedures are stored inside a database schema and which can be invoked from within an SQL command call. A stored procedure can provide additional layers of security between the user interface and the database by abstracting data access because end users may manipulate data through procedure calls without exposing embedded queries in a graphical user interface. Additionally, stored procedures can be used to provide easier interface for complex or batch operations to be executed on database records.

Unlike user defined functions, stored procedures typically do not return data (although some SQL dialects may support this operation). Rather, procedures are used to perform many and/or complex operations or manipulate data in a batch. Additionally, stored procedures are not called through Queries (such as SELECT) and are instead called using a specific keyword, such as CALL, EXEC, or EXECUTE PROCEDURE the exact keyword(s) to use depend on the SQL dialect.

Real World Application

Real world applications include:

Updating database records in a batch
Providing interface for complex but frequently executed database operations
Database Error handling or logging
Performance optimization
Transaction Management
Implementation

Stored Procedures are created using the DML command CREATE. For the following examples, we are showcasing MySQL, but many SQL dialects support the creation of stored procedures. The exact syntax may vary from Language-to-Language.

Syntax for creating a procedure
CREATE PROCEDURE procedure_name ( IN | OUT | INOUT parameter_name parameter_datatype, … )
BEGIN    
    -- SQL statements
END
Note: MySQL allows data to be returned from a Stored Procedure using the OUT or INOUT declaration for parameters. This functionality is not supported in all SQL dialects.

Example
The following showcases how to create a procedure named UpdateEmployeeSalaries which will update the Salary column of all records in an Employees table based on a PercentageIncrease argument given.

DELIMITER //

CREATE PROCEDURE UpdateEmployeeSalaries(IN PercentageIncrease DECIMAL(5,2))
BEGIN
    -- Updating the salaries
    UPDATE Employees
    SET Salary = Salary * (1 + PercentageIncrease / 100);

    -- Logging the update
    INSERT INTO SalaryUpdateLog (UpdateDate, PercentageIncrease)
    VALUES (NOW(), PercentageIncrease);
END //

DELIMITER ;
Let’s break down this example in more detail:

DELIMITER // is used to change the delimiter temporarily, so that semicolons within the procedure do not end the CREATE PROCEDURE statement prematurely.
This procedure takes a single input parameter, PercentageIncrease which is a DECIMAL datatype. This decimal can include up to 5 digits and has a precision (numbers to the right of the decimal point) of two decimal places.
BEGIN marks the start of the procedure statements and END marks the end of the procedure.
The UPDATE statement sets the salary of all employees in the Employees table to a new value increased by the PercentageIncrease argument passed when calling this procedure.
The INSERT statement records a log in a SalaryUpdateLog table which includes the date of the update and the argument given for the PercentageIncrease parameter.
Finally, DELIMITER ; resets the delimiter back to the default, semicolon.
To call this procedure, using MySQL, you would use the keyword CALL.

CALL UpdateEmployeeSalaries(3.0)
Syntax for deleting a procedure
DDL is also used to remove the procedure. As such, the DROP command can be used to remove a previously created stored procedure.

DROP PROCEDURE [IF EXISTS] stored_procedure_name;
Note: The brackets around IF EXISTS means that the operators are optional, and will check if the entity exists prior to completing the rest of the operation. In the example below you will see it used with and without:

To remove the UpdateEmployeeSalaries procedure, the syntax would look like this:

-- Remove the Procedure 'UpdateEmployeeSalaries' if it exists.
DROP PROCEDURE IF EXISTS UpdateEmployeeSalaries; 
alternatively:

-- Remove the Procedure 'UpdateEmployeeSalaries'. May throw an error is the procedure does not exist.
DROP PROCEDURE UpdateEmployeeSalaries;
Summary

A stored procedure is similar to functions in other programming languages, in which a set of statements are named and stored to executed at a future point in time as a batch
Procedures are used to perform many and/or complex operations or manipulate data in a batch
Unlike user defined functions, stored procedures typically do not return data (although some SQL dialects may support this operation)


## Optional-class
Learning Objectives

After completing this module, associates should be able to:

Describe the need for the Optional class
Successfully implement a Java program that uses the Optional class
Description

The Optional class in Java is a container object that may or may not contain a non-null value. It was introduced in Java 8 as a way to handle scenarios where methods might return null values. Optional encourages developers to explicitly handle the case where the value may be absent. This makes the code more expressive and helps to avoid unexpected null pointer exceptions.

Optional methods
Optional is a generic class that can contain a value of type T, or it may be empty. It provides methods for accessing and manipulating the value if present. See the below methods:

of(value): Creates an Optional containing the specified non-null value. If the specified value is null, it throws a NullPointerException.
Example: Optional.of("biscuits")
ofNullable(value): Creates an Optional containing the specified value, which may be null. If the specified value is null, it returns an empty Optional.
Example: Optional.ofNullable(null)
empty(): Returns an empty Optional instance. Represents the absence of a value.
Example: Optional.empty()
isPresent(): Returns true if the Optional contains a non-null value, otherwise returns false.
Example: optional.isPresent()
ifPresent(consumer): If a value is present, performs the given action with the value, otherwise does nothing. Note that this method does not return a value.
Example: optional.ifPresent(value -> System.out.println(value))
get(): If a value is present, returns the value, otherwise throws a NoSuchElementException.
Example: optional.get()
orElse(other): If a value is present, returns the value, otherwise returns the specified other value.
Example: optional.orElse("some other value")
orElseGet(Supplier): If a value is present, returns the value, otherwise returns the result produced by the specified Supplier.
Example: optional.orElseGet(() -> "some other value")
orElseThrow(Supplier): If a value is present, returns the value, otherwise throws an exception created by the provided Supplier.
Example: optional.orElseThrow(() -> new RuntimeException("Value not present"))
map(Function): If a value is present, applies the given mapping function to it and returns an Optional containing the result, otherwise returns an empty Optional.
Example: optional.map(value -> value.toUpperCase())
flatMap(Function): If a value is present, applies the given mapping function to it, which must return an Optional, and returns the result, otherwise returns an empty Optional.
Example: optional.flatMap(value -> Optional.of(value.toUpperCase()))
filter(Predicate): If a value is present and matches the given predicate, returns an Optional containing the value, otherwise returns an empty Optional.
Example: optional.filter(value -> value.length() > 5)
When to Use orElse() vs orElseGet()
It is recommended to use orElse() when the value to be returned is a simple constant value or an already computed value. For a value resulting from a complex or expensive computation, then it is recommended to use orElseGet(). The reason for this is that orElseGet() takes a Supplier function as an argument, and that method will be evaluated only if the Optional does not have a value to return. This is known as lazy evaluation. If you were to pass an equivalent method call into the OrElse() function as an argument, that call will be evaluated to retrieve its value, even if the Optional did already have a value present.

When to Use map() vs flatMap()
It is recommended to use map() when the transformation function returns a non-optional value and you want to keep the result wrapped in an Optional. Use flatMap() when the transformation function returns an Optional itself, and you want to flatten the result to avoid nested Optional instances.

Resources
Java documentation: https://docs.oracle.com/javase/8/docs/api/java/util/Optional.html
Oracle article: https://www.oracle.com/technical-resources/articles/java/java8-optional.html
Real World Application

The Java Optional class provides a way to represent values that may or may not be present. While it might not seem immediately obvious how this class could be used in real-world applications, there are several scenarios where it can be beneficial:

API Design: Optional can be used in the design of APIs to clearly communicate to users whether a method can return null or not. By returning Optional instead of null, API designers can force users to explicitly handle the case where the value might be absent, improving the clarity and reliability of the API.
Stream Processing: Optional integrates well with Java Streams, allowing developers to process streams of data containing optional values. This can be useful when dealing with collections where some elements might be null or absent.
Configuration Settings: Optional can be used to represent optional configuration settings or parameters. For example, a method that reads configuration settings from a file could return an Optional containing the value of a specific setting, or an empty Optional if the setting is not present.
Overall, the Java Optional class can be a powerful tool for writing more robust, clear, and expressive code, especially in scenarios where values may or may not be present. By using Optional, developers can make their code more explicit about the possibility of null values and provide better error handling and clarity in their applications.

Implementation

Below are some examples of using the Optional class.

Example 1:

// Java program to illustrate
// optional class methods
 
import java.util.Optional;
 
class Rev {
 
    // Driver code
    public static void main(String[] args)
    {
 
        // creating a string array
        String[] str = new String[5];
 
        // Setting value for 2nd index
        str[2] = "hello world";
 
        // It returns an empty instance of Optional class
        Optional<String> empty = Optional.empty();
        System.out.println(empty);
 
        // It returns a non-empty Optional
        Optional<String> value = Optional.of(str[2]);
        System.out.println(value);
    }
}
Output:

Optional.empty
Optional[hello world]
In the above example, we used Optional.empty() to create an Optional object that does not hold a value. We then used Optional.of() to create another Optional object that contains a String value within it.

Example 2:

// Java program to illustrate
// optional class methods
 
import java.util.Optional;
 
class Rev {
 
    // Driver code
    public static void main(String[] args)
    {
 
        // creating a string array
        String[] str = new String[5];
 
        // Setting value for 2nd index
        str[2] = "hello world";
 
        // It returns a non-empty Optional
        Optional<String> value = Optional.of(str[2]);
 
        // It returns value of an Optional.
        // If value is not present, it throws
        // an NoSuchElementException
        System.out.println(value.get());
 
        // It returns hashCode of the value
        System.out.println(value.hashCode());
 
        // It returns true if value is present,
        // otherwise false
        System.out.println(value.isPresent());
    }
}
Output:

hello world
1967487235
true
In the above example, we use an Optional object's get() method to get it's value, if it has one. We want to be careful when calling this method, and it is recommended that we first ensure that there is a value with isPresent(), otherwise an exception will be thrown.

Exercises (Optional)
The exercises will focus around a PhoneBook class. The phone book already has some initial values:

Ted Striker Roger Murdock Elaine Dickinson

The partial code is below:

package optionals;

import java.util.*;

public class PhoneBook {

    private static final HashMap<String, String> PHONE_NUMBERS = new HashMap<String, String>() {
        {
            put("Ted Striker", "5551212");
            put("Roger Murdock", "3879812");
            put("Elaine Dickinson", "8675309");
            put("Jos de Vos", "016/161616");
        }
    };

    private HashMap<String, String> phoneBookEntries = PHONE_NUMBERS;

    PhoneBook() { }

    public HashMap<String, String> getPhoneBookEntries() {
        return phoneBookEntries;
    }

    public Optional<String> findPhoneNumberByName(String name){
        return null;
    }

    public Optional<String> findNameByPhoneNumber(String phoneNumber){
        return null;
    }

    @Override
    public String toString() {
        System.out.println("Hello from PhoneBook's toString method");
        return "PhoneBook{" +
                "phoneBookEntries=" + phoneBookEntries +
                '}';
    }
}

Exercise: Implement findPhoneNumberByName() and findNameByPhoneNumber() in PhoneBook class that returns an Optional. An empty Optional must be returned if nothing is found.

Test your implementation with the code below:

package optionals;

import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import java.util.NoSuchElementException;
import java.util.Optional;

import static org.assertj.core.api.Assertions.assertThat;

public class PhoneBookTest {

    @Rule
    public ExpectedException expectedException = ExpectedException.none();

    private PhoneBook phoneBook = new PhoneBook();

    @Test
    public void findPhoneNumberByName() {
        Optional<String> phoneNumber = phoneBook.findPhoneNumberByName("Jos de Vos");

        assertThat(phoneNumber.get()).isEqualTo("016/161616");
    }

    @Test
    public void findPhoneNumberByName_NotFound() {
        expectedException.expect(NoSuchElementException.class);

        Optional<String> phoneNumber = phoneBook.findPhoneNumberByName("Jos de Voss");

        phoneNumber.get();
    }

    @Test
    public void findNameByPhoneNumber() {
        Optional<String> name = phoneBook.findNameByPhoneNumber("016/161616");

        assertThat(name.get()).isEqualTo("Jos de Vos");
    }

    @Test
    public void findNameByPhoneNumber_NotFound() {
        expectedException.expect(NoSuchElementException.class);

        Optional<String> phoneNumber = phoneBook.findPhoneNumberByName("016/161619");

        phoneNumber.get();
    }

}


## stream-api
Learning Objectives

After completing this module, associates should be able to:

Describe the Stream API
Successfully implement a Java program that uses the Stream API
Description
Stream API
The Java 8 Stream API is a functional-style way of defining operations on a stream of elements. Streams are an abstraction which allow defining operations which do not modify the source data and are lazily executed. Streams do not store data, they simply define operations like filtering, mapping, or reducing, and can be combined with other operations and then executed. Some built-in Streams are located in the java.util.stream package. The Stream API enables developers to perform complex data processing operations on collections in a functional and declarative manner.

A stream is a sequence of elements that can be processed sequentially or in parallel. It represents a pipeline of data processing operations that can be applied to a collection. Streams are created from various data sources such as collections, arrays, or generator functions.

Streams are divided into intermediate and terminal operations. Intermediate streams return a new stream and are always lazy - they don't actually execute until a terminal operation is called. Some common intermediate operations include map, filter, distinct, sorted, limit, and skip. Terminal operations trigger the execution of the stream pipeline, which allows efficiency by perfoming all operations in a single pass over the data. Examples of terminal operations include forEach, collect, reduce, count, min, max, anyMatch, allMatch, and noneMatch.

Finally, reduction operations take a sequence of elements and combine them into a single result. Stream classes have the reduce() and collect() methods for this purpose, with many built-in operations defined in the Collectors class. Collectors are used to accumulate stream elements into a result container such as a List, Set, Map, or custom data structure. Common collectors include toList, toSet, toMap, joining, groupingBy, and partitioningBy.

Example:

List<Student> students = new ArrayList<>();
// add students...
List<Double> grades = students.stream()
                          .filter(s -> s.isAttending())
						  .mapToDouble(s -> s.getGrade)
						  .collect(Collectors.toList());
The Function Package
The Stream API makes extensive use of functional interfaces such as Predicate, Function, Consumer, and Supplier. These interfaces enable developers to pass lambda expressions or method references as arguments to stream operations, making it easy to define custom behavior for processing stream elements.

Java documentation: https://docs.oracle.com/javase/8/docs/api/java/util/function/package-summary.html
Real World Application

The Java Stream API is an essential component of the Java programming language, providing developers with a powerful tool for processing collections of objects in a functional and declarative manner. Here are some key reasons highlighting the importance of the Java Stream API:

Expressiveness and Readability: Streams enable developers to write code that is concise, expressive, and easy to understand. With features like lambda expressions and method chaining, complex data processing operations can be expressed in a clear and declarative style, leading to more maintainable code.
Functional Programming Paradigm: Streams embrace functional programming concepts such as immutability, higher-order functions, and lazy evaluation. This paradigm shift allows developers to write code that is more modular, composable, and scalable, leading to better software design and architecture.
Efficient and Parallel Processing: Streams provide built-in support for parallel execution, allowing developers to leverage multi-core processors and improve the performance of data processing tasks. By simply invoking the parallel() method on a stream, the Stream API can automatically parallelize operations across multiple threads, making it easy to take advantage of parallel processing without the need for low-level threading constructs.
Rich Set of Operations: The Stream API offers a wide range of intermediate and terminal operations for filtering, mapping, sorting, aggregating, and reducing data. These operations, combined with collectors and collectors' factory methods, provide developers with a comprehensive toolkit for manipulating and transforming collections of objects.
In summary, the Java Stream API plays a crucial role in modern Java development, offering developers a flexible, efficient, and expressive way to work with collections of objects. Its adoption has led to cleaner, more maintainable codebases and has empowered developers to tackle complex data processing tasks with ease.

Implementation

Below is an Associate class:

public class Associate {

  private int age;
  private String firstName;
  private String lastName;

	// ... constructors and methods omitted
}
Let's say that in our main() method we creat a list of Associate objects:

List<Associate> associateList = new ArrayList<>();
    associateList.add(new Associate(1, "Juan", "Lopez"));
    associateList.add(new Associate(2, "Ariel", "Gomez"));
    associateList.add(new Associate(3, "Peter", "Alagna"));
    associateList.add(new Associate(4, "Bobbert", "Lesley"));
Below are several examples of working with streams:

/** Iterate **/
System.out.println("Iterating over all list: ");
associateList.stream().forEach((Associate a) -> {
	System.out.println(a.getFirstName());
});
/** Filter **/
String filter = "r";
System.out.println("\nIterating over list with filter(" + filter + ")");
associateList.stream().filter((Associate a) -> new StringBuilder(a.getFirstName()).indexOf(filter) != -1)
.forEach((Associate a) -> {
	System.out.println(a.getFirstName());
});
/** Getting Max Age Value **/
    int maxAge = associateList.stream().mapToInt(Associate::getAge).max().getAsInt();
    System.out.println("\nThe highest age: " + maxAge);
/** Getting Average Age Value **/
    double avgAge = associateList.stream().mapToDouble(Associate::getAge).average().getAsDouble();
    System.out.println("\nThe average age: " + avgAge);
Exercises (Optional)
In these exercises, there are two domain classes: Country and City. Each city belongs to a country defined by the attribute, countryCode. Each country has a unique code and has many cities.

package com.example.domain;

import java.util.ArrayList;
import java.util.List;

public class Country {
   private String code;
   private String name;
   private String continent;
   private double surfaceArea;
   private int population;
   private double gnp;
   private int capital;
   private List<City> cities;
   {
      cities = new ArrayList<>();
   }

   public Country() {
   }

   public Country(String code, String name, String continent, int population,
         double surfaceArea, double gnp, int capital) {
      this.code = code;
      this.name = name;
      this.continent = continent;
      this.surfaceArea = surfaceArea;
      this.population = population;
      this.capital = capital;
      this.gnp = gnp;
   }

   // getters and setters

   @Override   
   public String toString() {
      return "Country [ name=" + name + ", population=" + population + "]";
   }

}
package com.example.domain;

public class City {
   private int id;
   private String name;
   private int population;
   private String countryCode;

   public City() {
   }

   public City(int id, String name, String countryCode, int population) {
      this.id = id;
      this.name = name;
      this.population = population;
      this.countryCode = countryCode;
   }

   // getters and setters
   
   @Override   
   public String toString() {
      return "City [id=" + id + ", name=" + name + ", population=" + population + ", countryCode=" + countryCode + "]";
   };

}

EXERCISE 1:

Write a Java program to find the highest populated city of each country.

EXERCISE 2:

Write a Java program to find the highest populated city of each continent.

Summary

The Java 8 Stream API is a functional-style way of defining operations on a stream of elements.
Streams are an abstraction which allow defining operations which do not modify the source data and are lazily executed.
Streams do not store data, they simply define operations like filtering, mapping, or reducing, and can be combined with other operations and then executed.
Some built-in Streams are located in the java.util.stream package.
Streams are divided into intermediate and terminal operations.
Intermediate streams return a new stream and are always lazy - they don't actually execute until a terminal operation is called.
Terminal operations trigger the execution of the stream pipeline, which allows efficiency by performing all operations in a single pass over the data.
Finally, reduction operations take a sequence of elements and combine them into a single result.
Stream classes have the reduce() and collect() methods for this purpose, with many built-in operations defined in the Collectors class.


## reflection-api
Learning Objectives

After completing this module, associates should be able to:

Describe the Reflection API
Successfully implement a Java program using the Reflection API
Description

Reflection in Java is a feature that allows a program to examine and manipulate its own structure, behavior, and metadata at runtime. It provides a way to inspect and modify classes, interfaces, fields, methods, and constructors, even if they are private or inaccessible through normal means. Reflection is particularly useful for building generic code, implementing frameworks, and writing debugging tools. The API can be found in the java.lang.reflect package.

Using Reflection
The java.lang.Class class represents classes and interfaces in Java. It provides methods to query information about a class, such as its name, superclass, implemented interfaces, fields, methods, and constructors. There are several ways to obtain a Class object:

Using the .class syntax: Class<?> clazz = MyClass.class;
Using the getClass() method: Class<?> clazz = obj.getClass();
Using the Class.forName() method: Class<?> clazz = Class.forName("com.example.MyClass");
Once you have a Class object, you can inspect its structure using methods such as getFields(), getMethods(), getConstructors(), getDeclaredFields(), getDeclaredMethods(), and getDeclaredConstructors(). These methods return arrays of Field, Method, or Constructor objects, which represent the fields, methods, and constructors of the class, respectively.

The difference between methods that contain the word "declared" in the name and those that don't is that they only include members defined within the class itself, including private members. Methods that do not include the word "declared" include both inherited members and members defined within the class itself, except for private members.
Just as there is a class that represents Class, there are classes that represent constructors, fields, and methods. These classes contain methods for accessing metadata about or using these class members.

Resources
Java documentation: https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/package-summary.html and https://docs.oracle.com/javase/8/docs/api/java/lang/Class.html
Java tutorial: https://docs.oracle.com/javase/tutorial/reflect/index.html
Oracle article: https://www.oracle.com/technical-resources/articles/java/javareflection.html
Real World Example

Knowing reflection in Java is important for several reasons:

Introspection: Reflection enables you to inspect the structure and behavior of classes, interfaces, fields, methods, and constructors at runtime. This introspective capability is valuable for building frameworks, libraries, and tools that need to analyze or manipulate classes dynamically.
Dependency Injection and IoC Containers: Many dependency injection frameworks and Inversion of Control (IoC) containers use reflection to inspect and wire dependencies at runtime. By using reflection, these frameworks can automatically instantiate and inject dependencies into objects based on their annotations or configuration.
Testing and Debugging: Reflection can be helpful in testing and debugging scenarios where you need to access and modify private fields or invoke private methods for testing purposes. It enables you to inspect and manipulate the internal state of objects, which can be useful for writing unit tests or diagnosing issues in production environments.
Annotations and Custom Annotations: Reflection is often used with annotations to process and interpret metadata associated with classes, fields, methods, and other program elements. It allows you to extract and analyze annotation metadata at runtime, enabling powerful runtime behavior based on annotations.
Framework and Library Development: Reflection is frequently employed in the development of frameworks and libraries to provide flexible and extensible behavior. Frameworks can use reflection to dynamically discover and invoke extension points, configure components based on annotations or configuration files, and enforce runtime constraints or policies.
Overall, understanding reflection in Java empowers developers to build more flexible, extensible, and dynamic applications by leveraging the power of introspection and runtime manipulation of classes and objects. While reflection can be a powerful tool, it should be used judiciously due to its potential performance overhead and the complexity it introduces to the codebase.

Implementation

When using the Reflection API, we use the following three steps:

Obtain a Class object
Get one or more of its class members
Manipulate or use the class member
Obtaining a Class Object
Class<?> classObj = String.class;
System.out.println(classObj);

Class<?> classObj2 = Class.forName("java.lang.String");
System.out.println(classObj2);

Class<?> classObj3 = "hello world".getClass();
System.out.println(classObj3);
Output:

class java.lang.String
class java.lang.String
class java.lang.String
Above are three examples of obtaining the Class object for the String class. Note the use of wildcards in the generic parameters. The reason for this is for some of these examples, the return type may be uncertain or not exactly of the type we are expecting.

Obtaining Class Members
// step 1
Class<?> classObj = Object.class;
System.out.println(classObj + "\n");

// step 2
Member[] members = classObj.getDeclaredMethods();
for (Member member : members) {
   System.out.println(member);
}
In the above example, we obtain a Class object for the Object class, and then we get its methods and print them out to the console. Note that getDeclaredMethods() returns an array of objects of the type Member. This is the supertype for the Field, Method, and Constructor classes.

The following code is the output:

class java.lang.Object

protected void java.lang.Object.finalize() throws java.lang.Throwable
public final void java.lang.Object.wait(long,int) throws java.lang.InterruptedException
public final void java.lang.Object.wait() throws java.lang.InterruptedException
public final native void java.lang.Object.wait(long) throws java.lang.InterruptedException
public boolean java.lang.Object.equals(java.lang.Object)
public java.lang.String java.lang.Object.toString()
public native int java.lang.Object.hashCode()
public final native java.lang.Class java.lang.Object.getClass()
protected native java.lang.Object java.lang.Object.clone() throws java.lang.CloneNotSupportedException
public final native void java.lang.Object.notify()
public final native void java.lang.Object.notifyAll()
Using Class Members

// step 3
Method method = (Method) classObj.getDeclaredMethod("getClass");
System.out.println(method);
System.out.println(method.invoke("hello world"));
If we continue with the previous example, we can use classObj to get a specific method with the name getClass and invoke it.

The following code is the output:

public final native java.lang.Class java.lang.Object.getClass()
class java.lang.String
Exercises (Optional)
Normally it is up to the programmer to write a toString() method for each class one creates. This exercise is about writing a general toString() method once and for all. As part of the Reflection API for Java, it is possible to find out which fields exist for a given object, and to get their values. The purpose of this exercise is to make a toString() method that gives a printed representation of any object, in such a manner that all fields are printed, and references to other objects are handled as well.

To solve the exercises, you will need to examine the java.lang.reflect API.

Write a class ToString with one static method "String toString(Object o)". The first version should just return the name of the class the object is an instance of. Write another class, ToStringTest, which prints the result of calling "ToString.toString("Hans")".
Extend the toString method. This time it should find out which fields exist in the object, and return a string of the format "classname{fieldName1, fieldName2,....,fieldNameN}". When this works, make sure you do not print out a superfluous comma just before the closing brace.
Extend the toString method, so that each field is printed in the form "fieldName: fieldType".
We do not want static fields to be included in the printout. Make sure no static fields are printed. Keep testing the method using the ToStringTest method.
Extend the method to print out the values of each field using Java's built in toString method. The format for each field should now be "fieldName: fieldType = value". Note, the value of a private field can be read after you use the "setAccessible(true)" method that fields inherit from AccessibleObject.
A field might be an Array. If it is, write each value in the array as "[val1, val2, val3,..., valN]". Optional extra. If the array has more than 15 elements, only the first 15 should be printed, and the rest should be printed as "...". Hint on retrieving values from an Array: Try looking at the documentation of the Array class in the API.
Summary

Reflection allows an executing Java program to examine or "introspect" upon itself and manipulate internal properties of the program.
The API can be found in the java.lang.reflect package..
The java.lang.Class class represents classes and interfaces in Java and it provides methods to query information about a class.
Once you have a Class object, you can inspect its structure using methods such as getFields(), getMethods(), etc.
Just as there is a class that represents Class, there are classes that represent constructors, fields, and methods. These classes contain methods for accessing metadata about or using these class members.


## thread-class
Learning Objectives

After completing this module, associates should be able to:

Describe the Thread class topic
Successfully implement a Java program using the Thread class
Description
Thread Class
In Java, the Thread class is a fundamental part of the multithreading capabilities provided by the language. It represents a separate thread of execution, allowing concurrent execution of multiple tasks within a single Java program. Multithreading is achieved via the Thread class and/or the Runnable interface.

Thread Methods
A few important methods in the Thread class include:

getters and setters for id, name, and priority
interrupt() to explicitly interrupt the thread
isAlive(), isInterrupted() and isDaemon() to test the state of the thread
join() to wait for the thread to finish execution
start() to actually begin thread execution after instantiation
A few important static methods are also defined:

Thread.currentThread() which returns the thread that is currently executing
Thread.sleep(long millis) which causes the currently executing thread to temporarily stop for a specified number of milliseconds
Thread Priorities
Priorities signify which order threads are to be run. The Thread class contains a few static variables for priority:

MIN_PRIORITY = 1
NORM_PRIORITY = 5, default
MAX_PRIORITY = 10
Creating Threads using Thread class
You can create a new thread by subclassing Thread and overriding its run() method, or by providing a Runnable object or lambda to the Thread constructor:

class MyThread extends Thread {
  @Override
  public void run() {
    System.out.println("Inside the MyThread thread");
  }
}

public class Main {

  public static void main(String[] args) throws Exception {
	// create threads
    Thread myRunnable = new Thread(() -> {
      System.out.println("Inside the Runnable thread");
    });
    Thread myThread = new MyThread();

	// start threads
    myRunnable.start();
    myThread.start();
  }

}
In the above example, we created two threads and started them. The first thread was created by providing a lambda that satisfied Runnable's run() method to a Thread() constructor. The second thread was created by instantiating a class we created, MyThread, that extended the Thread class.

The output is as follows:

Inside the Runnable thread
Inside the MyThread thread
Real World Application

A very good example of thread-based multithreading is a word processing program that checks the spelling of words in a document while the user is writing the document. This is possible only if each action is performed by a separate thread.
Another familiar example is a browser that starts rendering a web page while it is still downloading the rest of page.
Background jobs like running application servers which will come into action whenever a request comes.
Performing some execution while I/O blocked.
Gathering information from different web services running in parallel.
Games are very good examples of threading. You can use multiple objects in games like cars, motor bikes, animals, people etc. All these objects are nothing but just threads that run your game application.
Airplane ticket reservation system where multiple customers accessing the server.
Multiple bank account holders accessing their accounts simultaneously on the bankSA server.
Implementation
Thread Creation by Extending the Thread Class
In the following example we will make an Employee class that extends the Thread class. Here we also override the run() method that is made available to us through the Thread class. A thread begins its life inside run() method.

Employee.java

package com.revature.threads.intro;

public class Employee extends Thread {

	@Override
	public void run() {
		
		for(int i = 0; i < 10; i++) {
			System.out.println(Thread.currentThread().getName() + " is working...");
			
			try {
				Thread.sleep(2000);
			} catch (InterruptedException e) {
				
				/*
				 * InterruptedException is thrown when the Employee's interrupt()
				 * method is called. We will break out if this occurs.
				 */
				e.printStackTrace();
				break;
			}
		}
	}
}
Using the Thread Through ThreadDriver Class
In the below example we will be using the above Employee class and use the thread we started in there. Here you will see the use of start() and join() methods.

We create an object of our Employee class and call the start() method to start the execution of a thread. start() invokes the run() method on the Thread object.

We then use the join() method to tell our thread to wait until the specified thread completes its execution.


package com.revature.threads.intro;

public class ThreadDriver {

	public static void main(String[] args) {
		
		Employee emp1 = new Employee(); // Thread state: NEW
		emp1.setPriority(1);
//		emp1.run();	// does not actually create a new thread
		emp1.start(); // Thread state: RUNNING
		
		Employee emp2 = new Employee();
		emp2.setPriority(2);
		emp2.start();
		
		/*
		 * join() method
		 * 
		 * Using join(), we tell our thread to wait until the specified thread completes
		 * its execution. There are overloaded versions of the join() method, which allows
		 * us to specify the time for which you want to wait for the specified thread to
		 * terminate.
		 */
		try {
			emp1.join(); // Waiting for emp1 to finish its execution
		} catch (InterruptedException e) {
			e.printStackTrace();
		}
		// Display the priority of threads. The default priority is 5.
		System.out.println(emp1.getPriority());
		System.out.println(emp2.getPriority());
		
		// Check to see if a given thread is alive or dead
		System.out.println(emp1.isAlive());
		System.out.println(emp2.isAlive());
		
		
	}
}

Exercise (Optional)
Can you get errors by using an unsynchronized counter with multiple threads? Write a program to find out. Use the following unsynchronized Counter class, which you can include as a nested class in your program:

static class Counter { int count; void inc() { count = count+1; } int getCount() { return count; } }

Write a Thread class that will repeatedly call the inc() method in an object of type Counter. The object should be a shared global variable. Create several threads, start them all, and wait for all the threads to terminate. Print the final value of the counter, and see whether it is correct.

Let the user enter the number of threads and the number of times that each thread will increment the counter. You might need a fairly large number of increments to see an error. And of course there can never be any error if you use just one thread. Your program can use join() to wait for a thread to terminate.

Summary

The Thread class represents a separate thread of execution, allowing concurrent execution of multiple tasks within a single Java program.
Multithreading is achieved via the Thread class and/or the Runnable interface.
The Thread class has methods for managing threads, such as isAlive(), isInterrupted(), join(), and start().
Priorities signify which order threads are to be run.
You can create a new thread by subclassing Thread and overriding its run() method, or by providing a Runnable object or lambda to the Thread constructor.


## runnable-interface
Learning Objectives

After completing this module, associates should be able to:

Describe the Runnable Interface
Successfully implement a Java program using the Runnable Interface
Description
Runnable Interface
In Java, multithreading is achieved via the Thread class and/or the Runnable interface. java.lang.Runnable is an interface that is to be implemented by a class whose instances are intended to be executed by a thread.

Creating Threads using Runnable Interface
Create a class that implements the Runnable functional interface
implement the run() method
pass an instance of your class to a Thread constructor
call the start() method on the thread
Example:

	public class MyRunnable implements Runnable {
		@Override
		public void run() {
			System.out.println("Inside the MyRunnable class");
		}
	}
Runnable and Lambda Expressions
Because Runnable is a functional interface, we can use a lambda expression to define thread behavior inline instead of implementing the interface in a separate class. We pass a lambda expression as the Runnable type required in the Thread constructor.

Example:

public class ThreadLambda {
  public static main(String[] args) {
    Thread willRun = new Thread(() -> {
	  System.out.println("Running!");
	});
	willRun.start();
  }
}
Real World Application

Here are several real-world applications where the Runnable interface is commonly used:

Web Server Implementation: In web server applications, the Runnable interface is used to handle incoming client requests concurrently. Each client request is encapsulated as a Runnable task, which is then executed by a thread pool to ensure responsiveness and scalability of the server.
Background Processing: Many applications perform background processing tasks, such as batch processing, data synchronization, or periodic maintenance tasks. The Runnable interface allows you to encapsulate these tasks as separate Runnable units of work, which can be executed asynchronously or on separate threads.
Parallel Processing: In parallel processing applications, such as scientific computing or data analysis, the Runnable interface can be used to define parallelizable tasks that can be executed concurrently on multiple processor cores or nodes in a cluster.
Testing and Mocking: In unit testing and mocking frameworks, the Runnable interface can be used to define test cases or mock behaviors that need to be executed asynchronously or in parallel. This allows you to simulate concurrent execution scenarios and verify the behavior of your code under different conditions.
Overall, the Runnable interface is a versatile and widely-used component of the Java platform, providing a simple and flexible way to define and execute concurrent tasks in various types of applications and frameworks. Its usage is foundational to building responsive, scalable, and efficient software solutions that leverage the power of concurrency and parallel processing.

Implementation
Steps to create a new thread using Runnable
Create a Runnable implementer and implement the run() method.
Instantiate the Thread class and pass the implementer to the Thread. Thread has a constructor which accepts Runnable instances.
Invoke the start() method of Thread instance. Start internally calls run() of the implementer. Invoking start() creates a new Thread that executes the code written in run(). Calling run() directly doesn’t create and start a new Thread, it will run in the same thread.
Example 1
public class RunnableDemo {
 
    public static void main(String[] args)
    {
        System.out.println("Main thread is- "
                        + Thread.currentThread().getName());
        Thread t1 = new Thread(new RunnableDemo().new RunnableImpl());
        t1.start();
    }
 
    private class RunnableImpl implements Runnable {
 
        public void run()
        {
            System.out.println(Thread.currentThread().getName()
                             + ", executing run() method!");
        }
    }
}
Output:

Main thread is- main
Thread-0, executing run() method!
The output shows two active threads in the program – main thread and Thread-0. The main() method is executed by the main thread but invoking the start on RunnableImpl creates and starts a new thread – Thread-0. What happens when Runnable encounters an exception? Runnable can’t throw checked exceptions but RuntimeException can be thrown from the run(). Uncaught exceptions are handled by the exception handler of the thread, and if JVM can’t handle or catch exceptions, it prints the stack trace and terminates the flow.

Example 2
import java.io.FileNotFoundException;
 
public class RunnableDemo {
 
    public static void main(String[] args)
    {
        System.out.println("Main thread is- " +
                          Thread.currentThread().getName());
        Thread t1 = new Thread(new RunnableDemo().new RunnableImpl());
        t1.start();
    }
 
    private class RunnableImpl implements Runnable {
 
        public void run()
        {
            System.out.println(Thread.currentThread().getName()
                             + ", executing run() method!");
            /**
             * Checked exception can't be thrown, Runnable must
             * handle checked exception itself.
             */
            try {
                throw new FileNotFoundException();
            }
            catch (FileNotFoundException e) {
                System.out.println("Must catch here!");
                e.printStackTrace();
            }
 
            int r = 1 / 0;
            /*
             * Below commented line is an example
             * of thrown RuntimeException.
             */
            // throw new NullPointerException();
        }
    }
}
Output:

Thread-0, executing run() method!
Must catch here!
java.io.FileNotFoundException
    at RunnableDemo$RunnableImpl.run(RunnableDemo.java:25)
    at java.lang.Thread.run(Thread.java:745)
Exception in thread "Thread-0" java.lang.ArithmeticException: / by zero
    at RunnableDemo$RunnableImpl.run(RunnableDemo.java:31)
    at java.lang.Thread.run(Thread.java:745)
The output shows that Runnable can’t throw checked exceptions, like FileNotFoundException, to the callers. It must handle checked exceptions in the run() method but RuntimeExceptions (thrown or auto-generated) are handled by the JVM automatically.

Exercise (Optional)
Write a program to find the number in the range 1 to 100000 that has the largest number of divisors. By using threads, your program will take less time to do the computation when it is run on a multiprocessor computer. At the end of the program, output the elapsed time, the integer that has the largest number of divisors, and the number of divisors that it has. For this exercise, you should simply divide up the problem into parts and create one thread to do each part.

Summary

In Java, multithreading is achieved via the Threadclass and/or the Runnable interface.
The java.lang.Runnable interface needs to be implemented by a class whose instances are intended to be executed by a thread.
To create a class that implements the Runnable functional interface
implement the run() method
pass an instance of your class to a Thread constructor
call the start() method on the thread
Because Runnable is a functional interface, we can use a lambda expression to define thread behavior inline instead of implementing the interface in a separate class.
We pass a lambda expression as the Runnable type required in the Thread constructor.


## states-of-a-thread
Learning Objectives

After completing this module, associates should be able to:

Describe the states of a thread
Successfully implement a Java program demonstrating the different states of a thread
Description
States of a Thread
At any given time, a thread can be in one of these states:

New: newly created thread that has not started executing
Runnable: either running or ready for execution but waiting for its resource allocation
Blocked: waiting to acquire a monitor lock to enter or re-enter a synchronized block/method
Waiting: waiting for some other thread to perform an action without any time limit
Timed_Waiting: waiting for some other thread to perform a specific action for a specified time period
Terminated: has completed its execution
Thread lifecycle

Life Cycle of a thread
New Thread: When a new thread is created, it is in the new state. The thread has not yet started to run when the thread is in this state. When a thread lies in the new state, its code is yet to be run and hasn’t started to execute.
Runnable State: A thread that is ready to run is moved to a runnable state. In this state, a thread might actually be running or it might be ready to run at any instant of time. It is the responsibility of the thread scheduler to give the thread, time to run. A multi-threaded program allocates a fixed amount of time to each individual thread. Each and every thread runs for a short while and then pauses and relinquishes the CPU to another thread so that other threads can get a chance to run. When this happens, all such threads that are ready to run, waiting for the CPU and the currently running thread lie in a runnable state.
Blocked/Waiting state: When a thread is temporarily inactive, then it’s in one of the following states:
Blocked
Waiting
Timed Waiting: A thread lies in a timed waiting state when it calls a method with a time-out parameter. A thread lies in this state until the timeout is completed or until a notification is received. For example, when a thread calls sleep or a conditional wait, it is moved to a timed waiting state.
Terminated State: A thread terminates because of either of the following reasons:
Because it exits normally. This happens when the code of the thread has been entirely executed by the program.
Because there occurred some unusual erroneous event, like segmentation fault or an unhandled exception.
Real World Application

Knowing the different thread states in Java is important for understanding and debugging concurrent programs. Here are several reasons why understanding thread states is important:

Debugging: When troubleshooting concurrency issues, understanding thread states can help identify the cause of problems such as deadlocks, livelocks, and race conditions. By examining the state of threads in the system, developers can gain insights into the behavior of concurrent code and diagnose issues more effectively.
Performance Optimization: Understanding thread states can aid in performance optimization by identifying bottlenecks and inefficiencies in concurrent programs. By analyzing the state transitions of threads, developers can identify areas where threads spend excessive time in certain states, leading to potential performance improvements.
Concurrency Control: Thread states are closely related to synchronization and concurrency control mechanisms in Java, such as locks, monitors, and atomic operations. Understanding thread states helps developers reason about the correctness and effectiveness of synchronization techniques used to coordinate access to shared resources in concurrent programs.
In summary, knowing the different thread states in Java is crucial for building reliable, efficient, and scalable concurrent applications. Thread states provide insights into the behavior of concurrent programs and aid in debugging and performance optimization.

Implementation
Implementing the Thread States in Java
In Java, to get the current state of the thread, use the getState() method to get the current state of the thread. Java provides java.lang.Thread.State inner class that defines the ENUM constants for the state of a thread.

Example program

// Java program to demonstrate thread states
class thread implements Runnable {
    public void run()
    {
        // moving thread2 to timed waiting state
        try {
            Thread.sleep(1500);
        }
        catch (InterruptedException e) {
            e.printStackTrace();
        }
 
        System.out.println(
            "State of thread1 while it called join() method on thread2 -"
            + Test.thread1.getState());
        try {
            Thread.sleep(200);
        }
        catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
 
public class Test implements Runnable {
    public static Thread thread1;
    public static Test obj;
 
    public static void main(String[] args)
    {
        obj = new Test();
        thread1 = new Thread(obj);
 
        // thread1 created and is currently in the NEW
        // state.
        System.out.println(
            "State of thread1 after creating it - "
            + thread1.getState());
        thread1.start();
 
        // thread1 moved to Runnable state
        System.out.println(
            "State of thread1 after calling .start() method on it - "
            + thread1.getState());
    }
 
    public void run()
    {
        thread myThread = new thread();
        Thread thread2 = new Thread(myThread);
 
        // thread1 created and is currently in the NEW
        // state.
        System.out.println(
            "State of thread2 after creating it - "
            + thread2.getState());
        thread2.start();
 
        // thread2 moved to Runnable state
        System.out.println(
            "State of thread2 after calling .start() method on it - "
            + thread2.getState());
 
        // moving thread1 to timed waiting state
        try {
            // moving thread1 to timed waiting state
            Thread.sleep(200);
        }
        catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println(
            "State of thread2 after calling .sleep() method on it - "
            + thread2.getState());
 
        try {
            // waiting for thread2 to die
            thread2.join();
        }
        catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println(
            "State of thread2 when it has finished it's execution - "
            + thread2.getState());
    }
}
Output

State of thread1 after creating it - NEW
State of thread1 after calling .start() method on it - RUNNABLE
State of thread2 after creating it - NEW
State of thread2 after calling .start() method on it - RUNNABLE
State of thread2 after calling .sleep() method on it - TIMED_WAITING
State of thread1 while it called join() method on thread2 -WAITING
State of thread2 when it has finished it's execution - TERMINATED
When a new thread is created, the thread is in the NEW state. When the start() method is called on a thread, the thread scheduler moves it to Runnable state. Whenever the join() method is called on a thread instance, the current thread executing that statement will wait for this thread to move to the Terminated state. So, before the final statement is printed on the console, the program calls join() on thread2 making the thread1 wait while thread2 completes its execution and is moved to the Terminated state. thread1 goes to Waiting state because it is waiting for thread2 to complete its execution as it has called join on thread2.

Summary

At any given time, a thread can be in one of these states:

New: newly created thread that has not started executing
Runnable: either running or ready for execution but waiting for its resource allocation
Blocked: waiting to acquire a monitor lock to enter or re-enter a synchronized block/method
Waiting: waiting for some other thread to perform an action without any time limit
Timed_Waiting: waiting for some other thread to perform a specific action for a specified time period
Terminated: has completed its execution


## multithreading
Learning Objectives

After completing this module, associates should be able to:

Describe multithreading
Successfully implement a Java program demonstrating multithreading
Description
Concurrency
Concurrency refers to breaking up a task or piece of computation into different parts that can be executed independently, out of order, or in partial order without affecting the final outcome. One way - but not the only way - of achieving concurrency is by using multiple threads in the same program.

Operating systems use concurrency to manage the many different programs that run on them. The GUI - graphical user interface - for example, is run at the same time as other processes. Without this, any process that took too long in the background, like reading / writing to files or making an HTTP request, would block the GUI and prevent any other user input.

Multi-core Processing
Most computers these days have multiple cores or CPUs, which means that calculations at the hardware level can be done in parallel. Without multiple cores, operating systems can still achieve concurrency with a process called time splicing - this means running one process for a short time, then switching to another, and back very rapidly. This ensures that no process or application is completely blocked.

On multi-core systems, different processes can be run on different CPUs entirely. This enables true parallelization and is a key benefit of writing multithreaded programs.

Introduction to Threads
A thread is a subset of a process that is also an independent sequence of execution, but threads of the main process run in the same memory space, managed independently by a scheduler. So, we can think of a thread as a "path of execution", but they can access the same objects in memory.

Every thread that is created in a program is given its own call stack, where it stores local variables references. However, all threads share the same heap, where the objects live in memory. Thus, two threads could have separate variable references on two different stacks that still point to the same object in the heap.

Multithreading
Multithreading extends the idea of multitasking into applications where you can subdivide operations in a single application into individual, parallel threads. Each thread can have its own task that it performs. The OS divides processing time not just with applications, but between threads. Multi-core processors can actually run multiple different processes and threads concurrently, enabling true parallelization.

In Java, multithreading is achieved via the Thread class and/or the Runnable interface.

A Note on Best Practices
In general, it is best to avoid implementing multithreading, if possible. The benefit of multithreaded applications is better performance due to non-blocking execution. However, you should always measure or attempt to estimate the performance benefit you will get by using threads versus the tradeoff in complexity and subtle bugs that might be generated. Usually there are frameworks, tools, or libraries that have implemented the problem you are trying to solve, and you can leverage those instead of trying to build your own solution. For example, web servers like Apache Tomcat have multithreading built-in and provide APIs for dealing with network requests without having to worry about threads.

Real World Application

Understanding multithreading is crucial for developing modern software applications that need to efficiently utilize available hardware resources, improve performance, and enhance user experience. Here's why understanding multithreading is important:

Concurrency: Multithreading allows programs to perform multiple tasks concurrently, making better use of available CPU cores and reducing idle time. This concurrency is essential for applications that need to handle multiple tasks simultaneously, such as web servers, database servers, and multimedia applications.
Responsiveness: Multithreading enables applications to remain responsive to user input while performing background tasks. For example, in GUI applications, background threads can handle time-consuming operations such as file I/O or network communication without blocking the user interface, ensuring a smooth and interactive user experience.
Parallelism: Multithreading enables parallel execution of tasks on multicore processors, leading to improved performance and throughput. By dividing tasks into smaller units and executing them concurrently on different cores, multithreading allows applications to leverage the full processing power of modern hardware architectures.
Asynchronous Programming: Multithreading supports asynchronous programming models, where tasks can execute independently and asynchronously without blocking the main thread of execution. Asynchronous programming is crucial for building responsive and non-blocking applications, such as web servers, real-time systems, and event-driven applications.
Fault Tolerance: Multithreading can enhance fault tolerance and robustness in distributed systems by isolating failure-prone components or tasks in separate threads. By decoupling tasks and handling failures gracefully, multithreaded applications can continue to operate even in the presence of faults or errors.
In summary, understanding multithreading is essential for building high-performance, scalable, and responsive software applications that can take full advantage of modern hardware architectures and meet the increasing demands of users and workloads.

Implementation

Multithreading is a Java feature that allows concurrent execution of two or more parts of a program for maximum utilization of CPU. Each part of such program is called a thread. So, threads are light-weight processes within a process.

Threads can be created by using two mechanisms :

Extending the Thread class
Implementing the Runnable Interface
Thread creation by extending the Thread class
We create a class that extends the java.lang.Thread class. This class overrides the run() method available in the Thread class. A thread begins its life inside run() method. We create an object of our new class and call start() method to start the execution of a thread. start() invokes the run() method on the Thread object.


// Java code for thread creation by extending
// the Thread class
class MultithreadingDemo extends Thread {
    public void run()
    {
        try {
            // Displaying the thread that is running
            System.out.println(
                "Thread " + Thread.currentThread().getId()
                + " is running");
        }
        catch (Exception e) {
            // Throwing an exception
            System.out.println("Exception is caught");
        }
    }
}
 
// Main Class
public class Multithread {
    public static void main(String[] args)
    {
        int n = 8; // Number of threads
        for (int i = 0; i < n; i++) {
            MultithreadingDemo object
                = new MultithreadingDemo();
            object.start();
        }
    }
}
Output

Thread 15 is running
Thread 14 is running
Thread 16 is running
Thread 12 is running
Thread 11 is running
Thread 13 is running
Thread 18 is running
Thread 17 is running
Thread creation by implementing the Runnable Interface
We create a new class which implements java.lang.Runnable interface and override run() method. Then we instantiate a Thread object and call start() method on this object.


// Java code for thread creation by implementing
// the Runnable Interface
class MultithreadingDemo implements Runnable {
    public void run()
    {
        try {
            // Displaying the thread that is running
            System.out.println(
                "Thread " + Thread.currentThread().getId()
                + " is running");
        }
        catch (Exception e) {
            // Throwing an exception
            System.out.println("Exception is caught");
        }
    }
}
 
// Main Class
class Multithread {
    public static void main(String[] args)
    {
        int n = 8; // Number of threads
        for (int i = 0; i < n; i++) {
            Thread object
                = new Thread(new MultithreadingDemo());
            object.start();
        }
    }
}
Output

Thread 13 is running
Thread 11 is running
Thread 12 is running
Thread 15 is running
Thread 14 is running
Thread 18 is running
Thread 17 is running
Thread 16 is running
Thread Class vs Runnable Interface
If we extend the Thread class, our class cannot extend any other class because Java doesn’t support multiple inheritance. But, if we implement the Runnable interface, our class can still extend other base classes.
We can achieve basic functionality of a thread by extending the Thread class because it provides some built-in methods like yield(), interrupt() etc. that are not available in Runnable interface.
Using Runnable will give you an object that can be shared amongst multiple threads.
Summary

A thread is a subset of a process that is also an independent sequence of execution, but threads of the main process run in the same memory space, managed independently by a scheduler. So, we can think of a thread as a "path of execution", but they can access the same objects in memory.
Multithreading extends the idea of multitasking into applications where you can subdivide operations in a single application into individual, parallel threads.
In general, it is best to avoid implementing multithreading yourself if possible.


## synchronization
Learning Objectives

After completing this module, associates should be able to:

Describe synchronization
Successfully implement a Java program demonstrating synchronization
Description
Synchronization
Synchronization in Java is a crucial concept for ensuring proper access and modification of shared resources in a multi-threaded environment. It plays a vital role in preventing race conditions, deadlocks, and other concurrency-related issues that can lead to data corruption, unpredictable behavior, and system crashes.

Synchronized keyword
In a multithreaded environment, a race condition occurs when 2 or more threads attempt to access the same resource. Using the synchronized keyword on a piece of logic enforces that only one thread can access the resource at any given time. This keyword is used to mark a block of code or a method as a critical section, ensuring that only one thread can execute that code at a time. This is achieved by automatically acquiring an intrinsic lock (also known as a monitor lock) associated with the object or class on which the synchronized block or method is defined. When a thread encounters a synchronized block or method, it attempts to acquire the lock associated with the object or class. If the lock is available, the thread acquires it and proceeds to execute the synchronized code. If the lock is already held by another thread, the current thread waits for the lock to be released.

synchronized blocks or methods can be created using the keyword synchronized. Examples:

// synchronized block
synchronized(objectidentifier) {
   // the specified object is now locked and in use
}

// synchronized method
public synchronized void myMethod() {
   // the object using this method is now locked and in use
}
Real World Application

Here are some key reasons why it's important to know how to use synchronization in Java:

Thread Safety: Synchronization ensures that only one thread can access a shared resource at a given time, preventing multiple threads from modifying the same data concurrently. This guarantees thread safety and maintains data integrity.
Avoiding Race Conditions: Race conditions occur when two or more threads access a shared resource concurrently, and the final result depends on the relative timing of their execution. Synchronization helps prevent race conditions by controlling the order in which threads execute critical sections of code.
Concurrent Data Structures: Java provides various concurrent data structures, such as ConcurrentHashMap, CopyOnWriteArrayList, and BlockingQueue, which are designed to be thread-safe. Understanding synchronization helps developers effectively use and reason about these concurrent data structures.
In summary, understanding synchronization in Java is crucial for developing reliable, thread-safe, and efficient multi-threaded applications. It helps prevent data corruption, ensures correct behavior in concurrent environments, and enables developers to write scalable and maintainable code.

Implementation

Below is an example of using synchronization. In the example, we have a class that creates two threads. Both threads have print statements before and after a loop. One thread's loop increments a value by 1 and the other thread's loop increments the same value by 2.

public class Main {

    int value = 0;

    public static void main(String[] args) {
        // create object to work with
        Main myObj = new Main();

        // create threads
        Thread thread1 = new Thread(() -> {
            try {
                myObj.updateValueFiveTimesByAddingOne();
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        });

        Thread thread2 = new Thread(() -> {
            try {
                myObj.updateValueFiveTimesByAddingTwo();
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        });

        // start threads
        thread1.start();
        thread2.start();
    }
}
Methods called by threads without using synchronization:

public void updateValueFiveTimesByAddingOne() throws InterruptedException {
    System.out.println("Print statement before addingOne loop");

    // iterate 5 times, update value by 1 every time
    for (int i = 1; i <= 5; i++) {
      System.out.println(++value);
      Thread.sleep(500);
    }

    System.out.println("Print statement after addingOne loop");
}

public void updateValueFiveTimesByAddingTwo() throws InterruptedException {

    System.out.println("Print statement before addingTwo loop");

    // iterate 5 times, update value by 2 every time
    for (int i = 1; i <= 5; i++) {
      value += 2;
      System.out.println(value);
      Thread.sleep(500);
    }

    System.out.println("Print statement after addingTwo loop");
}
Output:

Print statement before addingOne loop
1
Print statement before addingTwo loop
3
4
6
7
9
10
12
13
15
Print statement after addingOne loop
Print statement after addingTwo loop
Notice how the output is jumbled as both threads take turns executing. The value is sometimes incremented by 1, then 2, and back and forth. Thread.sleep() is used to slow down execution.

Example with synchronized blocks:

public void updateValueFiveTimesByAddingOne() throws InterruptedException {
    System.out.println("Print statement before addingOne loop");

    synchronized (this) {
      // iterate 5 times, update value by 1 every time
      for (int i = 1; i <= 5; i++) {
        System.out.println(++value);
        Thread.sleep(500);
      }

    }

    System.out.println("Print statement after addingOne loop");
  }

  public void updateValueFiveTimesByAddingTwo() throws InterruptedException {
    System.out.println("Print statement before addingTwo loop");

    synchronized (this) {
      // iterate 5 times, update value by 2 every time
      for (int i = 1; i <= 5; i++) {
        value += 2;
        System.out.println(value);
        Thread.sleep(500);
      }
    }

    System.out.println("Print statement after addingTwo loop");
  }
Output:

Print statement before addingOne loop
Print statement before addingTwo loop
1
2
3
4
5
Print statement after addingOne loop
7
9
11
13
15
Print statement after addingTwo loop
We see that once a synchronization block is entered, the statements within it execute sequentially. We do see the print statements are not guaranteed to run in order since they are not within the synchronization blocks.

Example with using synchronized methods:

public synchronized void updateValueFiveTimesByAddingOne() throws InterruptedException {
    System.out.println("Print statement before addingOne loop");

    // iterate 5 times, update value by 1 every time
    for (int i = 1; i <= 5; i++) {
      System.out.println(++value);
      Thread.sleep(500);
    }

    System.out.println("Print statement after addingOne loop");
  }

  public synchronized void updateValueFiveTimesByAddingTwo() throws InterruptedException {

    System.out.println("Print statement before addingTwo loop");

    // iterate 5 times, update value by 2 every time
    for (int i = 1; i <= 5; i++) {
      value += 2;
      System.out.println(value);
      Thread.sleep(500);
    }

    System.out.println("Print statement after addingTwo loop");
  }
Output:

Print statement before addingOne loop
1
2
3
4
5
Print statement after addingOne loop
Print statement before addingTwo loop
7
9
11
13
15
Print statement after addingTwo loop
Notice that the entire body of the each method is synchronized and runs from beginning to end without program execution switching to another thread.

Summary

Synchronization is the capability to control the access of multiple threads to any shared resource.
In a multithreaded environment, a race condition occurs when 2 or more threads attempt to access the same resource.
Using the synchronized keyword on a piece of logic enforces that only one thread can access the resource at any given time.
synchronized blocks or methods can be created using the keyword.


## deadlock
Learning Objectives

After completing this module, associates should be able to:

Describe "deadlock" using both real-world examples and Java code.
Successfully implement a Java program demonstrating how to prevent deadlock.
Description
What is a Deadlock?
A deadlock occurs when two or more threads are blocked forever, waiting for each other to release resources Each thread holds a resource and waits for another resource held by another thread

Four Conditions for Deadlock:
Mutual Exclusion: Resources cannot be shared simultaneously
Hold and Wait: Threads hold resources while waiting for others
No Preemption: Resources cannot be forcibly taken from threads
Circular Wait: A circular chain of threads waiting for resources
Deadlock Detection:
Use Java's ThreadMXBean to detect deadlocks programmatically The findDeadlockedThreads() method returns thread IDs involved in deadlock Thread dumps can also help identify deadlocks

Prevention Strategies:
a) Avoid Nested Locks:

Follow a consistent order when acquiring multiple locks Never acquire locks in different orders in different threads

b) Lock Timeout:

Use tryLock() with timeout instead of synchronized Release locks if unable to acquire all necessary resources

c) Resource Ordering:

Assign a numerical order to resources Always acquire resources in ascending order

d) Lock Hierarchy:

Define a hierarchy for lock acquisition Never acquire a lock at a lower level after acquiring a higher-level lock

Real World Application

A simple real-time example of deadlock is that suppose there are two friends John and Jerry that are drawing a diagram. During drawing, John needs an eraser, so he will use (lock) the eraser.

Meanwhile, Jerry needs ruler, so he will use (lock) the ruler. Currently, eraser and ruler are not occupied by any of the friends. Now John needs a ruler to continue his drawing. But Jerry will not give it because he is using it currently.

Later on, Jerry needs an eraser but John will not give it as his drawing is not completed because of waiting for the ruler. Thus, none of the friends will release stationary objects and both will wait infinitely for each other to release stationary. This situation is called deadlock in Java.

Similarly, assume that there are two threads t1 and t2. Both threads are running concurrently (simultaneously). During execution, thread t1 is waiting for data that is locked by thread t2 and t2 is waiting for data that is locked by thread t1.

In this case, none of the threads will unlock the lock because of not completing their execution process. This situation is called deadlock.

When thread deadlock occurs in a program, the further execution of program will stop. Therefore, thread deadlock is a drawback in a program. We should take care to avoid deadlock while coding.

Implementation

Now we will see the example code of deadlock in Java. This example is from the Concurrency Java tutorial: https://docs.oracle.com/javase/tutorial/essential/concurrency/deadlock.html

public class Deadlock {

    public static void main(String[] args) throws InterruptedException {
        // two friend objects
        Friend alphonse = new Friend("Alphonse");
        Friend gaston = new Friend("Gaston");

        // two threads each using an object
        Thread thread1 = new Thread(() -> {
            alphonse.bow(gaston);
        });

        Thread thread2 = new Thread(() -> {
            gaston.bow(alphonse);
        });

        // start threads
        thread1.start();
        thread2.start();

        // debugging
        // use main thread to check state of other threads periodically
        for (int i = 0; i <= 5; i++) {
            Thread.sleep(1000);
            System.out.println(thread1.getName() + "- State:" + thread1.getState() + " & IsAlive:" + thread1.isAlive());
            System.out.println(thread2.getName() + "- State:" + thread2.getState() + " & IsAlive:" + thread2.isAlive());
        }

    }
}

class Friend {
    private String name;

    public Friend(String name) {
        this.name = name;
    }

    public String getName() {
        return this.name;
    }

    public synchronized void bow(Friend bower) {
        System.out.format("%s: %s"
                + "  has bowed to me!%n",
                this.name, bower.getName());

        // debugging statement
        System.out.println(Thread.currentThread().getName() + " holds lock on " + this.name + "? "
                + Thread.holdsLock(this) + "\n" + Thread.currentThread().getName() + " holds lock on " + bower.name
                + "? " + Thread.holdsLock(bower));

        bower.bowBack(bower); // deadlock happens here where this method tries to use another object
    }

    public synchronized void bowBack(Friend bower) {
        System.out.format("%s: %s"
                + " has bowed back to me!%n",
                this.name, bower.getName());
    }
}
Output:

Alphonse: Gaston  has bowed to me!
Gaston: Alphonse  has bowed to me!
Thread-0 holds lock on Alphonse? true
Thread-0 holds lock on Gaston? false
Thread-1 holds lock on Gaston? true
Thread-1 holds lock on Alphonse? false
Thread-0- State:BLOCKED & IsAlive:true
Thread-1- State:BLOCKED & IsAlive:true
Thread-0- State:BLOCKED & IsAlive:true
Thread-1- State:BLOCKED & IsAlive:true
Thread-0- State:BLOCKED & IsAlive:true
Thread-1- State:BLOCKED & IsAlive:true
... 
From what we can see, both threads each are using an object. Once the object's bowBack() method is called in each thread, the threads attempt to access the other object that is currently unavailable.

Summary

Deadlock is a condition when two or more threads try to access the same resources at the same time. Then these threads can never access the resource and eventually go into the blocked state forever.
Though it is not possible to completely get rid of the deadlock problem, we can take precautions to avoid such deadlock conditions. These preventive measures are as follows:
By avoiding nested locks
By avoiding unnecessary locks


## livelock
Learning Objectives

After completing this module, associates should be able to:

Describe "livelock" using both real-world examples and Java code.
Successfully implement a Java program demonstrating how to prevent livelock.
Description

Livelock is another concurrency problem and is similar to deadlock. In livelock, two or more threads keep on transferring states between one another instead of being blocked infinitely as we saw in the deadlock example. Consequently, the threads are not able to perform their respective tasks.

A great example of livelock is a messaging system where, when an exception occurs, the message consumer rolls back the transaction and puts the message back to the head of the queue. Then the same message is repeatedly read from the queue, only to cause another exception and be put back on the queue. The consumer will never pick up any other message from the queue.

Real World Application

Example 1:

A simple example of livelock would be two people who meet face-to-face in a corridor, and both of them move aside to let the other pass. They end up moving from side to side without making any progress as they move the same way at the time. Here, they never cross each other.

Example 2:

Another example of livelock is when two cars stop at a 4-way stop at the exact same time. Both drivers wave at each other to go first, so neither one goes.

Implementation

The below program demonstrates livelock.

Criminal class:

public class Criminal {
  private boolean hostageReleased = false;

  public synchronized void releaseHostage(Police police) {
    while (!police.isRansomSent()) {
      System.out.println("Criminal: waiting for police to give ransom.");

      try {
        Thread.sleep(1000);
      } catch (InterruptedException ex) {
        ex.printStackTrace();
      }
    }

    System.out.println("Criminal: Releasing Hostage");
    this.hostageReleased = true;
  }

  public boolean isHostageReleased() {
    return this.hostageReleased;
  }
}
Police class:

public class Police {
  private boolean ransomSent = false;

  public synchronized void giveRansom(Criminal criminal) {
    while (!criminal.isHostageReleased()) {
      System.out.println("Police: waiting for criminal to release hostage.");

      try {
        Thread.sleep(1000);
      } catch (InterruptedException ex) {
        ex.printStackTrace();
      }
    }

    System.out.println("Police: sent ransom");
    this.ransomSent = true;
  }

  public boolean isRansomSent() {
    return this.ransomSent;
  }
}
the Main class:


public class Main {
  static final Police police = new Police();
  static final Criminal criminal = new Criminal();

  private static Thread t1;
  private static Thread t2;

  // Notice that this code will never stop...
  public static void main(String[] args) throws Exception {
    t1 = new Thread(() -> {
      t1.setName("Thread 1");
      police.giveRansom(criminal);
    });
    t1.start();

    t2 = new Thread(() -> {
      t2.setName("Thread 2");
      criminal.releaseHostage(police);
    });
    t2.start();

    // debugging: print state and name of threads over time
    for (int i = 0; i <= 10; i++) {
      System.out.print("-----------------\n" +
          "Name: " + t1.getName() + "\n" +
          "State: " + t1.getState() + "\n" +
          "-----------------\n");

      System.out.print("-----------------\n" +
          "Name: " + t2.getName() + "\n" +
          "State: " + t2.getState() + "\n" +
          "-----------------\n");
      Thread.sleep(1000);
    }
  }
}

Now, let's run this example:

Police: waiting for criminal to release hostage.
Criminal: waiting for police to give ransom.
-----------------
Name: Thread 1
State: TIMED_WAITING
-----------------
-----------------
Name: Thread 2
State: TIMED_WAITING
-----------------
Police: waiting for criminal to release hostage.
Criminal: waiting for police to give ransom.
-----------------
Name: Thread 1
State: TIMED_WAITING
-----------------
-----------------
Name: Thread 2
State: TIMED_WAITING
-----------------
Police: waiting for criminal to release hostage.
Criminal: waiting for police to give ransom.
-----------------
Name: Thread 1
State: TIMED_WAITING
-----------------
-----------------
Name: Thread 2
State: TIMED_WAITING
-----------------
Police: waiting for criminal to release hostage.
Criminal: waiting for police to give ransom.
...
As we can see in the logs, the program doesn't end and both threads are not blocked, but cannot progress any further as they wait on one another.

Summary

Livelock is another concurrency problem and is similar to deadlock.
In livelock, two or more threads keep on transferring states between one another instead of waiting infinitely as we saw in the deadlock example.
Consequently, the threads are not able to perform their respective tasks.


## producer-consumer-problem
Learning Objectives

After completing this module, associates should be able to:

Describe the producer-consumer problem using both real-world examples and Java code.
Successfully implement a Java program demonstrating how to prevent the producer-consumer problem.
Description
Producer-Consumer Problem
The Producer-Consumer problem is a classic example of a multi-process synchronization problem. Here, we have a fixed-size buffer and two classes of threads - producers and consumers. Producers produces the data to the queue and Consumers consume the data from the queue. Both producer and consumer shares the same fixed-size buffer as a queue.

Problem - The producer should produce data only when the queue is not full. If the queue is full, then the producer shouldn't be allowed to put any data into the queue. The consumer should consume data only when the queue is not empty. If the queue is empty, then the consumer shouldn't be allowed to take any data from the queue.

Solution - We can solve the Producer-Consumer problem by using wait() & notify()methods to communicate between producer and consumer threads. The wait() method to pause the producer or consumer thread depending on the queue size. The notify() method sends a notification to the waiting thread.

The Producer thread will keep on producing data for Consumer to consume. It will use wait() method when Queue is full and use notify() method to send notification to Consumer thread once data is added to the queue.

The Consumer thread will consume the data form the queue. It will also use wait() method to wait if queue is empty. It will also use notify() method to send notification to producer thread after consuming data from the queue.

Real World Application

A very typical application is message passing between applications which happens all the time in today's multi-processing operating systems.

Consider the printing of documents. You can print from several applications, i.e. multiple producers can create printing "messages" that are enqueued by a printer spooler and later consumed by a printer. Many kinds of shared access to resources are producer consumer problems.

In real-time critical applications with multi-tasking there also may be also some kind of events that are passed from one task to another which requires synchonization mechanisms to avoid race conditions, especially in event-triggered systems. For example, an embedded real-time system may receive a message from the producer, which one or multiple tasks (consumer) may be waiting for.

Implementation

Below is an example of a program that avoids the producer-consumer problem by using synchronization and the wait() and notify() thread methods. This example is of a cookie store that has a Baker (producer) and Cashier (consumer).

Cookie class:

public class Cookie {

}
Main class:

import java.util.ArrayDeque;

public class Main {
  public static ArrayDeque<Cookie> cookies = new ArrayDeque<>();
  public static final int MAX_COOKIES = 5;

  public static void main(String[] args) {
    // create producer and consumer
    Baker baker = new Baker();
    Cashier cashier = new Cashier();

    // creates threads that produce and consume
    Thread thread1 = new Thread(() -> {
      try {
        for(int i = 0; i < 5; i++) {
           baker.produce(new Cookie());
        }
        
      } catch (InterruptedException e) {
        System.out.println("Producer interrupted!");
      }
    });

    Thread thread2 = new Thread(() -> {
      try {
        for(int i = 0; i < 5; i++) {
          cashier.consume();
        }
        
      } catch (InterruptedException e) {
        System.out.println("Consumer interrupted!");
      }
    });

    // start threads
    thread1.start();
    thread2.start();
  }
}
The Main class has a queue that contains cookies, and there is a maximum of 5 cookies that it can contain. When the program starts, it creates a Baker and Cashier. We then have two threads, one of which asks the Baker to make cookies 5 times, and the other asks the Cashier to take 5 cookies to sell.

Baker class:

import java.util.ArrayDeque;

public class Baker {

    ArrayDeque<Cookie> cookies = Main.cookies;

    public void produce(Cookie cookie) throws InterruptedException {

        synchronized(cookies) {

            // check if baker can produce
            while (cookies.size() == Main.MAX_COOKIES) {
                // if not, wait
                System.out.println("Baker waits!");
                cookies.wait();
            }


            // if there is space on shelf, produce
            cookies.add(cookie);
            System.out.println("Baker baked a cookie!\n" + 
            "Cookies currently ready for sale: " + cookies.size());
            cookies.notify();

        }
    }
}
The Baker class will attempt to bake cookies. When its produce() method runs, it acquires a lock on the Main class's cookies object. It then checks if it can produce by checking the size of the cookies queue. If it can, it produces a cookie. If it can't, it uses the wait() method to release its lock and will only run if it has been notified by another thread that the cookies object is ready to be used again.

Cashier class:

import java.util.ArrayDeque;

public class Cashier {
    ArrayDeque<Cookie> cookies = Main.cookies;

    public void consume() throws InterruptedException {

        synchronized(cookies) {

            // check if baker can produce
            while (cookies.isEmpty()) {
                // if not, wait
                System.out.println("Cashier waits!");
                cookies.wait();
            }


            // if there is space on shelf, produce
            cookies.remove();
            System.out.println("Cashier sold a cookie!\n" + 
            "Cookies currently ready for sale: " + cookies.size());
            cookies.notify();

        }
    }
}
The Cashier class will attempt to consume cookies. When its consume() method runs, it acquires a lock on the Main class's cookies object. It then checks if it can take a cookie by checking the size of the cookies queue. If it can, it consumes a cookie. If it can't, it uses the wait() method to release its lock and will only run if it has been notified by another thread that the cookies object is ready to be used again.

Output:

Cashier waits!
Baker baked a cookie!
Cookies currently ready for sale: 1
Baker baked a cookie!
Cookies currently ready for sale: 2
Cashier sold a cookie!
Cookies currently ready for sale: 1
Cashier sold a cookie!
Cookies currently ready for sale: 0
Cashier waits!
Baker baked a cookie!
Cookies currently ready for sale: 1
Cashier sold a cookie!
Cookies currently ready for sale: 0
Cashier waits!
Baker baked a cookie!
Cookies currently ready for sale: 1
Cashier sold a cookie!
Cookies currently ready for sale: 0
Cashier waits!
Baker baked a cookie!
Cookies currently ready for sale: 1
Cashier sold a cookie!
Cookies currently ready for sale: 0
Summary

The Producer-Consumer problem is a classic example of a multi-process synchronization problem.
Here, we have a fixed-size buffer and two classes of threads - producers and consumers.
Producers produces the data to the queue
Consumers consume the data from the queue.
Both producer and consumer shares the same fixed-size buffer as a queue.
The producer should produce data only when the queue is not full.
If the queue is full, then the producer shouldn't be allowed to put any data into the queue.
The consumer should consume data only when the queue is not empty.
If the queue is empty, then the consumer shouldn't be allowed to take any data from the queue.
We can solve the Producer-Consumer problem by using wait() & notify()methods to communicate between producer and consumer threads
The wait() method to pause the producer or consumer thread depending on the queue size.
The notify() method sends a notification to the waiting thread.
Producer thread will keep on producing data for Consumer to consume.
It will use wait() method when Queue is full and use notify() method to send notification to Consumer thread once data is added to the queue.




# Spring/Spring Boot Basics
## Intro To Spring
Learning Objectives

After completing this module, associates should be able to:

Define the Spring Core Framework
Description
Introduction to Spring Framework
The Spring Framework is a Java platform that provides infrastructure support to develop Java applications.

As a developer, the process of handling business logic is important when compared to managing the infrastructure. With Spring a developer can focus on the application and Spring manages the infrastructure.

Spring helps to build applications using "plain old java objects" (POJOs) and to apply enterprise services non-invasively to POJOs.

Advantages of Spring Framework
Use of POJO: Spring framework helps developers to develop enterprise applications using POJO. An enterprise container like an application server is not required while using POJOs.
Flexibility for configuring Spring: Spring provides both XML configuration and Java-based annotations.
No need for Server: Spring framework provides a lightweight container and it can be activated without any web server or application server.
No need for reinvention: Spring uses technologies such as JDK timers, ORM frameworks, Java EE, etc. So developers need not have to learn all those technologies or frameworks to develop applications.
Modularity: Spring framework provides modularity. The developers can decide which packages can be used or ignored based on the requirements.
Ease of Testability: Spring Dependency Injection simplifies the injection of test data by using JavaBean POJO.
Inversion Control and APIs: Spring framework provides inversion control and APIs to translate exceptions thrown by JDBC and Hibernate into unchecked and consistent.
Modules of Spring Framework
Features of the spring framework are organized into 20 modules.

The Spring modules are grouped into Core Container, Data Access/Integration, Web, AOP (Aspect Oriented Programming), Instrumentation, and Test, as shown in the following diagram.

Modules of Spring Framework

Spring Core Module
Spring Core follows the principle of Inversion of Control, where the control of object creation and lifecycle management is shifted from the application code to the Spring container. The Spring Core module provides the Inversion of Control (IoC) container. This container manages the creation and configuration of objects (beans) in a Spring application. The Spring Container can be implemented in two different ways namely, BeanFactory and ApplicationContext.

This modules also provides functionality for managing the lifecycle of a bean and for configuring your application, either with XML configuration or through annotations.

Real World Application

Knowing Spring Core is crucial for effectively using the Spring Framework to build enterprise Java applications. Here are some reasons why understanding Spring Core is important:

Dependency Injection (DI): Spring Core implements a powerful dependency injection container that manages object creation and wiring of components. Understanding DI enables developers to write loosely coupled and easily maintainable code by decoupling dependencies and promoting modularization.
Inversion of Control (IoC): Spring Core follows the principle of Inversion of Control, where the control of object creation and lifecycle management is shifted from the application code to the Spring container. This leads to more flexible and modular application design, making it easier to manage and extend.
Bean Lifecycle Management: Spring Core provides mechanisms for managing the lifecycle of beans, including instantiation, initialization, and destruction. Knowing how to configure bean lifecycle callbacks allows developers to perform custom initialization and cleanup tasks when beans are created or destroyed.
Annotations and XML Configuration: Spring Core allows developers to configure beans and application contexts using annotations or XML-based configuration. Knowing how to use annotations and XML configuration provides flexibility and choice in configuring Spring applications according to project requirements and preferences.
Implementation

The Spring Core, beans and context dependencies can be added to a Maven project:

<!-- https://mvnrepository.com/artifact/org.springframework/spring-core -->
<dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-core</artifactId>
    <version>5.3.23</version>
</dependency>
<dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-beans</artifactId>
    <version>5.3.23</version>
</dependency>
<dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-context</artifactId>
    <version>5.3.23</version>
</dependency>
<dependency>
An XML file can be created for Spring configuration.

Spring Configuration is done by adding the following code to the XML file. It can be obtained from Spring documentation.

<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://www.springframework.org/schema/beans
        https://www.springframework.org/schema/beans/spring-beans.xsd">

    <!-- add your beans here -->
</beans>
Summary

Spring is a Java framework used to develop Java applications.
Spring as multiple modules like Core Container, Data Access/Integration, Web, AOP (Aspect Oriented Programming), Instrumentation, and Test.
The Spring Core module provides the Inversion of Control (IoC) container.
Spring IoC container is used for handling objects and adding dependencies.
The Spring Core module also provides functionality for managing the lifecycle of a bean and for configuring your application, either with XML configuration or through annotations.

## Spring Ioc Container
Learning Objectives

After completing this module, associates should be able to:

Define the Spring IOC container.
Define a Spring bean.
Description
Dependency Injection
Injecting objects into other objects is called Dependency Injection.

IOC
In a Spring Application, the process of adding dependencies and calling the objects of classes is done in the Spring container, which was traditionally done by the programmer explicitly.

The transfer of control of objects or portions of a program to a container or a framework is called Inversion of Control.

Advantages of IoC
Decoupling the task execution from the implementation.
Easy switching between different Implementation
Greater modularity of a program
Easy testing of a program by isolating the component or mocking its dependencies.
The IoC container
IoC container is a core container that uses Dependency Injection or IoC pattern to implicitly provide an object reference in a class during runtime.

org.springframework.beans.factory.BeanFactory is the actual Spring IoC container.

The BeanFactory interface is responsible to instantiate, configure objects and assemble the dependencies between these objects.

Spring IoC Container

Bean Factory vs Application Context
The org.springframework.beans and org.springframework.context packages are the basis for the Spring framework's IoC container.

The ApplicationContext is built on top of BeanFactory.

The BeanFactory provides the configuration framework and the basic functionality, while ApplicationContext adds additional functionality like easy integration with Spring AOP features, message resource handling, event propagation, and contexts specific to the application-layer like WebApplicationContext for use in web applications.

Bean
The object that is instantiated, assembled and managed by the IoC container is called a bean. Beans form the backbone of any Spring application.

Initializing a container
The Spring IoC container can be initialized either by the BeanFactory or ApplicationContext.

BeanFactory:

Resource resource = new FileSystemResource("beans.xml");
BeanFactory factory = new XmlBeanFactory(resource);
ApplicationContext:

ApplicationContext context = new ClassPathXmlApplicationContext(
new String[] {"applicationContext.xml", "applicationContext-part2.xml"});
Real World Application

Spring IoC container is used to initialise, assemble and manage beans.
Consider classes named car and bike which implement the vehicle interface, the objects for these classes are initialized, assembled and managed as spring beans.
These objects can be injected as dependencies for the other classes.
If the HP laptop is a class, the hard drive is an interface. The classes like SanDisk and Toshiba implement the Hard drive interface. SanDisk and Toshiba are dependencies for the HP laptop.
Implementation

Spring XML configuration

The following boilerplate code is added in the spring.xml file.
The following code can be acquired directly from the spring website.
<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context"
    xmlns:aop="http://www.springframework.org/schema/aop"
    xsi:schemaLocation="http://www.springframework.org/schema/beans
    http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
    http://www.springframework.org/schema/context
    http://www.springframework.org/schema/context/spring-context-3.0.xsd
    http://www.springframework.org/schema/aop
    http://www.springframework.org/schema/aop/spring-aop-3.0.xsd
    " >
    <!-- bean definitions here -->
</beans>
Dependency Injection and Bean definition

Consider an interface named Vehicle with an abstract method drive() and two classes named Car and Bike that implement the interface Vehicle.
The class App has the main method that created an ApplicationContext object and used dependency injection.
As both car and bike are vehicles any of them can be used as a dependency.
Spring.xml:

<?xml version="1.0" encoding="UTF-8"?>

<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context"
    xmlns:aop="http://www.springframework.org/schema/aop"
    xsi:schemaLocation="http://www.springframework.org/schema/beans
    http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
    http://www.springframework.org/schema/context
    http://www.springframework.org/schema/context/spring-context-3.0.xsd
    http://www.springframework.org/schema/aop
    http://www.springframework.org/schema/aop/spring-aop-3.0.xsd
    ">
    <bean id="vehicle" class = "Revature.Bike"></bean>
</beans>
App.java:

import org.springframework.context.ApplicationContext;
import org.springframework.context.support.ClassPathXmlApplicationContext;

public class App {
    public static void main(String[] args) {
        
        ApplicationContext context = new ClassPathXmlApplicationContext("spring.xml");
        Vehicle obj = (Vehicle)context.getBean("vehicle");
        obj.drive();
        
    }

}
Car.java:

package Revature;

public class Car implements Vehicle{

    public void drive() {
        System.out.println("I am riding a car");
        
    }
    public static void main(String[] args) {
        System.out.println("Hello World");
    }

}
Bike.java:

package Revature;

public class Bike implements Vehicle{

    public void drive() {
        System.out.println("I am riding a bike");
        
    }

}
Vehicle.java:

package Revature;

public interface Vehicle {
    void drive();

}
The output is:

I am riding a bike
if the bean definition is changed as follows:
   <bean id="vehicle" class = "Revature.Car"></bean>
The output is

I am riding a car
Summary

In a Spring Application, the process of adding dependencies and calling the objects of classes is done in the Spring container, which was traditionally done by the programmer explicitly.
IoC container is a core container that uses Dependency Injection or IoC pattern to implicitly provide an object reference in a class during runtime.
The BeanFactory provides the configuration framework and the basic functionality
The ApplicationContext adds additional functionality like easy integration with Spring AOP features, message resource handling, event propagation, and contexts specific to the application-layer like WebApplicationContext for use in web applications.
A Bean is an object that is instantiated, assembled and managed by the IoC container is called a bean. Beans form the backbone of any Spring application.


## Overview Of Inversion Of Control
Learning Objectives

After completing this module, associates should be able to:

Understand the concept of Inversion of Control (IoC) and its significance in software development.
Recognize the benefits of IoC in terms of code modularity, reusability, and testability.
Identify the key principles and characteristics of IoC, such as decoupling, dependency injection, and the use of containers.
Description

Inversion of Control (IoC) is a software design principle that promotes loose coupling and modularity by shifting the responsibility of managing dependencies from the application itself to an external container or framework. In IoC, the control flow of a program is inverted, with the framework or container taking charge of creating and managing objects, and injecting their dependencies.

Key points about Inversion of Control include:

Decoupling Dependencies: IoC aims to decouple components and reduce their direct dependencies, allowing for easier maintenance, testing, and scalability.
Dependency Injection: IoC relies on Dependency Injection (DI), where the dependencies of a component are provided externally, typically through constructor injection, setter injection, or interface injection.
Loose Coupling: By removing the responsibility of creating and managing dependencies, IoC enables loose coupling between components, promoting better code organization and reusability.
Inversion of Control Containers: IoC containers, such as Spring Framework in Java or .NET Core's built-in container, facilitate IoC by managing the lifecycle of objects, performing dependency injection, and handling other aspects of object creation and configuration.
Dependency Inversion Principle: IoC is closely related to the Dependency Inversion Principle (DIP), which suggests that high-level modules should not depend on low-level modules, but both should depend on abstractions.
Software Frameworks and Libraries: IoC is a commonly used principle in various software frameworks and libraries, providing a foundation for building flexible and modular applications.
Benefits: Inversion of Control offers several benefits, including improved code maintainability, testability, reusability, and the ability to easily swap or configure different implementations of components.
Application Design Flexibility: IoC provides flexibility in designing applications, as it allows developers to focus on implementing business logic without worrying about managing dependencies.
In summary, Inversion of Control is a powerful design principle that promotes loosely coupled and modular code by shifting the responsibility of managing dependencies to an external container or framework. It enhances the flexibility, maintainability, and testability of software applications.

Real World Application

Inversion of Control (IoC) is beneficial in a wide range of real-world applications, especially in complex software systems that require modularity, flexibility, and maintainability. Here are some examples of the types of applications where IoC is commonly used:

Enterprise Applications: IoC is widely employed in enterprise applications, such as Customer Relationship Management (CRM) systems, Enterprise Resource Planning (ERP) systems, and Human Resources (HR) management systems. These applications often consist of multiple modules with complex dependencies, and IoC helps manage these dependencies effectively.

Web Applications: Web applications, ranging from simple websites to large-scale web portals, benefit from IoC by facilitating the management of various components, such as controllers, services, and repositories. IoC frameworks like Spring provide built-in support for web development, making it easier to achieve loose coupling and modular design.

Microservices Architecture: In microservices-based architectures, where applications are divided into small, independent services, IoC is crucial. Each microservice can have its own IoC container, enabling the deployment and scaling of individual services independently while ensuring proper dependency management and loose coupling between services.

Test-Driven Development (TDD): IoC plays a vital role in test-driven development by allowing easy substitution of dependencies with mock or stub implementations during unit testing. By decoupling dependencies, developers can isolate and test individual components in isolation, promoting testability and ensuring the correctness of the application's behavior.

Plugin and Extension Systems: IoC is highly valuable in applications that support plugin or extension systems. By employing IoC, the application can dynamically load and manage plugins without explicit knowledge of their concrete implementations, enhancing extensibility and allowing third-party developers to contribute to the application ecosystem.

GUI Applications: Graphical User Interface (GUI) applications can benefit from IoC to manage the various components, views, and controllers. IoC frameworks provide mechanisms for handling the lifecycle and dependencies of these components, facilitating their integration and ensuring proper management of user interface interactions.

In summary, IoC is beneficial in a wide range of applications, including enterprise systems, web applications, microservices architectures, TDD practices, plugin and extension systems, and GUI applications. By embracing IoC, developers can achieve better modularity, flexibility, testability, and maintainability in their software projects.

Implementation

Below is an example of using inversion of control and dependency injection in Spring.

Identify Dependencies: Identify the dependencies within your application that need to be managed and injected, such as service dependencies or external libraries.

Define Interfaces: Create interfaces that represent the contracts for the dependencies. These interfaces will allow for loose coupling and abstraction.

// Example interface
public interface MyDependency {
    void doSomething();
}
Implement Dependencies: Implement the concrete classes that fulfill the interfaces. These classes will encapsulate the functionality of the dependencies.
// Example implementation of the dependency
public class MyDependencyImpl implements MyDependency {
    public void doSomething() {
        // Implementation logic
    }
}
Define Injection Points: Identify the injection points within your application where dependencies need to be injected. Annotate those points accordingly.
// Example injection point in a class using constructor injection
public class MyClass {
    private final MyDependency myDependency;

    public MyClass(MyDependency myDependency) {
        this.myDependency = myDependency;
    }

    // Use myDependency within class methods
}
Configure Dependency Injection: Configure the IoC container or framework to wire the dependencies into the injection points. This can be done either through annotations or XML configuration, depending on the chosen framework.
Java-based:

// Example configuration class
@Configuration
public class AppConfig {
    @Bean
    public MyDependency myDependency() {
        return new MyDependencyImpl();
    }
}
Using XML:

// Example Spring XML configuration for dependency injection
<beans>
    <bean id="myClass" class="com.example.MyClass">
        <constructor-arg ref="myDependency"/>
    </bean>

    <bean id="myDependency" class="com.example.MyDependencyImpl"/>
</beans>
Instantiate the Container: Create an instance of the IoC container or let the framework instantiate it. This will initialize and manage the dependencies based on the configuration.
public class App {

    public static void main(String[] args) {

        // Example instantiation of the Spring IoC container
        ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml");
        MyClass myClass = context.getBean("myClass", MyClass.class);
    }

}
Access Dependencies: Access the injected dependencies from within your application and utilize their functionality.
public class App {

    public static void main(String[] args) {

        ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml");
        MyClass myClass = context.getBean("myClass", MyClass.class);

        // Example usage of the injected dependency
        myClass.getMyDependency().doSomething();
    }

}
By following these steps, you can successfully implement Inversion of Control in your application, allowing for loose coupling, better modularity, and easier maintenance of dependencies.

Summary

Inversion of Control (IoC) is a software design principle that promotes loose coupling and modularity by shifting the responsibility of managing dependencies from the application itself to an external container or framework. IoC allows for the decoupling of components and improves code organization, testability, and maintainability.

Key points about IoC include:

Dependency Management: IoC separates the creation and management of dependencies from the application code, making it easier to manage and swap out components.
Inversion of Control Containers: IoC containers, such as Spring, manage the lifecycle and dependencies of objects, providing a convenient way to configure and wire components.
Dependency Injection: IoC relies on Dependency Injection (DI) to inject dependencies into objects, allowing for loose coupling and facilitating unit testing.
Flexible Configuration: IoC provides flexible configuration options, allowing developers to configure dependencies through annotations, XML, or Java-based configuration classes.
Modularity and Reusability: IoC promotes modularity and reusability by encapsulating dependencies and providing a clear separation of concerns.
Dynamic Extension: IoC facilitates dynamic extension through plugins or modules, enabling the application to be easily extended or customized.
By adopting Inversion of Control, developers can achieve a more modular, maintainable, and flexible architecture in their software applications.


## Overview Of Dependency Injection
Learning Objectives

After completing this module, associates should be able to:

Define what Dependency Injection (DI) is and explain its purpose in software development.
Identify the problems that DI solves in software development.
Recognize the role of DI in the context of the Spring Framework (or any other relevant framework).
Understand how DI contributes to the testability and maintainability of code.
Explain how DI supports the principles of modular design.
Description

Dependency Injection (DI) is a design pattern in programming that enhances the modularity and testability of software. It's a form of Inversion of Control (IoC) where the responsibility of creating dependencies shifts from the class to a framework or container.

DI addresses issues like tightly coupled code, code duplication, and challenges in unit testing. It encourages creation of loosely coupled and maintainable code, which leads to efficient and scalable software systems.

Within frameworks such as Spring, DI is a central principle, advocating for modular and testable code. It enables building systems where components can be independently replaced and tested, resulting in highly modular architecture.

In practice, dependencies are injected into classes via constructors (Constructor Injection), setter methods (Setter Injection), or directly into fields (Field Injection). Various DI containers and frameworks exist to facilitate this process and manage the lifecycle of objects.

In essence, DI is a key tool in modern software development, promoting principles of modularity, testability, and maintainability.

Real World Application

Dependency Injection (DI) has a wide range of applications in real-world software development. It aids in managing and controlling complex code bases, improving testability, and enhancing modularity. Here are some real-world examples of dependency injection:

Spring Framework: Spring Framework is a prime example of dependency injection in action. In Spring, dependencies are injected into beans either through constructor injection, setter injection, or field injection. For example, in a Spring-based web application, dependencies such as data access objects (DAOs), service components, and controllers are injected into each other using annotations or XML configuration.
Java EE (Enterprise Edition): Java EE also supports dependency injection through its Contexts and Dependency Injection (CDI) framework. CDI enables managed beans to be injected into each other using annotations such as @Inject. For instance, in a Java EE web application, managed beans representing components like servlets, EJBs, and CDI beans can have their dependencies injected at runtime.
Google Guice: Guice is a lightweight dependency injection framework for Java developed by Google. It enables developers to declare dependencies and their bindings using Java code. With Guice, dependencies are injected into objects by declaring them as constructor parameters or fields. Guice is commonly used in various Java projects, including web applications, desktop applications, and libraries.
AngularJS and Angular: Front-end frameworks like AngularJS (version 1.x) and Angular (version 2+) use dependency injection to manage components and services. In Angular, dependencies are injected into components and services using TypeScript annotations or Angular's built-in dependency injection system. This allows developers to write modular and testable front-end code.
JUnit and Mockito: Dependency injection is also prevalent in testing frameworks like JUnit and Mockito. In JUnit, dependencies such as mocks and stubs are injected into test classes to isolate the code under test. Mockito, a popular mocking framework for Java, uses dependency injection to inject mock objects into test classes, enabling developers to simulate behavior of dependencies during testing.
Android Development: In Android development, dependency injection frameworks like Dagger 2 are commonly used to manage dependencies between Android components such as activities, fragments, and services. Dagger 2 generates dependency injection code at compile time, providing efficient and type-safe dependency injection in Android applications.
Node.js and JavaScript: Dependency injection is not limited to Java-based technologies. In Node.js and JavaScript applications, libraries like InversifyJS and Awilix provide dependency injection containers that enable developers to manage and inject dependencies into JavaScript classes and modules.
These examples illustrate how dependency injection is used across different platforms and frameworks to facilitate modular, maintainable, and testable software development. By decoupling components and promoting inversion of control, dependency injection enables flexible and scalable software architecture in a wide range of applications.

Implementation

Let's dive into a simple yet effective example to understand its practical use. Imagine we are creating an online book store. We have a BookStore class, and it depends on a BookService class to get a list of books.

Without DI, the BookStore class might look something like this:

public class BookStore {
    private BookService bookService;

    public BookStore() {
        this.bookService = new BookService();
    }

    public List<Book> getBooks() {
        return bookService.getBooks();
    }
}
In the above code, the BookStore class is tightly coupled with the BookService class. It directly instantiates the BookService object using the new keyword. This setup makes the BookStore class hard to test and maintain. Any changes in the BookService class can directly impact the BookStore class.

Now, let's implement this with Dependency Injection. Implementing Dependency Injection (DI) can be quite straightforward. The following steps provide a basic guideline on how to get started with DI in your project.

Step 1: Identify Dependencies
The first step in implementing DI is to identify the dependencies in your code. Dependencies are instances of classes that your class needs to function. In our example, our BookStore class needs an instance of BookService to operate, so BookService is a dependency of the BookStore class.

Step 2: Provide Injection Points
Once you have identified the dependencies, the next step is to provide a way for these dependencies to be injected into the class. This is usually done through the constructor (Constructor Injection), through setter methods (Setter Injection), or directly into fields (Field Injection). The method you choose will depend on your specific use case and the complexity of your dependencies.

For example, if we choose Constructor Injection, our BookStore class would look like this:

public class BookStore {
    private BookService bookService;

    public BookStore(BookService bookService) {
        this.bookService = bookService;
    }

    // rest of the class implementation
}
In this updated version, the BookService object is passed to the BookStore class via its constructor. The BookStore class is no longer responsible for creating the BookService object. It doesn't need to know which implementation of BookService it is using, just that it can use it to get books. This allows us to change or update our BookService without impacting the BookStore class. We could easily substitute the real BookService with a mock object during testing, making our tests more reliable and isolated.

Step 3: Create Instances of Dependencies
The third step is to create instances of your dependencies within your App.java class. These instances will be injected into your class. The way these instances are created can vary. For a small project, you might just create these instances manually. For larger projects, you might use a DI container or framework like Spring, which can manage your instances for you.

In the App.java class, instantiate a BookService, as well as a BookStore object which you pass the BookService to via the constructor in order to fulfill its dependencies.

public class App {

    public static void main(String[] args) {

        // instantiate BookService object
        BookService bookService = new BookService();

    }

}
Step 4: Inject Dependencies
The final step is to inject the dependencies into your class as we've done above by passing the bookService object through the constructor of the bookStore object. If you're using a framework like Spring, this might be handled for you. If you're doing it manually, you would pass the instances when you create a new instance of your class like below:

public class App {

    public static void main(String[] args) {

        // instantiate BookService object
        BookService bookService = new BookService();

        // instantiate a BookStore object by passing the bookService object through its constructor
        BookStore bookStore = new BookStore(bookService);
    }

}
Step 5: Use Your Dependencies
Now you can use your dependencies as if they were part of your class. The key benefit is that your class is not responsible for creating these instances. This makes your code easier to test and maintain.

Remember, while you can implement DI manually, many projects use a DI container or framework. These tools manage the lifecycle and configuration of your instances for you, making it easier to deal with complex dependencies. Spring is a popular choice in the Java world, but other languages have their own options, such as Dagger for Android, Guice for Java, Autofac for .NET, and many more.

Summary

Dependency Injection (DI) is a design pattern in programming that enhances the modularity and testability of software. It's a form of Inversion of Control (IoC) where the responsibility of creating dependencies shifts from the class to a framework or container.

DI addresses issues like tightly coupled code, code duplication, and challenges in unit testing. It encourages creation of loosely coupled and maintainable code, which leads to efficient and scalable software systems.

Within frameworks such as Spring, DI is a central principle, advocating for modular and testable code. It enables building systems where components can be independently replaced and tested, resulting in highly modular architecture.

In practice, dependencies are injected into classes via constructors (Constructor Injection), setter methods (Setter Injection), or directly into fields (Field Injection). Various DI containers and frameworks exist to facilitate this process and manage the lifecycle of objects.

In essence, DI is a key tool in modern software development, promoting principles of modularity, testability, and maintainability.


## Types Of Dependency Injection
Learning Objectives

After completing this module, associates should be able to:

Understand the different types of Dependency Injection techniques - Constructor Injection, Setter Injection, and Field Injection.

Compare and contrast the pros and cons of Constructor Injection, Setter Injection, and Field Injection.

Identify the circumstances and scenarios in which each type of Dependency Injection is most appropriately used.

Demonstrate the ability to implement each of the three types of Dependency Injection in practical coding scenarios.

Recognize the role and implementation of the different types of Dependency Injection within the context of different frameworks, such as the Spring Framework.

Understand how different types of Dependency Injection can affect the testability and maintainability of code.

Analyze the contribution of different Dependency Injection types towards creating modular, decoupled, and robust software designs.

Description

Dependency Injection (DI) in Spring can be categorized into three types: Constructor Injection, Setter Injection, and Field Injection.

Constructor Injection
What it is: This form of Dependency Injection is accomplished when a class receives its dependencies through its constructor during the instantiation process.
Advantages: It allows for the creation of immutable objects because once a constructor is used to set a dependency, the field can be made final. This type of injection is also less prone to null pointer exceptions.
Disadvantages: The class could end up with a large number of constructor arguments if there are many dependencies, which can make the code difficult to understand.
Best used when: The dependencies are required for the class to function and should not be changed after the class is constructed.
Setter Injection
What it is: This type of Dependency Injection happens when a class receives its dependencies through setter methods after it has been constructed.
Advantages: It provides a way to change the dependencies of a class after it has been constructed. It also makes it easier to handle optional dependencies.
Disadvantages: The object is not guaranteed to be in a fully initialized state because it might not have received all its dependencies.
Best used when: The dependencies can be changed during the lifespan of the class or when some dependencies are optional.
Field Injection
What it is: Field Injection happens when the dependencies are injected directly into the fields of a class, bypassing the constructor or setters.
Advantages: It reduces the amount of boilerplate code, as no explicit setter or constructor is needed.
Disadvantages: It makes your classes harder to test because you can't easily replace the dependencies with mocks. It also breaks encapsulation because you're modifying private fields outside the class.
Best used when: You need to reduce the amount of boilerplate code and you are not concerned about the testing implications.
Real World Application

Let's explore the real-world applications of the different types of Dependency Injection (DI) in Spring:

Constructor Injection
Large-scale enterprise applications: In large applications where various services are interacting with each other, constructor injection ensures all dependencies are present at the time of object creation, making the application more stable and less prone to null pointer exceptions.

Immutable Objects: Constructor injection is useful in scenarios where you want your injected dependencies to be immutable (i.e., they can't be changed after being set). This is helpful in maintaining data consistency and integrity across the application.

Mandatory Dependencies: Whenever an object has mandatory dependencies (dependencies it cannot function without), constructor injection is preferred.

Setter Injection
Configurable Components: Setter injection is useful in scenarios where you want to provide the option for dependencies to be reconfigured even after an object has been created. This is common in applications that have plugable or interchangeable components.

Optional Dependencies: In cases where an object can function without certain dependencies, or where default dependencies can be overridden, setter injection is often used.

Field Injection
Simplifying Code with Spring Annotations: Field injection is commonly used in scenarios where you want to reduce boilerplate code. By directly injecting dependencies into fields with Spring's @Autowired annotation, the need for explicit setter or constructor methods is eliminated.

Rapid Prototyping and Development: Field injection can speed up development time as it requires less code. This makes it a good choice for prototyping or when speed of development is a priority over other considerations such as testability.

Implementation

Understanding the theoretical concepts of Dependency Injection (DI) types is useful, but let's look at how to implement them. Here are some examples for each type of DI:

Constructor Injection
Constructor injection is commonly used in scenarios where the injected dependencies are required for the class to function, and those dependencies shouldn't change during the lifespan of the class. Let's look at an example using a PaymentService class:

@Service
public class PaymentService {
    private final BankService bankService;

    public PaymentService(BankService bankService) {
        this.bankService = bankService;
    }

    public void makePayment(double amount) {
        bankService.transferFunds(amount);
    }
}
Here, PaymentService requires BankService to function and cannot change BankService after it has been constructed.

Setter Injection
Setter injection is often used when you need to provide the ability to change dependencies after an object has been constructed, or when a dependency is optional. Let's see an example with a NotificationService class:

@Service
public class NotificationService {
    private EmailService emailService;

    public void setEmailService(EmailService emailService) {
        this.emailService = emailService;
    }

    public void sendNotification(String message) {
        if(emailService != null) {
            emailService.sendEmail(message);
        }
    }
}
Here, NotificationService has an optional dependency on EmailService. The EmailService can be changed at any time using the setEmailService method.

Field Injection
Field Injection is typically used in scenarios where you want to reduce the amount of boilerplate code. An example of this is when using Spring's @Autowired annotation:

@Service
public class ProductService {
    @Autowired
    private ProductRepository productRepository;

    public List<Product> getAllProducts() {
        return productRepository.findAll();
    }
}
In this example, the ProductRepository dependency is directly injected into the ProductService class. No constructor or setter methods are required. However, this method of injection can make testing harder as it's more difficult to replace ProductRepository with a mock for testing.

Summary

Dependency Injection (DI) is a technique widely used in Spring to develop loosely coupled, testable, and maintainable code. There are three main types of DI in Spring:

Constructor Injection: Dependencies are provided through a class constructor. Ideal for mandatory and immutable dependencies, and often used in large-scale applications for better stability.

Setter Injection: Dependencies are injected through setter methods after an object is constructed. It is commonly used for optional dependencies and offers the flexibility to change dependencies during the object's lifespan.

Field Injection: Dependencies are injected directly into the fields, eliminating the need for explicit setter or constructor methods. While this reduces boilerplate code and speeds up development, it can complicate testing and violate encapsulation principles.

Each type of DI has its own benefits and use cases, and the choice among them depends on specific requirements of your project. Spring supports all these types of DI, making it a versatile framework for a wide variety of applications.


## Injection Using Xml Based Configuration
Learning Objectives

After completing this module, associates should be able to:

Understand the purpose and benefits of XML-based configuration in Spring.
Learn how to create a Spring configuration file using XML.
Identify how to define beans and their dependencies in the XML configuration file.
Recognize how to perform Dependency Injection using constructor, setter, and field injection in the XML configuration.
Description

Spring provides a powerful and flexible XML-based configuration option for managing beans and their dependencies. In Spring, the beans and their dependencies can be configured in an XML file. This file, often named applicationContext.xml, serves as the centerpiece of the application configuration.

In the XML configuration file, each bean is defined within the <bean> tag. The id attribute is used to uniquely identify the bean, and the class attribute specifies the fully qualified class name of the bean.

Example:

<bean id="exampleBean" class="com.example.ExampleBean" />
Constructor-based DI is accomplished by using the <constructor-arg> tag within the bean definition. The ref attribute is used to specify the bean that should be injected.

Example:

<bean id="anotherBean" class="com.example.AnotherBean">
    <constructor-arg ref="exampleBean" />
</bean>
Setter-based DI is achieved by using the <property> tag. The name attribute specifies the name of the property (matching the setter method), and the ref attribute identifies the bean to be injected.

Example:

<bean id="yetAnotherBean" class="com.example.YetAnotherBean">
    <property name="exampleBean" ref="exampleBean" />
</bean>
Spring XML configuration also supports the injection of complex data types such as lists, sets, maps, and properties using specific tags like <list>, <set>, <map>, and <props>. XML configuration also allows setting bean scope (singleton, prototype, etc.) using the scope attribute in the <bean> tag.

Spring provides a set of XML namespaces to simplify the configuration. For instance, the context namespace can be used to configure features like component-scanning and property placeholder configuration.

This XML-based configuration provides a clear and centralized way to manage the beans and their dependencies, making the application easy to configure and maintain.

Real World Application

In real-world applications, the use of XML configuration in Spring has become less common over the years, with many developers preferring the annotation-based configuration for its simplicity and less verbosity. However, there are still certain scenarios where using XML configuration might be beneficial or necessary:

Legacy Applications: Many older Spring applications were originally built using XML configuration, and updating them to annotation-based configuration may not be feasible due to time, resource constraints, or risk of introducing bugs. Maintaining and extending such applications often requires knowledge of XML configuration.

Highly Configurable Applications: XML configuration can be useful in applications where a high degree of configurability is required. With XML, it's easier to switch out implementations or configuration details without needing to recompile source code.

Fine-Grained Control Over Configuration: XML configuration provides more explicit control over the configuration details, making it a good choice when fine-grained control over the beans and their wiring is needed.

Sharing Configuration Across Applications: XML configuration files can be shared across multiple applications, which can be beneficial in large systems where common beans are used across different applications.

Remember, the choice between XML and annotation-based configuration often depends on the specific needs of your project and the preferences of your development team.

Implementation

Below is an example of using XML configuration.

Step 1: Setting Up the Project
We're going to use Maven as a build tool for this example. We'll need to set up a basic Maven project.

Here's a simple structure for our project:

/myapp
|-- pom.xml
|-- src
    |-- main
        |-- java
            |-- com
                |-- myapp
                    |-- MyApp.java
                    |-- MyService.java
        |-- resources
            |-- applicationContext.xml
Step 2: Configuring pom.xml
The pom.xml file should include dependencies for Spring Context, which provides core functionality, including XML configuration:

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>com.myapp</groupId>
  <artifactId>myapp</artifactId>
  <version>1.0-SNAPSHOT</version>
  <packaging>jar</packaging>

  <name>myapp</name>
  <url>http://maven.apache.org</url>

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <spring.version>5.3.0</spring.version> <!-- Use the version you need -->
  </properties>

    <!-- Add the Spring Context Module -->
  <dependencies>
    <dependency>
      <groupId>org.springframework</groupId>
      <artifactId>spring-context</artifactId>
      <version>${spring.version}</version>
    </dependency>
  </dependencies>

</project>
Step 3: Creating a Bean Class
We'll create a simple service class (MyService.java) that we'll later define as a bean in the XML configuration.

package com.myapp;

public class MyService {
    public void sayHello() {
        System.out.println("Hello, World!");
    }
}
Step 4: Configuring Bean in applicationContext.xml
In the applicationContext.xml file, we'll define MyService as a Spring bean.

<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
       http://www.springframework.org/schema/beans/spring-beans.xsd">

    <bean id="myService" class="com.myapp.MyService" />

</beans>
Step 5: Using the Bean
In our main application (MyApp.java), we'll load the application context from the XML configuration file and retrieve our MyService bean to use it.

package com.myapp;

import org.springframework.context.ApplicationContext;
import org.springframework.context.support.ClassPathXmlApplicationContext;

public class MyApp {
    public static void main(String[] args) {
        ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml");
        MyService myService = context.getBean("myService", MyService.class);
        myService.sayHello();
    }
}
Now we can run MyApp.java as a Java application, and it will print "Hello, World!" to the console. This demonstrates a very simple use of XML configuration in Spring. Real-world applications would typically have more complex dependencies and more configuration details.

Summary

XML configuration in Spring is a traditional way of defining and managing beans and their dependencies.
It provides a clear and centralized way of configuring the application, making it easier to understand and maintain.
Beans are defined within the <bean> tag in the XML configuration file. Each bean is given a unique id and the fully qualified class name is specified with the class attribute.
Dependency Injection can be done through constructor injection, where dependencies are specified within the <constructor-arg> tag, or through setter injection, where dependencies are specified within the <property> tag.
XML configuration also supports injection of complex data types, such as lists, sets, and maps.
Even though annotation-based configuration is becoming more popular due to its simplicity, XML configuration is still in use, especially in legacy applications or applications that require a high degree of configurability and explicit control over configuration details.
The use of XML configuration enables sharing of configuration details across multiple applications, which can be beneficial in large systems.


## Injection Using Java Based Configuration
Learning Objectives

After completing this module, associates should be able to:

Understand what Java-based configuration in Spring is and how it differs from XML-based configuration.
Learn to create and configure Spring beans using Java-based configuration.
Understand the benefits and drawbacks of Java-based configuration, and learn to choose the appropriate configuration method based on the specific needs of the project.
Gain knowledge on how Java-based configuration contributes to better testability of the application.
Learn to migrate from XML-based configuration to Java-based configuration.
Description
Java-Based Configuration
Java-based configuration in Spring is an alternative to XML-based configuration. It offers a type-safe and more flexible method of configuring beans and their dependencies. The configuration is done entirely in Java classes using annotations. Java-based configuration uses annotations to configure your beans. With Java-based configuration, you would create a configuration class that defines your beans. These classes are similar to XML configuration files in functionality.

@Configuration classes are used to define beans. They are similar to XML configuration files in functionality.
@Bean is used to declare a bean in a @Configuration class.
@Qualifier: This annotation is used when there are multiple beans of the same type and you want to inject one of them. It allows for specifying the exact bean to be wired.
@Scope: This annotation is used to specify the scope of the beans (singleton, prototype, etc.).
@ComponentScan: This is used to specify the packages to scan for @Component annotations or its derivatives. Spring will automatically discover and instantiate the beans.
Benefits of Java-Based Configuration
Type safety: As everything is done in Java, the compiler can check the code and prevent errors that can happen due to typos in XML files.
Autocompletion and debugging: IDEs can offer autocomplete, navigation, and debugging out-of-the-box for Java classes, but not necessarily for XML files.
Flexibility: Java-based configuration allows for more dynamic and complex scenarios, as it's capable of leveraging the full power of Java language.
Despite these benefits, Java-based configuration might not be the best choice for all situations, such as when the configuration is shared among multiple projects, or when there's a need to separate configuration from code. It's always important to choose the appropriate configuration method based on the specific needs of the project.

Real World Application

Java-based Dependency Injection (DI) and configuration in Spring offer several advantages over XML-based configuration. Here are some real-world applications where Java-based DI and configuration are preferred:

Improved Readability and Maintainability: Java-based configuration allows developers to express their configuration directly in code, making it more readable and easier to maintain compared to XML files.
Type Safety: With Java-based configuration, the compiler can catch type-related errors at compile-time, reducing the likelihood of runtime issues due to incorrect or misspelled bean names.
Enhanced Refactoring and IDE Support: Java-based configuration enables IDEs to provide advanced features such as auto-completion, refactoring, and navigation, which can significantly improve development productivity and reduce errors.
Easy Integration with Java Libraries and Frameworks: Java-based configuration seamlessly integrates with other Java libraries and frameworks, enabling developers to leverage the full power of the Java ecosystem without the need for additional configuration files.
Dynamic and Conditional Configuration: Java-based configuration allows for dynamic and conditional bean creation and configuration, making it more suitable for complex scenarios where runtime conditions or external factors determine the configuration.
Annotation-Driven Development: Java-based configuration relies heavily on annotations such as @Component, @Autowired, and @Bean, promoting a more annotation-driven development approach, which is widely adopted and understood in the Java community.
Faster Development and Deployment: Java-based configuration eliminates the need to maintain separate XML files, reducing configuration overhead and making development and deployment cycles faster.
It's important to note that both XML and Java-based configurations have their merits, and the choice between them depends on the specific requirements and preferences of the project. Java-based configuration is generally preferred in modern Spring applications due to its improved readability, maintainability, and integration capabilities.

Implementation

Below is a demonstration of defining Dependency Injection (DI) using Java-based configuration with @Bean and @Configuration annotations. We will have an Employee entity and a basic controller-service-repository architecture in our application.

Step 1: Setup
To begin, we can set up a project in Java by utilizing the build tool of our choice, such as Maven or Gradle. We will incorporate the required Spring dependencies into the projects build file.

Example:

<dependencies>
    <!-- Other dependencies -->
    <dependency>
        <groupId>org.springframework</groupId>
        <artifactId>spring-context</artifactId>
        <version>5.3.9</version> <!-- Replace with the desired version -->
    </dependency>
</dependencies>
Step 2: Create Bean Classes
We need to identify the beans that need to be managed by the Spring container and create the corresponding bean classes:

Employee class:

public class Employee {
    // fields
    private Integer id;
    private String firstName;
    private String lastName;

    // ... other class members omitted
}
EmployeeController class:

public class EmployeeController {
    private EmployeeService employeeService;

    // consructor injection
    public EmployeeController(EmployeeService employeeService) {
        this.employeeService = employeeService;
    }

}
EmployeeService class:

public class EmployeeService {
    private EmployeeRepository employeeRepository;

    // setter injection
    public void setEmployeeRepository(EmployeeRepository employeeRepository) {
        this.employeeRepository = employeeRepository;
    }
}
EmployeeRepository class:

public class EmployeeRepository {
    
}
Step 2: Create the Configuration Class
Within the configuration class we can define methods annotated with @Bean that will return instances of the beans.

import org.springframework.context.annotation.*;

@Configuration
public class EmployeeConfig {

    // uses constructor injection
    @Bean
    public EmployeeController employeeController(EmployeeService employeeService) {
        return new EmployeeController(employeeService);
    }

    // uses setter injection
    @Bean
    public EmployeeService employeeService(EmployeeRepository employeeRepository) {
        EmployeeService bean = new EmployeeService();
        bean.setEmployeeRepository(employeeRepository);
        return bean;
    }

    @Bean
    public EmployeeRepository employeeRepository() {
        return new EmployeeRepository();
    }
}
In the above code, we used construction and setter injection depending on how the bean classes were constructed.

Step 3: Application Entry Point
In the main method of our application's entry point class, we can create an instance of the Spring ApplicationContext and use it to get and use the beans:

package com.example.myapp;

import org.springframework.context.annotation.AnnotationConfigApplicationContext;

public class Main {

    public static void main(String[] args) {
        // #1. create container and configure with a Java config class
        AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(EmployeeConfig.class);

        // #2. get and use beans
        String[] allBeanNames = context.getBeanDefinitionNames();
        System.out.println("\nBeans created involving Java-based config:");
        for (String beanName : allBeanNames) {
            System.out.println(beanName);
        }

        // #4. close container
        ((AnnotationConfigApplicationContext) context).close();
    }
}
Step 4: Run the Application
Now we can run the application to verify that the beans are created and dependencies are injected correctly. Output:
Beans created involving Java-based config:
org.springframework.context.annotation.internalConfigurationAnnotationProcessor
org.springframework.context.annotation.internalAutowiredAnnotationProcessor
org.springframework.context.event.internalEventListenerProcessor
org.springframework.context.event.internalEventListenerFactory
employeeConfig
employeeController
employeeService
employeeRepository
By following these steps and customizing the code according to your specific requirements, you can successfully incorporate Dependency Injection using Java-based configuration with @Bean and @Configuration annotations in your Spring application.

Summary

Dependency Injection (DI) using Java-based configuration in Spring provides a more readable and maintainable approach compared to XML-based configuration.
Java-based configuration uses annotations such as @Configuration, @Bean, and @Autowired to define beans and their dependencies.
@Configuration marks a class as a configuration class, where bean definitions and dependencies are declared.
@Bean annotates methods that define and return instances of beans.
@ComponentScan is used to specify the packages to scan for @Component annotations or its derivatives. Spring will automatically discover and instantiate the beans.
Java-based configuration promotes type safety and integrates well with IDE tools.
It offers flexibility for dynamic and conditional bean configuration.
It seamlessly integrates with other Java libraries and frameworks.
Java-based configuration in Spring simplifies dependency management, improves code readability, and provides a flexible approach to configure and wire beans in applications.


## Annotation Based Configuration
Learning Objectives

After completing this module, associates should be able to:

Understand the concept of annotation-based configuration in the Spring framework.
Learn about the component and stereotype annotations used in Spring configuration.
Learn about the different types of dependency injection annotations.
Understand how to use annotation-based configuration in real-world applications.
Description

The Spring Framework is a robust and versatile framework that's widely used in the Java ecosystem for creating enterprise-level applications. In Spring, the application configuration can be done either using XML or annotations. This document will focus on the annotation-based configuration.

Annotation-based configuration is another type of configuration that uses Component annotations only, such as @Component, @Service, @Repository, and @Controller for declaring beans, and @Autowired for Dependency Injection.

@Component is a general-purpose stereotype annotation indicating that the class is a Spring component.
@Service, @Repository, and @Controller are specializations of @Component for specific use cases.
@Autowired: It is used to achieve Dependency Injection. The Spring container auto-wires beans by looking at the classes in the classpath, matching them by type or by name.
Benefits of Annotation-Based Configuration:

Simplicity: Using annotations often leads to cleaner and more readable code as the metadata (annotations) are placed right next to the code they are related to.
Elimination of XML configuration: Annotations eliminate the need for XML configuration files, providing a means to move towards a zero-XML configuration. The XML files could get quite verbose and difficult to manage in large projects.
Improved productivity: With annotations, developers can avoid writing long XML configuration files and spend more time writing the business logic.
Type Safety: Since annotations are strongly typed, errors will be caught at compile time rather than at runtime, as could happen with string-based XML configurations.
Better for Collaboration and Control: With annotations, the configurations are done in the code, so it's easier for multiple team members to understand what's happening in the codebase and maintain version control.
Contextual Configuration: Annotations allow configurations to be applied right where they're needed, making the configuration context-specific and easier to understand.
Real World Application

Here are some real-world application types where annotation-based configuration in Spring is commonly used:

Web Applications: Annotation-based configuration in Spring is widely used in the development of web applications. This includes applications utilizing Spring MVC, where controllers can be easily configured using @Controller annotation and request mapping annotations like @RequestMapping, @GetMapping, etc.

Restful Services: In the development of RESTful web services using Spring Boot, annotations like @RestController and @RequestMapping are used for configuring endpoints.

Enterprise Applications: Annotation-based configurations are extensively used in larger enterprise level applications. Spring's @Service, @Repository and @Component annotations are used to denote service classes, data access objects (DAOs), and other Spring components respectively.

Implementation

Below is a demonstration of defining Dependency Injection (DI) using annotation-based configuration with @Controller, @Service, and Repository annotations. We will have a Gallery entity and a basic controller-service-repository architecture in our application.

Step 1: Setup
To begin, we can set up a project in Java by utilizing the build tool of our choice, such as Maven or Gradle. We will incorporate the required Spring dependencies into the projects build file.

Example:

<dependencies>
    <!-- Other dependencies -->
    <dependency>
        <groupId>org.springframework</groupId>
        <artifactId>spring-context</artifactId>
        <version>5.3.9</version> <!-- Replace with the desired version -->
    </dependency>
</dependencies>
Step 2: Create Bean Classes
We need to identify the beans that need to be managed by the Spring container and create the corresponding bean classes:

Gallery class:

package com.example.annotationbased;

public class Gallery {

    // fields
    private Integer id;
    private String name;
    private String description;

    // ... other class members omitted
}
GalleryController class:

package com.example.annotationbased;

import org.springframework.stereotype.Controller;

@Controller
public class GalleryController {
    
    private GalleryService galleryService;

    public GalleryController(GalleryService galleryService) {
        this.galleryService = galleryService;
    }
}
GalleryService class:

package com.example.annotationbased;

import org.springframework.stereotype.Service;

@Service
public class GalleryService {
    
    private GalleryRepository galleryRepository;

    public void setGalleryService(GalleryRepository galleryRepository) {
        this.galleryRepository = galleryRepository;
    }
}
GalleryRepository class:

package com.example.annotationbased;

import org.springframework.stereotype.Repository;

@Repository
public class GalleryRepository {
    
}
Step 3: Application Entry Point
In the main method of our application's entry point class, we can create an instance of the Spring ApplicationContext and use it to get and use the beans:

package com.example.annotationbased;

import org.springframework.context.annotation.AnnotationConfigApplicationContext;

public class AnnotationBasedApplication {

	public static void main(String[] args) {
		// #1. create container and configure by specifying package to find beans in
		AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext("com.example.annotationbased");

		// #2. get and use beans
		String[] allBeanNames = context.getBeanDefinitionNames();
		System.out.println("\nBeans created involving annotation-only config:");
		for (String beanName : allBeanNames) {
			System.out.println(beanName);
		}

		// #4. close container
		((AnnotationConfigApplicationContext) context).close();
	}

}
In this example, we created a container that manages beans for the package all of our sources files are within, the com.example.annotationbased package.

Step 4: Run the Application
Now we can run the application to verify that the beans are created and dependencies are injected correctly. Output:
Beans created involving annotation-only config:
org.springframework.context.annotation.internalConfigurationAnnotationProcessor
org.springframework.context.annotation.internalAutowiredAnnotationProcessor
org.springframework.context.event.internalEventListenerProcessor
org.springframework.context.event.internalEventListenerFactory
galleryController
galleryRepository
galleryService
By following these steps and customizing the code according to your specific requirements, you can successfully incorporate Dependency Injection using annotation-based configuration.

Summary

Annotation-based configuration is an alternative to XML-based configuration that was introduced in Spring 2.5. It uses annotations to handle dependency injection, bean creation, and configuration.
They key annotations are the @Autowired, @Service, Controller, and Repository annotations.
There are many benefits to using annotation-based configuration such as simplicity, elimination of XML configuration, and type safety.


## Component Scanning
Learning Objectives

After completing this module, associates should be able to:

Understand the concept and purpose of component scanning in Spring.
Learn how to use the @ComponentScan annotation.
Description

In Spring, component scanning is a mechanism that allows the framework to automatically detect and register the classes annotated with Stereotype annotations (@Component, @Service, @Repository, @Controller) as beans in the Spring application context.

When the application starts up, Spring scans the classpath, looks for classes annotated with these annotations, creates instances (beans) of these classes, and registers them in the application context.

This automated process greatly simplifies the configuration of a Spring application by reducing the need for explicit bean definitions in XML or Java configuration files. It also encourages a convention-over-configuration programming model, which leads to cleaner and more manageable code.

The @ComponentScan annotation or <context:component-scan> element in XML is used to specify the packages that Spring should scan for component classes. If no specific packages are specified, Spring will scan from the package of the class that declares the @ComponentScan annotation and its sub-packages.

Real World Application

Here are some real-world application types where component scanning in Spring is commonly used:

Web Applications: Component scanning is extensively used in web applications developed using Spring MVC. Controllers, services, and repositories are often marked with Stereotype annotations (@Controller, @Service, @Repository) and automatically detected and registered by Spring.

RESTful Services: When developing RESTful services with Spring Boot, @RestController annotated classes are automatically detected and registered into the application context.

Enterprise Applications: Larger enterprise applications benefit from component scanning to help manage the complexity of numerous service and repository classes.

Microservices: In a Spring Boot microservice architecture, component scanning is used to automatically detect and register beans, streamlining the process of setting up and configuring the microservices.

Spring Data JPA Applications: In applications using Spring Data JPA, component scanning is used to automatically detect and register repository interfaces.

Implementation

Below is a step-by-step guide on how to implement component scanning in a Spring application. The guide assumes you have a basic understanding of Java and Maven, as well as an Integrated Development Environment (IDE) like IntelliJ IDEA or Eclipse:

In the following example, we'll outline the steps of using a Maven project with Spring context dependency and use Java-based configuration (@Configuration and @ComponentScan annotations) to illustrate component scanning. Note that Spring Boot makes this process easier and more straightforward, but it's valuable to understand how things work under the hood.

Let's assume we have a Maven project with the following Spring context dependency added to the POM.xml file:

<dependencies>
    <dependency>
        <groupId>org.springframework</groupId>
        <artifactId>spring-context</artifactId>
        <version>5.3.10</version> <!-- use the latest version available -->
    </dependency>
</dependencies>
Step 1: Create a Service Class
In src/main/java/com/example, under the root package, let's create a new package named service.
In the service package, we'll create a class GreetingService with the following code:
package com.example.service;

import org.springframework.stereotype.Service;

@Service
public class GreetingService {
    public String greet() {
        return "Hello, World!";
    }
}
The @Service annotation marks this class as a Spring managed bean. Spring will automatically detect this class during component scanning.

Step 2: Create a Configuration Class
In src/main/java/com/example, under the root package, let's create a new package named config.
In the config package, we'll create a class AppConfig with the following code:
package com.example.config;

import org.springframework.context.annotation.ComponentScan;
import org.springframework.context.annotation.Configuration;

@Configuration
@ComponentScan(basePackages = "com.example")
public class AppConfig {

}
The @Configuration annotation tells Spring that this class contains bean definitions. The @ComponentScan annotation tells Spring to scan the com.example package (and its sub-packages) for classes marked with Stereotype annotations.

Step 3: Create a Main Class to Test the Setup
Let's create a Main class with a main() method that retrieves the GreetingService bean from the application context and calls the greet() method:

package com.example;

import com.example.config.AppConfig;
import com.example.service.GreetingService;
import org.springframework.context.ApplicationContext;
import org.springframework.context.annotation.AnnotationConfigApplicationContext;

public class Main {
    public static void main(String[] args) {
        ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);
        GreetingService greetingService = context.getBean(GreetingService.class);
        System.out.println(greetingService.greet());  // Prints "Hello, World!"
    }
}
When you run this Main class, Spring will create an application context with the beans defined in AppConfig, perform component scanning, and inject the GreetingService bean where needed.

Summary

Component scanning is a feature in Spring that allows for the automatic detection and instantiation of classes annotated with Stereotype annotations (@Component, @Service, @Repository, @Controller).

These detected classes are automatically registered as beans in the Spring application context. This means you don't have to explicitly define these beans in your configuration.

The process of component scanning simplifies the configuration of your Spring application and makes your code cleaner and easier to manage.

To use component scanning, you need to specify the packages to scan. You can do this using the @ComponentScan annotation or the <context:component-scan> element in XML.

Spring will scan the specified packages, and any sub-packages, looking for classes with Stereotype annotations.

While Spring Boot automatically sets up component scanning, you can also set it up manually in a Spring Framework application using a configuration class annotated with @Configuration and @ComponentScan.


## Stereotype Annotations
Learning Objectives

After completing this module, associates should be able to:

Define stereotype annotations
Description

Spring provides stereotype annotations. These annotations are used to create Spring beans automatically in ApplicationContext.

Stereotype AnnotationsThe following are the Spring stereotype annotations:

1. @Component

@Component is the main Stereotype Annotation. It is a class-level annotation. @Component is used across the application to mark the beans as Spring's managed components.

2. @Service

It is a specialization of @Component. The business logic of an application is in the service layer, @Service annotation is used to indicate that a class belongs to the service layer.

3. @Repository

It is a specialization of @Component. It is very similar to the DAO pattern. DAO classes provide CRUD operations for database tables.

4. @Controller

It is a specialisation of @Component. The @Controller indicates that a particular class is a controller. It is mostly used with Spring MVC applications. It is used in combination with annotated methods based on the @RequestMapping annotation. The dispatcher scans the class and detects methods with @RequestMapping.

Real World Application

Consider an application that is used to get and display the student data.
An entity class Student is created with required fields.
Service classes are created for services like getting students with percentages above 90 or students who are engineering majors etc.
Repository classes are created to save the student and find the student by id etc.
A controller class is created for registration and displaying data after registration.
Implementation

@Component
Consider a class in the Spring application
package com.sample.controller;

@Component
public class Student {
    //fields
    //methods
}
@Controller
Consider a controller class StudentController
package com.sample.controller;

@Controller
public class StudentController {

    @RequestMapping(value = "" )
    //method

}
@Service
Consider a service class that converts marks to GPA.
package com.sample.service;

import org.springframework.stereotype.Service;

@Service
public class MarksToGpa {

}
@Repository
Consider a DAO class StudentDAO.
package com.sample.repository;

import org.springframework.stereotype.Repository;

@Repository
public class StudentDAO {

    //methods to get data from the database and update the data in the database

}
Summary

Stereotype annotations are used to create beans automatically in the application context.
The most common stereotype annotation is @Component
@Controller, @Service, and @Repository are specializations of @Component

## Bean Definition And Instantiation
Learning Objectives

After completing this module, associates should be able to:

Understand the concept of bean definition and instantiation in the context of dependency injection frameworks like Spring.
Learn the various ways to define beans, such as XML configuration, Java-based configuration, and annotations.
Description

Bean definition and instantiation are core concepts in the Spring framework, governing the creation, configuration, and lifecycle management of objects, known as beans. Here is a brief overview of bean definition and instantiation:

Bean Definition: In Spring, a bean definition serves as a blueprint for creating bean instances. It defines the characteristics and configuration of a bean, including its class, scope, properties, and dependencies.
Instantiation: Bean instantiation refers to the process of creating an instance of a bean based on its bean definition. Spring provides various mechanisms for bean instantiation, such as constructor injection, setter injection, and factory methods.
Configuration Options: Spring offers multiple approaches to define beans, including XML configuration, Java-based configuration using annotations or @Bean methods, and component scanning.
Scopes: Beans can have different scopes, such as singleton, prototype, request, session, and more. The scope determines the lifecycle and availability of a bean within an application context.
Lifecycle Management: Spring supports lifecycle callbacks for beans, allowing custom logic to be executed during bean initialization and destruction phases.
Dependencies: Beans often depend on other beans to fulfill their functionality. Spring facilitates dependency injection, where dependencies are resolved and injected into beans, promoting loose coupling and modular design.
Lazy Initialization: Spring supports lazy initialization, allowing beans to be created and initialized only when they are requested, improving application startup time and resource usage.
Advanced Features: Spring provides advanced features for bean definition and instantiation, including conditional bean creation, dynamic bean registration, and the ability to create beans using factory methods.
Bean Naming and Aliases: Beans can be assigned unique names and aliases, enabling easy referencing and retrieval of beans within an application context.
By understanding bean definition and instantiation in Spring, developers gain the ability to configure and manage beans effectively, leverage dependency injection, and optimize the lifecycle and usage of objects within their applications.

Bean Definition
The BeanDefinition objects contain the following metadata

Class: The name of the class that instantiates the bean.
<bean id="bean1" class="..."/>
Name: Bean's name started with a lowercase letter and was followed by the camel case.

id: an id is a unique name given to the bean within the container.
name: name attribute is used to provide multiple bean ids. the names can be separated by "," or ";".
<bean id="bean1" name="alias1,alias2,alias3" class="..."/>
alias: alias is used to add a name to a bean defined elsewhere.
<alias name="fromName" alias="toName"/>

Scope: scope is used to define the bean behaviour in the container(prototype, singleton, auto wiring mode etc)
<bean id="bean1" class="..." scope = "scopeName"/>
Real World Application

The concept of bean definition and instantiation in Spring finds applications in various real-world scenarios, providing flexibility and modularity to software development. Here are some examples:

Enterprise Applications: Bean definition and instantiation are commonly used in enterprise applications to define and configure components such as service classes, data access objects (DAOs), and business logic beans. This approach allows for easy management of dependencies and promotes modular design.

Web Applications: Web applications leverage bean definition and instantiation for configuring controllers, services, data sources, and other components. This enables separation of concerns and facilitates the integration of different layers within the application.

Microservices Architecture: In a microservices architecture, bean definition and instantiation are employed to define and configure individual microservices as independent components. Each microservice can have its own bean configuration, allowing for flexible scaling and deployment.

Testing and Mocking: Bean definition and instantiation are invaluable in testing scenarios. By defining beans for mock or stub implementations, developers can easily swap real dependencies with test-specific ones, enabling comprehensive unit testing and promoting test-driven development practices.

Dynamic Plugin Systems: Applications that support dynamic plugin systems utilize bean definition and instantiation to manage and instantiate plugins or extensions. This allows for the dynamic addition or removal of functionality, enhancing the extensibility and customization of the application.

Command-Line Applications: Command-line applications can benefit from bean definition and instantiation by defining and configuring beans for various command handlers or processors. This approach allows for easy integration of different command-line components.

Integration with External Systems: Bean definition and instantiation are often used in integrating with external systems or services. By defining beans to handle communication and interaction with external APIs or services, the application can encapsulate the necessary logic and manage the dependencies effectively.

Desktop Applications: Bean definition and instantiation are applicable in desktop applications for configuring GUI components, event handlers, and other UI-related beans. This enables separation of concerns and promotes modularity in desktop application development.

These are just a few examples of how bean definition and instantiation play a vital role in various real-world applications. By utilizing these concepts, developers can achieve better organization, maintainability, and extensibility in their software projects.

Implementation

Create a Java Project: Set up a new Java project using your preferred IDE or build tool, such as Maven or Gradle.

Add Spring Dependencies: Include the necessary Spring dependencies in your project's build configuration file (e.g., pom.xml for Maven) to ensure access to the Spring framework and related functionality. For example:

<!-- Maven dependency example -->
<dependencies>
    <dependency>
        <groupId>org.springframework</groupId>
        <artifactId>spring-context</artifactId>
        <version>5.3.10</version>
    </dependency>
    <!-- Include other dependencies as required -->
</dependencies>
Define a Bean: Create a class that you want to define as a bean. Add the appropriate annotations to mark it as a bean, such as @Component, @Service, or @Repository. For example:
import org.springframework.stereotype.Component;

@Component
public class MyBean {
    // Bean implementation code
}
Configure Component Scanning: Ensure that component scanning is enabled in your Spring configuration to automatically detect and register beans. This can be done via XML configuration or Java-based configuration. For example, using Java-based configuration:
import org.springframework.context.annotation.ComponentScan;
import org.springframework.context.annotation.Configuration;

@Configuration
@ComponentScan(basePackages = "com.example")
public class AppConfig {
    // Configuration code
}
Create Application Context: Instantiate the Spring application context based on your configuration. This will load and initialize the defined beans.
import org.springframework.context.ApplicationContext;
import org.springframework.context.annotation.AnnotationConfigApplicationContext;

public class MainApp {
    public static void main(String[] args) {
        ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);
        // Application logic
    }
}
Retrieve and Use Beans: Access the beans from the application context and utilize their functionality in your application. This can be done by calling the appropriate methods on the bean instances.
import org.springframework.context.ApplicationContext;
import org.springframework.context.annotation.AnnotationConfigApplicationContext;

public class MainApp {
    public static void main(String[] args) {
        ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);

        MyBean myBean = context.getBean(MyBean.class);
        myBean.doSomething();
    }
}
Run the Application: Build and run your Spring application. The application context will initialize the defined beans, and you can observe the output or behavior of the beans as per your implementation.
By following these steps, you can practice the implementation of defining beans and instantiating them in a Spring application. This will help you understand the concepts of dependency injection, bean configuration, and usage within the Spring framework.

Summary

Bean definition and instantiation are fundamental concepts in the Spring framework that contribute to the flexible and modular design of applications. Here's a brief summary:

Bean Definition: In Spring, a bean definition serves as a blueprint for creating bean instances. It specifies the configuration, dependencies, and lifecycle details of a bean.
Instantiation: Bean instantiation is the process of creating an actual instance of a bean based on its bean definition. Spring provides various mechanisms for instantiation, including constructor injection, setter injection, and factory methods.
Configuration Options: Spring offers multiple approaches to define beans, such as XML configuration, Java-based configuration with annotations or @Bean methods, and component scanning.
Scopes: Beans can have different scopes, such as singleton, prototype, request, session, and more. The scope defines the lifecycle and visibility of a bean within an application context.
Lifecycle Management: Spring provides lifecycle callbacks for beans, allowing custom logic to be executed during bean initialization and destruction phases.
Dependencies: Beans often have dependencies on other beans. Spring's dependency injection mechanism resolves and injects these dependencies, promoting loose coupling and modularity.
Lazy Initialization: Spring supports lazy initialization of beans, where they are created and initialized only when requested, improving application startup time and resource utilization.
Advanced Features: Spring offers advanced features for bean definition and instantiation, including conditional bean creation, dynamic bean registration, and the ability to use factory methods.
Bean Naming and Aliases: Beans can be assigned unique names and aliases, enabling easy referencing and retrieval within an application context.
Understanding bean definition and instantiation in Spring is crucial for effective dependency management and creating modular applications with flexible configuration options.


## Bean Lifecycle
Learning Objectives

After completing this module, associates should be able to:

Define bean lifecycle
Define and implement custom init and destroy methods
Description
Bean Lifecycle
The bean life cycle is as follows:

Spring IOC starts
Bean Instantiated
Dependencies Injected
Internal Spring processing
Custom init method( ready for use)
Container shutdown
Custom destroy method
Bean Life Cycle

Custom Init method
In any Spring application for most of the classes some methods should be implemented before the main execution.
Method calls for these methods are grouped in an init method.
With the help of Spring, the init method can be invoked after bean instantiation.
Custom destroy method
After execution of the init method and bean, before the shutdown of the container, the custom destroy method is invoked.
Similar to the init method, the destroy methods of all the classes are grouped under a common method.
With the help of Spring, destroy method can be invoked along with container shutdown.
Configuration
The configuration for custom init and custom destroy methods is done in three methods:

XML-based configuration
Annotation configuration
Interface configuration
Note: Interface based configuration is not commonly used and added to the content to provide wide explanation.

Real World Application

An object for a class that is initialized, managed and destroyed by a Spring IoC container is called a bean.
Consider a DAO class with a JDBC connection. In the bean lifecycle, the first process is bean initialization.
The second step in the process is passing dependencies like driver name, user name and password etc.
The third step is the custom init method. Creating the connection is a common and repeated step in JDBC, this can be added to the init method.
The next step is bean execution. Operations like fetching data and manipulating data from databases come under this step.
The next step is container shutdown. Before the container shuts down custom destroy method is invoked. Closing database connection can be added to the destroy method.
Finally, the container is shut down.
Implementation

In this example, we will be working with a Student entity and database. We have a StudentDAO class that allows the application to connect to and work with a database. We also have an XML file for configuring our beans.

Setting Up Our Files
Student:

public class Student {
    private int id;
    private String firstName;
    private String lastName;
    private String major;

    // ... other code omitted
}
StudentDAO:

package StudentData;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;

import javax.annotation.PostConstruct;
import javax.annotation.PreDestroy;

public class StudentDAO {
    static String driver ;
    static Connection con;
    static String url;
    static String userName;
    static String password;

    static void createConnection() throws ClassNotFoundException, SQLException {
        Class.forName(driver);
        con = DriverManager.getConnection(url,userName,password);
            
    }
    static void closeConnection() throws SQLException {
        con.close();
        
    }
    void getAllRecords() throws SQLException {
        String sql = "Select * from student;";
        PreparedStatement st = con.prepareStatement(sql);
        ResultSet rs = st.executeQuery();
        while(rs.next()) {
            System.out.println(rs.getInt(1)+ " "+rs.getString(2)+" "+rs.getString(3)+" "+rs.getString(4));
        }
    }

}
The bean configuration is done for the StudentDAO class in an XML file:

<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context"
    xmlns:aop="http://www.springframework.org/schema/aop"
    xsi:schemaLocation="http://www.springframework.org/schema/beans
    http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
    http://www.springframework.org/schema/context
    http://www.springframework.org/schema/context/spring-context-3.0.xsd
    http://www.springframework.org/schema/aop
    http://www.springframework.org/schema/aop/spring-aop-3.0.xsd
    ">
    <bean id = "StudentDAO" class="StudentData.StudentDAO"></bean>
   
</beans>
Setting Up Injection
Fields like Driver name, username, password and url can be considered properties and injected. To do this, setter and getter methods are generated for all the fields. A print statement is added in every setter method for reference.

StudentDAO:

    public static String getDriver() {
        return driver;
    }
    public static void setDriver(String driver) {
        System.out.println("setting Driver");
        StudentDAO.driver = driver;
    }
    public static String getUrl() {
        
        return url;
    }
    public static void setUrl(String url) {
        System.out.println("setting url");
        StudentDAO.url = url;
    }
    public static String getUserName() {
        return userName;
    }
    public static void setUserName(String userName) {
        System.out.println("setting userName");
        StudentDAO.userName = userName;
    }
    public static String getPassword() {
        return password;
    }
    public static void setPassword(String password) {
        System.out.println("setting password");
        StudentDAO.password = password;
    }
In the XML, the property tag is added in the bean tag with name and value attributes.

<bean id = "StudentDAO" class="StudentData.StudentDAO">
    <property name="driver" value="com.mysql.cj.jdbc.Driver"></property>
    <property name="userName" value="root"></property>
    <property name="password" value="12345"></property>
    <property name="url" value="jdbc:mysql://localhost:3306/content"></property>
</bean>
Using init() and destroy()
The process of creating a connection is common for every method. If new methods like getRecord and deleteRecord are created, the created connection is repeated. To avoid this, a new method is created for connection. The init method is created and the method call for the connection method is written within in. A print statement is added in the init and destroy method for reference.

void init() throws ClassNotFoundException, SQLException {
        System.out.println("Inside init method");       
        createConnection();
    }
The process of closing the connection can be done in destroy method by invoking the closeConnection method.

void destroy() throws SQLException {
        System.out.println("Inside Destroy Method");
        closeConnection();
    }
Using Annotation Configuration
Configuring the init method
@PostConstruct is used to configure the init method. Before using annotations, <context:annotation-config> or <bean class="org.springframework.context.annotation.CommonAnnotationBeanPostProcessor" /> should be added to the XML file.

StudentDAO:

@PostConstruct
    void init() throws ClassNotFoundException, SQLException {
        System.out.println("Inside init method");       
        createConnection();
    }
Configuring the destroy method
@PreDestroy is used to configure destroy method. In a web application the application context is closed implicitly but in a desktop application, the application should be explicitly closed.

StudentDAO:

@PreDestroy
    void destroy() throws SQLException {
        System.out.println("Inside Destroy Method");
        closeConnection();
    }
By using the below java code, the application context is closed:

((AbstractApplicationContext) context).close();
A lifecycle hook registerShutdownHook() can also be used to destroy the bean. This is prefered over context.close() because registerShutdownHook() is called once the main thread ends and closes the container. So irrespective of the line where registerShutdownHook() is written a new getBean() call be written anywhere in the main method without any exception.

Using the registerShutdownHook:

((AbstractApplicationContext) context).registerShutdownHook();
XML-based configuration
For XML-based configuration, rather than using the @PostConstruct and PreDestroy annotations, the init-method and destroy-method attributes are added to the bean tag in the XML file:

<bean id = "StudentDAO" class="StudentData.StudentDAO" init-method="init" destroy-method="destroy">
If there are multiple classes, the init and destroy method names are standardized to init and destroy and default-init-method and default-destroy-method are added to the beans tag.

<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context"
    xmlns:aop="http://www.springframework.org/schema/aop"
    xsi:schemaLocation="http://www.springframework.org/schema/beans
    http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
    http://www.springframework.org/schema/context
    http://www.springframework.org/schema/context/spring-context-3.0.xsd
    http://www.springframework.org/schema/aop
    http://www.springframework.org/schema/aop/spring-aop-3.0.xsd
    " default-init-method="init" default-destroy-method="destroy">
Interface-based configuration
InitializingBean and DisposableBean interfaces are implemented and afterPropertiesSet() and destroy() methods are overridden . Which are similar to the init and destroy methods.

StudentDAO:

package StudentData;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;

import javax.annotation.PostConstruct;
import javax.annotation.PreDestroy;

import org.springframework.beans.factory.DisposableBean;
import org.springframework.beans.factory.InitializingBean;

public class StudentDAO implements InitializingBean, DisposableBean{
    static String driver ;
    public static String getDriver() {
        return driver;
    }
    public static void setDriver(String driver) {
        System.out.println("setting Driver");
        StudentDAO.driver = driver;
    }
    static Connection con;
    static String url;
    static String userName;
    static String password;
    
    
    public static Connection getCon() {
        return con;
    }
    public static void setCon(Connection con) {
        StudentDAO.con = con;
    }
    public static String getUrl() {
        
        return url;
    }
    public static void setUrl(String url) {
        System.out.println("setting url");
        StudentDAO.url = url;
    }
    public static String getUserName() {
        return userName;
    }
    public static void setUserName(String userName) {
        System.out.println("setting userName");
        StudentDAO.userName = userName;
    }
    public static String getPassword() {
        return password;
    }
    public static void setPassword(String password) {
        System.out.println("setting password");
        StudentDAO.password = password;
    }
    void init() throws ClassNotFoundException, SQLException {
        System.out.println("Inside init method");       
        createConnection();
    }
    
    static void createConnection() throws ClassNotFoundException, SQLException {
        Class.forName(driver);
        con = DriverManager.getConnection(url,userName,password);
            
    }
    static void closeConnection() throws SQLException {
        con.close();
        
    }
    void getAllRecords() throws SQLException {
        String sql = "Select * from student;";
        PreparedStatement st = con.prepareStatement(sql);
        ResultSet rs = st.executeQuery();
        while(rs.next()) {
            System.out.println(rs.getInt(1)+ " "+rs.getString(2)+" "+rs.getString(3)+" "+rs.getString(4));
        }
        
        
    }
    public void afterPropertiesSet() throws Exception {
         System.out.println("Inside init method");      
         createConnection();
        
    }
    public void destroy() throws SQLException {
        System.out.println("Inside Destroy Method");
        closeConnection();
    }

}
The output of the code for all the implementations is as follows

setting Driver
setting userName
setting password
setting url
Inside init method
1 Krishna Urlaganti Computer Science
3 Stephen Hawking Physics
Inside Destroy Method
Student table:

student_id	first_name	last_name	major
1	Krishna	Urlaganti	Computer Science
3	Stephen	Hawking	Physics
------------	------------	-----------	------------------
Summary

The bean life cycle:

Spring IOC starts-> Bean Instantiated-> Dependencies Injected-> Internal Spring processing-> Custom init method( ready for use)-> Container shutdown initiated -> Custom destroy method -> Container shutdown completed.
All the operations like initialization can be done in the init method and all the closing persons can be done in destroy method
Annotation-based configuration, XML-based configuration and Interfaces can be used to implement init and destroy methods.


## Scopes Of A Bean
Learning Objectives

After completing this module, associates should be able to:

Define the scopes of a bean
Description

The scope of a bean defines its life cycle and visibility of that bean.

Bean Scopes
Singleton:
In singleton scope, the container creates a single instance of that bean.
It is defined explicitly by using @Scope("singleton") or @Scope(value= ConfigurableBeanFactory.SCOPE_SINGLETON)
All requests for that bean name will return the same object, which is cached.
Any modifications to the object will be reflected in all references to the bean.
This scope is the default value if no other scope is specified.
Note: Singleton scope creates a single instance of the class for an ApplicationContext whereas the Singleton Design pattern ensures that a single instance of the class is used for the entire application.

Prototype:
The prototype scope will return a different instance every time it is requested from the container.
It is defined by setting the value prototype to the @Scope("prototype") annotation or ``@Scopevalue=(ConfigurableBeanFactory.SCOPE_PROTOTYPE)` in the bean definition.
A prototype bean is not initialized by Spring IoC until an object is created.
If a Singleton class has a prototype class dependency, the prototype class is initialized along with the singleton class.
When a prototype class object is injected into the singleton class a single instance of the prototype class is created.
This can be avoided by using AOP proxy object and @Lookup.
Web Aware Scopes
Request:

The request scope creates a bean instance for a single HTTP request.
Session:

The session scope creates a bean instance for an HTTP Session.
Application:

The application scope creates the bean instance for the lifecycle of a ServletContext.
Websocket:

The WebSocket scope creates it for a particular WebSocket session.
Real World Application

Singleton scope is used for stateless beans (Business logic) and Prototype scope is used for stateful beans (Business logic and State).
Implementation

Singleton Example
A class named Singleton is created with singleton scope:

package com.bean.app;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Lookup;
import org.springframework.context.annotation.Scope;
import org.springframework.context.annotation.ScopedProxyMode;
import org.springframework.stereotype.Component;

@Component
public class Singleton {
    
    Singleton(){
        System.out.println("Singleton bean initialized");

}
App.java

package com.bean.app;

import org.springframework.context.ApplicationContext;
import org.springframework.context.support.ClassPathXmlApplicationContext;

public class App {
    
    public static void main(String[] args) {
        ApplicationContext context = new ClassPathXmlApplicationContext("spring.xml");
        Singleton s1 = (Singleton) context.getBean("singleton");
        Singleton s2 = (Singleton) context.getBean("singleton");
        
        if(s1==s2)
        {
            System.out.println(s1);
            System.out.println(s2);
            System.out.println("Same object is created");
        }
    }

}
Console:

Singleton

Prototype
Prototype.java

package com.bean.app;

import org.springframework.context.annotation.Scope;
import org.springframework.stereotype.Component;

@Component
@Scope(value = "prototype")
public class Prototype {
    Prototype(){
        System.out.println("Prototype Class initialized");
    }

    void printMessage() {
        System.out.println("Prototype");
    }
}
App.java

package com.bean.app;

import org.springframework.context.ApplicationContext;
import org.springframework.context.support.ClassPathXmlApplicationContext;

public class App {
    
    public static void main(String[] args) {
        ApplicationContext context = new ClassPathXmlApplicationContext("spring.xml");
        Prototype  p1 = (Prototype) context.getBean("prototype"); 
        Prototype  p2 = (Prototype) context.getBean("prototype"); 
        
        if(p1!=p2) {
            System.out.println(p1);
            System.out.println(p2);
            System.out.println("Objects are not same");
        }
        
        
    }

}

Console:

Prototype

Injecting prototype dependency to a singleton class
Singleton.java

package com.bean.app;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Lookup;
import org.springframework.context.annotation.Scope;
import org.springframework.context.annotation.ScopedProxyMode;
import org.springframework.stereotype.Component;

@Component
public class Singleton {
    
    @Autowired
    private Prototype prototype;
    
    
    
    public Prototye getPrototype() {
        return prototype;
    }
    
    public void setPrototype(Prototye prototype) {
        this.prototype = prototype;
    }
    Singleton(){
        System.out.println("Singleton bean initialized");
        
    }
    void printMessage() {
        System.out.println("Singleton");
    
    }
    

}
App.java

package com.bean.app;

import org.springframework.context.ApplicationContext;
import org.springframework.context.support.ClassPathXmlApplicationContext;

public class App {
    
    public static void main(String[] args) {
        ApplicationContext context = new ClassPathXmlApplicationContext("spring.xml");
        Singleton s1 = (Singleton) context.getBean("singleton");
        Singleton s2 = (Singleton) context.getBean("singleton");
        
        if(s1==s2)
        {
            System.out.println(s1.getPrototype());
            System.out.println(s2.getPrototype());
        }
        
        
    }

}
Console:

Prototype Dependency

Using AOP proxy
Prototype.java

package com.bean.app;

import org.springframework.context.annotation.Scope;
import org.springframework.context.annotation.ScopedProxyMode;
import org.springframework.stereotype.Component;

@Component
@Scope(value = "prototype", proxyMode = ScopedProxyMode.TARGET_CLASS)
public class Prototype {
    Prototype(){
        System.out.println("Prototype Class initialized");
    }

    void printMessage() {
        System.out.println("Prototype");
    }
}
Proxy

Using @Lookup
Singleton.java

package com.bean.app;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Lookup;
import org.springframework.context.annotation.Scope;
import org.springframework.context.annotation.ScopedProxyMode;
import org.springframework.stereotype.Component;

@Component
public class Singleton {
    
    @Autowired
    private Prototype prototype;
    
    
    
    public Prototype getPrototype() {
        return prototype;
    }
    
    public void setPrototype(Prototype prototype) {
        this.prototype = prototype;
    }
    Singleton(){
        System.out.println("Singleton bean initialized");
        
    }
    void printMessage() {
        System.out.println("Singleton");
    
    }
    
    @Lookup
    Prototype createPrototype() {
        return null;
    }

}
App.java

package com.bean.app;

import org.springframework.context.ApplicationContext;
import org.springframework.context.support.ClassPathXmlApplicationContext;

public class App {
    
    public static void main(String[] args) {
        ApplicationContext context = new ClassPathXmlApplicationContext("spring.xml");
        Singleton s1 = (Singleton) context.getBean("singleton");
        Singleton s2 = (Singleton) context.getBean("singleton");
        
        if(s1==s2)
        {
            System.out.println(s1.createPrototype());
            System.out.println(s2.createPrototype());
        }   
        
    }

}
Console:

Lookup

Summary

The visibility of bean is called as ban scope, Different Scopes of the bean are:
Singleton
Prototype
Request
Session
Application
WebSocket


## Lombok
Learning Objectives

After completing this module, associates should be able to:

Define Lombok
Description
Lombok
Lombok is a Java library that helps reduce boilerplate code in Java classes by providing annotations that automatically generate common methods such as getters, setters, constructors, equals, hashCode, and toString. It aims to improve code readability, maintainability, and productivity by eliminating the need to write repetitive and mundane code manually. Lombok works by plugging into our build process and auto-generating Java bytecode into the .class files as per several annotations we introduce in our code.

Features of Lombok
Getters, Setters and Constructors: In many IDE getters and setters can be auto-generated. However, the code is present in the class and if a new field is added the getters and setters could be generated for the respective field. If a field is renamed the getters and setters should be edited.

Lombok annotations @Getter and @Setter are used to generate getters and setters.
Lazy Getter: The process of getting the data only when it is needed is called lazy loading. The process of retrieving the data only when the corresponding getter for the field is needed is called a lazy getter.
@Getter(lazy=true) annotation can be used to create a lazy getter.
Core Java methods: The methods like toString(), equals() and hashCode() can be auto generated using lambok annotations @ToString and @EqualsAndHashCode.

Value Classes/DTOs: In some situations, a data type is defined to represent complex values as Data Transfer Objects(DTOs). In most cases these are immutable. Instead of adding the code for the constructor to take all the fields and check that they are not null Lombok annotations @RequiredArgsConstructor and @NonNull.

@Data Annotation: Lombok annotation @Data is a bundle of @ToString, @EqualsAndHashCode, @Getter / @Setter and @RequiredArgsConstructor

Configuring API: Instead of providing getters, setters and constructor methods, @Builder annotation in Lombok is used.

Checked Exceptions: Instead of using catch blocks or throws, @SneakyThrows annotation of lombok can be used.

Closing the resources after use: The @Cleanup annotation of Lombok is a good alternative for the try with resources and close().

Logging: Annotations like @Log4j and @Slf4j can be used using Lombok.

Thread Safe Methods: The @Synchronized annotation of Lombok is used to get an autogenerated, private, unexposed field.

Real World Application

In any application, the entities need setters and getters or constructors. Consider an entity class Student with fields id, firstName, lastName and major. All these fields need a setter and getters or constructors.
The process of generating setters and getters and maintaining those methods is tedious. This can be easily handled by Lombok so the developer can concentrate on the business logic and less on writing boilerplate code.
Implementation

Below is an example of using Lombok.

Adding Lombok to the Project
Lombok maven dependency is added to the pom.xml file.

<!-- https://mvnrepository.com/artifact/org.projectlombok/lombok -->
<dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
    <version>1.18.24</version>
    <scope>provided</scope>
</dependency>
The dependency of the required version can be taken from the website Lombok Maven Dependency

Using Lombok
A student entity class is simplified as follows:

package com.student.entity;

import javax.persistence.Entity;
import javax.persistence.Id;

import lombok.AllArgsConstructor;
import lombok.Getter;
import lombok.NoArgsConstructor;
import lombok.NonNull;
import lombok.Setter;

@Entity
@Getter
@Setter
@NoArgsConstructor
public class Student {
    @Id
    public String studentID;
    public String firstName;
    public String lastName;
    public String major;
    public Student(String studentID, String firstName, String lastName, String major) {
        this.studentID = studentID;
        this.firstName = firstName;
        this.lastName = lastName;
        this.major = major;
    }
    

}
The getters, setters, and no-args constructor will be provided during compilation.

An example of a lazy getter:

package com.student.entity;

import javax.persistence.Entity;
import javax.persistence.Id;

import lombok.AllArgsConstructor;
import lombok.Getter;
import lombok.NoArgsConstructor;
import lombok.NonNull;
import lombok.Setter;

@Entity
@Getter(lazy=true)
@Setter @NoArgsConstructor
public class Student {
    @Id
    public String studentID;
    public String firstName;
    public String lastName;
    public String major;
    public Student(String studentID, String firstName, String lastName, String major) {
        this.studentID = studentID;
        this.firstName = firstName;
        this.lastName = lastName;
        this.major = major;
    }
}
Summary

Lombok is a Java library that helps reduce boilerplate code in Java classes by providing annotations that automatically generate common methods
Lombok works by plugging into our build process and auto-generating Java bytecode into the .class files as per several annotations we introduce in our code.


## Overview Of Spring Boot
Learning Objectives

After completing this module, associates should be able to:

Understand the purpose and features of Spring Boot
Description

Spring Boot is a powerful framework within the Spring ecosystem that simplifies the development of Java-based applications. It makes developing web applications and microservices with Spring Framework faster and easier through three core capabilities:

Autoconfiguration.
An opinionated approach to configuration.
Additional features such as an embedded server and start packs.
user

Autoconfiguration
One of the key features of Spring Boot is its auto-configuration mechanism, which automatically configures beans and components based on the dependencies present in the classpath. We no longer need to write XML configuration as Spring Boot analyzes the environment and classpath to configure beans, reducing the need for manual configuration and improving productivity.

Opinionated
Spring Boot adopts an opinionated approach to configuration, providing sensible defaults and conventions for various aspects of application development. This allows developers to quickly bootstrap new projects and focus on writing business logic rather than spending time on boilerplate configuration.

Additional Features
Spring Boot comes with built-in support for embedded servers such as Tomcat, Jetty, and Undertow and it also provides a set of starter packs, which are curated sets of dependencies for common use cases such as web applications, data access, security, and messaging. Starter packs include all the necessary dependencies and configurations to get started with specific types of applications, making it easy to add functionality to projects without worrying about version compatibility or dependency conflicts.

Spring Boot also seamlessly integrates with other components of the Spring ecosystem, such as Spring Framework, Spring Data, Spring Security, and Spring Cloud. This allows developers to leverage the full power of Spring ecosystem technologies and frameworks to build robust and feature-rich applications.

Its simplicity, ease of use, and focus on convention over configuration make it a popular choice for building a wide range of applications, from simple REST APIs to complex microservices architectures.

spring boot

Comparison of the Spring Framework and Spring Boot
Spring Framework	Spring Boot
It is a comprehensive and modular framework for building enterprise Java applications	It is an opinionated framework built on top of the Spring Framework that aims to simplify and streamline the development of Spring-based applications
It provides a wide range of features and functionalities for various aspects of application development	It includes embedded server support and seamless integration with the Spring ecosystem
It requires developers to configure and manage many aspects of their applications manually	It is designed to be easy to use and quick to get started with because of auto-configuration
The Spring Boot Flow
spring

Real World Application

Understanding Spring Boot is essential for modern Java developers due to several reasons:

Rapid Application Development: Spring Boot simplifies the process of building Spring-based applications by providing a set of opinionated defaults and auto-configuration capabilities. Developers can quickly bootstrap new projects and focus on writing business logic instead of spending time on boilerplate configuration.
Microservices Architecture: Spring Boot is well-suited for building microservices-based architectures, where applications are developed as a set of small, independently deployable services. Its lightweight nature, embedded server support, and seamless integration with Spring Cloud enable developers to create scalable and resilient microservices applications efficiently.
Embedded Server Support: Spring Boot comes with embedded server support for popular web servers such as Tomcat, Jetty, and Undertow. This eliminates the need for manual server configuration and deployment, simplifying the deployment process and making it easier to run Spring applications as standalone executable JAR files.
Auto-Configuration: Spring Boot's auto-configuration feature automatically configures beans and components based on the dependencies present in the classpath. This reduces configuration overhead and eliminates the need for explicit configuration in many cases, leading to cleaner and more concise code.
Dependency Management: Spring Boot simplifies dependency management by providing a curated set of dependencies through starter packs. Starter packs include common libraries and configurations for specific use cases (e.g., web applications, data access, security), allowing developers to quickly add required dependencies to their projects without worrying about version conflicts or compatibility issues.
In summary, understanding Spring Boot is crucial for modern Java developers looking to build scalable, maintainable, and cloud-native applications efficiently. Its simplicity, productivity features, embedded server support, and seamless integration with the Spring ecosystem make it a popular choice for building a wide range of Java-based applications.

Implementation

Below is an example that demonstrates how quickly we can set up an application with Spring Boot. The example can be found here: https://spring.io/guides/gs/spring-boot

Create a Maven project and add the spring-boot-starter dependency to the pom.XML:
<dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter</artifactId>
      <version>3.2.5</version>
  </dependency>

Set Up Your Driver Class:
import java.util.Arrays;

import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.ApplicationContext;
import org.springframework.context.annotation.Bean;

@SpringBootApplication
public class App 
{
    public static void main( String[] args )
    {
        SpringApplication.run(App.class, args);
    }

    @Bean
    public CommandLineRunner commandLineRunner(ApplicationContext ctx) {
        return args -> {

            System.out.println("Let's inspect the beans provided by Spring Boot:");

            String[] beanNames = ctx.getBeanDefinitionNames();
            Arrays.sort(beanNames);
            for (String beanName : beanNames) {
                System.out.println(beanName);
            }

        };
    }
}

We can see the @SpringBootApplication annotation. This annotation tags the class as a source of bean definitions, enables auto-configuration, and tells the program to look for beans in other classes of the current package. The main() method uses Spring Boot’s SpringApplication.run() method to launch the application.

There is also a CommandLineRunner method marked as a @Bean, and this runs on start up. It retrieves all the beans that were created by your application or that were automatically added by Spring Boot. It sorts them and prints them out.

Output:


  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v3.2.5)

2024-04-30T09:36:38.608-05:00  INFO 9028 --- [           main] com.example.App                          : Starting App using Java 17.0.8.1 with PID 9028 (C:\Users\User\Desktop\demo\target\classes started by User in C:\Users\User\Desktop\demo)
2024-04-30T09:36:38.618-05:00  INFO 9028 --- [           main] com.example.App                          : No active profile set, falling back to 1 default profile: "default"
2024-04-30T09:36:40.693-05:00  INFO 9028 --- [           main] com.example.App                          : Started App in 3.011 seconds (process running for 4.229)
Let's inspect the beans provided by Spring Boot:
app
applicationAvailability
applicationTaskExecutor
commandLineRunner
fileWatcher
forceAutoProxyCreatorToUseClassProxying
lifecycleProcessor
org.springframework.aop.config.internalAutoProxyCreator
org.springframework.boot.autoconfigure.AutoConfigurationPackages
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration$ClassProxyingConfiguration
org.springframework.boot.autoconfigure.availability.ApplicationAvailabilityAutoConfiguration
org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration
org.springframework.boot.autoconfigure.context.LifecycleAutoConfiguration
org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration
org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration
org.springframework.boot.autoconfigure.internalCachingMetadataReaderFactory
org.springframework.boot.autoconfigure.sql.init.SqlInitializationAutoConfiguration
org.springframework.boot.autoconfigure.ssl.SslAutoConfiguration
org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration
org.springframework.boot.autoconfigure.task.TaskExecutorConfigurations$SimpleAsyncTaskExecutorBuilderConfiguration
org.springframework.boot.autoconfigure.task.TaskExecutorConfigurations$TaskExecutorBuilderConfiguration        
org.springframework.boot.autoconfigure.task.TaskExecutorConfigurations$TaskExecutorConfiguration
org.springframework.boot.autoconfigure.task.TaskExecutorConfigurations$ThreadPoolTaskExecutorBuilderConfiguration
org.springframework.boot.autoconfigure.task.TaskSchedulingAutoConfiguration
org.springframework.boot.autoconfigure.task.TaskSchedulingConfigurations$SimpleAsyncTaskSchedulerBuilderConfiguration
org.springframework.boot.autoconfigure.task.TaskSchedulingConfigurations$TaskSchedulerBuilderConfiguration
org.springframework.boot.autoconfigure.task.TaskSchedulingConfigurations$ThreadPoolTaskSchedulerBuilderConfiguration
org.springframework.boot.context.internalConfigurationPropertiesBinder
org.springframework.boot.context.properties.BoundConfigurationProperties
org.springframework.boot.context.properties.ConfigurationPropertiesBindingPostProcessor
org.springframework.boot.context.properties.EnableConfigurationPropertiesRegistrar.methodValidationExcludeFilter
org.springframework.boot.sql.init.dependency.DatabaseInitializationDependencyConfigurer$DependsOnDatabaseInitializationPostProcessor
org.springframework.context.annotation.internalAutowiredAnnotationProcessor
org.springframework.context.annotation.internalCommonAnnotationProcessor
org.springframework.context.annotation.internalConfigurationAnnotationProcessor
org.springframework.context.event.internalEventListenerFactory
org.springframework.context.event.internalEventListenerProcessor
propertySourcesPlaceholderConfigurer
simpleAsyncTaskExecutorBuilder
simpleAsyncTaskSchedulerBuilder
spring.info-org.springframework.boot.autoconfigure.info.ProjectInfoProperties
spring.lifecycle-org.springframework.boot.autoconfigure.context.LifecycleProperties
spring.sql.init-org.springframework.boot.autoconfigure.sql.init.SqlInitializationProperties
spring.ssl-org.springframework.boot.autoconfigure.ssl.SslProperties
spring.task.execution-org.springframework.boot.autoconfigure.task.TaskExecutionProperties
spring.task.scheduling-org.springframework.boot.autoconfigure.task.TaskSchedulingProperties
sslBundleRegistry
sslPropertiesSslBundleRegistrar
taskExecutorBuilder
taskSchedulerBuilder
threadPoolTaskExecutorBuilder
threadPoolTaskSchedulerBuilder

As we can see, our application already has a lot of autoconfiguration created for us.

Summary

Spring Boot provides a good platform for Java developers to develop a stand-alone Spring application that you can just run. The main aim of Spring Boot was to achieve the Auto-Configuration feature. With the help of this and other features, we can create a stand-alone Spring web application.


## Using spring initializr
Learning Objectives

After completing this module, associates should be able to:

Understand and use Spring Initializr
Description

Spring Initializr is a web-based tool provided by Spring, which allows developers to easily bootstrap a Spring Boot application. It sets up an application with the best practices of Spring, so you can focus more on your business code and less on the project setup. Some features of Spring Initializr are:

Easy project setup: Spring Initializr simplifies the process of setting up a new Spring project by generating a basic project structure and build configuration.

Project Customization: You can customize your Spring Boot project according to your needs. You can select the build system (Maven or Gradle), programming language (Java, Kotlin or Groovy), Spring Boot version, and dependencies.

Dependency Management: With Spring Initializr, you don't have to manually search and specify the dependencies in your pom.xml or build.gradle file. It provides a user-friendly interface to search and add required dependencies.

IDE Integration: Spring Initializr is integrated with popular IDEs like Eclipse, IntelliJ IDEA, and Visual Studio Code, which means you can create and import the project directly from your preferred IDE.

Command-Line Interface: For those who prefer to work from the command-line, Spring Initializr also provides a web API that can be accessed using tools like curl, HTTPie, or Spring Boot’s CLI tool.

Community Support: Spring Initializr is a part of the larger Spring community, which means you have access to a vast amount of resources, including documentation, Stack Overflow, and GitHub.

Real World Application

Spring Initializr is extensively used in the real-world development of Spring Boot applications. Here are a few common ways it's used:

Project Kick-off: Developers use Spring Initializr when starting a new Spring Boot project. It quickly generates a project with a standard structure, build configuration, and desired dependencies, which significantly speeds up the project initiation phase.

Learning and Experimentation: For learners and developers trying out new features or libraries, Spring Initializr is a handy tool. It allows them to quickly set up a project with specific configurations without going through the manual setup process.

Microservice Development: In a microservice architecture, multiple small services work together. Each service is a separate application, often a Spring Boot application. Spring Initializr is frequently used to bootstrap these individual services.

Workshops and Training: In workshops and training sessions, instructors often use Spring Initializr to set up the base project. This way, they ensure all participants have the same project setup, which simplifies the teaching process and allows focusing on the topic at hand.

Prototyping: When developers need to create a quick prototype to validate an idea or a design approach, Spring Initializr helps by providing a quick and easy way to generate a Spring Boot application with necessary dependencies.

Overall, Spring Initializr serves as a time-saving tool for Spring Boot developers, aiding in efficient and effective software development.

Implementation

Spring Initializr is a great tool for setting up a new Spring Boot project quickly and easily. In this guide, we'll go through the process of creating a new Spring Boot Maven project with Web and Spring Data JPA dependencies.

Note: The next steps are optional however, we will walk you through how to setup a Spring Project. You would need your own IDE setup locally to do the following steps.

Step 1: Open Spring Initializr
Open your web browser and go to Spring Initializr's website.

Step 2: Select Project Metadata
In the "Project Metadata" section, you'll see a few options:

Project: Choose Maven Project.
Language: Choose Java.
Spring Boot: Choose the version you want to use, typically the default latest stable version.
Project Metadata:
Group: Enter the group ID for your project, usually in the format of a reverse domain (like com.example).
Artifact: Enter the name for your project. This will also be the name of your project's root directory.
Name: This field is auto-filled based on your artifact but can be changed if necessary.
Description: Provide a short description of your project.
Package Name: This field is auto-filled based on your group and artifact. It's the package structure of your Java classes.
Packaging: Choose Jar to keep things simple and portable.
Java: Choose the version of Java you want to use.
Step 3: Add Dependencies
In the "Dependencies" section, search for and add the following:

Spring Web: It includes Spring MVC and Tomcat, allowing you to write both web apps and REST services.
Spring Data JPA: This dependency makes it easy to access data stored in relational databases.
Just start typing the names into the search box and then click on the matching result to add them.

Step 4: Generate Project
After all details are filled out, click on the Generate button at the bottom. This will download a ZIP file containing your new project.

Step 5: Extract and Open Project
Extract the downloaded ZIP file to a location of your choice. Then, open this project in your favorite IDE (like IntelliJ IDEA, Eclipse, or Visual Studio Code).

Your new Spring Boot Maven project is now set up with Web and Spring Data JPA dependencies and ready to be developed!

Running the Project
You can run the project from both the command line and within an Integrated Development Environment (IDE):

Command Line:
Open a terminal and navigate to the root directory of your project (the directory containing the pom.xml file).

Run the following command:

mvn spring-boot:run
This command uses Maven (indicated by the mvn prefix) to start your Spring Boot application.

From an IDE:
Most modern IDEs (such as IntelliJ IDEA, Eclipse, and VS Code) have built-in support for Maven and can directly import and run your project.

IntelliJ IDEA:
Open IntelliJ IDEA and click File > Open....
Navigate to your project directory and click Open.
Once the project is open, you should see a Maven tab on the right side. Under Plugins, find spring-boot and double-click on spring-boot:run.
Eclipse:
Open Eclipse and click File > Import....
In the import wizard, expand the Maven folder and choose Existing Maven Projects. Click Next.
Browse to your project directory and click Finish.
Right-click on the project in the Project Explorer, select Run As, and then Maven Build.... In the Goals field, enter spring-boot:run, then click Run.
VS Code:
Open VS Code, click on File > Open Folder... and navigate to your project directory.
Click on the Spring Boot Dashboard in the Activity Bar on the side, right-click your project and select Start.
Please note that you need to have the necessary plugins (Spring Boot Tools for VS Code, Spring Boot support in IntelliJ IDEA, M2Eclipse in Eclipse) installed in your IDE for these instructions to work.

Summary

Spring Initializr is a web-based tool provided by the Spring Framework team.
It offers a fast way to create Spring Boot projects with specific configurations.
Allows developers to choose the desired programming language (Java, Kotlin, Groovy), build system (Maven or Gradle), and Spring Boot version.
Provides a well-structured project, which adheres to the best practices of Spring Boot.
Ideal for both beginners learning Spring Boot and experienced developers who want to quickly bootstrap a Spring Boot application.


## Auto Configuration
Learning Objectives

After completing this module, associates should be able to:

Understand the purpose and functionality of auto-configuration in Spring Boot.
Recognize the role of @EnableAutoConfiguration or @SpringBootApplication annotations in enabling auto-configuration.
Identify how auto-configuration simplifies the application setup process by reducing explicit configuration.
Description

Auto-configuration is one of the most powerful features of Spring Boot. Some key points about auto-configuration:

It aims to drastically simplify the setup and configuration of Spring applications by automatically configuring your Spring application based on the libraries present on your project's classpath.
Auto-configuration is enabled with @EnableAutoConfiguration annotation, or more commonly with @SpringBootApplication which includes the former as part of its own definition.
It's important to note that auto-configuration is 'opinionated', meaning it makes certain assumptions about what you might need based on your classpath.
Spring Boot auto-configurations are designed to work well with default settings, but they can be manually overridden for customization according to application needs.
This feature works well with "Starters" provided by Spring Boot, which are a set of convenient dependency descriptors that you can include in your application.
Important Spring Boot Annotations
@SpringBootApplication:

It is a combination of three annotations @EnableAutoConfiguration, @ComponentScan, and @Configuration.

@SpringBootApplication
public class AssociateApplication {
    public static void main(String[] args) {
        ApplicationContext context = SpringApplication.run(AssociateApplication.class, args);
    }
}
@EnableAutoConfiguration:

It enables Spring Boot to auto-configure the application context. Therefore, it automatically creates and registers beans based on both the included jar files in the classpath and configures it to run the methods.

@Configuration
@EnableAutoConfiguration
public class AssociateApplication {
    public static void main(String[] args) {
        ApplicationContext context = SpringApplication.run(AssociateApplication.class, args);
    }
}
@ComponentScan:

When we develop an application, we need to tell the Spring framework to look for Spring components. @ComponentScan enables Spring to scan for things like configurations, controllers, services, and other components we define. The @ComponentScan annotation is used with the @Configuration annotation to specify the package for Spring to scan for components such as @Component, @Controller, @Service, @Repository.

@Configuration
@ComponentScan
public class AssociateApplication {
    public static void main(String[] args) {
        ApplicationContext context = SpringApplication.run(AssociateApplication .class, args);
    }
}
We can also use @ComponentScan as below here spring can be read the specified package.

@Configuration
@ComponentScan(basePackage = "com.revature")  
@Configuration:

The most important annotation in spring is the @Configuration annotation which indicates that the class has @Bean definition methods. So Spring container can process the class and generate Spring Beans to be used in the application. This annotation is part of the spring core framework.

@Configuration
public class AppConfig {
 
    @Bean(name="demoService")
    public DemoClass service() 
    {
        
    }
}
Real World Application

Auto-configuration in Spring Boot has transformed the way enterprise level applications are built and deployed. Here are some of its key benefits and real-world applications:

Speed Up Development Process: Auto-configuration eliminates the need for manual setup and configuration of common functionalities. This helps in accelerating the development process.

Focus on Business Logic: Developers can focus more on writing the business logic, rather than setting up and configuring the application.

Reduction in Code and Configuration Errors: As Spring Boot auto-configures many components based on classpath settings, the likelihood of configuration errors reduces drastically.

Scalable for Large Applications: Auto-configuration handles a lot of under-the-hood tuning based on the libraries detected in the classpath. This makes it more efficient and scalable for large enterprise applications.

Simplified Dependency Management: Together with Spring Boot Starters, auto-configuration simplifies dependency management. Starters bring required dependencies together, while auto-configuration sets up application components based on these dependencies.

Customizable: Despite its 'opinionated' approach, auto-configuration can be easily overridden and customized as per the specific needs of an enterprise application.

Integrated Developer Experience: Auto-configuration, combined with other Spring Boot features like embedded servers, provides a seamless and integrated developer experience.

In conclusion, auto-configuration is one of the primary reasons why Spring Boot has become a go-to framework for quick, robust, and efficient enterprise application development.

Implementation

Auto-configuration is a feature of Spring Boot that works out-of-the-box and doesn't need explicit code to be written to make it work. Instead, it's about understanding how it works and how you can customize it when necessary. Below is a step-by-step guide to understanding how to leverage auto-configuration in a Spring Boot application.

Note: The next steps are optional. You would need your own IDE setup locally to do the following steps.

Step 1: Create a New Spring Boot Project
Open your web browser and navigate to the Spring Initializr website at https://start.spring.io.
On the Spring Initializr page, you'll see a simple interface to configure your project.
Fill in the necessary information such as Group, Artifact, and Package names. These fields are used to define the project's structure and naming conventions.
Choose the desired project type. For a Java Maven project, select "Maven Project" from the "Project" dropdown.
Select the desired Spring Boot version. It's recommended to choose the latest stable version.
Add dependencies by searching for them in the "Search for dependencies" field. In this case, add the "Spring Web" and "Spring Data JPA" dependencies by typing their names and selecting them from the suggestions.
Once you've selected the desired dependencies, click the "Generate" button to download a ZIP file containing your project.
Extract the downloaded ZIP file to a location of your choice. This will create a new directory with the project structure and configuration files.
Step 2: Understand Auto-Configuration
Auto-configuration is activated when you add @SpringBootApplication annotation to your main class. This annotation includes @EnableAutoConfiguration which activates auto-configuration.

@SpringBootApplication
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
Step 3: View Auto-Configuration Report
To understand what configurations Spring Boot auto-configures for you, you can enable the auto-configuration report.

# In your application.properties file
debug=true
When you start the application, you'll see an auto-configuration report in the console showing which auto-configuration classes got applied and which did not, along with the reason.

Step 4: Customizing Auto-Configuration
Auto-configuration configures beans based on the jars in your classpath and other beans. However, if you define your own configuration, Spring Boot backs off and lets your configuration take precedence.

For example, Spring Boot auto-configures a DataSource if it sees database related jars in the classpath. But if you define your own DataSource bean, the auto-configured one will not be initialized.

@Bean
@ConfigurationProperties(prefix="com.example.datasource")
public DataSource dataSource() {
    return DataSourceBuilder.create().build();
}
Step 5: Excluding Auto-Configuration Classes
Sometimes, you may want to exclude a certain auto-configuration class entirely. This can be done using the exclude attribute of @SpringBootApplication or @EnableAutoConfiguration.

@SpringBootApplication(exclude = {DataSourceAutoConfiguration.class})
public class MyApplication { ... }
In this example, the DataSourceAutoConfiguration class will not be applied, and Spring Boot will not auto-configure a DataSource even if it sees database related jars in your classpath.

Remember, Spring Boot's auto-configuration is a powerful feature that makes it easier and quicker to create Spring applications, allowing developers to focus on the unique parts of their application rather than the repetitive and boilerplate configuration.

Remember to replace com.mycompany.app and my-app with your actual group ID and artifact ID. Also replace com.example.datasource with your actual datasource details.

Summary

Auto-configuration aims to drastically simplify the setup and configuration of Spring applications by automatically configuring your Spring application based on the libraries present on your project's classpath
It is 'opinionated', meaning it makes certain assumptions about what you might need based on your classpath
It is enabled with @EnableAutoConfiguration annotation, or more commonly with @SpringBootApplication
Spring Boot auto-configurations are designed to work well with default settings, but they can be manually overridden for customization according to application needs


## Common Spring Boot Starters
Learning Objectives

After completing this module, associates should be able to:

Understand the purpose and usage of Spring Boot starters
Learn to include commonly used starters for web development, database access, security, testing, etc.
Use Maven or Gradle to manage starter dependencies
Follow best practices for efficient usage of starters
Description

Spring Boot Starters are pre-configured dependencies that provide opinionated configurations and dependencies to simplify the development of specific functionalities in a Spring Boot application. They allow developers to quickly add required dependencies and configurations for common use cases without the need for manual configuration.

Examples of Spring Boot Starters include:

spring-boot-starter-web: Includes everything needed to build a web application, including embedded Tomcat or Jetty server, Spring MVC, and other web-related dependencies.
spring-boot-starter-data-jpa: Provides support for using Spring Data JPA for database access, including Hibernate as the default JPA implementation.
spring-boot-starter-security: Offers Spring Security configurations for securing applications, including authentication, authorization, and common security features.
spring-boot-starter-test: Includes dependencies and configurations for writing tests in a Spring Boot application, such as JUnit, Mockito, and Spring Test.
These starters simplify the development process by providing sensible defaults, auto-configuration, and dependency management. Developers can include the desired starters in their projects to quickly set up the required functionality without spending time on manual configuration.

Spring Boot Starters are highly customizable, allowing developers to override defaults, exclude specific dependencies, and provide their own configurations based on their application requirements.

Real World Application

Spring Boot Starters are widely used in real-world applications to accelerate the development process and provide robust functionality out-of-the-box. Here's an example scenario:

Let's consider the development of a social media application. In this application, we would like to include the following features:

Web interface for user registration, login, and posting updates
Database persistence for storing user data and posts
Security measures to protect user data and authenticate users
Automated testing to ensure the reliability of the application
To achieve these functionalities, we can leverage Spring Boot Starters:

We can use the spring-boot-starter-web starter to quickly set up a web interface with Spring MVC, handle user registration and login endpoints, and serve static resources.
The spring-boot-starter-data-jpa starter can be used for easy integration with a database, allowing us to persist user data and posts using Spring Data JPA and Hibernate.
With the spring-boot-starter-security starter, we can enable authentication and authorization features, securing the application's endpoints and protecting user data.
The spring-boot-starter-test starter provides essential dependencies and configurations for writing unit tests and integration tests, allowing us to verify the functionality of our application automatically.
By including these starters in our project, we can significantly reduce the time and effort required to set up these functionalities manually. Spring Boot's auto-configuration capabilities, coupled with the provided starters, ensure that the necessary dependencies, configurations, and defaults are readily available.

This real-world example highlights the practicality of Spring Boot Starters in rapidly developing robust web applications with minimal boilerplate code and configuration overhead.

Implementation

Note: The next steps are optional. You would need your own IDE setup locally to do the following steps.

Here's a step-by-step guide on how to use Spring Initializr to generate a Spring Boot project with some common Spring Boot starters.

Visit the Spring Initializr website at https://start.spring.io/.

Fill in the project metadata:

Choose your Project details (e.g., Group, Artifact, Package Name).
Select the desired Spring Boot version.
Configure project settings:

Choose the build system as Maven.
Select the programming language as Java.
Add dependencies:

In the "Dependencies" section, search for "Web" and select Spring Web.
Search for "JPA" and select Spring Data JPA.
You can also add any other required dependencies based on your project's needs.
Click on the "Generate" button to download the generated project as a ZIP file.

Extract the downloaded ZIP file to your desired project directory.

Open the extracted project directory in your preferred Integrated Development Environment (IDE).

Open the pom.xml file located in the project's root directory.

The pom.xml file will look something like this:

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.example</groupId>
    <artifactId>demo</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>demo</name>
    <description>Demo project for Spring Boot</description>

    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.5.2</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>

    <properties>
        <java.version>11</java.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-jpa</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <!-- Other dependencies based on your selections -->
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>
This pom.xml file includes the necessary dependencies for Spring Data JPA and Spring Web.

Customize the pom.xml file as needed, such as adding additional dependencies or modifying build configurations.

Build and run your Spring Boot application using your IDE or command-line tools.

By following these steps, you have generated a Spring Boot project using Spring Initializr with Maven and obtained the complete `pom.xml`` file with the required starter dependencies for Spring Data JPA and Spring Web.

Summary

Spring Starters are pre-configured dependencies provided by Spring Boot to simplify the setup of common functionalities in your applications.
They are opinionated and curated combinations of libraries that work together seamlessly.
Spring Starters provide a convenient way to add essential dependencies to your project without manually configuring each one.


## Overview Of Spring Mvc & Architecture
Learning Objectives

After completing this module, associates should be able to:

Understand Spring's MVC Architecture
Define the three main logical components: model, view, and controller
Understand MVC architecture's impact in web development
Description
MVC Architecture
This is an architectural design pattern that is leveraged by multiple programming languages and frameworks as the basis for handling requests from users. Below are the basic components of the MVC architecture.

Model: The component that deals with all the data-related logic or business logic.

View: This component deals with the UI logic of the application.

Controller: It is an interface between the Model and View. It is used to process business logic and incoming requests, manipulate data using the Model component and interact with views to give the final output.

Spring MVC
Spring MVC (Model-View-Controller) is a web framework provided by the Spring Framework for building web applications in Java which follows the MVC architectural pattern. In Spring MVC, the Model typically consists of POJOs (Plain Old Java Objects) or domain objects that represent the entities in the application domain. The View is typically implemented using template engines or another seperate application entirely, and controllers are implemented as classes annotated with @Controller or other stereotype annotations such as @RestController for creating RESTful web services.

In scenarios where views are not needed, such as building RESTful web services or providing APIs for client-server communication, you can create controller methods that return data directly without involving views. These controller methods typically return data objects (e.g., domain objects, DTOs) serialized into the desired format (e.g., JSON, XML) using libraries like Jackson.

Key Features
Annotation-based Configuration: Spring MVC provides support for annotating controllers and mapping request URLs to controller methods using annotations such as @RequestMapping, @GetMapping, @PostMapping, etc. This allows for clean and concise configuration of request mappings.
Data Binding and Validation: Spring MVC supports automatic data binding between HTTP request parameters and Java objects, as well as validation of form data using annotations.
View Resolvers: Spring MVC supports view resolvers, which are responsible for resolving logical view names to actual View implementations. This allows for flexible and configurable rendering of views, supporting various view technologies such as JSP, Thymeleaf, FreeMarker, etc.
Spring MVC also follows the Front Controller pattern. The Front Controller pattern is a design pattern commonly used in web applications to handle requests from clients and route them to the appropriate components for processing. In Spring MVC, the Front Controller is represented by the DispatcherServlet, which acts as the entry point for all incoming requests. When a request is received by the DispatcherServlet, it consults the request mappings configured in the application to determine which controller should handle the request. The DispatcherServlet then delegates the request to the corresponding controller for further processing.

Example Flow of a Spring MVC Application
SpringMVC Architecture

The request from the client is sent to our Embedded Tomcat Server which sends the request to the DispatcherServlet (Front Controller).
Based on the request, it is sent to a specific controller through the FrontController's delegation.
The FrontController is aware of all the controllers annotated with the @Controller annotation.
The response from the controller is a model, given back to the FrontController which then interacts with the view template to render a response to the User.
The model can be either in raw data formats such as JSON, XML, etc. or utilizes view technologies such as JSP.
In this architecture, as the DispatcherServlet(FrontController) handles all the requests and sends the response there is no direct interaction between the client and the controllers. As DispatcherServlet interacts with the view template, at any point in time the developer can change the view from JSP to thymeleaf or any other view template easily.

Real World Application

The Model-View-Controller (MVC) architectural pattern plays a crucial role in web development for several reasons:

Separation of Concerns: MVC promotes a clear separation of concerns by dividing an application into three distinct components: Model, View, and Controller. This separation allows developers to focus on different aspects of the application independently, making the codebase more modular, maintainable, and scalable.
Modularity and Reusability: By separating the application into three components, MVC encourages modularity and reusability of code. Developers can reuse models, views, and controllers across different parts of the application or in other projects, leading to faster development cycles and reduced duplication of code.
Scalability: MVC makes it easier to scale applications by isolating different components. For example, if the application experiences increased traffic, developers can scale the Controller layer to handle more requests without impacting the Model or View layers. Similarly, changes to one component (e.g., updating the business logic in the Model) can be made without affecting other components, allowing for easier maintenance and evolution of the application over time.
Testability: MVC promotes testability by separating concerns and isolating components. Each component can be unit tested independently, allowing developers to write focused, targeted tests for models, views, and controllers. This makes it easier to verify the correctness of individual components and ensure the overall quality of the application.
User Interface Design: MVC facilitates the design and development of user interfaces by separating presentation logic (View) from application logic (Model and Controller). Designers and frontend developers can focus on creating visually appealing and interactive user interfaces (views) without needing to understand or modify the underlying business logic (models and controllers).
Flexibility and Adaptability: MVC provides flexibility and adaptability to changing requirements and technologies. Because each component is independent, developers can easily swap out or upgrade components without affecting the rest of the application. For example, it's possible to change the view layer from HTML-based views to a JavaScript-based frontend framework (e.g., Angular, React) without touching the backend logic.
Overall, the MVC architectural pattern is essential for building robust, maintainable, and scalable web applications. It promotes best practices in software design, facilitates collaboration among developers, designers, and testers, and ultimately leads to better-quality software products.

Implementation

Example SpringMVC setup:

We can add a dependency for Spring Boot to utilize Spring Web in the pom.xml file:
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
</dependency>
We ca configure a Controller class with the appropriate @Controller annotation above the class definition. We can include a @RequestMapping annotation and @ResponseBody to the method signature as seen below for a simple HTTP GET Request.
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.ResponseBody;

@Controller
public class TestController {
    @RequestMapping(method = RequestMethod.GET)
    public @ResponseBody String testRequest(){
        return "Hello from the TestController";
    }
}
Once above is completed, we can run our Spring Boot application and make the request to http://localhost:8080.
NOTE: Be mindful if you've made any adjustments to your server.port number property in your application.properties/yaml.
Summary

Spring MVC is used to implement the Model-View-Controller architecture.
It provides a front controller named DispatcherServlet, which directly handles all the requests and responses for.
All the controllers are annotated with @Controller and are managed by the DispacherServlet(FrontController).
The response from the controller is a model.


## Dev Tools
Learning Objectives

After completing this module, associates should be able to:

Understand the benefits of using Spring Boot DevTools
Description

Spring Boot DevTools is a set of tools designed to improve the development experience for Spring Boot applications. It provides features that help developers to quickly iterate, debug, and reload their applications during the development process. Here are some key aspects of Spring Boot DevTools:

Automatic Restart: DevTools automatically restarts the Spring Boot application when changes are detected in the classpath. This eliminates the need to manually stop and restart the application after making code changes, speeding up the development workflow.
Live Reload: With Live Reload support, DevTools reloads the browser automatically when changes are made to static resources such as HTML, CSS, or JavaScript files. This allows developers to see the changes reflected immediately without manually refreshing the browser.
Remote Debugging: DevTools supports remote debugging, allowing developers to connect a debugger to a running Spring Boot application. This enables debugging of the application code in real-time, even in a production-like environment.
Development-time Configuration: DevTools provides development-time configuration options that can be used to customize the behavior of the application during development. For example, developers can disable certain features or enable additional logging for debugging purposes.
Property Defaults: DevTools sets sensible defaults for development properties, such as disabling caching and enabling debug logging. These defaults help optimize the development experience and ensure that the application behaves as expected during development.
Real World Application

Understanding Spring Boot DevTools is important for developers working on Spring Boot applications because it offers several benefits that can significantly improve the development workflow:

Faster Development: Spring Boot DevTools automates many tasks that developers would otherwise have to perform manually, such as restarting the application or refreshing the browser after making changes. This automation speeds up the development process and allows developers to focus more on writing code and less on repetitive tasks.
Improved Productivity: By providing features like automatic restart and Live Reload, DevTools enables developers to see their changes in real-time without having to manually stop and restart the application or refresh the browser. This immediate feedback loop increases productivity and allows developers to iterate more quickly.
Better Debugging Experience: DevTools supports remote debugging, allowing developers to connect a debugger to a running Spring Boot application. This feature is invaluable for troubleshooting issues in real-time and gaining insights into the application's behavior during development.
Enhanced Collaboration: DevTools simplifies the setup process for new developers joining a project by automatically adding development-specific dependencies and setting sensible defaults for development properties. This ensures consistency across development environments and facilitates collaboration within development teams.
Implementation

To work with Spring Boot DevTools, we have to add the below dependency in pom.xml:

<dependency>  
    <groupId>org.springframework.boot</groupId>  
    <artifactId>spring-boot-devtools</artifactId>  
</dependency>   
Now, when modifying and saving source code, the server picks up our changes and the application restarts as shown below:

Dev Tool

Summary

Spring Boot DevTools is a set of tools designed to improve the development experience for Spring Boot applications
It provides features that help developers to quickly iterate, debug, and reload their applications during the development process
Several features of DevTools are LiveReload, Automatic Restart, Remote Debugging, and more.


## Spring Environments
Learning Objectives

After completing this module, associates should be able to:

Understand how to use external configuration to use the same application code in different environments
Undertand configuration-specific annotations
Description

An application environment refers to the context in which a software application operates, such as the configuration settings that define the behavior application when it runs. Spring Boot allows to externalize configuration so we can work with the same application code in different environments. We can use external sources such as properties files, YAML files, environment variables, command-line arguments, and more, rather than hardcoding configuration values directly into the application code.

Configuration Annotations
Spring introduces the new @PropertySource annotation as a convenient mechanism for adding property sources to the environment.

We can use this annotation in conjunction with the @Configuration:

@Configuration
@PropertySource("classpath:application2.properties")
public class RevatureApplication {
    //.....
}
Another very useful way to register a new properties file is using a placeholder, which allows us to dynamically select the right file at runtime:

@PropertySource({ 
  "classpath:persistence-${envTarget:mysql}.properties"
})
Property values can be injected directly into your beans using the @Value annotation, accessed via Spring’s Environment abstraction or bound to structured objects via @ConfigurationProperties.

Injecting a property with the @Value annotation:

@Value( "${jdbc.url}" )
private String jdbcUrl;
Real World Application

Externalized configuration provides several benefits:

Flexibility: Externalized configuration allows you to change configuration settings without modifying the application code. This makes it easier to customize the behavior of your application for different environments (e.g., development, testing, production) or specific deployments.
Security: Externalizing sensitive configuration properties (such as database credentials, API keys, and encryption keys) to external sources helps improve security by reducing the risk of exposing sensitive information in code repositories or production environments.
Portability: By separating configuration from code, you can deploy the same application binary to different environments without modifications. Configuration can be customized for each environment using external sources, making deployments more consistent and predictable.
Ease of Management: Managing configuration settings in external files or environment variables simplifies configuration management tasks. Configuration files can be version-controlled, documented, and managed separately from the application code, making it easier to track changes and collaborate with team members.
Implementation

Below is an example of running a project with different configurations.

Let's say we have the below project. Notice there is an application.yaml file and an application2.yaml file in our src/main/resources directory. This file will be auto-detected by Spring Boot. We can refer to this file in our application to work with the properties from within it.

yml

Let's say the following code is within the application.yml file:

server.port = 8005
spring.jpa.show-sql = true
spring.datasource.url= jdbc:mysql://localhost:3306/revatureDB
spring.datasource.username=root
spring.datasource.password=root
spring.jpa.hibernate.ddl-auto=update
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL8Dialect
management.security.enabled=false
We can add the @PropertySource annotatoin as shown below to our Driver class:

package com.revature;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.PropertySource;

@SpringBootApplication
@PropertySource("classpath:application.yml")
public class RevatureApplication {

    public static void main(String[] args) {
        SpringApplication.run(RevatureApplication.class, args);
    }
}
We see the application start up just fine:

env

Similarly, we add different configurations in the application2.yml file:

server.port = 8009
spring.jpa.show-sql = true
spring.datasource.url= jdbc:mysql://localhost:3306/revatureDB1
spring.datasource.username=root
spring.datasource.password=root
spring.jpa.hibernate.ddl-auto=update
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL8Dialect
management.security.enabled=false
We can then run the program using this alternate file:

package com.revature;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.PropertySource;

@SpringBootApplication
@PropertySource("classpath:application2.yml")
public class RevatureApplication {

    public static void main(String[] args) {
        SpringApplication.run(RevatureApplication.class, args);
    }
}
Output:

envn

Just now we have executed the same application with two different configuration files. This is how we can use the different configuration files without touching our application logic.

Summary

Spring Boot allows to externalize configuration so we can work with the same application code in different environments
We can use external sources such as properties files, YAML files, environment variables, command-line arguments, and more, rather than hardcoding configuration values directly into the application code
Spring introduces the new @PropertySource annotation as a convenient mechanism for adding property sources to the environment
Property values can be injected directly into your beans using the @Value annotation


## @Controller MVC Annotations
Learning Objectives

After completing this module, associates should be able to:

Define the @Controller annotation
Description

In Spring MVC, the controller is a component responsible for handling incoming HTTP requests from clients and processing them. It acts as an intermediary between the user interface (typically a web browser) and the business logic of the application. The controller receives requests, processes them by invoking appropriate methods or services, and returns responses to the client.

Controllers in Spring MVC can be annotated with @Controller to signal to Spring that the class is a controller and should be managed by the framework. This annotation is a specialization of the @Component annotation and replaces the process of expanding any controller base class or referencing the Servlet-specific features.

Real World Application

Every Spring MVC application has multiple controllers to perform various business logic.
Consider any website. The login, signup and logout are a few examples for controllers.
Implementation

Below is an example of a basic controller in Spring MVC.

import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.GetMapping;

@Controller
public class HelloController {

    @GetMapping("/hello")
    public String hello() {
        return "hello"; 
    }
}
In the above example, we have a class annotated with the Controller annotation. Within it is one method that will run if an HTTP GET request is made to the /hello path. This method will return hello in the body of the HTTP response.

Summary

@Controller is a stereotype annotation that is similar to @Component. It provides additional information that a class is a controller.
In Spring MVC controllers are used to handle incoming HTTP requests from clients and process them.


## @requestmapping & @responsebody
Learning Objectives

After completing this module, associates should be able to:

Define the @RequestMapping and @ResponseBody annotations
Understand when each of these annotations are required
Description
RequestMapping
In Spring MVC, the @RequestMapping annotation is used to map HTTP requests to handler methods in controllers. It allows developers to define which method should handle a particular URL pattern and HTTP method (such as GET, POST, PUT, DELETE, etc.). The @RequestMapping annotation can be applied at the class level and/or method level.

You can use @RequestMapping at the class level to specify a common base URL for all handler methods within that controller. This is useful for grouping related endpoints under a common base path:

@Controller
@RequestMapping("/api")
public class MyController {
    // Handler methods...
}
You can use @RequestMapping at the method level to further specify the URL pattern and HTTP method for individual handler methods. This allows you to have multiple handler methods within the same controller, each handling different requests.

@Controller
public class MyController {
    
    @RequestMapping("/hello", method = RequestMethod.GET)
    public String hello() {
        // Handler logic...
    }

    @RequestMapping("/save", method = RequestMethod.POST)
    public String saveData() {
        // Handler logic...
    }
}
The @RequestMapping annotation provides attributes for configuring request mappings to further refine the mapping conditions. Some examples are:

value: This attribute specifies the URL pattern to which the handler method should be mapped.

@RequestMapping(value = "/hello")
method: This attribute specifies the HTTP request method(s) that the handler method should handle.

@RequestMapping(value = "/save", method = RequestMethod.POST)
params: This attribute specifies request parameters that must be present in the request for the handler method to be invoked.

@RequestMapping(value = "/users", params = "id")
ResponseBody
The @ResponseBody annotation in Spring MVC is used to indicate that the return value of a controller method should be serialized directly to the HTTP response body. This annotation is typically used at the class level in combination with the @Controller annotation to specify that the return value of the methods in the class should not be interpreted as a view name but rather as the response content itself. Alternatively, the annotation could be placed on methods rather than the class itself.

Example:

import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.ResponseBody;

@Controller
public class ExampleController {

    @RequestMapping("/hello")
    @ResponseBody
    public String hello() {
        return "Hello, World!";
    }
}
Real World Application

URLs like "/login", "/signup", "/user", "/admin" and "/addProduct" etc. can be mapped to methods using the @RequestMapping annotation.
The @ResponseBody ensures that the view type returned for each of the above requests will generate the appropriate response.
Implementation

Below is an example of using the @RequestMapping and @ResponseBody annotations.

@Controller
@RequestMapping("/api/users")
public class UserController {

    @RequestMapping("/register", method = RequestMethod.POST)
    public @ResponseBody User register(@RequestBody User newUser) {
        // Logic to register a new user
    }

    @RequestMapping("/login", method = RequestMethod.POST)
    public @ResponseBody User login(@RequestBody LoginRequest loginRequest) {
        // Logic to authenticate user login
    }

    @RequestMapping("/{userId}", method = RequestMethod.GET)
    public @ResponseBody User getUserProfile(@PathVariable Long userId) {
        // Logic to retrieve user profile
    }

    @RequestMapping("/{userId}", method = RequestMethod.PUT)
    public @ResponseBody User updateUserProfile(@PathVariable Long userId, @RequestBody User updatedUser) {
        // Logic to update user profile
    }

    // Other handler methods for user operations...
}
We can see clients can perform requests such as a POST request to /api/users/register or a POST request to /api/users/login using the @RequestMapping annotation. We can see the @ResponseBody annotation in the method definitions.

Summary

You can use @RequestMapping at the class level to specify a common base URL for all handler methods within that controller.
You can use @RequestMapping at the method level to further specify the URL pattern and HTTP method for individual handler methods.
The @RequestMapping annotation provides attributes for configuring request mappings to further refine the mapping conditions.
The @ResponseBody annotation in Spring MVC is used to indicate that the return value of a controller method should be serialized directly to the HTTP response body.


## Http Method Annotations
Learning Objectives

After completing the following module, associates should be able to:

Define HTTP method annotations
Differentiate these HTTP method annotations from @RequestMapping
Description

Spring MVC includes several convenience annotations for each of the HTTP request types. These annotations are all @RequestMapping annotated with their appropriate method value assigned already and have the same properties as the @RequestMapping annotation. The following request mapping annotations are available to use:

@GetMapping
@PostMapping
@PutMapping
@DeleteMapping
@PatchMapping
We can see that the naming conventions handles each of those respective HTTP request methods.

Real World Application

Each controller method can instead now include the convenience annotation for their particular request when called rather than the RequestMapping annotation. Examples are:

@PostMapping for "addProduct",
@GetMapping for "findAllProducts",
@DeleteMapping for "removeProduct", etc.
Implementation

Below is an example of using the HTTP method annotations.

@Controller
@ResponseBody
@RequestMapping("/api/users")
public class UserController {

    @PostMapping("/register")
    public User register(@RequestBody User newUser) {
        // Logic to register a new user
    }

    @PostMapping("/login")
    public User login(@RequestBody LoginRequest loginRequest) {
        // Logic to authenticate user login
    }

    @GetMapping("/{userId}")
    public User getUserProfile(@PathVariable Long userId) {
        // Logic to retrieve user profile
    }

    @PutMapping("/{userId}")
    public User updateUserProfile(@PathVariable Long userId, @RequestBody User updatedUser) {
        // Logic to update user profile
    }

    // Other handler methods for user operations...
}
We can see clients can perform requests such as a POST request to /api/users/register or a POST request to /api/users/login using the relevant HTTP method annotations.

Summary

@RequestMapping is at the core of each of the HTTP method annotations, incorporating all the same annotation properties with the "method" value defaulted to their respective HTTP request.
Utilize the new HTTP Method Annotations for each of our Methods accordingly
Spring MVC includes the following HTTP request Annotations:
@GetMapping
@PostMapping
@PutMapping
@DeleteMapping
@PatchMapping


## Request Parameters And Path Variables
Cumulative for the Request Parameters and Path Variables
Learning Objectives
Learning Objectives
Define Request Parameters with @RequestParam and Path Variables with @PathVariable
Differentiate the uses of @RequestParam and @PathVariable
Understand how the information from each annoation is being extracted from the URL
Showcase the implementation and real-world application of @PathVariable and @PathParam
Understand the various parameters for each annotation
Description
Description
@RequestParam and @PathVariable annotations are used to extract values from the request URI.

@RequestParam
@RequestParam extracts values from the query string.
It is encoded as the values are taken from a query string but not directly from the path.
@PathVariable
@PathVariable extracts values from the URI path.
As it extracts values from the URI path, it is not encoded.
Real World Application
Real-World Application
A simple application of @PathVariable is getting data from an endpoint that identifies an entity with a primary key like the student_id, employerr_id, branch_id and user_id etc.
Using RequestParam all the data provided by the user on the registration page and bind it to the request parameter.
Implementation
Implementation
Consider the same example user for student registration where our email acts as our unique identifier for each student.

@PathVariable will help users request student information through the displayInfo handler methods addition parameter "email" that must match exactly to the contents of the @GetMapping path value inside of the '{}', in this case "email".
@RequestParam will help users register through the submitInfo handler method to use of query parameters inside of their URL. Our method requires an @RequestParam attached to each defined method parameter requiring to be included in the URL query.
For a temporary solution, we will use an ArrayList studentList to store information in memory for student information requests and registrations.
WE would update the Student Controller to reflect the below.

Student Controller

package com.sample.controller;

import com.sample.model.Student;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.*;

import java.util.ArrayList;
import java.util.List;

@Controller
@RequestMapping("student")
public class StudentController {

    private List<Student> studentList = new ArrayList<>();

    {
        studentList.add(new Student("admin@mail.com", "IT",100,"adminPass"));
        studentList.add(new Student("Charles@mail.com", "Biology",72,"password"));
        studentList.add(new Student("Nick@mail.com", "Computer Science",18,"superPass"));
    }

    @GetMapping("info/{email}")
    public @ResponseBody Student displayInfo(@PathVariable String email){

        for (Student student:studentList){
            if(student.getEmail().equals(email))
                return student;
        }
        return null;

    }

    @PostMapping("submit")
    public @ResponseBody String submit(@RequestParam String email,
                                          @RequestParam String major,
                                          @RequestParam int age,
                                          @RequestParam String password){

        studentList.add(new Student(email, major, age, password));
        return "Successfully submitted";

    }
}
Once we made the updates we would be able to boot up our Spring Boot application and using Postman make the following requests:

A GET request to http://localhost:8080/student/info/admin@mail.com to invoke the displayInfo handler method

This should return back all the information about that student in the form of JSON information as seen in the image below:


A POST request to localhost:8080/student/submit?email=test@mail.com&major=testing&age=42&password=megaPass to invoke the submitInfo handler method.

This will register the new student by adding it to the temporary list that exists in memory. Invoked the GET request above to see the newly registered student by http://localhost:8080/student/info/{newly-registered-email-here}


Summary
Summary
Both @RequestParam and @PathVariable annotations are used to extract values from the request URI. But @RequestParam gets values from the query string and @PathVariable gets variables from URI.


## Request Body And @requestbody Annotation
Learning Objectives

After completing this module, associates should be able to:

Define usage of @RequestBody
Understand how the @RequestBody is unmarshalling JSON to a Java object
Description

The @RequestBody annotation in Spring MVC is used to indicate that a method parameter should be bound to the body of the HTTP request. It is typically used in controller methods to extract data from the request body and convert it into a Java object.

When a method parameter is annotated with @RequestBody, Spring MVC automatically extracts the content of the HTTP request body and converts it into the appropriate Java object based on the content type of the request. @RequestBody REQUIRES that the information provided matches an object available to be mapped to.

Example:

@PostMapping("/create")
public User createUser(@RequestBody User newUser) {

    // ... logic

}

Real World Application

A simple application of @RequestBody might be when information is being sent in a POST request, such as when a user is being created, and we want our handler method to directly translate the body of the request into a User Java object.
Implementation

Consider an example application where we have students that can register using their email as a unique identifier. @RequestBody will help users request updating their student information through the update handler methods by observing the information inside of the request body and unmarshalling the JSON information into a Java object.

Note: We will need a JSON processing library, like Jackson, as a dependency to our project to unmarshall JSON into Java objects.

Student Controller:

package com.sample.controller;

import com.sample.model.Student;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.*;

import java.util.ArrayList;
import java.util.List;

@Controller
@RequestMapping("student")
public class StudentController {

    private List<Student> studentList = new ArrayList<>();

    {
        studentList.add(new Student("admin@mail.com", "IT",100,"adminPass"));
        studentList.add(new Student("Charles@mail.com", "Biology",72,"password"));
        studentList.add(new Student("Nick@mail.com", "Computer Science",18,"superPass"));
    }

    @PutMapping("update")
    public @ResponseBody String update(@RequestBody Student updatedStudent){
        if(studentList.removeIf(student -> student.getEmail().equals(updatedStudent.getEmail()))) {
            studentList.add(updatedStudent);
            return "Successfully updated";
        }
        return "Email was not registered, check email and try again. Or register new student.";
    }

}
Below are examples of using this application:

We can include the following JSON object in our request body using an application like Postman:

{
    "email": "admin@mail.com",
    "major": "ADMIN",
    "age": 100,
    "password": "adminPass"
}
We can submit a PUT request to http://localhost:8080/student/update to invoke the update handler method

This should return back that it was successfully updated:


Summary

The @RequestBody annotation in Spring MVC is used to indicate that a method parameter should be bound to the body of the HTTP request.
When a method parameter is annotated with @RequestBody, Spring MVC automatically extracts the content of the HTTP request body and converts it into the appropriate Java object based on the content type of the request.


## Responseentity Class
Learning Objectives

After completing this module, associates should be able to:

Define and use the ResponseEntity class
Description

ResponseEntity is a class that is meant to represent the entire HTTP response, offering the ability to control anything included in the response such as status codes, headers, and the response body. This allows for a lot more flexibility when it comes to the following:

The body is included and can be given a generic to help ensure the type safety of the API's HTTP responses.
Optionally, we can include header information without requiring to directly touch the HttpServletResponse.
Incorporating status codes, especially for handler methods that may not always behave the same way.
HTTP Status Code
ResponseEntity allows you to specify the HTTP status code to be returned in the response. You can use predefined constants from the HttpStatus enum or provide a custom status code. For example, HttpStatus.OK, HttpStatus.CREATED, HttpStatus.NOT_FOUND, etc.

Headers
You can add custom headers to the response using the header() method of ResponseEntity. This allows you to include additional metadata in the response, such as content type, caching directives, authentication tokens, etc.

Body
ResponseEntity supports returning a response body along with the status code and headers. You can pass the body content as a parameter to the constructor or use the body() method to set the body content. The body can be of any type, including Java objects, collections, strings, etc.

The ResponseEntity class offers flexible control of our HTTP Response through several static methods such as:

.status() method takes in either the int status code or the HttpStatus enum
.body() allows for information & objects to be passed back in the HTTP response body
.header() which takes in two strings for the key-value pair in the HTTP Response headers
.build() for anything that doesn't have a body included with it
Example:

import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class ExampleController {

    @GetMapping("/hello")
    public ResponseEntity<String> hello() {
        String message = "Hello, World!";
        return ResponseEntity.ok(message);
    }

    @GetMapping("/error")
    public ResponseEntity<String> error() {
        String errorMessage = "Internal Server Error";
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                             .body(errorMessage);
    }
}

Real World Application

ResponseEntity helps build flexible responses by giving us complete control of our HTTP response. By utilizing the static methods in the ResponseEntity class, we can have the ability to create custom HTTP responses.

Implementation

Consider an example application where we have students that can submit their information. ResponseEntity<Object> will replace the method signature return type of our handler method, and Object would be replaced with that type that we intend on returning. Our return statement will now contain ResponseEntity static methods for constructing our HTTP response.

Student Controller:

package com.sample.controller;

import com.sample.model.Student;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.*;

import java.util.ArrayList;
import java.util.List;

@Controller
@RequestMapping("student")
public class StudentController {

    private List<Student> studentList = new ArrayList<>();

    {
        studentList.add(new Student("admin@mail.com", "IT",100,"adminPass"));
        studentList.add(new Student("Charles@mail.com", "Biology",72,"password"));
        studentList.add(new Student("Nick@mail.com", "Computer Science",18,"superPass"));
    }

    @PostMapping("submit")
    public @ResponseBody ResponseEntity<String> submitInfo(@RequestParam String email,
                                          @RequestParam String major,
                                          @RequestParam int age,
                                          @RequestParam String password){

        studentList.add(new Student(email, major, age, password));
        return ResponseEntity.status(201)
                .body("Successfully Registered");

    }

}
Summary

ResponseEntity offers flexible control of our HTTP Response through several static methods.


## Http Status Code & Exception Handling With @exception Handler
Learning Objectives

After completing this module, associates should be able to:

Respond with appropriate HTTP Status Codes using @ResponseStatus
Handle exceptions with @ExceptionHandler
Understand the use of the HttpStatus enum
Description

@ResponseStatus is a method level annotation in Spring Boot and is used to declare the HTTP status code to be returned by a handler method in case of a specific exception being thrown. Spring Boot uses the status code specified in the annotation to construct the HTTP response. You can specify any HTTP status code using the value attribute of the @ResponseStatus annotation. The value attribute takes in a value of type HttpStatus. The HttpStatus enum provides the "named" values for all of the status codes, i.e. HttpStatus.CREATED for status code 201, HttpStatus.Accepted for status code 202.

@ExceptionHandler is an annotation on the method level that handles any exceptions specified in the annotation properties. These properties can include a generic catch-all with the Exception.class, a single instance of any exception class, or an array of multiple exception classes.

Example of using both annotations:

import org.springframework.http.HttpStatus;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.bind.annotation.ResponseStatus;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class ExampleController {

    @ExceptionHandler(ResourceNotFoundException.class)
    @ResponseStatus(HttpStatus.NOT_FOUND)
    public String handleResourceNotFoundException(ResourceNotFoundException ex) {
        return "Resource not found: " + ex.getMessage();
    }

    // Other controller methods...
}

If no @ExceptionHandler or @ResponseStatus annotation is found for an exception, Spring Boot returns a default status code of 500 (INTERNAL_SERVER_ERROR) to indicate that an unexpected error has occurred.

Real World Application

The @ExceptionHandler and @ResponseStatus annotations in Spring Framework are essential for handling exceptions and customizing HTTP responses in web applications. Here's why they are important:

Granular Error Handling: With @ExceptionHandler, you can handle different types of exceptions differently. This granularity allows you to provide specific error messages, status codes, and responses based on the nature of the exception. For example, you can return a custom error message and a 404 status code for a ResourceNotFoundException, while returning a generic error message and a 500 status code for other exceptions.
Custom Responses: The @ExceptionHandler annotation gives you full control over the HTTP response sent back to the client when an exception occurs. You can customize the status code, headers, and body of the response to provide meaningful feedback to the user. This helps in communicating the cause of the error and guiding the user on how to proceed.
Fine-Grained Control: @ResponseStatus allows you to specify the HTTP status code to be returned for a specific exception. This gives you fine-grained control over the status codes returned by your application, ensuring that they align with the HTTP specification and the requirements of your API clients.
Cleaner Code: By using @ExceptionHandler and @ResponseStatus, you can keep your controller methods focused on handling business logic, while moving error handling logic to separate methods. This leads to cleaner and more maintainable code, with better separation of concerns.
Implementation

Consider an example application where we have students that can submit, update, or get their information. See the below controller handler methods we are starting with.

Student Controller:

package com.sample.controller;

import com.sample.model.Student;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.*;

import java.util.ArrayList;
import java.util.List;

@Controller
@RequestMapping("student")
public class StudentController {

    private List<Student> studentList = new ArrayList<>();

    {
        studentList.add(new Student("admin@mail.com", "IT",100,"adminPass"));
        studentList.add(new Student("Charles@mail.com", "Biology",72,"password"));
        studentList.add(new Student("Nick@mail.com", "Computer Science",18,"superPass"));
    }

    @GetMapping("info/{email}")
    public @ResponseBody ResponseEntity<Student> displayInfo(@PathVariable String email) {

        for (Student student:studentList){
            if(student.getEmail().equals(email))
                return ResponseEntity.status(HttpStatus.ACCEPTED)
                        .body(student);
        }

    }

    @PostMapping("submit")
    public @ResponseBody ResponseEntity<String> submitInfo(@RequestParam String email,
                                          @RequestParam String major,
                                          @RequestParam int age,
                                          @RequestParam String password){

        studentList.add(new Student(email, major, age, password));
        return ResponseEntity.status(201)
                .body("Successfully Registered");

    }

    @PutMapping("update")
    public @ResponseBody ResponseEntity<Void> update(@RequestBody Student updatedStudent){
        if(studentList.removeIf(student -> student.getEmail().equals(updatedStudent.getEmail()))) {
            studentList.add(updatedStudent);
            return ResponseEntity.noContent()
                    .header("update-status", "true")
                    .build();
        }
    }

}
In our controller handler methods, our responses should be able to reasonably handle any exceptions thrown at them and not simply return a String. As the above code works for successfull calls, the exceptions where we cannot find the student by email we cannot perform our GET or PUT requests to acheive the appropriate response. Along with this we may have issues with our POST requests that are missing required params.

Let's edit the controller's handlers so that we throw exceptions when necessary and create @ExceptionHandler methods to handle these exceptions:

package com.sample.controller;

import com.sample.model.Student;
import com.sample.exception.CustomException;
import org.springframework.http.HttpStatus;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.*;

import java.util.ArrayList;
import java.util.List;

@Controller
@RequestMapping("student")
public class StudentController {

    private List<Student> studentList = new ArrayList<>();

    {
        studentList.add(new Student("admin@mail.com", "IT",100,"adminPass"));
        studentList.add(new Student("Charles@mail.com", "Biology",72,"password"));
        studentList.add(new Student("Nick@mail.com", "Computer Science",18,"superPass"));
    }

    @GetMapping("info/{email}")
    @ResponseStatus(HttpStatus.ACCEPTED)
    public @ResponseBody Student displayInfo(@PathVariable String email) {

        for (Student student:studentList){
            if(student.getEmail().equals(email))
                return student;
        }
        throw new CustomException("Email was not found, check email and try again.");

    }

    @PostMapping("submit")
    @ResponseStatus(HttpStatus.CREATED)
    public @ResponseBody String submitInfo(@RequestParam String email,
                                          @RequestParam String major,
                                          @RequestParam int age,
                                          @RequestParam String password){

        studentList.add(new Student(email, major, age, password));
        return "Successfully submitted";

    }

    @PutMapping("update")
    @ResponseStatus(HttpStatus.NO_CONTENT)
    public @ResponseBody void update(@RequestBody Student updatedStudent){
        if(studentList.removeIf(student -> student.getEmail().equals(updatedStudent.getEmail()))) {
            studentList.add(updatedStudent);
            return;
        }
        throw new RuntimeException("Email was not registered, check email and try again. Or register new student.");
    }

    @ExceptionHandler({RuntimeException.class, CustomException.class})
    @ResponseStatus(HttpStatus.NOT_FOUND)
    public @ResponseBody String handleNotFound(RuntimeException ex) {
        return ex.getMessage();
    }

    @ExceptionHandler(MissingServletRequestParameterException.class)
    @ResponseStatus(HttpStatus.BAD_REQUEST)
    public @ResponseBody String handleMissingParams(MissingServletRequestParameterException ex) {
        return ex.getParameterName() + " is missing in the query parameters and is required.";
    }

}
First, we updated our displayInfo and update to instead throw a CustomException or RuntimeException. Then we included two @ExceptionHandlermethods, each to handle their respective exceptions. Along with this, we included @ResponseStatus annotations with the appropriate HttpStatus enum above our two new methods.

Summary

@ResponseStatus allowed us to include the appropriate http status code through the HttpStatus enum in Java when returning our HTTP Response to the user/frontend.
@ExceptionHandler is an annotation on the method level that handles any exceptions specified in the annotation properties.


## Restful Api Development With @restcontroller Annotation
Learning Objectives

After completing this module, associates should be able to:

Define RESTful API with @RestController
Understand @RestController's combination of @Controller and @ResponseBody to apply @ResponseBody to all methods by default
Description

@RestController is a specialized version of the controller that incorporates the @Controller and @ResponseBody resulting in easier controller implementation as your methods no longer require the need for including the @ReponseBody before the return type in each method signature.

Example:

import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class HelloController {

    @GetMapping("/hello")
    public String hello() {
        return "Hello, World!";
    }
}

Notice that we no longer need the @ReponseBody annotation anywhere in the class.

Real World Application

@RestController helps with rapid development and ensuring that a response is always give to the users based on the HTTP requests they've given to the API such as logging in, requesting information, updating information, registering, etc.

Implementation

Consider an example application where we have students that can submit their information. Note the use of RestController.

Student Controller:

package com.sample.controller;

import com.sample.model.Student;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.*;

import java.util.ArrayList;
import java.util.List;

@RestController
@RequestMapping("student")
public class StudentController {

    private List<Student> studentList = new ArrayList<>();

    {
        studentList.add(new Student("admin@mail.com", "IT",100,"adminPass"));
        studentList.add(new Student("Charles@mail.com", "Biology",72,"password"));
        studentList.add(new Student("Nick@mail.com", "Computer Science",18,"superPass"));
    }

    @PostMapping("submit")
    public ResponseEntity<String> submitInfo(@RequestParam String email,
                                          @RequestParam String major,
                                          @RequestParam int age,
                                          @RequestParam String password){

        studentList.add(new Student(email, major, age, password));
        return ResponseEntity.status(201)
                .body("Successfully Registered");

    }

}
Summary

@RestController offers a much easier implmentation of our controllers by handling the responsibility that all the methods will contain a @ResponseBody and automatically serialize return objects into HTTP responses.


# Spring Projects
##Spring Data Overview
Learning Objectives

After completing this module, you should be able to:

Understand what Spring Data is and how it can be used to interact with different types of databases.
Description

Spring Data is an umbrella project within the larger Spring ecosystem that aims to simplify database access and persistence in Spring-based applications. It provides a unified and consistent programming model for working with various data access technologies, including relational databases, NoSQL databases, and other data stores.

Here are some key aspects of Spring Data:

Abstraction Layer: Spring Data provides a unified abstraction layer on top of different data access technologies. It abstracts away the complexities of interacting with various databases and data stores, allowing developers to focus on application-specific business logic rather than low-level database interactions.
Repository Pattern: At the core of Spring Data is the Repository pattern, which provides a higher-level interface for interacting with data. Repositories encapsulate data access logic and provide methods for common CRUD (Create, Read, Update, Delete) operations as well as custom query methods.
Automatic Query Generation: Spring Data repositories can automatically generate queries based on method names or custom query annotation such as @Query. This eliminates the need for writing boilerplate SQL or JPQL queries and simplifies the development process.
Support for Various Data Stores: Spring Data supports a wide range of data stores, including relational databases (e.g., MySQL, PostgreSQL), NoSQL databases (e.g., MongoDB, Cassandra), key-value stores, document databases, and more. Each database technology is supported through a dedicated Spring Data module.
Real World Application

Knowing how to use Spring Data is important for several reasons:

Rapid Development: Spring Data provides a higher-level abstraction for interacting with databases, allowing developers to focus on business logic rather than boilerplate database code. This accelerates the development process and reduces time-to-market for applications.
Consistency: Spring Data promotes consistency in data access across different parts of an application and among team members. By providing a unified programming model and best practices for data access, it helps maintain a consistent and cohesive codebase.
Flexibility: Spring Data supports a wide range of data stores, including relational databases, NoSQL databases, and other data stores. This flexibility allows developers to choose the most suitable database technology for their application's requirements without having to learn new data access frameworks.
Testability: Spring Data promotes testability by providing support for writing unit tests and integration tests for data access code. Developers can easily test repository methods, custom queries, and persistence logic using standard testing frameworks and techniques.
Performance and Scalability: Spring Data offers features like caching, pagination, and lazy loading to optimize database performance and scalability. By leveraging these features, developers can build efficient and scalable applications that handle large volumes of data and concurrent users.
Implementation

Spring Data Commons provides all the common interfaces that are used to connect with different data stores.
Crud Repository
The key important interface in Spring Data Commons is the CrudRepository. It provides CRUD operations irrespective of databases. It extends Repository which is the base class for all the repositories providing access to databases.

All the methods in the CrudRepository interface are listed below:

public interface CrudRepository<T, ID> extends Repository<T, ID> {
    <T1 extends T> T1 save(T1 entity);
    <T1 extends T> Iterable<T1> saveAll(Iterable<T1> entities);
    Optional<T> findById(ID id);
    boolean existsById(ID id);
    Iterable<T> findAll();
    Iterable<T> findAllById(Iterable<ID> ids);
    void deleteById(ID ids);
    void delete(T entity);
    void deleteAll(Iterable<? extends T> entities);
    void deleteAll();
}
PagingAndSortingRepository
The other interface in Spring Data is PagingAndSortingRepository. PagingAndSortingRepository provides options to Sort the data using the Sort interface and options to paginate using the Pageable interface, which provides methods for pagination getPageNumber(), next(), previousOrFirst() ,getPageSize()etc.

public abstract interface PagingAndSortingRepository extends CrudRepository {
public Iterable findAll(Sort s);
public Page findAll(Pageable p);
}
Custom Repositories
We can create a custom repository extending any of the repository classes like Repository, PagingAndSortingRepository, and CrudRepository. For example:

interface StudentRepository extends CrudRepository<Student, Long> { }
Spring Data Implementation
The following are Spring Data Modules which are specific to the databases.

Spring Data JPA: Connect to relational databases using ORM frameworks.
Spring Data MongoDB: Repositories for MongoDB.
Spring Data REST: Used for RESTful resources around Spring Data repositories.
Spring Data JPA
Spring Data JPA helps in connecting relational databases using ORM frameworks. The dependency we use:

<dependencies>
  <dependency>
    <groupId>org.springframework.data</groupId>
    <artifactId>spring-data-jpa</artifactId>
  </dependency>
<dependencies>
public interface JpaRepository<T1, ID> 
    extends PagingAndSortingRepository<T1, ID>, 
            QueryByExampleExecutor<T1>
            { }
Some of the additional methods it provides are shown below.

<T1 extends T> T1 saveAndFlush(T1 entity);
void deleteInBatch(Iterable<T> entities);
void deleteAllInBatch();
Spring Data MongoDB
Spring Data MongoDB provides support for using MongoDB as a database. The key interface is MongoRepository.

public interface MongoRepository<T, ID> extends PagingAndSortingRepository<T, ID>, QueryByExampleExecutor<T> 
{ }
Some of the important methods are.

    @Override
    <T1 extends T> List<T1> findAll(Example<T1> example);
    @Override
    <T1 extends T> List<T1> findAll(Example<T1> example, Sort sort);
Spring Data REST
Spring Data REST can be used for REST web services that connect to Spring Data repositories An example using JPA is shown below.

@RepositoryRestResource(collectionResourceRel = "example", path = "example")
public interface ExampleRepository
        extends PagingAndSortingRepository<Example, Long> {

    List<Example> findByUser(@Param("user") String user);

}
example request is shown below:

URL : http://localhost:8080/example?user=bob123
Summary

Spring Data is one of the key projects from the Spring Framework.
Its purpose is to make it easy for storing and retrieving the different kinds of relational and non-relational data from databases by providing abstractions/interfaces.
Some of the modules of Spring Data are Spring Data JPA, Spring Data MongoDB, Spring Data REST, etc.

## Relationship Between Jpa Hibernate And Spring Data Jpa
Learning Objectives

After completing this module, associates should be able to:

Explain what Java Persistence API (JPA), Hibernate, and Spring Data JPA are, and identify their main functions and features.
Understand and articulate the relationship between JPA, Hibernate, and Spring Data JPA.
Differentiate between JPA, Hibernate, and Spring Data JPA in terms of use cases, advantages, and limitations.
Description

The Java Persistence API (JPA), Hibernate, and Spring Data JPA are three key technologies used in Java applications to interact with databases. While they are distinct, they work together to provide a robust and simplified way to access and manipulate data.

Java Persistence API (JPA)
JPA is a specification that defines a standard for Object-Relational Mapping (ORM) in Java. It outlines the mapping between Java classes and database tables, allowing developers to interact with databases using Java objects. JPA provides a set of annotations and APIs to define how objects are mapped to tables, define relationships between objects, and perform basic CRUD operations.

Hibernate
Hibernate is a popular open-source ORM framework that provides an implementation for JPA. It goes beyond the standard specifications and offers additional features such as caching, dirty checking, lazy loading, and more. Hibernate converts Java objects to SQL statements, allowing developers to interact with databases using Java code instead of SQL.

Spring Data JPA
Spring Data JPA, a part of the larger Spring Data family, makes it easier to implement JPA-based repositories. It is a data access abstraction that works on top of the ORM, providing a way to reduce the boilerplate code. With Spring Data JPA, developers can perform complex database operations without having to write a lot of code. It provides ready-to-use interfaces and allows developers to define custom queries just by naming a method in a certain way.

The Relationship
In a typical Spring Data JPA setup, JPA provides the standard specifications, Hibernate provides the implementation of those specifications, and Spring Data JPA provides the abstraction to simplify data access even further.

These technologies work together to provide a comprehensive data access solution in Java. Developers can write code in a simple and consistent way across different types of databases (relational, NoSQL, etc.), reducing the amount of boilerplate code and the potential for errors.

Real World Application

In enterprise-level applications, the integration of JPA, Hibernate, and Spring Data JPA is crucial in building robust and efficient data access layers. Here are a few real-world applications:

eCommerce Platforms
Consider a large eCommerce platform like Amazon or eBay. They have complex data models with numerous relationships between different entities such as users, products, orders, reviews, etc. The use of JPA and Hibernate allows these complex relationships to be handled smoothly, mapping these entities to database tables. Spring Data JPA simplifies the data access layer, making it easier to implement the repositories required to manage these entities.

Content Management Systems
Content management systems (CMS) like WordPress or Drupal also benefit from these technologies. They need to manage a variety of content types, such as posts, pages, comments, tags, and users. JPA and Hibernate can handle the mapping of these entities, while Spring Data JPA can reduce the boilerplate code required to implement the data access layer.

Banking and Financial Systems
Banking and financial systems deal with a multitude of entities like customers, accounts, transactions, and more. These systems need robust and efficient data access mechanisms to ensure data integrity and consistency. The combination of JPA, Hibernate, and Spring Data JPA allows these systems to manage their data efficiently, with transactions and concurrency control provided by JPA and Hibernate. Spring Data JPA's easy-to-use repositories streamline the development of the data access layer.

Enterprise Resource Planning (ERP) Systems
ERP systems like SAP manage a vast amount of data across different domains like finance, HR, sales, procurement, etc. JPA, Hibernate, and Spring Data JPA can be used to model the complex relationships between different entities and manage them efficiently. The abstraction provided by Spring Data JPA can significantly reduce the code complexity, making it easier to maintain and extend the system.

In all these scenarios, the use of JPA, Hibernate, and Spring Data JPA reduces the amount of code that needs to be written, simplifies the data access layer, and ensures efficient and reliable interaction with the database.

Implementation

This example will demonstrate a Spring Data JPA project with Hibernate as the JPA provider. Let's assume that the following dependencies are used in the project: Spring Web, Spring Data JPA, and a database driver. This example will use MySQL.

Database Configuration
In the application.properties file, the datasource and JPA are configured:

spring.datasource.url=jdbc:mysql://localhost:3306/mydb
spring.datasource.username=root
spring.datasource.password=root

spring.jpa.hibernate.ddl-auto=update
spring.jpa.show-sql=true
Entity Classes
To represent a database entity, we create a new class. For example, if you're creating a simple book management system, you might have a Book entity like this:

import javax.persistence.Entity;
import javax.persistence.GeneratedValue;
import javax.persistence.GenerationType;
import javax.persistence.Id;

@Entity
public class Book {

    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private Long id;
    private String title;
    private String author;

    // Getters and setters omitted for brevity
}
Creating a Repository
We can create a new interface that extends JpaRepository:

import org.springframework.data.jpa.repository.JpaRepository;

public interface BookRepository extends JpaRepository<Book, Long> {
}
Now we can use the BookRepository to perform CRUD operations.

Using the Repository in a Service
We can now autowire the repository in a service class and use it:

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

@Service
public class BookService {

    @Autowired
    private BookRepository bookRepository;

    public List<Book> getAllBooks() {
        return bookRepository.findAll();
    }
    
    public Book saveBook(Book book) {
        return bookRepository.save(book);
    }

    // Additional methods omitted for brevity
}
In these steps, you've seen how to set up a Spring Data JPA project with Hibernate. We've created a simple entity and a repository, and used the repository in a service class to perform CRUD operations.

Summary

JPA is a specification that defines a standard for Object-Relational Mapping (ORM) in Java.
Hibernate is a popular open-source ORM framework that provides an implementation for JPA
Spring Data is a data access abstraction that works on top of the ORM, providing a way to reduce the boilerplate code.

## Jparepository Vs Crud Repository
Learning Objectives

After completing this module, associates should be able to:

Understand the basic concept and purpose of Spring Data JPA's CrudRepository and JpaRepository.
Describe the main differences between CrudRepository and JpaRepository.
Understand how to perform CRUD operations using both repositories.
Customize queries using both CrudRepository and JpaRepository.
Understand the relationship between CrudRepository, JpaRepository, and the Spring Data ecosystem as a whole.
Description

In Spring Data JPA, CrudRepository and JpaRepository are two interfaces used for creating repository implementations automatically. They provide a layer of abstraction to the underlying data persistence system and allow you to perform CRUD operations without the need for boilerplate code.

CrudRepository
CrudRepository is a simple interface provided by Spring Data JPA that you can use to perform CRUD operations on an entity. It provides methods like save(), findOne(), findAll(), count(), delete(), and exists() which are used for accessing and manipulating database records.

Here is an example of a CrudRepository:

public interface PersonRepository extends CrudRepository<Person, Long> {
}
JpaRepository
JpaRepository is an interface provided by Spring Data JPA, extending both the CrudRepository and PagingAndSortingRepository interfaces.

As it contains all methods provided by these two interfaces, it offers a complete suite of functionalities including not only basic CRUD operations, but also additional capabilities for pagination and sorting.

Moreover, JpaRepository comes with some JPA-specific features not found in CrudRepository. For example, it provides methods to flush the persistence context, as well as to refresh, save and flush an entity in one method call.

One important feature is its ability to handle locking instances for concurrent access. It also supports creating instances of TypedQuery for an already given query, or instances of Query for a native SQL statement.

Here is a simple example of a JpaRepository:

public interface PersonRepository extends JpaRepository<Person, Long> {
}
By using JpaRepository, you will have access to a wider array of JPA functionalities, which could be beneficial for more complex applications. However, it's important to note that these additional features come at the cost of increased complexity compared to CrudRepository.

Comparison
While both CrudRepository and JpaRepository play an important role in simplifying database operations, the choice between the two generally depends on the specific needs of your project.

CrudRepository: Provides basic CRUD operations. This is a good choice for simple applications where only basic database operations (Create, Read, Update, Delete) are required.

JpaRepository: Extends CrudRepository and adds additional JPA-specific features. It provides functionalities like flushing and locking mechanisms, and the ability to write criteria queries. If your application requires these more advanced features, then JpaRepository is the appropriate choice.

In conclusion, if your application only requires basic CRUD operations, CrudRepository is a lighter and simpler choice. However, for more complex applications where advanced JPA features are required, JpaRepository would be the way to go.

Real World Application
Web Applications
Web applications that require a persistent backend database extensively use Spring Data JPA. Whether it's an e-commerce site where you need to store user data, orders, and product details, or a content management system, Spring Data JPA provides the necessary tools to handle CRUD operations with ease.

Microservices
Microservices often require their own databases. Spring Data JPA is typically used in this scenario to handle the data persistence needs of individual services.

Enterprise Applications
Enterprise applications with complex business operations often require a robust and reliable ORM tool. Spring Data JPA, combined with Hibernate, can cater to these needs, making it a popular choice in the enterprise world.

Use Cases
User Management System: In a user management system, there are often complex relationships between entities (such as users, roles, and permissions). Spring Data JPA makes managing these relations easier with its easy-to-use and feature-rich APIs.

E-Commerce Applications: For an e-commerce application, managing products, orders, and customer details can become cumbersome with plain SQL. Spring Data JPA can streamline these operations, enhancing readability and maintainability of the code.

Content Management Systems: CMSs often involve frequent database interactions. Here, Spring Data JPA aids in simplifying the code base by abstracting the boilerplate code.

Data Analytics Applications: In data analytics applications where data transactions are complex and heavy, Spring Data JPA's transaction management features come in handy.

Spring Data JPA is a powerful tool in the Java ecosystem. While it's not suitable for every use case, it shines in applications where there is a need to interact with relational databases in an efficient, maintainable, and flexible manner.

Implementation

This is an example of the process of implementing JpaRepository or CrudRepository using Spring Data JPA.

Step 1: Create a Spring Boot Application
You can use Spring Initializr to create a new Spring Boot application. Make sure to include Spring Data JPA as a dependency.

Alternatively, you can add the dependencies manually in your pom.xml:

<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-jpa</artifactId>
    </dependency>

    <!-- If using an H2 in-memory database -->
    <dependency>
        <groupId>com.h2database</groupId>
        <artifactId>h2</artifactId>
        <scope>runtime</scope>
    </dependency>
</dependencies>
Step 2: Configure your DataSource
In your application.properties file, define your database configuration.

spring.datasource.url=jdbc:h2:mem:testdb
spring.datasource.driverClassName=org.h2.Driver
spring.datasource.username=sa
spring.datasource.password=password
spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
Step 3: Create an Entity
We will create a simple Person entity. Create a class and annotate it with @Entity. This is to denote that this class will be mapped to a database table.

import javax.persistence.Entity;
import javax.persistence.GeneratedValue;
import javax.persistence.Id;

@Entity
public class Person {

    @Id
    @GeneratedValue
    private Long id;
    private String name;
    private String email;

    // getters and setters
}
Step 4: Create a Repository
You can create a repository interface that extends either CrudRepository or JpaRepository. For this guide, let's use JpaRepository.

import org.springframework.data.jpa.repository.JpaRepository;

public interface PersonRepository extends JpaRepository<Person, Long> {
}
Step 5: Use the Repository
You can now use the repository in your service classes to perform database operations. Here's a simple example:

import org.springframework.stereotype.Service;
import org.springframework.beans.factory.annotation.Autowired;

@Service
public class PersonService {

    private final PersonRepository personRepository;

    @Autowired
    public PersonService(PersonRepository personRepository) {
        this.personRepository = personRepository;
    }

    public List<Person> getAllPeople() {
        return personRepository.findAll();
    }

    public Person savePerson(Person person) {
        return personRepository.save(person);
    }

    // more methods as needed
}
For a typical Spring Boot application, you wouldn't directly interact with repositories or services from the main method. The main method's purpose is to bootstrap the Spring Application context and start the application. However, if you want to interact with the repository or services for some testing or quick prototyping purposes, you can do so by obtaining the ApplicationContext and getting the bean you need.

Here is an example:

```java
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.ApplicationContext;

@SpringBootApplication
public class DemoApplication {

    public static void main(String[] args) {
        ApplicationContext ctx = SpringApplication.run(DemoApplication.class, args);

        PersonService personService = ctx.getBean(PersonService.class);

        Person person = new Person();
        person.setName("John Doe");
        person.setEmail("john.doe@example.com");

        // Save the person using the service
        personService.savePerson(person);

        // Print all people
        List<Person> people = personService.getAllPeople();
        for (Person p : people) {
            System.out.println(p.getName());
        }
    }
}
```
This code will start a Spring Boot application and then use the PersonService to create a new Person and then print all the people. Please remember that this is not the usual way to use Spring Boot, as typically the database operations would be triggered by web requests, scheduled tasks, or other events rather than directly from the main method.

With these steps, you have implemented a simple application that uses either CrudRepository or JpaRepository. You can create, read, update, and delete Person entities using the methods provided by the repository interface.

Summary

In Spring Data JPA, CrudRepository and JpaRepository provide a layer of abstraction to the underlying data persistence system and allow you to perform CRUD operations without the need for boilerplate code.
CrudRepository is a simple interface provided by Spring Data JPA that you can use to perform CRUD operations on an entity.
JpaRepository extends both the CrudRepository and PagingAndSortingRepository interfaces, meaning it contains all methods provided by these two interfaces, as well as some JPA-specific features.


## Property Expressions
Learning Objectives

After completing this module, associates should be able to:

Understand the concept of property expressions in Spring Data JPA and JpaRepository.

Learn how to derive queries directly from method names using property expressions.

Understand how to handle property expressions for nested properties and handle related exceptions.

Become proficient in resolving ambiguity in property expressions when multiple properties could match.

Description

In Spring Data JPA, property expressions are an integral part of the query derivation mechanism. They are used in method names to express the properties that the query should consider.

What are Property Expressions?
Property expressions refer to the chaining of properties within method names. These expressions form the findBy, readBy, getBy, and queryBy parts in method names defined in the Spring Data JPA repository interfaces.

For example, if we have a User entity with a name property, and we want to find users based on their name, we would use a property expression like so:

public interface UserRepository extends JpaRepository<User, Long> {
    List<User> findByName(String name);
}
In this case, name in findByName is a property expression.

Handling Nested Properties
Property expressions can also handle nested properties. Suppose our User has an Address entity associated with it:

public class User {
    // other fields...

    private Address address;
}
And the Address entity has a country field. We can find users based on their address's country by using a nested property in the method name:

public interface UserRepository extends JpaRepository<User, Long> {
    List<User> findByAddress_Country(String country);
}
In this case, Address_Country in findByAddress_Country is a nested property expression.

Resolving Ambiguity
Sometimes, a property expression could match more than one property (e.g., when the properties have the same name but are in different nested paths). Spring Data JPA provides a @Param annotation to disambiguate in these cases. You can specify which property the method parameter should be bound to:

public interface UserRepository extends JpaRepository<User, Long> {
    @Query("select u from User u where u.address.country = :country")
    List<User> findByCountry(@Param("country") String country);
}
Property expressions are a powerful feature in Spring Data JPA that help in deriving queries from method names. They provide a convenient and intuitive way to declare methods in your repository to fetch data based on entity properties, including nested ones.

Real World Application

Property expressions in Spring Data JPA provide a quick and readable way to write queries using method names in the repositories. These can be used in a multitude of real-world applications where querying the database based on certain entity properties is required.

Example: E-Commerce Application
Consider an e-commerce application where there are User, Product, and Order entities. Users can place orders for various products. The Order entity might look something like this:

public class Order {
    private Long id;
    private User user;
    private Product product;
    private LocalDate date;
    // other fields, getters, and setters...
}
We can use property expressions in an OrderRepository to find orders based on properties of User or Product.

public interface OrderRepository extends JpaRepository<Order, Long> {
    List<Order> findByUser_Name(String userName);
    List<Order> findByProduct_Name(String productName);
    List<Order> findByDate(LocalDate date);
}
In this repository:

findByUser_Name(String userName) uses the property expression User_Name to find all orders placed by a user with a specific name.

findByProduct_Name(String productName) uses the property expression Product_Name to find all orders that contain a specific product.

findByDate(LocalDate date) uses the property expression Date to find all orders placed on a specific date.

This way, property expressions can simplify the process of writing complex queries, especially in a real-world application with numerous entities and relationships.

Implementation

Property expressions in Spring Data JPA allow us to create complex database queries just by defining methods in the repository interface. Here's a step-by-step guide on how to implement this feature:

Step 1: Define Your Entity
First, define your entities. Here's an example of a User entity with an embedded Address:

@Entity
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private Long id;
    private String name;
    
    @Embedded
    private Address address;
    
    // Getters and Setters...
}
The Address entity could look like this:

@Embeddable
public class Address {
    private String street;
    private String city;
    private String country;
    
    // Getters and Setters...
}
Step 2: Define Your Repository
Define your UserRepository that extends the JpaRepository interface:

public interface UserRepository extends JpaRepository<User, Long> {
}
Step 3: Create Methods Using Property Expressions
You can then create methods in your UserRepository using property expressions to form the query:

public interface UserRepository extends JpaRepository<User, Long> {
    List<User> findByName(String name);
    List<User> findByAddress_Country(String country);
}
In these methods:

findByName(String name) uses the property expression Name to find all users with a specific name.

findByAddress_Country(String country) uses the nested property expression Address_Country to find all users living in a specific country.

Step 4: Use the Repository
Finally, you can use your repository in a service or a controller to get the data from the database. Here's an example of a UserService that uses UserRepository:

@Service
public class UserService {
    private final UserRepository userRepository;

    public UserService(UserRepository userRepository) {
        this.userRepository = userRepository;
    }

    public List<User> getUsersByName(String name) {
        return userRepository.findByName(name);
    }

    public List<User> getUsersByCountry(String country) {
        return userRepository.findByAddress_Country(country);
    }
}
That's it! You have successfully used property expressions in Spring Data JPA to create complex queries directly from method names in your repository.

Summary

Property expressions refer to the chaining of properties within method names.
Property expressions can also handle nested properties.
Sometimes, a property expression could match more than one property (e.g., when the properties have the same name but are in different nested paths). Spring Data JPA provides a @Param annotation to disambiguate in these cases.


## Annotations
Learning Objectives

After completing this module, associates should be able to:

Understand how Spring Data Annotations are useful for decreasing the amount of effort the developer needs to invest in setting up or configuring a project.
Description

Spring Data annotations are used to configure and customize various aspects of data access in Spring applications. These annotations are part of the Spring Data project, which provides a simplified and unified programming model for interacting with different types of data stores. Here are some commonly used Spring Data annotations and their purposes:

@Entity: This annotation is used to mark a Java class as an entity, which represents a table in a relational database. It is typically used in conjunction with JPA (Java Persistence API) to map Java objects to database tables.
@Repository: This annotation is used to indicate that a class is a repository, which is responsible for performing CRUD (Create, Read, Update, Delete) operations on entities. Spring Data repositories are interfaces that extend Repository or its subinterfaces and are automatically implemented by Spring at runtime.
@Query: This annotation is used to define custom query methods in Spring Data repositories. It allows developers to write JPQL (Java Persistence Query Language) or native SQL queries directly within repository interfaces, using method names as placeholders for query parameters.
@Param: This annotation is used to specify named parameters in @Query annotations. It allows developers to reference method parameters by name in query methods, making the queries more readable and maintainable.
@Transactional: This annotation is used to mark a method, class, or interface as transactional, indicating that the method should be executed within a transactional context. It is typically used in service layer classes to demarcate transactional boundaries around business logic.
Real World Application

Knowing Spring Data annotations is important for several reasons:

Data Access Configuration: Spring Data annotations allow developers to configure and customize various aspects of data access in Spring applications. By understanding these annotations, developers can effectively configure repositories, define custom queries, map entities to database tables, and specify transactional behavior.
Consistency: Spring Data annotations promote consistency in data access across different parts of an application and among team members. By adhering to a common set of annotations and best practices, developers can maintain a consistent and cohesive codebase, making it easier to understand, debug, and maintain.
Flexibility: Spring Data annotations provide a flexible and extensible framework for data access configuration. Developers can customize repositories, define custom queries, map entities to database tables, and specify transactional behavior according to their application's requirements, providing flexibility and adaptability to changing business needs.
Testability: Spring Data annotations facilitate unit testing and integration testing of data access code. By understanding these annotations, developers can write comprehensive tests to validate repository methods, custom queries, and persistence logic, ensuring the reliability and robustness of their applications.
Implementation

Spring Data Annotations
Spring Data provides an abstraction over data storage technologies. Hence the business logic code is independent. Below are some examples of Spring Data annotations.

@Id
@Id marks a field in a model or an entity class as the primary key. Since it’s implementation-independent, it makes an entity class easy to use with multiple data stores.

class Associate{

    @Id
    private Long id;
    private String name;

      // Getters and Setters 
    
}
@Param
A parameter annotated with @Param must have a value string matching the corresponding query parameter name.

@Repository
public interface AssociateRepository extends CrudRepository<Student, Serializable> {

    @Query("select a from Associate a where a.name = :name")
    List<Associate> findByName(@Param("name") String name);
@Transactional
Transactional is to configure the transactional behavior of a method, we can do it with @Transactional annotation.

@Transactional
    public List<Associate> saveAllAssociates(List<Associates> associateList) {
        List<Associate> response = (List<Associate>) associateRepository.saveAll(associateList);
        return response;
    }

Spring Data JPA Annotations
Below are some example of Spring Data JPA annotations.

@Query
Using @Query, we can provide a JPQL implementation for a repository method

@Query("SELECT COUNT(*) FROM Person p")
long getPersonCount();
We can also use named parameters:

@Query("FROM Person p WHERE p.name = :name")
Person findByName(@Param("name") String name);
We can also use native SQL queries if we set the native query argument to true.

@Query(value = "SELECT AVG(p.age) FROM person p", nativeQuery = true)
int getAverageAge();
Summary

Spring Data annotations are used to configure and customize various aspects of data access in Spring applications.


## @transactional
Learning Objectives

After reviewing this lesson you should be able to:

Explain how the Spring @Transactional work in place of JDBC and hibernate frameworks.
Description

A transaction is a type of request which will perform multiple operations or queries against a database as a single unit of work. Transactions typically result in a state change to the database (newly created, removed or updated data), but all operations for a transaction must be successful, or the entire transaction will fail.

The @Transactional annotation is a declarative transaction management annotation in Spring which acts as a key element in managing database transactions more simply. When applied to a method or class, @Transactional instructs the Spring framework to manage transactions automatically. When applied at the method level, it specifies that the annotated method should be executed within a transactional context. When applied at the class level, it specifies that all public methods of the class should be transactional. Using this annotation allows developers to control various aspects of transaction management, such as propagation behavior, isolation level, and rollback rules.

Propagation: Propagation behavior determines how transactions propagate between nested method calls or when calling multiple transactional methods within the same transactional context. The propagation attribute of @Transactional allows developers to specify different propagation behaviors, such as REQUIRED, REQUIRES_NEW, NESTED, etc.
Isolation Level: Isolation level defines the degree of isolation between concurrent transactions. The isolation attribute of @Transactional allows developers to specify the desired isolation level, such as READ_COMMITTED, REPEATABLE_READ, SERIALIZABLE, etc.
Rollback Rules: Rollback rules determine under what conditions a transaction should be rolled back. The rollbackFor and noRollbackFor attributes of @Transactional allow developers to specify exception types that should trigger a rollback or should not trigger a rollback, respectively.
Please note, if we use Spring without Spring Boot, we need to activate the transaction management by annotating the application class with @EnableTransactionManagement. In Spring Boot we use the @Transactional annotation instead.

Simple queries (selecting all records from a database with no other operations taking place) do not necessarily require the use of transaction management tools, although, the inclusion may still be beneficial depending on the type of operations performed.

Real World Application

Transaction management is a vital skill for developers working with databases, especially when providing functionality for complex queries and provides further control through workflows involved with data persistence. Real-world applications manage transactions for a myriad of reasons, from propagating behavior across an application, to security concerns, read-only access, allowing for time-outs during transactions for efficiency or security, or providing safety nets against human error through roll-backs.

Implementation

Here, we can see some examples of leveraging Spring’s built-in Transaction management annotations.

Example of syntax for a Spring project without a Spring Boot:

@Configuration
@EnableTransactionManagement
public class Example {

    @Bean
    public PlatformTransactionManager taxManager() {
        return yourTaxManager; 
    }

}
Example of syntax for a project with Spring Boot:

public class AssociateService {

    @Transactional
    public Long registerAssociate(Associate associate) {

        // Query to the database to check if the username already exists and         
        // inserts the Associate into the database and retrieves the autogenerated id
        // AssociateDao.save(associate);

        return id;
    }

}
The code example above would be equivalent to the following implementation using JDBC:

public class AssociateService {

    public Long registerAssociate(Associate associate) {
        Connection connection = dataSource.getConnection(); 
        try (connection) {
            connection.setAutoCommit(false); 

            // Query to the database to check if the username already exists and         
            // inserts the Associate into the database and retrieves the autogenerated id
            // AssociateDao.save(associate);

            connection.commit();

        } catch (SQLException e) {
            connection.rollback();
        }
    }
}

Please note, the @Transactional annotation can be applied to either an individual class method, or it may be applied to a class, in which case all public methods from the class will be executed with the transactional context:

import org.springframework.transaction.annotation.Transactional;

@Transactional
public class ExampleService {
    public void doBusinessLogic() {
        // Business logic here
    }
}

// The transaction will be started before doBusinessLogic() is executed and committed after it completes.
Example using @Transactional attributes:

import org.springframework.transaction.annotation.Transactional;

@Service
public class UserService {

    @Autowired
    private UserRepository userRepository;

    @Transactional(propagation = Propagation.REQUIRED, isolation = Isolation.READ_COMMITTED)
    public void updateUser(User user) {
        // Perform database operations
        userRepository.save(user);
    }

    // Other methods...
}
Example of using rollback rules:

import org.springframework.transaction.annotation.Transactional;
import org.springframework.dao.DataAccessException;

@Service
public class UserService {

    @Autowired
    private UserRepository userRepository;

    @Transactional(rollbackFor = DataAccessException.class)
    public void updateUser(User user) {
        try {
            // Perform database operations
            userRepository.save(user);
        } catch (DataAccessException e) {
            // Handle exception
            // Optionally, perform additional error handling or logging
            throw e; // This will trigger a rollback
        }
    }

    // Other methods...
}

Summary

The @Transactional annotation is a declarative transaction management annotation in Spring which acts as a key element in managing database transactions more simply.
When applied to a method or class, @Transactional instructs the Spring framework to manage transactions automatically.
Using this annotation allows developers to control various aspects of transaction management, such as propagation behavior, isolation level, and rollback rules.


## Acid Properties Of Transactions
Learning Objectives

After completing the following module, associates should be able to:

Understand the ACID properties of a transaction
Use the @Transactional annotation
Description

ACID is an acronym that stands for atomicity, consistency, isolation, and durability.

Atomicity: All the operations performed within the transaction get executed or none of them.

Consistency: Consistency ensures that your transaction takes a system from one consistent state to another consistent state.

Isolation: ensures that the transaction is isolated from other transactions.

Durability: Durability ensures that your committed changes get persisted.

Relational databases support ACID transactions, and the JDBC specification enables you to control them. Spring provides annotations and different transaction managers to integrate transaction management into their platform and to make it easier to use.

Real World Application

Understanding ACID (Atomicity, Consistency, Isolation, Durability) principles and how to use the @Transactional annotation in Spring are crucial for building robust and reliable database transactions in enterprise applications. Here's why:

Data Integrity: ACID principles ensure data integrity by guaranteeing that database transactions are processed reliably and consistently. Atomicity ensures that transactions are either fully completed or fully rolled back in case of failure, preserving data consistency.
Concurrency Control: ACID principles, particularly Isolation, help manage concurrent access to shared data. Understanding isolation levels such as Read Uncommitted, Read Committed, Repeatable Read, and Serializable is essential for controlling the visibility of changes made by concurrent transactions.
Transaction Management: The @Transactional annotation in Spring provides a declarative way to manage transactions in Spring-based applications. It allows developers to demarcate transactional boundaries around methods or classes, ensuring that database operations within those boundaries are executed atomically and consistently.
Error Handling and Rollback: By using @Transactional, developers can specify how transactions should behave in case of errors. They can configure rollback behavior, propagation rules, and exception handling strategies to ensure that transactions are handled gracefully and consistently.
Consistency Across Layers: @Transactional allows developers to define transactional behavior at the service layer, promoting a separation of concerns and encapsulation of business logic. This ensures consistency and reliability across multiple layers of the application, from controllers to repositories.
Implementation
Transactions with JDBC
The three important things to control ACID properties are

void setAutoCommit(boolean status): It is true by default means each transaction is committed by default.
void commit(): commits the transaction.
void rollback(): cancels the transaction.
Example:

import java.sql.*;  
class AssociateRecords{  
public static void main(String args[])throws Exception{  
Class.forName("com.mysql.jdbc.Driver");  
Connection con=DriverManager.getConnection(  
"jdbc:mysql://localhost:3306/sample","root","root");

con.setAutoCommit(false);  
  
Statement stmt=con.createStatement();  
stmt.executeUpdate("insert into associates values(1,'jhon')");  
stmt.executeUpdate("insert into associates values(2,'smith')");  

con.commit();  
con.close();  
}
}  
Starting a transaction by getting a Connection and deactivating the auto-commit. This gives you control over the database transaction. Otherwise, you will automatically execute each SQL statement within a separate transaction.
Commit a transaction by calling the commit() method on the Connection interface. This tells your database to perform all required consistency checks for the changes permanently.
Roll back all operations performed during the transaction by calling the rollback() method on the Connection interface. You usually perform this operation if an SQL statement failed or if you detected an error in your business logic.
jdbc transaction 

Transactions with Spring
Spring provides all the boilerplate code that is required to start, commit, or rollback a transaction.
Spring also integrates with Hibernate and JPA transaction handling. Using Spring Boot, we use a @Transactional annotation on each interface, method, or class that will be executed within a transactional context.
In Spring without Spring Boot, you need to activate the transaction management by annotating your application class with @EnableTransactionManagement.
Examples :

@Service
public class AssociateService {

  private final AssociateRepository associateRepository;

  public AssociateService(AssociateRepository associateRepository) {
    this.associateRepository = associateRepository;
  }

  @Transactional
    public void updateAssociaterNameTransaction() {         
        Associate associate = AssociateRepository.findById(1L).get(); 
        associate.setName("revature");     
    } 
}
@Transactional annotation tells Spring that a transaction is required to execute this method.
The @Transactional annotation supports a set of attributes that you can use to customize the behavior.
 @Transactional(readOnly = true)
@Transactional(propagation = Propagation.REQUIRES_NEW)
Summary

Spring Boot and Spring Data JPA provide an easy to use transaction handling. You only need to annotate your interface, class, or method with Spring's @Transactional annotation.
Relational databases support ACID transactions, and the JDBC specification enables you to control them. Spring provides annotations and different transaction managers to integrate transaction management into their platform and to make it easier to use.


## Transaction Propagation Strategies
Learning Objectives

After completing this module, associates should be able to:

Understand how to specify the propogation type of a transaction in Spring Data
Description

Propagation defines our business logic transaction limit. Spring can start and pause a transaction according to our propagation setting. Spring will call the TransactionManager::getTransaction to create a transaction according to the propagation. It also supports some of the propagations for all types of TransactionManager.

Different types of propagations and usages are discussed below.

REQUIRED is to join an active transaction or to start a new one. This is the default behavior.

SUPPORTS is to join an active transaction if one exists. This method will be executed without a transactional context.

MANDATORY is to join an active transaction if one exists or to throw an Exception if the method gets called without an active transaction.

NEVER is to throw an Exception if the method gets called in the context of an active transaction.

NOT_SUPPORTED is to pause an active transaction and execute the method without any transactional context.

REQUIRES_NEW is to start a new transaction for the method always. If the method gets called with an active transaction. that transaction gets suspended until this method got executed.

NESTED is to start a new transaction, If it gets called with an active transaction, Spring sets a savepoint and rolls back to that savepoint if an Exception occurs.

Real World Application

Knowing transaction propagation strategies is crucial for building complex and reliable database transactions in enterprise applications. Here's why understanding transaction propagation is important:

Control Over Transaction Scope: Transaction propagation strategies allow developers to control the scope and behavior of transactions. They determine how transactions are propagated across different layers of the application, such as service methods, allowing for fine-grained control over transactional boundaries.
Consistency and Data Integrity: By understanding transaction propagation, developers can ensure data consistency and integrity across multiple database operations. Properly managing transaction boundaries helps prevent data corruption, concurrency issues, and inconsistent states in the database.
Concurrency Control: Transaction propagation strategies play a vital role in managing concurrent access to shared resources. They help define isolation levels and ensure that transactions are executed in a consistent and predictable manner, even in a multi-threaded or distributed environment.
Implementation

Below is an example of setting the propogation type of a transaction:

@Service
public class AssociateService {
 
    private AssociateRepository associateRepository;
 
    public AssociateService(AssociateRepository associateRepository) {
        this.associateRepository = associateRepository;
    }
 
    @Transactional(propagation = Propagation.REQUIRES_NEW)
    public void updateAssociateNameTransaction() {
        Associate associate = associateRepository.findById(10).get();
        associate.setName("name");
    }
}
Propogation.REQUIRES_NEW indicates that a new transaction must be created. If there is a current transaction, it is suspended.

Summary

Spring can start and pause a transaction according to our propagation setting.
Spring will call the TransactionManager::getTransaction to create a transaction according to the propagation.
The different propagation types are REQUIRED, SUPPORTS, MANDATORY, NEVER, NOT_SUPPORTED, REQUIRES_NEW, and NESTED


## Spring Boot Actuator Overview
Learning Objectives

After completing this module, associates should be able to:

Understand what Spring Boot Actuator is and it's benefit within enterprise level applications
Description

Spring Boot Actuator is a sub-project of the Spring Boot Framework that aims to assist you in monitoring and managing your application when it's pushed to production.

Key Features
End Points: Actuator provides several built-in endpoints (such as /health, /info, /metrics, etc.) for various monitoring purposes. Each one provides different insights into the application.

Health Check: It provides detailed health information about the application. This is beneficial in determining whether or not the application is running as expected.

Metrics Gathering: It collects and provides various metrics about the application, such as memory usage, garbage collection, web request statistics, and many more. These can be used to identify potential issues or bottlenecks.

Application Info: Provides detailed information about the application, such as version number, Git commit ID, and others.

Loggers: Actuator provides the ability to view and modify the log level of your application in real-time.

Auditing: It automatically audits events in your application and accesses it through the /auditevents endpoint.

HTTP Tracing: Provides trace information about the last 100 HTTP request-response exchanges via /trace endpoint.

Security
Actuator endpoints expose sensitive information of your application, so it’s always recommended to secure your Actuator endpoints.

Spring Boot Actuator is an essential tool for monitoring and managing Spring Boot applications, making it a must-have tool for any serious Spring Boot developer.

Real World Application

Here are some real-world applications of Spring Boot Actuator:

Continuous Monitoring: Operations teams can continuously monitor the health of the application to identify any issues or failures quickly.
Integration with Monitoring Tools: Health checks provided by Spring Boot Actuator can be integrated with external monitoring tools like Prometheus, Grafana, or Datadog for comprehensive monitoring and alerting.
Performance Optimization: Developers can use metrics to identify performance bottlenecks and optimize critical parts of the application.
Capacity Planning: Operations teams can use metrics to plan for capacity upgrades based on trends in resource usage over time.
Debugging and Troubleshooting: Developers and operations teams can use logging endpoints to troubleshoot issues and debug problems in production.
Compliance and Security: Audit endpoints provide insights into application security and compliance by tracking changes and access to sensitive data.
Configuration Management: Operations teams can use environment information to manage application configurations dynamically without redeploying the application.
Troubleshooting Configuration Issues: Developers can use environment information to troubleshoot configuration-related issues in production environments.
Business-specific Monitoring: Developers can create custom endpoints to monitor business-specific metrics or gather domain-specific insights.
Operational Tasks: Operations teams can create custom endpoints to perform operational tasks such as cache eviction, database cleanup, or system diagnostics.
Implementation

This is an example of setting up Spring Boot Actuator in a Spring Boot application and monitoring its basic endpoints. Let's say we have a Spring Boot project that has includes Spring Web and Spring Boot Actuator as dependencies.

Step 1: Enable Actuator Endpoints
Spring Boot Actuator endpoints are secured by default. To enable the endpoints, we need to add the following configuration to the application.properties file:

management.endpoints.web.exposure.include=*
This configuration will expose all actuator endpoints. However, exposing all endpoints can pose a security risk, as some of them can reveal sensitive information. For the sake of this example and testing purposes, we're exposing all, but in a production application, you should expose only necessary endpoints.

Step 2: Access the Actuator Endpoints
Once the application is running, we can access the Actuator endpoints from the web browser or using a tool like Postman or curl.

The most basic endpoint is the /health endpoint, which shows the health status of the application. We can access it at:

http://localhost:8080/actuator/health
We should see a response similar to the following:

{
  "status": "UP"
}
Congratulations! We've successfully set up and tested a basic Spring Boot Actuator endpoint in your application. Remember that these endpoints are typically secured when deploying an application to a production environment.

Summary

Spring Boot Actuator is a sub-project of the Spring Boot Framework that aims to assist you in monitoring and managing your application when it's pushed to production.
Actuator provides several built-in endpoints for various monitoring purposes.
Actuator endpoints expose sensitive information of your application, so it’s always recommended to secure your Actuator endpoints.


## Built In Actuator Endpoints
Learning Objectives

After completing this module, associates should be able to:

Understand the purpose and usage of various built-in Actuator endpoints
Know how to interact with these endpoints using different tools like curl, Postman, or a web browser.
Understand how to expose or hide specific endpoints
Description

Spring Boot Actuator comes with several built-in endpoints that offer valuable insights into the running application and the environment it operates in.

List of Endpoints
/health: This endpoint provides basic health information about the application. It's often used by software orchestration tools to check the application's health status.

/info: It returns arbitrary application information. By default, this endpoint is empty. You can populate it with information such as build version, Git commit ID, and more by customizing it via the application properties.

/metrics: It provides detailed metrics information about the JVM, application usage, and system details. These metrics are useful for monitoring and diagnosing performance issues.

/loggers: It allows you to view and change the logging level of your application dynamically. This can be particularly useful when diagnosing problems in a live application.

/threadDump: This endpoint is useful for troubleshooting and diagnostics. It performs a thread dump, allowing you to see what every thread in the JVM is doing.

/heapdump: This endpoint returns a heap dump from the JVM used by the application. This is especially useful when diagnosing memory-related issues.

/auditevents: It exposes audit events in your application, including authentication events (like login and logout). You can customize which events get captured based on your needs.

/httptrace: It keeps trace information about the last 100 HTTP request-response exchanges. It's an invaluable tool for diagnosing issues or understanding how traffic flows through the application.

/env: It provides details about the environment in which the application is running. This includes property sources and system properties.

Each of these endpoints plays a critical role in monitoring and managing your application, and understanding how to use them effectively can be a major advantage when running your application in production.

Real World Application

In a real-world enterprise application, each Spring Boot Actuator endpoint provides unique benefits that assist in monitoring, diagnosing, and managing the application effectively. Here's how these endpoints can be applied:

/health: In a microservice architecture, orchestration tools use this endpoint to determine the health status of each service. If a service reports a DOWN status, the orchestrator can spin up a new instance, ensuring high availability.

/info: It can be configured to provide details like application version, Git commit information, and deployment metadata, assisting in tracking which version of the application is running.

/metrics: In a large-scale application, performance monitoring is crucial. This endpoint provides vital metrics data that can be used for capacity planning, identifying performance bottlenecks, and tuning the system for optimal performance.

/loggers: During incident response, this endpoint allows support engineers to dynamically change the logging level to DEBUG or TRACE in a running application, enabling them to gather more information without requiring a restart.

/threadDump: In a high-load scenario where the application might be slowing down or hanging, a thread dump can help engineers identify deadlocks, thread starvation, or any other threading-related issues.

/heapdump: This endpoint helps troubleshoot memory leaks or heavy memory consumption by generating a heap dump, which can then be analyzed using tools like Eclipse MAT or VisualVM.

/auditevents: For compliance or security audits, this endpoint helps track significant events like user logins and configuration changes, which can be critical in identifying security incidents or anomalies.

/httptrace: This endpoint is handy when diagnosing issues related to HTTP request-response exchanges, allowing engineers to understand how requests flow through the system and identify any issues.

/env: In complex environments where the application might behave differently due to external configurations, this endpoint can help troubleshoot by providing a snapshot of the system properties and environment variables.

These endpoints, collectively, help maintain a smooth, highly available, and secure operation of enterprise-scale applications, thereby ensuring the provision of top-notch services.

Implementation

This guide will show you how to access various types of built-in Actuator endpoints in a Spring Boot application.

Assuming you've already set up a Spring Boot project with Actuator dependency and enabled Actuator endpoints as shown in previous guides, let's access multiple types of endpoints.

Configure the Exposed Endpoints
To expose all actuator endpoints, add the following line to your application.properties file:

management.endpoints.web.exposure.include=*
For production-grade applications, exposing all endpoints might be a security risk. It is recommended to expose only the necessary endpoints. You can specify multiple endpoints separated by a comma like this:

management.endpoints.web.exposure.include=health,info,metrics,loggers
In this case, only /health, /info, /metrics, and /loggers will be exposed. Be sure to save your changes within application.properties before continuing.

Step 1: Run Your Application
Start your Spring Boot application:

If you're running from the terminal, navigate to the root directory of your project and use the following command:
./mvnw spring-boot:run
Alternatively, you can run the application directly from your IDE by running the main class.

Step 2: Access the /health Endpoint
The /health endpoint shows the health of your application. To access this endpoint:

Open your web browser or use a REST client tool like Postman.
Enter the following URL:
http://localhost:8080/actuator/health
You should see a response indicating the health status of your application.

Step 3: Access the /info Endpoint
The /info endpoint provides arbitrary application information. Access it using:

http://localhost:8080/actuator/info
Step 4: Access the /metrics Endpoint
The /metrics endpoint provides details of your application's metrics. Access it using:

http://localhost:8080/actuator/metrics
You will get a list of available metrics. To view a specific metric, append its name to the URL. For example, to view jvm.memory.used metric:

http://localhost:8080/actuator/metrics/jvm.memory.used
Step 5: Access the /loggers Endpoint
The /loggers endpoint allows you to see and change logging levels. Access it using:

http://localhost:8080/actuator/loggers
To view a specific logger, append its name to the URL. For example, to view the root logger:

http://localhost:8080/actuator/loggers/root
Step 6: Access the /env Endpoint
The /env endpoint provides details about the environment in which the application is running. You can access it using:

http://localhost:8080/actuator/env
Remember, the actual availability of these endpoints depends on the security configuration of your application. It's crucial to secure sensitive endpoints in a production environment.

Summary

Spring Boot Actuator comes with several built-in endpoints, each providing unique insights about your application and its operating environment.
Each endpoint is a powerful tool for monitoring and managing your Spring Boot application effectively.


## Unit Testing Service Layer Methods With Junit And Mockito
Learning Objectives

By the end of this module, you will be able to:

Understand the role of unit testing in Spring Boot applications.
Set up and configure JUnit and Mockito for testing Spring Boot applications.
Use Mockito to create mocks and stubs for isolating the component under test.
Description
Unit Testing in Spring Boot Applications with JUnit and Mockito
Unit testing is an essential practice in software development that helps ensure the quality and correctness of your application's individual components. In the context of Spring Boot applications, unit testing focuses on isolating individual components or classes and verifying their behavior without depending on external dependencies like databases, web services, or other components in the system.

JUnit and Mockito are widely used tools in the Java ecosystem to facilitate unit testing. JUnit is a powerful testing framework that simplifies the creation, execution, and maintenance of tests. Mockito is a powerful and flexible mocking framework that allows you to create mock objects for dependencies and control their behavior during testing.

JUnit
JUnit is an open-source testing framework that provides a set of annotations and assertions to write tests efficiently. It simplifies the process of creating, organizing, and executing tests, making it easier to ensure the correctness and reliability of your application code.

Some of the key JUnit features are:

Annotations to define test methods, test classes, setup, and tear-down procedures.
Assertions to verify expected outcomes.
Test runners to execute tests and report results.
Mockito
Mockito is a popular open-source mocking framework for Java applications. It allows you to create mock objects for external dependencies, control their behavior, and verify that they are being used correctly in the code under test. This is particularly important when testing components that interact with databases, web services, or other external systems.

Some of the key Mockito features are:

Creating mock objects for classes and interfaces.
Stubbing methods to return specific values or throw exceptions.
Verifying the number of times a method has been called, or if certain methods were called at all.
Argument matchers to verify that methods are called with the correct parameters.
Testing the Service Layer with Mocking
To test the service layer in a Spring Boot application, developers can use JUnit to write unit tests for each service method. To isolate the service layer from the repository layer, they can use Mockito to mock the repository interface.

Mocking the repository layer enables developers to create test scenarios where the service layer returns specific data or throws certain exceptions. Developers can use the @Mock annotation to create a mock object for the repository interface and then use the when method to stub the behavior of the mock object.

Together, JUnit and Mockito provide a comprehensive set of tools for writing, organizing, and executing unit tests in Spring Boot applications. By leveraging these frameworks, developers can create a robust test suite that helps to ensure the stability and quality of their software.

Real World Application

In this document, we will be discussing real-world applications and use cases of unit testing in Spring Boot applications using JUnit and Mockito. Unit testing plays a crucial role in ensuring the reliability and maintainability of software applications by validating individual components to make sure they work as intended. Let's explore how unit testing is used in various industries and scenarios.

E-commerce Applications: E-commerce platforms require robust and efficient backend systems to manage customer data, product inventory, and order processing. Unit testing with JUnit and Mockito allows developers to verify the functionality of each component, such as services for user registration, login, adding products to the cart, and processing payments, ensuring the overall system's stability and reliability.

Financial Services: Banking and financial applications require a high level of security and accuracy in processing transactions, managing accounts, and generating reports. Unit testing with Spring Boot, JUnit, and Mockito helps developers build and maintain complex financial tools while minimizing the risk of bugs and ensuring compliance with industry regulations.

Internet of Things (IoT): IoT applications often involve a multitude of interconnected devices that require reliable and efficient software components. Spring Boot is a popular choice for building microservices that power IoT applications, and unit testing with JUnit and Mockito allows developers to validate these components' functionality individually, ensuring the overall system performs smoothly and securely.

Health Care Systems: Healthcare applications handle sensitive data and require high availability, accuracy, and security. Unit testing in Spring Boot applications with JUnit and Mockito enables developers to rigorously test components responsible for patient information management, appointment scheduling, and billing, thereby improving the quality and reliability of healthcare software systems.

Content Management Systems (CMS): CMS applications involve various components for managing user-generated content, such as text, images, and videos. Spring Boot is widely used for building scalable and modular CMS applications, and unit testing with JUnit and Mockito ensures that components such as content editors, media uploaders, and authentication systems work as intended, providing a stable and secure platform for content management.

In conclusion, unit testing with JUnit and Mockito in Spring Boot applications is an essential practice that can significantly improve the reliability and maintainability of software systems in various real-world scenarios. By conducting thorough component-level tests, developers can catch potential issues early in the development process, leading to more stable and secure applications in various industries.

Implementation

In this guide, we will explore the implementation process of unit testing in Spring Boot applications using JUnit and Mockito. Here, we will learn how to write and run effective unit tests for our Spring Boot applications.

Step 1: Add Dependencies
To implement unit testing with JUnit and Mockito, we add the following dependencies to our pom.xml (Maven). We will also add Lombok to make the testing process faster:

Maven
<dependencies>
    <!-- JUnit 5 and Spring Boot Test with Mockito -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <scope>test</scope>
    </dependency>
    <!-- Lombok -->
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <version>1.18.26</version>
        <scope>provided</scope>
    </dependency>
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <version>RELEASE</version>
        <scope>compile</scope>
    </dependency>
Step 2: Creating an Example Model Class:
import lombok.Data;

@Data
public class Customer {
    private Long id;
    private String firstName;
    private String lastName;
}
Step 3: Creating a CustomerRepository Interface to define the CRUD methods - typically we use Spring DataJPA:
import java.util.List;

@Repository
public interface CustomerRepository {
        Customer save(Customer customer);
        Customer findById(Long id);
        void delete(Customer customer);
        List<Customer> findAll();
}
Step 4: Creating a CustomerService class that depends on the CustomerRepository
import lombok.AllArgsConstructor;
import org.springframework.stereotype.Service;
import java.util.List;

@Service
@AllArgsConstructor
public class CustomerService {

    private final CustomerRepository customerRepository;

    public List<Customer> getAllCustomers() {
        return customerRepository.findAll();
    }

    public Customer getCustomerById(Long id) {
        return customerRepository.findById(id)
                .orElseThrow(() -> new RuntimeException("Customer not found with id " + id));
    }

    public Customer createCustomer(Customer customer) {
        return customerRepository.save(customer);
    }

    public Customer updateCustomer(Long id, Customer customer) {
        Customer existingCustomer = getCustomerById(id);
        existingCustomer.setFirstName(customer.getFirstName());
        existingCustomer.setLastName(customer.getLastName());
        return customerRepository.save(existingCustomer);
    }

    public void deleteCustomer(Long id) {
        Customer customerToDelete = customerRepository.findById(id)
                .orElseThrow(() -> new RuntimeException("Customer not found with id " + id));
        customerRepository.delete(customerToDelete);
    }
}
Note that we don't need to add @Autowired annotation in this Service class because we are already using Lombok's @AllArgsConstructor which automatically injects the dependencies, including CustomerRepository, via constructor injection.

Step 5: Creating a CustomerServiceTest class to test the CustomerService:
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.junit.jupiter.MockitoExtension;
import static org.junit.jupiter.api.Assertions.*;

@ExtendWith(MockitoExtension.class)
class CustomerServiceTest {

}
The @ExtendWith(MockitoExtension.class) annotation is used to register the JUnit 5 extension provided by Mockito, which allows you to use Mockito's mocking and verification capabilities in your JUnit 5 tests.

The MockitoExtension class is a JUnit 5 extension that initializes and cleans up Mockito mocks in your test class. By using this extension, Mockito can automatically inject mock objects into your test class's fields annotated with @Mock, and create the instance of the test class with @InjectMocks.

Step 6: Mocking the dependencies of the CustomerService class:
@ExtendWith(MockitoExtension.class)
public class CustomerServiceTest {

    @Mock
    private CustomerRepository customerRepository;

    @InjectMocks
    private CustomerService customerService;
}
In Mockito, @Mock is used to create a mock instance of a class or interface. This is useful when you want to test a specific class or method that depends on another class or interface, without actually having to instantiate that dependency.

In the given code snippet, @Mock is used to create a mock instance of the CustomerRepository interface, which is a dependency of the CustomerService class.

@InjectMocks is another annotation in Mockito which is used to inject the mocks that were created using the @Mock annotation into the target object (in this case, the CustomerService class).

Together, @Mock and @InjectMocks allow us to isolate the class under test (CustomerService in this case) from its dependencies (CustomerRepository in this case), and control the behavior of those dependencies in our tests.

Step 6: Completing CustomerServiceTest:

import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;
import java.util.Optional;
import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.*;

@ExtendWith(MockitoExtension.class)
class CustomerServiceTest {


    @Mock
    private CustomerRepository customerRepository;

    @InjectMocks
    private CustomerService customerService;

    @Test
    public void testCreateCustomer() {
        // given
        Customer customer = new Customer();
        customer.setFirstName("John");
        customer.setLastName("Doe");

        when(customerRepository.save(customer)).thenReturn(customer);

        // when
        Customer savedCustomer = customerService.createCustomer(customer);

        // then
        assertEquals(customer, savedCustomer);
        verify(customerRepository, times(1)).save(customer);
    }

    @Test
    public void testGetCustomerById() {
        // given
        Long customerId = 1L;
        Customer customer = new Customer();
        customer.setId(customerId);
        customer.setFirstName("John");
        customer.setLastName("Doe");

        when(customerRepository.findById(customerId)).thenReturn(Optional.of(customer));

        // when
        Customer foundCustomer = customerService.getCustomerById(customerId);

        // then
        assertEquals(customer, foundCustomer);
        verify(customerRepository, times(1)).findById(customerId);
    }

    @Test
    public void testDeleteCustomer() {
        // given
        Customer customer = new Customer();
        customer.setId(1L);
        customer.setFirstName("John");
        customer.setLastName("Doe");

        when(customerRepository.findById(1L)).thenReturn(Optional.of(customer));

        // when
        customerService.deleteCustomer(1L);

        // then
        verify(customerRepository, times(1)).findById(1L);
        verify(customerRepository, times(1)).delete(customer);
    }

}
Summary of the Above Test Suite:
The test suite uses the JUnit 5 testing framework and the Mockito library to test the functionality of a CustomerService class. The @ExtendWith annotation is used to enable the MockitoExtension for the test suite. The @Mock annotation is used to mock the CustomerRepository dependency, and the @InjectMocks annotation is used to inject the mocked dependency into the CustomerService instance.

There are three test methods in the test suite: testCreateCustomer(), testGetCustomerById(), and testDeleteCustomer(). The testCreateCustomer() method tests the createCustomer() method by creating a new Customer object and setting its properties, mocking the save() method of the CustomerRepository to return the created Customer, calling the createCustomer() method of the CustomerService, and then asserting that the returned Customer is the same as the created one and that the save() method was called exactly once.

The testGetCustomerById() method tests the getCustomerById() method by creating a new Customer object, mocking the findById() method of the CustomerRepository to return an Optional containing the created Customer, calling the getCustomerById() method of the CustomerService with the created Customer's ID, and then asserting that the returned Customer is the same as the created one and that the findById() method was called exactly once with the correct ID.

The testDeleteCustomer() method tests the deleteCustomer() method by creating a new Customer object and mocking the findById() method of the CustomerRepository to return an Optional containing the created Customer, calling the deleteCustomer() method of the CustomerService with the created Customer's ID, and then asserting that the findById() and delete() methods were called exactly once with the correct ID and customer object, respectively.

Summary

When unit testing a Spring Boot service layer, it is important to isolate dependencies and mock the repository. JUnit 5 and Mockito are commonly used for this purpose.
JUnit 5 is a testing framework that provides several annotations and assertions for writing unit tests in Java. It is the default testing framework in Spring Boot 2.
Mockito is a Java library that provides tools for creating mock objects and verifying their behavior. Mockito is commonly used to mock dependencies, such as repositories, when unit testing Spring Boot service layer.
To mock the repository, the @Mock annotation is used to create a mock instance of the repository interface. The @InjectMocks annotation is then used to inject the mock repository into the service instance.


## Intro To Integration Testing
Learning Objectives

After completing this module, associates should be able to:

Understand the role and importance of Integration Testing.
Explore Spring Boot's support for simplified Integration Testing.
Description

Integration testing is a key phase in the software testing lifecycle. It is the process of combining individual software modules and testing them as a group.

Why is Integration Testing Important?
Software systems today are often comprised of multiple modules or components that are developed independently. These components must work together seamlessly to deliver the intended functionality of the system.

While unit testing ensures that individual parts of a system work correctly, integration testing aims to confirm that these parts work correctly when combined. This is critical as the interaction between individual components can often lead to bugs that weren't visible when the components were tested in isolation.

Levels of Integration Testing
Integration testing is typically performed at several levels including:

Component Integration Testing: Verifies the interaction between software components. Usually done after component testing.
System Integration Testing: Verifies the interaction between different systems and may involve testing interfaces to external organizations.
Integration Testing in the Context of Spring
The Spring framework provides robust support for integration testing. It provides annotations such as @SpringBootTest and @DataJpaTest that can be used to create application contexts with different levels of detail, depending on the requirements of your tests.

In a Spring application, you might need to test the interactions between:

Different layers of your application, such as the service and repository layers.
Your application and your database.
Your application and other Spring Boot features, like caching or the Actuator.
In addition, the Spring Boot Test module provides several useful tools and annotations to streamline integration testing with other parts of the Spring ecosystem.

Real World Application

Integration testing plays a critical role in ensuring software quality across a wide array of applications. Here are a few examples where integration testing is vital.

E-commerce Platforms
An e-commerce platform typically integrates various components like user interface, payment gateway, product catalog, and shipping module. Integration testing ensures that these components work seamlessly together to provide a smooth user experience.

Banking Systems
Banking systems involve numerous integrated sub-systems such as user accounts, transactions, credit card processing, etc. Proper integration testing helps in avoiding glitches that could potentially lead to significant financial loss and affect the reputation of the bank.

Healthcare Systems
In healthcare, integration testing is vital for systems like Electronic Health Records (EHR). Such systems need to integrate with numerous modules like patient records, billing, insurance, pharmacy, and lab systems. Any communication issues among these modules could have serious implications for patient care.

Social Media Platforms
Social media platforms typically involve integration of various modules like user profiles, messaging, notifications, feeds, etc. Integration testing ensures smooth and seamless interaction between these modules, ensuring a good user experience.

Enterprise Resource Planning (ERP) Systems
ERP systems are often complex and integrate many business functions like finance, human resources, supply chain, etc. Integration testing in ERP systems ensures that data flows correctly between different business functions and reduces the risk of issues during real-world use.

Implementation

Here is a simplified step-by-step guide for setting up integration testing in a Spring Boot application. We will use Spring Boot's built-in support for integration testing with the @SpringBootTest annotation.

Step 1: Add Required Dependencies
First, we need to add the necessary dependencies in your Maven's pom.xml or Gradle's build.gradle file.

If you're using Maven:

<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-test</artifactId>
    <scope>test</scope>
</dependency>
Step 2: Create an Integration Test Class
Create a new test class for your integration test. This test class should be located in the same package as the class you're testing, but in your src/test/java directory.

You can use the @SpringBootTest annotation to indicate that this is an integration test that requires the Spring Boot context.

Here's a basic example:

import org.springframework.boot.test.context.SpringBootTest;
import org.junit.jupiter.api.Test;

@SpringBootTest
class YourApplicationIT {

    @Test
    void contextLoads() {
    }
}
Step 3: Inject Dependencies
You can use Spring's dependency injection features in your integration tests. For instance, you might need to inject a TestRestTemplate to make HTTP requests to your application, or you might need to inject your services or repositories to perform assertions on them.

Here's how you might inject a TestRestTemplate:

import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.boot.test.web.client.TestRestTemplate;
import org.springframework.beans.factory.annotation.Autowired;
import org.junit.jupiter.api.Test;

@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)
class YourApplicationIT {

    @Autowired
    private TestRestTemplate restTemplate;

    @Test
    void contextLoads() {
    }
}
Step 4: Write Your Integration Test
You can now write your integration test. You can use the restTemplate to make requests to your application and then use JUnit's assertions to verify the results. You can also use your injected services or repositories to set up test data or verify that operations were performed.

@Test
void testGetEndpoint() {
    ResponseEntity<String> response = restTemplate.getForEntity("/your-endpoint", String.class);
    assertEquals(HttpStatus.OK, response.getStatusCode());
    assertEquals("Expected response", response.getBody());
}
Step 5: Run Your Integration Test
You can now run your integration test in the same way that you would run a unit test. If you're using an IDE like IntelliJ IDEA or Eclipse, you can right-click on your test class and select "Run". Or you can run the test from the command line using Maven or Gradle. For example, with Maven, you would use mvn test.

Remember, integration tests typically take longer to run than unit tests, as they involve starting the Spring context and often involve IO operations like network or database access. This is a normal part of integration testing.

In conclusion, integration tests allow you to test larger chunks of your application in a more realistic setting compared to unit tests, making sure your components work well together. They're an essential part of any comprehensive testing strategy.

Summary

Integration testing is a level of software testing where individual units are combined and tested as a group.
While unit testing ensures that individual parts of a system work correctly, integration testing aims to confirm that these parts work correctly when combined.
Integration testing is typically performed at several levels including Component Integration Testing and System Integration Testing.
Spring Boot provides annotations such as @SpringBootTest and @DataJpaTest that can be used to create application contexts with different levels of detail, depending on the requirements of your tests.


## Testing Restful Apis With Mockmvc And Resttemplate
Learning Objectives

After completing this module, learners should be able to:

Understand the importance and benefits of integration testing in a Spring Boot application
Know how to write integration tests using MockMVC and RESTTemplate to test RESTful web services and web applications
Description
What is Integration Testing in Spring Boot?
In the context of a Spring Boot app, integration testing involves testing the interaction between different parts of the application to ensure that they work together correctly. This typically involves testing the application's behavior as a whole, rather than testing individual components in isolation.

Integration tests may involve starting up the Spring application context and making requests to the application's endpoints to ensure that the endpoints are correctly mapped, the responses are correctly generated, and that any required dependencies are correctly injected and used. This helps to ensure that the application is working correctly as a whole, and can help to catch issues that might not be caught by unit tests.

MockMVC and RESTTemplate are both used for testing REST APIs, but they serve different purposes.

What is MockMVC and what is its role?
It provides a mock implementation of the MVC framework, allowing developers to simulate requests and responses and test the behavior of their controllers in a controlled environment. MockMVC is designed to be lightweight, fast, and easy to use, making it a popular choice for integration testing in Spring Boot applications. It is used for testing the controller layer of the application.

What is RESTTemplate and what is its role in Integration Testing Spring Boot apps?
RESTTemplate is a class provided by Spring that makes it easy to make HTTP requests to RESTful web services. It provides methods for performing common HTTP operations, such as GET, POST, PUT, and DELETE, and supports both synchronous and asynchronous request processing. In the context of integration testing in Spring Boot applications, RESTTemplate can be used to test the interactions between the application and external RESTful services or APIs. It is used for testing the service layer of the application and is typically used for unit testing, where you test the behavior of individual components in isolation.

It is not uncommon to use RESTTemplate to test your own controller layer. However, keep in mind that using RESTTemplate will start up a server and make actual HTTP requests, which can be slower than using MockMVC. Using MockMVC allows you to test your controller layer in isolation without the overhead of starting up a server and making HTTP requests.

Also, note that using RESTTemplate to test your own controller layer does not provide the same level of isolation as using MockMVC. When you use RESTTemplate, you are testing the integration of your controller with the rest of the application, whereas with MockMVC you are testing just the controller layer.

Real World Application

Here's a sample enterprise-level application where MockMVC and RESTTemplate could be used for integration testing:

Application: E-commerce platform

Description: An online marketplace where vendors can sell their products to customers.

Scenario: An integration test needs to be written for the checkout process, which involves a customer adding items to their cart, entering their shipping and payment information, and submitting the order.

Steps:
Use MockMVC to simulate a customer adding items to their cart by sending a POST request to the /cart endpoint with the product IDs and quantities.
Use RESTTemplate to make a GET request to the /shipping endpoint to retrieve the available shipping options for the customer's location.
Use MockMVC to simulate the customer entering their shipping information by sending a POST request to the /checkout endpoint with the shipping option and customer details.
Use RESTTemplate to make a POST request to the payment gateway API with the customer's payment information.
Verify that the order was successfully placed by checking the database for the new order record and confirming that the order details match the expected values.
This is just one example of how MockMVC and RESTTemplate could be used in integration testing for an enterprise-level application like an e-commerce platform. By using these tools to simulate requests and responses from external systems, developers can thoroughly test the functionality of their applications and ensure that they are reliable and secure.

Implementation

This example will demonstrate how to test a controller layer with MockMVC and RESTTemplate, as well as setting up the basic components that comprise the app. Let's assume we're working with a Spring Boot application that has the following two dependencies: Spring Web and Lombok.

Step 1: Defining the User Model
import lombok.Data;

@Data
public class User {
    private Long id;
    private String username;
    private String email;
}
Step 2: Defining the UserRepository Interface
import java.util.List;
import java.util.Optional;

public interface UserRepository { 
   User save(User user);
   Optional<User> findById(Long id);
   List<User> findAll();
   void delete(User user);
}
Step 3: Defining the UserService Class
import lombok.AllArgsConstructor;
import org.springframework.stereotype.Service;
import java.util.List;

@Service
@AllArgsConstructor
public class UserService{

    private final UserRepository userRepository;

    public User createUser(User user) {
        return userRepository.save(user);
    }

    public User getUserById(Long id) {
        return userRepository.findById(id)
                .orElseThrow(() -> new RuntimeException("User not found with id " + id));
    }

    public List<User> getAllUsers() {
        return userRepository.findAll();
    }

    public User updateUser(Long id, User updatedUser) {
        User existingUser = userRepository.findById(id)
                .orElseThrow(() -> new RuntimeException("User not found with id " + id));

        existingUser.setUsername(updatedUser.getUsername());
        existingUser.setEmail(updatedUser.getEmail());

        return userRepository.save(existingUser);
    }

    public void deleteUser(Long id) {
        User userToDelete = userRepository.findById(id)
                .orElseThrow(() -> new RuntimeException("User not found with id " + id));
        userRepository.delete(userToDelete);
    }
}
Step 4: Defining the UserController Class
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import java.util.List;

@RestController
@RequestMapping("/api/users")
public class UserController {

    private final UserService userService;

    @Autowired
    public UserController(UserService userService) {
        this.userService = userService;
    }

    @PostMapping
    public ResponseEntity<User> createUser(@RequestBody User user) {
        User createdUser = userService.createUser(user);
        return new ResponseEntity<>(createdUser, HttpStatus.CREATED);
    }

    @GetMapping("/{id}")
    public ResponseEntity<User> getUserById(@PathVariable Long id) {
        User user = userService.getUserById(id);
        return new ResponseEntity<>(user, HttpStatus.OK);
    }

    @GetMapping
    public ResponseEntity<List<User>> getAllUsers() {
        List<User> users = userService.getAllUsers();
        return new ResponseEntity<>(users, HttpStatus.OK);
    }

    @PutMapping("/{id}")
    public ResponseEntity<User> updateUser(@PathVariable Long id, @RequestBody User user) {
        User updatedUser = userService.updateUser(id, user);
        return new ResponseEntity<>(updatedUser, HttpStatus.OK);
    }

    @DeleteMapping("/{id}")
    public ResponseEntity<Void> deleteUser(@PathVariable Long id) {
        userService.deleteUser(id);
        return new ResponseEntity<>(HttpStatus.NO_CONTENT);
    }
}
This is a Spring Boot controller for a RESTful API that handles CRUD (create, read, update, delete) operations on User resources. The controller is annotated with @RestController to indicate that it handles HTTP requests and returns the response in JSON format. The @RequestMapping annotation specifies the base URL path for all the controller's endpoints.

The controller has five methods that handle different HTTP requests. The methods use the @PostMapping, @GetMapping, @PutMapping, and @DeleteMapping annotations to specify the HTTP method of the request. The @RequestBody and @PathVariable annotations are used to extract the request body and URL path variables, respectively.

The methods return a ResponseEntity object that wraps the HTTP response body and status code. The HttpStatus enum is used to set the appropriate HTTP status code for the response. For example, HttpStatus.OK is returned when the resource is successfully retrieved, and HttpStatus.NO_CONTENT is returned when a resource is successfully deleted.

The controller depends on the UserService class, which is responsible for handling the business logic of the application. The UserService is injected into the constructor of the controller using the @Autowired annotation.

Step 5: Creating a Test Suite with MockMVC and RESTTemplate to test the UserController and Mocking the Repository and Service layers
import com.fasterxml.jackson.databind.ObjectMapper;
import org.junit.jupiter.api.Test;
import org.mockito.ArgumentCaptor;
import org.mockito.Captor;
import org.mockito.Mock;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.boot.test.mock.mockito.MockBean;
import org.springframework.http.*;
import org.springframework.test.web.servlet.MockMvc;
import org.springframework.test.web.servlet.request.MockMvcRequestBuilders;
import org.springframework.test.web.servlet.result.MockMvcResultMatchers;
import org.springframework.web.client.RestTemplate;

import java.util.ArrayList;
import java.util.List;

import static org.assertj.core.api.Assertions.assertThat;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.when;
import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content;
import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;

@SpringBootTest
@AutoConfigureMockMvc
public class UserControllerTest {

    @Autowired
    private MockMvc MockMvc;

    @MockBean
    private UserService userService;

    @MockBean
    private CustomerRepository customerRepository;

    @Mock
    private RestTemplate restTemplate;

    @Captor
    private ArgumentCaptor<User> userCaptor;

    @Autowired
    private ObjectMapper objectMapper;

}
The @SpringBootTest annotation is used to create an integration test that loads the complete Spring application context. This allows the tests to interact with the application as it would run in a production environment.
The @AutoConfigureMockMvc annotation configures the MockMVC instance for use in the tests. This is a Spring Framework test utility that provides a way to make requests and validate responses without starting up a server.
The @MockBean annotation is used to mock the UserService and CustomerRepository dependencies of the controller under test. This allows us to specify the behavior of these dependencies and focus on testing the behavior of the controller.
The @Mock annotation is used to mock the RESTTemplate dependency of the controller under test. This allows us to specify the behavior of this dependency and focus on testing the behavior of the controller.
The @Captor annotation is used to capture the argument passed to the userService.createUser() method during testing. This allows us to verify that the correct argument is being passed to this method.
The ObjectMapper is used to convert objects to and from JSON format, which is used for request and response bodies in the tests.
Step 6: Creating tests to test the controller layer with integration testing
    @Test
    public void testCreateUserWithMockMvc() throws Exception {
        // Creates a new user and sets username and email
        // Configures the behavior of userService.createUser() method to return the created user
        // Sends a POST request to "/api/users" with the user object as JSON payload
        // Performs assertions on the response status, content type, and JSON fields
        // Verifies that userService.createUser() is called with the captured user object
        User user = new User();
        user.setUsername("testuser");
        user.setEmail("testuser@example.com");

        when(userService.createUser(userCaptor.capture())).thenReturn(user);

        MockMvc.perform(MockMvcRequestBuilders.post("/api/users")
                        .contentType(MediaType.APPLICATION_JSON)
                        .content(objectMapper.writeValueAsString(user)))
                .andExpect(status().isCreated())
                .andExpect(content().contentType(MediaType.APPLICATION_JSON))
                .andExpect(MockMvcResultMatchers.jsonPath("$.username").value("testuser"))
                .andExpect(MockMvcResultMatchers.jsonPath("$.email").value("testuser@example.com"));

        verify(userService).createUser(userCaptor.capture());
        assertThat(userCaptor.getValue()).isEqualTo(user);
    }

    @Test
    public void testCreateUserWithRESTTemplate() {
        // Creates a new user and sets username and email
        // Configures the behavior of RESTTemplate.postForObject() method to return the created user
        // Sends a POST request to "/api/users" with the user object as JSON payload using RESTTemplate
        // Performs an assertion to check if the created user matches the expected user
        // Verifies that RESTTemplate.postForObject() is called with the expected parameters
        User user = new User();
        user.setUsername("testuser");
        user.setEmail("testuser@example.com");

        when(restTemplate.postForObject("/api/users", user, User.class)).thenReturn(user);

        User createdUser = restTemplate.postForObject("/api/users", user, User.class);

        assertThat(createdUser).isEqualTo(user);
        verify(restTemplate).postForObject("/api/users", user, User.class);
    }

    @Test
    public void testGetUserByIdWithMockMvc() throws Exception {
        // Creates a new user and sets id, username, and email
        // Configures the behavior of userService.getUserById() method to return the user
        // Sends a GET request to "/api/users/1" using MockMvc
        // Performs assertions on the response status, content type, and JSON fields
        // Verifies that userService.getUserById() is called with the expected id
        User user = new User();
        user.setId(1L);
        user.setUsername("testuser");
        user.setEmail("testuser@example.com");

        when(userService.getUserById(1L)).thenReturn(user);

        MockMvc.perform(MockMvcRequestBuilders.get("/api/users/1"))
                .andExpect(status().isOk())
                .andExpect(content().contentType(MediaType.APPLICATION_JSON))
                .andExpect(MockMvcResultMatchers.jsonPath("$.id").value(1))
                .andExpect(MockMvcResultMatchers.jsonPath("$.username").value("testuser"))
                .andExpect(MockMvcResultMatchers.jsonPath("$.email").value("testuser@example.com"));

        verify(userService).getUserById(1L);
    }

    @Test
    public void testGetUserByIdWithRESTTemplate() {
        // Creates a new user and sets id, username, and email
        // Configures the behavior of RESTTemplate.getForObject() method to return the user
        // Sends a GET request to "/api/users/1" using RESTTemplate
        // Performs an assertion to check if the retrieved user matches the expected user
        // Verifies that RESTTemplate.getForObject() is called with the expected URL and response type
        User user = new User();
        user.setId(1L);
        user.setUsername("testuser");
        user.setEmail("testuser@example.com");

        when(restTemplate.getForObject("/api/users/1", User.class)).thenReturn(user);

        User retrievedUser = restTemplate.getForObject("/api/users/1", User.class);

        assertThat(retrievedUser).isEqualTo(user);
        verify(restTemplate).getForObject("/api/users/1", User.class);
    }

    @Test
    public void testGetAllUsersWithMockMvc() throws Exception {
        // Creates a list of users and adds two users to it with different details
        // Configures the behavior of userService.getAllUsers() method to return the user list
        // Sends a GET request to "/api/users" using MockMVC
        // Performs assertions on the response status, content type, and JSON fields
        // Verifies that userService.getAllUsers() is called
        List<User> userList = new ArrayList<>();

        User user1 = new User();
        user1.setId(1L);
        user1.setUsername("testuser1");
        user1.setEmail("testuser1@example.com");

        User user2 = new User();
        user2.setId(2L);
        user2.setUsername("testuser2");
        user2.setEmail("testuser2@example.com");

        userList.add(user1);
        userList.add(user2);

        when(userService.getAllUsers()).thenReturn(userList);

        MockMvc.perform(MockMvcRequestBuilders.get("/api/users"))
                .andExpect(status().isOk())
                .andExpect(content().contentType(MediaType.APPLICATION_JSON))
                .andExpect(MockMvcResultMatchers.jsonPath("$[0].id").value(1))
                .andExpect(MockMvcResultMatchers.jsonPath("$[0].username").value("testuser1"))
                .andExpect(MockMvcResultMatchers.jsonPath("$[0].email").value("testuser1@example.com"))
                .andExpect(MockMvcResultMatchers.jsonPath("$[1].id").value(2))
                .andExpect(MockMvcResultMatchers.jsonPath("$[1].username").value("testuser2"))
                .andExpect(MockMvcResultMatchers.jsonPath("$[1].email").value("testuser2@example.com"));

        verify(userService).getAllUsers();
    }

    @Test
    public void testGetAllUsersWithRESTTemplate() {
        // Creates a list of users and adds two users to it with different details
        // Configures the behavior of RESTTemplate.getForObject() method to return the user list
        // Sends a GET request to "/api/users" using RESTTemplate
        // Performs an assertion to check if the retrieved user list matches the expected list
        // Verifies that RESTTemplate.getForObject() is called with the expected URL and response type
        List<User> userList = new ArrayList<>();

        User user1 = new User();
        user1.setId(1L);
        user1.setUsername("testuser1");
        user1.setEmail("testuser1@example.com");

        User user2 = new User();
        user2.setId(2L);
        user2.setUsername("testuser2");
        user2.setEmail("testuser2@example.com");

        userList.add(user1);
        userList.add(user2);

        when(restTemplate.getForObject("/api/users", List.class)).thenReturn(userList);

        List<User> retrievedUserList = restTemplate.getForObject("/api/users", List.class);

        assertThat(retrievedUserList).isEqualTo(userList);
        verify(restTemplate).getForObject("/api/users", List.class);
    }

    @Test
    public void testUpdateUserWithMockMvc() throws Exception {
        // Creates a user object with updated username and email
        // Creates an updatedUser object with the updated user details
        // Configures the behavior of userService.updateUser() method to return the updatedUser
        // Sends a PUT request to "/api/users/1" with the updated user object as JSON payload using MockMvc
        // Performs assertions on the response status, content type, and JSON fields
        // Verifies that userService.updateUser() is called with the expected id and updated user object
        User userToUpdate = new User();
        userToUpdate.setUsername("testuserupdated");
        userToUpdate.setEmail("testuserupdated@example.com");

        User updatedUser = new User();
        updatedUser.setId(1L);
        updatedUser.setUsername("testuserupdated");
        updatedUser.setEmail("testuserupdated@example.com");

        when(userService.updateUser(1L, userToUpdate)).thenReturn(updatedUser);

        MockMvc.perform(MockMvcRequestBuilders.put("/api/users/1")
                        .contentType(MediaType.APPLICATION_JSON)
                        .content(objectMapper.writeValueAsString(userToUpdate)))
                .andExpect(status().isOk())
                .andExpect(content().contentType(MediaType.APPLICATION_JSON))
                .andExpect(MockMvcResultMatchers.jsonPath("$.id").value(1))
                .andExpect(MockMvcResultMatchers.jsonPath("$.username").value("testuserupdated"))
                .andExpect(MockMvcResultMatchers.jsonPath("$.email").value("testuserupdated@example.com"));

        verify(userService).updateUser(1L, userToUpdate);
    }

    @Test
    public void testUpdateUserWithRESTTemplate() {
        // Creates a user object with updated username and email
        // Creates an updatedUser object with the updated user details
        // Configures the behavior of RESTTemplate.exchange() method to return the updatedUser as a ResponseEntity with HttpStatus.OK
        // Sends a PUT request to "/api/users/1" with the updated user object as JSON payload using RESTTemplate.exchange()
        // Retrieves the response entity and extracts the updated user from the response body
        // Performs assertions to check if the updated user matches the expected user and the response status code is HttpStatus.OK
        // Verifies that RESTTemplate.exchange() is called with the expected URL, HttpMethod, request entity, and response type
        User userToUpdate = new User();
        userToUpdate.setUsername("testuserupdated");
        userToUpdate.setEmail("testuserupdated@example.com");
        User updatedUser = new User();
        updatedUser.setId(1L);
        updatedUser.setUsername("testuserupdated");
        updatedUser.setEmail("testuserupdated@example.com");

        when(restTemplate.exchange("/api/users/1", HttpMethod.PUT, new HttpEntity<>(userToUpdate), User.class))
                .thenReturn(new ResponseEntity<>(updatedUser, HttpStatus.OK));

        ResponseEntity<User> responseEntity = restTemplate.exchange("/api/users/1", HttpMethod.PUT, new HttpEntity<>(userToUpdate), User.class);
        User updatedUserResponse = responseEntity.getBody();

        assertThat(updatedUserResponse).isEqualTo(updatedUser);
        assertThat(responseEntity.getStatusCode()).isEqualTo(HttpStatus.OK);
        verify(restTemplate).exchange("/api/users/1", HttpMethod.PUT, new HttpEntity<>(userToUpdate), User.class);
    }

    @Test
    public void testDeleteUserWithMockMvc() throws Exception {
        // Sends a DELETE request to "/api/users/1" using MockMVC
        // Performs an assertion to check if the response status is HttpStatus.NO_CONTENT
        // Verifies that userService.deleteUser() is called with the expected id
        MockMvc.perform(MockMvcRequestBuilders.delete("/api/users/1"))
                .andExpect(status().isNoContent());

        verify(userService).deleteUser(1L);
    }

    @Test
    public void testDeleteUserWithRESTTemplate() {
        // Sends a DELETE request to "/api/users/1" using RESTTemplate
        // Verifies that RESTTemplate.delete() is called with the expected URL
        restTemplate.delete("/api/users/1");

        verify(restTemplate).delete("/api/users/1");
    }
In the above code, we've used MockMVC to test the HTTP endpoints of the UserController and RESTTemplate to test the HTTP requests to the endpoints.

We've also used Mockito to mock the dependencies of the UserController and verify that the methods of the dependencies are called with the correct parameters. Finally, we've added assertions to check the response received from the endpoints, and the HTTP status codes returned by the endpoints.

Summary

In the context of a Spring Boot app, integration testing involves testing the interaction between different parts of the application to ensure that they work together correctly.
MockMVC is a mock implementation of the MVC framework, allowing developers to simulate requests and responses and test the behavior of their controllers in a controlled environment.
In the context of integration testing in Spring Boot applications, RESTTemplate can be used to test the interactions between the application and external RESTful services or APIs.


## Testing Database Interactions
Learning Objectives

By the end of this module, participants should be able to:

Understand the importance and benefits of integration testing the data layer in Spring Boot applications
Create an in-memory H2 database for integration testing purposes
Configure Spring Data JPA for testing
Write integration tests for Spring Data JPA repositories using JUnit and Mockito
Test the correctness of repository CRUD operations
Description

Data integration testing is a type of testing that verifies the interaction between different components of an application and the database. In a Spring Boot application, data integration testing ensures that the application interacts with the database correctly, by testing the data access layer of the application.

Here are some key points to keep in mind about data integration testing in a Spring Boot app:

Data integration testing verifies the interaction between the application and the database.
In a Spring Boot app, data integration testing focuses on testing the data access layer.
Spring Boot provides a number of tools and features to facilitate data integration testing.
To perform data integration testing, you can use an in-memory database, a test container, or a real database instance.
You can use tools like Spring Boot Test, JUnit, and Mockito to write and execute data integration tests.
Data integration testing helps ensure that your application functions correctly when interacting with a database, which is crucial for many modern applications.
Integration testing the data layer in Spring Boot applications involves:

Setting up a test database and configuring it in the application context
Writing test cases that interact with the database using the Spring Data JPA repositories
Using tools like Flyway or Liquibase to manage database schema changes in a version-controlled manner
Verifying that data is correctly persisted, retrieved, and updated in the database by the application
Ensuring that data integrity and transaction management features are working as expected
Mocking external dependencies of the data layer, such as the service layer or other systems that the application interacts with
Configuring test-specific properties such as connection pool size, transaction isolation levels, or database vendor-specific settings.
Real World Application

Let's consider an e-commerce web application built with Spring Boot. The application allows customers to browse products, add them to their cart, and place orders. The backend is responsible for managing the catalog of products, tracking inventory levels, and processing orders.

For the purpose of this example, let's focus on the data layer responsible for managing the products and their inventory levels. The product data is stored in a relational database, and the data layer is built using Spring Data JPA.

The objective of integration testing the data layer is to ensure that the application's CRUD (create, read, update, delete) operations on products and inventory levels work correctly, and that the data is properly persisted in the database. This includes testing the following scenarios:

Creating a new product and verifying that it is stored in the database with the correct attributes.
Updating an existing product's attributes and verifying that the changes are reflected in the database.
Deleting a product and verifying that it is removed from the database.
Retrieving a product by its ID and verifying that the returned data matches what is stored in the database.
Retrieving a list of products based on certain criteria (such as category or price range) and verifying that the returned data matches what is expected.
Managing inventory levels for a product by decrementing it when an order is placed and verifying that the correct inventory level is stored in the database.
To achieve these objectives, we can write integration tests that use an in-memory database to simulate the behavior of a real database. We can use the Spring Boot Test framework to set up the test environment, including configuring the in-memory database and loading the Spring context with the necessary components for testing the data layer. We can then write tests that use Spring Data JPA to interact with the in-memory database and validate the expected results.

Overall, integration testing the data layer of a Spring Boot application ensures that the application's data operations work as expected and that the data is correctly stored and retrieved from the database.

Implementation

The following is an example that demonstrates Integration Testing the Data Layer of a Spring Boot App. Let's assume we're working with a Spring Boot application that uses the following dependencies: Spring Data JPA, H2 Database, and Lombok.

Step 1: Defining the Entity Class
Creating an entity class that maps to a database table. For example:

import lombok.Data;
import javax.persistence.*;

@Entity
@Table(name = "users")
@Data
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "username")
    private String username;

    @Column(name = "email")
    private String email;
}
Step 2: Defining the Repository Interface
Creating a repository interface that extends Spring Data JPA's JpaRepository interface. For example:

import org.springframework.data.jpa.repository.JpaRepository;

public interface UserRepository extends JpaRepository<User, Long> {

}
Step 3: Configuring the H2 database properties and setting up a test-specific application.properties file
To specify different application properties when running tests, we can create a separate application-test.properties file in our project's src/test/resources directory. This file will be used instead of the regular application.properties file when running tests.

We can specify the database properties in this file, and they will be used only during testing. Here's an example of what our application-test.properties file might look like:
```
# DataSource
spring.datasource.url=jdbc:h2:mem:testdb
spring.datasource.driver-class-name=org.h2.Driver
spring.datasource.username=sa
spring.datasource.password=

# Hibernate
spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
spring.jpa.hibernate.ddl-auto=create-drop
```

With these settings, our tests will use an in-memory H2 database instead of the production database specified in your application.properties file. Note that we can customize the properties to match our specific database configuration.

When running tests, Spring Boot will automatically detect and use the application-test.properties file in the src/test/resources directory.

Step 4: Creating a Test Class for the Repository Layer
Within the test/java directory, we create the following class. It will most likely fall within a package called com.example.demo

import static org.assertj.core.api.Assertions.assertThat;

import java.util.List;
import java.util.Optional;

import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;
import org.springframework.boot.test.autoconfigure.orm.jpa.TestEntityManager;

@DataJpaTest
public class UserRepositoryTest {

    @Autowired
    private TestEntityManager entityManager;

    @Autowired
    private UserRepository userRepository;
}
@DataJpaTest: This annotation is used to configure the Spring test context for testing JPA repositories. It provides a set of useful annotations and tools for testing JPA repositories without the need for a complete Spring application context.
TestEntityManager: This class provides an alternative to using EntityManager in tests. It allows us to interact with the persistence context in a test-friendly way without having to set up an actual database. It is typically used to initialize the database with test data.
UserRepository: This is the repository interface that we want to test. It extends the JpaRepository interface, which provides CRUD functionality for our entities. The repository methods are defined as Java methods with special naming conventions that Spring Data JPA uses to generate queries automatically.
Step 5: Writing Integration Tests for the Repository Layer
The following example tests show how to write integration tests for the repository layer of a Spring Boot application. The tests use an in-memory database, H2, to store and retrieve data:

    @Test
    public void testSaveUser() {
        User user = new User();
        user.setUsername("testuser");
        user.setEmail("testuser@example.com");
        User savedUser = userRepository.save(user);
        assertThat(savedUser.getId()).isNotNull();
        assertThat(savedUser.getUsername()).isEqualTo("testuser");
        assertThat(savedUser.getEmail()).isEqualTo("testuser@example.com");
    }

    @Test
    public void testFindById() {
        User user = new User();
        user.setUsername("testuser");
        user.setEmail("testuser@example.com");
        User savedUser = entityManager.persistAndFlush(user);

        Optional<User> retrievedUser = userRepository.findById(savedUser.getId());
        assertThat(retrievedUser.isPresent()).isTrue();
        assertThat(retrievedUser.get().getUsername()).isEqualTo("testuser");
        assertThat(retrievedUser.get().getEmail()).isEqualTo("testuser@example.com");
    }

    @Test
    public void testFindAll() {
        User user1 = new User();
        user1.setUsername("testuser1");
        user1.setEmail("testuser1@example.com");
        entityManager.persistAndFlush(user1);

        User user2 = new User();
        user2.setUsername("testuser2");
        user2.setEmail("testuser2@example.com");
        entityManager.persistAndFlush(user2);

        List<User> userList = userRepository.findAll();
        assertThat(userList).hasSize(2);
        assertThat(userList.get(0).getUsername()).isEqualTo("testuser1");
        assertThat(userList.get(1).getUsername()).isEqualTo("testuser2");
    }

    @Test
    public void testDeleteUser() {
        User user = new User();
        user.setUsername("testuser");
        user.setEmail("testuser@example.com");
        User savedUser = entityManager.persistAndFlush(user);

        userRepository.delete(savedUser);
        assertThat(userRepository.findById(savedUser.getId())).isEmpty();
    }
In the testSaveUser method, a new user is created, saved to the repository, and then retrieved to assert that the user was successfully saved.
In the testFindById method, a user is persisted to the database, and then the findById method is used to retrieve the user and assert that it was correctly retrieved.
In the testFindAll method, two users are persisted to the database, and then the findAll method is used to retrieve all users and assert that they were correctly retrieved.
In the testDeleteUser method, a user is persisted
We should be able to run these tests and have them successfully pass.

Summary

In a Spring Boot application, data integration testing ensures that the application interacts with the database correctly, by testing the data access layer of the application.
To perform data integration testing, you can use an in-memory database, a test container, or a real database instance.
You can use tools like Spring Boot Test, JUnit, and Mockito to write and execute data integration tests.